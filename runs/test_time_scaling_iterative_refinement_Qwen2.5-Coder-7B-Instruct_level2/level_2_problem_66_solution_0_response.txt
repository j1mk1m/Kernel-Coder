Make sure you follow best practices for CUDA programming, such as using shared memory, coalesced memory access, and efficient grid/block configuration. Also, consider algorithmic optimizations such as inplace operations where possible.

### Constraints:

- Only use PyTorch's `load_inline` API to compile and link your CUDA kernels.
- Your solution should be self-contained and include all necessary imports.
- Ensure that the performance improvement is noticeable compared to the original implementation.
- Avoid unnecessary complexity; aim for simplicity and readability.
- If applicable, combine multiple operations into a single kernel for efficiency.
- Optimize for both GPU memory usage and computation time.
- Consider any potential issues related to precision (e.g., FP16 vs FP32) and how they might affect your implementation.

### Example of Using `load_inline`:

```python
from torch.utils.cpp_extension import load_inline

custom_op_source = """
// CUDA source code here
"""

custom_op_cpp_source = """
// C++ interface here
"""

custom_op = load_inline(
    name="custom_op",
    cpp_sources=custom_op_cpp_source,
    cuda_sources=custom_op_source,
    functions=["your_custom_function"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)
```

Use the above template to define and compile your custom CUDA kernels.