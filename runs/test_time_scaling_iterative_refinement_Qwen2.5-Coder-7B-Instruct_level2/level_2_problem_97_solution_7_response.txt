Please include a detailed explanation of your optimizations and the reasoning behind each change.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matmul, batch norm, bias add, div, and swish
matmul_bn_bias_div_swish_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matmul_bn_bias_div_swish_kernel(float* a, float* b, float* gamma, float* beta, float* running_mean, float* running_var, float* mean, float* var, float* out, int m, int n, int k, float eps, float scale) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        // Matrix multiplication
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }

        // Batch normalization
        mean[row] = sum / n;
        var[row] = 0.0f;
        for (int i = 0; i < n; ++i) {
            var[row] += (sum - mean[row]) * (sum - mean[row]);
        }
        var[row] /= n;
        var[row] = max(var[row], eps);
        out[row * n + col] = (sum - mean[row]) / sqrt(var[row]);

        // Bias addition
        out[row * n + col] += gamma[col];

        // Division
        out[row * n + col] /= scale;

        // Swish activation
        out[row * n + col] *= out[row * n + col] > 0 ? 1 : 0;
    }
}

void update_running_stats(float* mean, float* var, float* running_mean, float* running_var, float momentum) {
    int m = mean->size[0];
    for (int i = 0; i < m; ++i) {
        running_mean[i] = momentum * mean[i] + (1 - momentum) * running_mean[i];
        running_var[i] = momentum * var[i] + (1 - momentum) * running_var[i];
    }
}

torch::Tensor matmul_bn_bias_div_swish_cuda(torch::Tensor a, torch::Tensor b, torch::Tensor gamma, torch::Tensor beta, torch::Tensor running_mean, torch::Tensor running_var, float eps, float momentum, float scale) {
    auto m = a.size(0);
    auto n = b.size(1);
    auto k = a.size(1);

    auto mean = torch::zeros(m, device=a.device());
    auto var = torch::zeros(m, device=a.device());
    auto out = torch::zeros({m, n}, device=a.device());

    const int block_size_x = 32;
    const int block_size_y = 32;
    const int num_blocks_x = (n + block_size_x - 1) / block_size_x;
    const int num_blocks_y = (m + block_size_y - 1) / block_size_y;

    matmul_bn_bias_div_swish_kernel<<<num_blocks_y, num_blocks_x>>>(a.data_ptr<float>(), b.data_ptr<float>(), gamma.data_ptr<float>(), beta.data_ptr<float>(), running_mean.data_ptr<float>(), running_var.data_ptr<float>(), mean.data_ptr<float>(), var.data_ptr<float>(), out.data_ptr<float>(), m, n, k, eps, scale);

    update_running_stats(mean.data_ptr<float>(), var.data_ptr<float>(), running_mean.data_ptr<float>(), running_var.data_ptr<float>(), momentum);

    return out;
}
"""

matmul_bn_bias_div_swish_cpp_source = (
    "torch::Tensor matmul_bn_bias_div_swish_cuda(torch::Tensor a, torch::Tensor b, torch::Tensor gamma, torch::Tensor beta, torch::Tensor running_mean, torch::Tensor running_var, float eps, float momentum, float scale);"
)

# Compile the inline CUDA code for matmul, batch norm, bias add, div, and swish
matmul_bn_bias_div_swish = load_inline(
    name="matmul_bn_bias_div_swish",
    cpp_sources=matmul_bn_bias_div_swish_cpp_source,
    cuda_sources=matmul_bn_bias_div_swish_source,
    functions=["matmul_bn_bias_div_swish_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, bias_shape=(1,), divide_value=1.0):
        super(ModelNew, self).__init__()
        self.matmul_bn_bias_div_swish = matmul_bn_bias_div_swish

    def forward(self, x):
        gamma = torch.ones(out_features, device=x.device())
        beta = torch.zeros(out_features, device=x.device())
        running_mean = torch.zeros(out_features, device=x.device())
        running_var = torch.zeros(out_features, device=x.device())
        eps = 1e-5
        momentum = 0.1
        scale = divide_value

        return self.matmul_bn_bias_div_swish.matmul_bn_bias_div_swish_cuda(x, gamma, beta, running_mean, running_var, eps, momentum, scale)


batch_size = 1024
in_features = 8192
out_features = 8192
bn_eps = 1e-5
bn_momentum = 0.1
bias_shape = (1,)
divide_value = 1.0

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bn_eps, bn_momentum, bias_shape, divide_value]
```

This code defines a custom CUDA kernel that combines matrix multiplication, batch normalization, bias addition, division, and Swish activation into a single kernel. The kernel is then compiled using `load_inline` from PyTorch's C++ extension API and integrated into a new model class `ModelNew`. This approach aims to reduce the overhead of calling multiple operators and improve performance by leveraging parallel computation on the GPU.