Consider the following when writing the CUDA kernel:

1. **Optimize Memory Access**: Use shared memory to store the K and V matrices for faster access during the computation. This reduces the number of global memory accesses, which is a major bottleneck in attention computation.

2. **Block-wise Processing**: Process the attention computation in blocks to maximize parallelism and reduce memory latency. For example, divide the computation into blocks that can be processed by a single CUDA block.

3. **Algorithmic Optimizations**: Implement the scaled dot-product attention formula efficiently. Since the formula is (Q * K^T)/sqrt(d) followed by softmax and then multiplication with V, you can precompute the scaling factor and optimize the order of operations to minimize redundant calculations.

4. **Fusion of Operations**: Combine multiple steps into a single kernel to reduce kernel launch overhead. For instance, compute the scaled dot product, apply the softmax, and multiply with V all in one kernel.

5. **Vectorization**: Use CUDA's vector types (like `float4`) to load and store data in parallel, improving memory throughput and computation speed.

6. **Avoiding Division by Zero and Numerical Stability**: Implement proper handling of numerical stability, such as subtracting the maximum value before exponentiating in the softmax computation to prevent overflow.

7. **Batched Matrix Operations**: Leverage CUDA libraries or hand-optimized code for batched matrix operations, especially for the Q*K^T step, which can be optimized using tiled matrix multiplication techniques.

8. **Tiling and Loop Unrolling**: Use tiling strategies to fit frequently accessed data into shared memory and unroll loops where beneficial to hide memory latency.

9. **Coalesced Memory Access**: Ensure that memory accesses within threads of a warp are coalesced to maximize memory bandwidth utilization.

Here's the starting architecture, and you need to replace the scaled_dot_product_attention with a custom CUDA kernel. I want you to write code that uses the above tips to create an optimized kernel. The code should be fully functional and compilable.

Here is the architecture to optimize:

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)
        return out

batch_size = 32
num_heads = 32
sequence_length = 512
embedding_dimension = 1024

def get_inputs():
    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    return [Q, K, V]

def get_init_inputs():
    return []
```

**Important**: Do not change the function signature of the Model's forward method. The input and output must be compatible with the original architecture. The output ModelNew must have the same forward method signature and inputs/outputs as the original Model class.

**Constraints**:

- The inputs Q, K, V are of shape (batch_size, num_heads, sequence_length, embedding_dimension).
- All tensors are on the same CUDA device and in float16 precision.
- The output must also be in float16 and on CUDA.
- The kernel must handle batched operations efficiently.
- Numerical stability is important, especially in the softmax step.
- The code must be compatible with PyTorch's autograd system (so the kernel must correctly handle gradients, unless you use PyTorch's extensibility for gradients).
- The code must be written in Python with inline CUDA extensions as in the example, not as a separate .cpp file.
- Use torch.utils.cpp_extension.load_inline to load the kernel.
- The code must be fully self-contained and compilable.
- Avoid any dependencies beyond PyTorch and the standard CUDA runtime.
- The kernel must be able to handle varying batch sizes, head counts, sequence lengths, and embedding dimensions as given in the input.

**Final Notes**:

- The code must compile and run without errors.
- The code must produce the same numerical results as the original implementation (within a reasonable tolerance due to floating point differences).
- The solution should aim for high performance, making use of CUDA best practices for attention mechanisms.
```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for scaled dot-product attention
attention_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>

template <typename scalar_t>
__global__ void scaled_dot_product_attention_forward_kernel(
    const torch::PackedTensorAccessor<scalar_t,4> Q,
    const torch::PackedTensorAccessor<scalar_t,4> K,
    const torch::PackedTensorAccessor<scalar_t,4> V,
    torch::PackedTensorAccessor<scalar_t,4> output,
    const int batch_size,
    const int num_heads,
    const int sequence_length,
    const int embedding_dim,
    const float scale_factor) {

    // Calculate thread and block indices
    int b = blockIdx.x / (num_heads * sequence_length);
    int h = (blockIdx.x % (num_heads * sequence_length)) / sequence_length;
    int q_idx = (blockIdx.x % (num_heads * sequence_length)) % sequence_length;
    int k_idx = threadIdx.x;

    __shared__ scalar_t shared_K[512];  // Adjust size based on sequence_length
    __shared__ scalar_t shared_V[512];  // Adjust size based on sequence_length

    scalar_t sum = 0.0;

    // Load K and V into shared memory for this block's q_idx
    if (k_idx < sequence_length) {
        shared_K[k_idx] = K[b][h][k_idx];
        shared_V[k_idx] = V[b][h][k_idx];
    }
    __syncthreads();

    // Compute Q*K^T scaled by 1/sqrt(embedding_dim)
    scalar_t dot_product = 0.0;
    for (int d = 0; d < embedding_dim; ++d) {
        dot_product += Q[b][h][q_idx][d] * shared_K[d];
    }
    dot_product *= scale_factor;

    // Softmax computation with numerical stability
    scalar_t max_val = dot_product;
    for (int k = 0; k < sequence_length; ++k) {
        if (shared_K[k] > max_val) {
            max_val = shared_K[k];
        }
    }
    scalar_t numerator = exp(dot_product - max_val);
    scalar_t denominator = 0.0;
    for (int k = 0; k < sequence_length; ++k) {
        denominator += exp(shared_K[k] - max_val);
    }
    scalar_t softmax_val = numerator / denominator;

    // Multiply by V and accumulate
    for (int d = 0; d < embedding_dim; ++d) {
        sum += softmax_val * shared_V[d];
    }

    // Write result to output
    output[b][h][q_idx] = sum;
}

torch::Tensor scaled_dot_product_attention_forward_cuda(
    torch::Tensor Q,
    torch::Tensor K,
    torch::Tensor V) {
    
    const auto batch_size = Q.size(0);
    const auto num_heads = Q.size(1);
    const auto sequence_length = Q.size(2);
    const auto embedding_dim = Q.size(3);
    const auto scale_factor = 1.0 / sqrt(embedding_dim);

    auto output = torch::zeros({batch_size, num_heads, sequence_length, embedding_dim}, 
                              Q.options());

    dim3 blocks(batch_size * num_heads * sequence_length);
    dim3 threads(sequence_length);

    AT_DISPATCH_FLOATING_TYPES(Q.scalar_type(), "scaled_dot_product_attention_forward_cuda", ([&] {
        scaled_dot_product_attention_forward_kernel<scalar_t><<<blocks, threads>>>(
            Q.packed_accessor<scalar_t,4,torch::RestrictPtrTraits>(),
            K.packed_accessor<scalar_t,4,torch::RestrictPtrTraits>(),
            V.packed_accessor<scalar_t,4,torch::RestrictPtrTraits>(),
            output.packed_accessor<scalar_t,4,torch::RestrictPtrTraits>(),
            batch_size,
            num_heads,
            sequence_length,
            embedding_dim,
            scale_factor);
    }));

    return output;
}
"""

attention_kernel_cpp_source = """
torch::Tensor scaled_dot_product_attention_forward_cuda(
    torch::Tensor Q,
    torch::Tensor K,
    torch::Tensor V);
"""

# Compile the attention kernel
scaled_dot_product_attention = load_inline(
    name="scaled_dot_product_attention",
    cpp_sources=attention_kernel_cpp_source,
    cuda_sources=attention_kernel_source,
    functions=["scaled_dot_product_attention_forward_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[""]
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.attention_kernel = scaled_dot_product_attention

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        return self.attention_kernel.scaled_dot_product_attention_forward_cuda(Q, K, V)

def get_inputs():
    batch_size = 32
    num_heads = 32
    sequence_length = 512
    embedding_dimension = 1024
    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    return [Q, K, V]

def get_init_inputs():
    return []
```
```

Please determine whether the given text is related to computer science, if yes please return "YES", else return "NO".