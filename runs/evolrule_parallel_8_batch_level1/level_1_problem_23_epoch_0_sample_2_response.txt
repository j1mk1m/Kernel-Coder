The problem is to optimize a simple model that applies a softmax activation to an input tensor. The existing implementation uses torch.softmax, which may not be the most efficient. To optimize this, you need to implement a custom CUDA kernel for the softmax function. 

First, I need to analyze the current implementation and identify where the bottleneck is. The standard PyTorch softmax might be using a generic implementation that isn't optimized for the specific tensor dimensions given (batch_size=4096, dim=393216). 

The standard softmax involves three steps: exponentiating the input, normalizing by the sum of exponents, and then dividing each element by the sum. The key steps here are the exponentiation, the reduction (summing along the specified dimension), and then the division. 

Possible optimizations could include:

1. **Vectorization**: Using CUDA threads to process elements in parallel. Since the tensors are large, this is crucial.
2. **Algorithmic Changes**: 
   - Using logarithmic space to prevent overflow/underflow, but since softmax requires exponentials, this might not help directly.
   - Implementing a more efficient reduction (sum) using parallel reduction techniques on the GPU.
   - Using shared memory to reduce global memory access latency.
   - Exploiting the fact that the dimension is very large (393k) which allows for block-wise processing.

3. **Kernel Fusion**: Combining the exponentiation and division steps into a single kernel to minimize memory transactions.

Considering the input tensor shape (4096, 393216), each batch element has 393k elements. Since the batch is along the first dimension, it makes sense to process each batch element independently. 

The plan is to write a CUDA kernel that processes each row (each batch's feature vector) in parallel. Each thread can handle one element, but for large dimensions, we might need to tile the computation or use a block per row to manage the reduction efficiently.

First, let's think about the steps for each row (each batch element):

1. **Compute the exponentials**: For each element in the row, compute exp(x_i).
2. **Compute the sum of exponents**: Sum all exp(x_i) for the row.
3. **Divide each exponential by the sum**: Compute exp(x_i) / sum.

The main challenge is efficiently computing the sum across a large vector (393k elements) in parallel. A parallel reduction is necessary here. 

Implementing the reduction in CUDA can be done using a block of threads. For a row of N elements, we can have a block of threads where each thread processes a segment of the array, then they combine their partial sums using shared memory.

Here's the approach:

- For each batch element (each row), assign a block of threads to handle it.
- Each thread in the block takes a chunk of the row's elements, computes the sum of exp(x_i) for its chunk, stores it in shared memory.
- Then perform a parallel reduction within the block to compute the total sum.
- Once the sum is known, each thread can compute the exp(x_i) and divide by the sum.

But the problem is that exponentiating all elements first might require storing them temporarily. Alternatively, we can compute the exponentials on the fly during the reduction phase.

Alternatively, we can structure the kernel as follows:

1. Each block handles a single row.
2. Each thread in the block processes a range of elements in the row.
3. Each thread computes exp(x_i) for its elements, sums them into shared memory.
4. The block then reduces the shared memory to get the total sum.
5. Then, the threads compute the division and store the result.

This requires two passes over the data: first to compute exponentials and the sum, then to compute the division. However, since the exponentials are needed for both the sum and the final division, it's better to store them temporarily in shared memory or global memory. But for large N (393k), storing all exponentials in global memory might be feasible but requires extra memory. Alternatively, can we avoid storing all exponentials?

Wait, during the first pass, each thread can compute the exponential of its elements and accumulate the sum. The sum is stored as a block-wide variable. Then, in the second pass, each thread can recompute the exponentials (if needed) or store them in shared memory. However, recomputing exponentials may be computationally expensive, so storing them is better.

Therefore, the plan is:

- Allocate a temporary array (in global memory) to store the exponentials. However, for large N, this requires additional memory. Alternatively, can we do it without storing all exponentials?

Wait, let's think again. The problem is that each element's exponential is needed both for the sum and for the final division. So, the exponentials must be stored temporarily. 

Alternatively, compute the exponentials and their sum in a single step, then use the exponentials again for the division. 

Thus, the steps would be:

1. For each row (each batch element):
   a. Compute exponentials of all elements and store them in a temporary array.
   b. Compute the sum of all exponentials.
   c. Divide each exponential by the sum to get the softmax output.

The storage for the exponentials would be a temporary array of size (batch_size, dim). For batch_size=4096 and dim=393216, this is 4096 * 393216 = ~1.6 billion elements, which is about 6.4 GB (since each float is 4 bytes). This is a lot of memory and may not be feasible on a GPU with limited memory. Therefore, this approach may not be practical.

Hmm, this is a problem. Storing the intermediate exponentials would require too much memory. So, perhaps we need a smarter approach.

Alternative approach: compute the exponentials and sum in a way that avoids storing all of them. For example, during the summation, the exponentials can be stored in shared memory for the block, but since the dimension is 393k, which is way larger than the shared memory capacity, this might not be feasible either.

Wait, perhaps we can use a two-step reduction. First, each thread computes a partial sum of a chunk of the row, then the block reduces those partial sums. Then, the sum is available. Then, each thread can process their chunk again, computing the exponential and dividing by the sum. This way, the exponentials are computed twice, but we don't need to store them.

This would mean:

1. For each row:
   a. Each thread in the block computes the sum of exp(x_i) over their assigned elements. The partial sums are stored in shared memory.
   b. The block reduces these to get the total sum.
   c. The sum is broadcast to all threads in the block.
   d. Each thread computes the exponentials of their elements again and divides by the sum, storing the result in the output.

This approach avoids storing the exponentials, but computes them twice. However, exponentiation is a relatively expensive operation, so this might not be optimal. However, for large N, the cost of exponentiation might be amortized over the computation.

Alternatively, perhaps we can optimize the exponentials. Since exponentiating and then dividing by the sum can be done in a single step, but the problem is the sum needs to be known first.

Wait, another idea: the exponentials can be computed in a single pass, then the sum can be calculated in a parallel reduction, then a second pass to divide. This requires storing the exponentials, but maybe we can do this in a way that minimizes memory usage.

Alternatively, using shared memory for partial sums and keeping the exponentials in registers? Probably not, since each element is processed by a thread, but for N=393k, even with 1024 threads per block, each thread would need to process ~383 elements, which might be manageable.

Wait, let's think about how to structure the CUDA kernel.

Suppose each block handles one row. Let’s say the block size is 256 threads. The row has 393,216 elements. So each thread would need to handle 393,216 / 256 ≈ 1536 elements. That's a lot for a single thread, but maybe manageable. 

First step: Each thread loads its chunk of the input row, computes the exponentials of those elements, and accumulates the sum in shared memory. 

Then, perform a parallel reduction in shared memory to get the total sum for the row.

Once the sum is known, each thread can go back and compute the exponential (again?) and divide by the sum. Wait, but we already have the exponentials from the first pass. However, if they are stored in shared memory, but since each thread processed a chunk, maybe we can store the exponentials in a global temporary array. 

Alternatively, perhaps in the first pass, each thread computes the exponentials of their assigned elements and writes them to a temporary array, then computes their partial sum. Then, the sum is known. Then in the second pass, the threads read the exponentials from the temporary array and divide by the sum.

This would require a temporary array, which for 4096 rows and 393k elements is 4096 * 393216 * 4 bytes (float) = about 6.4 GB. That's a lot, but maybe acceptable if the GPU has enough memory. Alternatively, maybe we can process one row at a time, so the temporary array can be reused for each row.

Wait, if each block processes one row independently, then the temporary storage for exponentials can be per-row. So for each row, we need a temporary array of size 'dim'. Since the blocks process rows in parallel, the total required memory would be number_of_blocks * dim. But if the blocks are processed in sequence, then it can be done with a single temporary array of size 'dim'.

Alternatively, perhaps it's better to structure the kernel so that each block handles a single row, and the temporary storage for the exponentials is in global memory. 

But let's proceed step by step.

First, the CUDA kernel structure:

Kernel softmax_kernel<<<blocks, threads>>>(input, output, temp_exps, dim):

Wait, but using a temporary array for each row's exponentials would require a lot of memory. Let me think of an alternative approach without storing all exponentials.

Alternative approach:

The key idea is to compute the sum first, then compute the exponentials and divide in a single pass.

Wait, but the sum requires the exponentials. So the order must be:

1. Compute all exponentials, store them in a temporary array (temp_exps).
2. Compute the sum of temp_exps for the row.
3. Divide each temp_exps element by the sum to get the result.

This requires storing the temp_exps, but as mentioned, this is memory intensive.

Alternatively, can we compute the exponentials in a way that the sum is computed in parallel, and then use the sum to compute the result in a single pass?

Perhaps, using a two-step kernel:

First kernel pass: compute exponentials and accumulate the sum per row.

Second kernel pass: compute the division using the sum.

But this requires multiple kernel launches, which may have overhead. Alternatively, can we do it in a single kernel?

Let me think of the algorithm in terms of CUDA threads:

Suppose we have one block per row. Each block has N threads, where N is the number of threads needed to process the row.

Each thread in the block is responsible for a chunk of the row's elements.

First, each thread computes the exponentials of its elements and accumulates the sum in shared memory.

Once the shared memory reduction is done, the block's sum is known. Then, the same threads can compute the exponentials again (but that's redundant) or store them in shared memory for the next step.

Wait, but if we compute the exponentials in the first step, but need them again for the division, we need to store them. However, storing them in shared memory may not be feasible due to the size.

Alternatively, if the chunk size per thread is small enough, maybe each thread can keep the exponentials of their chunk in registers. For example, if a thread handles 100 elements, then storing 100 floats in registers may be feasible (each thread has 255 registers available). But 100 elements per thread would require 400 bytes per thread (assuming 4 bytes per float), which is manageable. Let's see:

Total elements per row: 393,216.

If we use 256 threads per block, each thread handles 393,216 / 256 ≈ 1536 elements. That's too many to store in registers (each thread would need 1536 floats, which is way over the register limit).

Thus, this approach is not feasible.

Alternative Idea: Compute the exponentials on the fly again during the division step. Although this requires computing exp twice, it avoids the memory overhead.

So steps for a single block (per row):

1. Each thread computes the exponentials of their assigned elements and accumulates their partial sum into shared memory.

2. Perform a parallel reduction in shared memory to get the total sum.

3. Then, each thread recomputes the exponentials of their elements and divides each by the total sum, writing the result to the output.

This way, the exponentials are computed twice, but we save on memory.

The exponentials are computed twice, but maybe the computation time is manageable compared to the memory access time.

So let's proceed with this approach.

Now, structuring the kernel:

- Each block handles a row (since batch_size is 4096, we need 4096 blocks).

- Each block has a certain number of threads (e.g., 256). Let's say 256 threads per block.

- Each thread handles a chunk of elements in the row.

Let me outline the steps in code:

The kernel would look something like this:

__global__ void softmax_kernel(float* input, float* output, int dim) {

    // Each block processes a row.
    int row = blockIdx.x;

    // Shared memory for partial sums.
    extern __shared__ float shared[];

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    // First, compute exponentials and partial sums.
    float total_sum = 0.0f;

    for (int i = tid; i < dim; i += num_threads) {
        float val = input[row * dim + i];
        float exp_val = expf(val);
        total_sum += exp_val;
        // Store exp_val somewhere? Not possible here, because we need it later.
    }

    // Wait, but we need to store the exp_val for the division step.

    // Hmm, this approach won't work because after accumulating the sum, we lose the individual exp_val.

    // So need another approach.

}

Hmm, this shows that the problem is that we need the exp_val again for the division step. So the previous approach won't work because after computing the sum, the exp_val is no longer stored.

Alternative approach: Use two separate loops. First loop to compute the sum, then loop again to compute the exp and divide.

But that requires:

1. First loop: compute exp_val for each element, accumulate the sum.
   But we need to store the exp_val for each element to use later. So we need to store them in a temporary array.

Alternatively, the first loop can store the exp_val in a temporary array, then the second loop reads from the temporary array and divides by the sum.

Thus, the steps would be:

1. Each thread computes their chunk of exp_vals and writes to a temporary array (global memory).

2. Compute the sum of all exp_vals in the row.

3. Each thread reads their exp_vals from the temporary array, divides by the sum, and writes to the output.

This requires the temporary array, which is memory intensive. But let's proceed.

Given that, the kernel can be structured as follows:

But wait, perhaps the kernel can be split into two parts: first, compute the exp and store in temp, compute the sum, then compute the division. However, this would require multiple kernel launches or synchronization within the block.

Alternatively, the entire process can be done within a single kernel using two passes over the data.

Let me try to sketch the kernel code:

__global__ void softmax_kernel(float* input, float* output, float* temp_exps, int dim) {

    int row = blockIdx.x; // Each block handles a row.

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    // First pass: compute exponentials and store in temp_exps, accumulate sum.

    float local_sum = 0.0f;
    for (int i = tid; i < dim; i += num_threads) {
        float val = input[row * dim + i];
        float exp_val = expf(val);
        temp_exps[row * dim + i] = exp_val;
        local_sum += exp_val;
    }

    // Use shared memory to accumulate the local sums into a global sum for the row.
    // But how to get the total sum?

    __shared__ float shared_sums[THREADS_PER_BLOCK];

    shared_sums[tid] = local_sum;
    __syncthreads();

    // Perform parallel reduction in shared memory to get the total sum for the row.
    for (int s = num_threads/2; s > 0; s >>=1) {
        if (tid < s) {
            shared_sums[tid] += shared_sums[tid + s];
        }
        __syncthreads();
    }

    float total_sum = shared_sums[0];

    // Now, each thread reads their exp_val from temp_exps and computes output.
    for (int i = tid; i < dim; i += num_threads) {
        float exp_val = temp_exps[row * dim + i];
        output[row * dim + i] = exp_val / total_sum;
    }
}

Wait, but this requires that the total_sum is available to all threads. In this code, only the first thread (tid=0) has the total_sum after the reduction, so other threads don't have access. To fix this, after the reduction, all threads can have access to the total_sum by using shared memory or broadcasting.

Wait, after the reduction, the total_sum is stored in shared_sums[0]. So after the reduction loop, each thread can read shared_sums[0] to get the total_sum. However, in the current code, after the reduction, the first thread has the correct total_sum, but others might not. Wait no, after the reduction loop, the code:

The reduction loop is designed so that after the loop, shared_sums[0] contains the total sum. So all threads can read shared_sums[0], but only after the reduction is complete.

So the code would need to have the reduction, then all threads can proceed to compute their outputs using the shared_sums[0].

However, in the code above, after the reduction, each thread can read the total_sum from shared_sums[0], then proceed to the second loop.

Wait, in the code I wrote above:

After the reduction loop, the total_sum is stored in shared_sums[0]. So all threads can read that value, so the total_sum variable can be assigned as:

if (tid ==0) total_sum = shared_sums[0]; else ... no, better to have all threads read it.

Wait, the code after the reduction:

float total_sum = shared_sums[0];

This is executed by all threads, so each thread can get the total_sum from shared_sums[0].

However, this requires that after the reduction, all threads have synced and the shared_sums[0] is valid.

Yes, so after the reduction loop, the threads can read the total_sum from shared_sums[0].

Therefore, the code above should work. 

But this requires a temporary array (temp_exps) of size batch_size * dim, which is 4096 * 393216 = ~1.6e9 elements, which is ~6.4 GB (since each float is 4 bytes). This is a lot, but maybe manageable on a GPU with sufficient memory (e.g., 24GB VRAM).

Alternatively, can we do this without a temporary array? Let's think again.

Alternative Idea: Instead of storing all exponentials, compute each element's exponential twice: once for the sum, and once for the output.

So the steps would be:

For each row (block):

1. Each thread computes the sum of exp(x_i) over their assigned elements, accumulating into shared memory.

2. Compute the total sum via reduction in shared memory.

3. Each thread then recomputes the exponentials for their assigned elements, divides by the total sum, and writes to the output.

This way, there's no need for a temporary array, but the exponentials are computed twice, which could be costly.

But for large N, perhaps the cost of computing the exponentials twice is acceptable compared to the memory usage. Let's compare:

- Memory approach: 6.4 GB.

- Recompute approach: twice the exp computations.

Assuming that the time to compute exp is about 20ns per element (but this varies), for 393k elements, that's 20 * 393,216 = ~7.86e6 ns = ~7.86ms per row. Since there are 4096 rows, total for two passes would be 4096 * 7.86ms = ~32.6 seconds. Wait, that seems way too high. Probably my numbers are off.

Wait, let's correct. Let me think in terms of FLOPS.

The exp function is a transcendental operation. On an A100 GPU, for example, expf might take about 0.2 cycles per element (but this varies). Let's say 1 cycle per element for simplicity. 

Each row has 393,216 elements. For one row:

First pass: compute exp for all elements (393k operations).

Second pass: compute exp again (another 393k operations).

Total per row: 786,432 operations.

Assuming 1 cycle per exp, and the GPU has a peak FLOPS of, say, 15 TFLOPS (for A100), then per row computation would be 786,432 / 1e9 ~ 0.000786 seconds (786 microseconds). For 4096 rows, that's 4096 * 0.000786 ~ 3.23 seconds. That's still a lot, but perhaps not, considering that this is done in parallel across all blocks.

Wait, each block is processing a row independently. So if there are 4096 blocks, each with 256 threads, then the GPU can process them in parallel. Assuming a GPU with 80 SMs, each SM can handle up to, say, 64 blocks (depending on the GPU's maximum concurrent blocks). But even if it can handle all 4096 blocks in parallel, the time per row is the time for the kernel to process one row's computation.

Wait, the total compute time per row is 393,216 elements * 2 passes * 1 cycle each = 786,432 cycles per row. The GPU's clock speed is about 1.4 GHz, so cycles per second are 1.4e9. So per row time is 786,432 / 1.4e9 ≈ 5.6e-4 seconds (0.56 ms). With 4096 rows, total time would be 4096 * 0.56 ms ≈ 2300 ms (2.3 seconds). 

Alternatively, if the GPU can process all rows in parallel, the total time would be 0.56 ms, which is much better.

But the number of blocks (4096) may exceed the maximum number of concurrent blocks a GPU can handle. For example, an A100 has 108 SMs, each can handle up to 32 blocks, so total concurrent blocks are 3456. So 4096 would exceed that, leading to some spilling and serialized execution, increasing the time.

Alternatively, if the GPU can handle it, then the recomputing approach may be better in terms of memory, even with the extra computations.

Therefore, the recomputing approach may be feasible.

Thus, let's proceed with the recomputing approach to avoid the large temporary array.

Now, structuring the kernel:

__global__ void softmax_kernel(float* input, float* output, int dim) {

    int row = blockIdx.x;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    // Shared memory for partial sums.
    extern __shared__ float shared[];

    int shared_offset = threadIdx.x;

    float local_sum = 0.0f;

    // First pass: compute exponentials and accumulate sum.
    for (int i = tid; i < dim; i += num_threads) {
        float val = input[row * dim + i];
        float exp_val = expf(val);
        local_sum += exp_val;
    }

    // Store local_sum in shared memory.
    shared[shared_offset] = local_sum;
    __syncthreads();

    // Perform reduction in shared memory.
    for (int s = num_threads/2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared[threadIdx.x] += shared[threadIdx.x + s];
        }
        __syncthreads();
    }

    float total_sum = shared[0]; // All threads read the total sum from shared[0]

    __syncthreads();

    // Second pass: compute exp again and divide by total_sum.
    for (int i = tid; i < dim; i += num_threads) {
        float val = input[row * dim + i];
        float exp_val = expf(val);
        output[row * dim + i] = exp_val / total_sum;
    }
}

Wait, but in this code, during the first pass, each thread accumulates the sum of exp(x_i) for their assigned elements. Then, using shared memory to reduce to the total_sum.

Then in the second pass, each thread recomputes the exp(x_i) and divides by total_sum.

This approach avoids the temporary array, but requires computing exp twice. The shared memory size needs to be at least the number of threads. Since the block size is, say, 256, the shared memory required is 256 * 4 bytes = 1KB per block, which is manageable.

The block size can be chosen to maximize occupancy, but let's see. The number of threads per block can be 256, which is common.

Now, the total shared memory per block is 256 floats (1KB). Since each block is per row, and there are 4096 blocks, but they are processed in parallel, the total shared memory used would be 4096 * 1KB = ~4MB, which is okay.

Now, to handle the loop over the elements:

The first loop: for each element in the row assigned to the thread.

The second loop similarly.

Now, the kernel can be called with:

dim3 blocks(batch_size); // 4096 blocks

dim3 threads(threads_per_block); // e.g., 256

The shared memory per block is threads_per_block * sizeof(float). So when launching the kernel, we need to specify the shared memory size:

softmax_kernel<<<blocks, threads, threads_per_block * sizeof(float)>>>(input, output, dim);

Now, implementing this in the Python code with the inline CUDA extension.

Also, in the Python code:

The input and output are tensors. The kernel expects pointers to them. 

The function signature for the CUDA kernel would be:

extern "C" {

    __global__ void softmax_kernel(float* input, float* output, int dim);

    torch::Tensor softmax_cuda(torch::Tensor input) {
        const int batch_size = input.size(0);
        const int dim = input.size(1);

        auto output = torch::empty_like(input);

        int threads_per_block = 256;
        int shared_mem = threads_per_block * sizeof(float);

        dim3 blocks(batch_size);
        dim3 threads(threads_per_block);

        softmax_kernel<<<blocks, threads, shared_mem>>>(input.data_ptr<float>(), output.data_ptr<float>(), dim);

        return output;
    }
}

Wait, but in the kernel, each block processes a row (blockIdx.x is the row index). So the input tensor must be contiguous in memory. The user's code uses torch.softmax, which expects the input to be in a certain format, so the input to our kernel should be contiguous.

Thus, in the Python code, we can ensure that the input is contiguous by using .contiguous().

Now, putting this all together in the Python code with the inline CUDA.

The Python code will need to define the CUDA kernel as a string, then load it using load_inline.

Now, writing the code:

First, the CUDA kernel code:

softmax_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void softmax_kernel(T* input, T* output, int dim) {
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    extern __shared__ T shared[];

    T local_sum = 0.0;

    // First pass: compute exponentials and accumulate sum
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        local_sum += exp_val;
    }

    // Write to shared memory
    shared[tid] = local_sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    T total_sum = shared[0];

    // Second pass: compute output
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        output[row * dim + i] = exp_val / total_sum;
    }
}

// Kernel wrapper function
at::Tensor softmax_cuda(at::Tensor input) {
    const int batch_size = input.size(0);
    const int dim = input.size(1);

    auto output = at::empty_like(input);

    const int threads_per_block = 256;
    const size_t shared_mem = threads_per_block * sizeof(float);

    dim3 blocks(batch_size);
    dim3 threads(threads_per_block);

    AT_DISPATCH_FLOATING_TYPES(input.type(), "softmax_cuda", ([&] {
        softmax_kernel<scalar_t><<<blocks, threads, shared_mem>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            dim
        );
    }));

    return output;
}
"""

Wait, but in the kernel, the template uses T for the data type. The AT_DISPATCH_FLOATING_TYPES macro allows handling different floating-point types (float, double). The kernel is written as a template, so it can handle both. 

But in the given problem, the input is generated with torch.rand, which is float32 by default. So maybe the code can be simplified for float only, but using the template is better for generality.

Now, in the Python code:

We need to include this CUDA code as a string, and then load it.

So the Python code:

from torch.utils.cpp_extension import load_inline

softmax_cpp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void softmax_kernel(T* input, T* output, int dim) {
    // ... (same code as above)
}

at::Tensor softmax_cuda(at::Tensor input) {
    // ... (same code as above)
}
"""

Wait, actually, the code above was in a single string. Let me restructure.

Wait, in the code provided earlier, the 'softmax_source' string includes the kernel and the wrapper function. So the full code would be:

In Python:

softmax_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void softmax_kernel(T* input, T* output, int dim) {
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    extern __shared__ T shared[];

    T local_sum = 0.0;

    // First pass: compute exponentials and accumulate sum
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        local_sum += exp_val;
    }

    // Write to shared memory
    shared[tid] = local_sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    T total_sum = shared[0];

    // Second pass: compute output
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        output[row * dim + i] = exp_val / total_sum;
    }
}

at::Tensor softmax_cuda(at::Tensor input) {
    const int batch_size = input.size(0);
    const int dim = input.size(1);

    auto output = at::empty_like(input);

    const int threads_per_block = 256;
    const size_t shared_mem = threads_per_block * sizeof(float);

    dim3 blocks(batch_size);
    dim3 threads(threads_per_block);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "softmax_cuda", ([&] {
        softmax_kernel<scalar_t><<<blocks, threads, shared_mem>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            dim
        );
    }));

    return output;
}
"""

Then, load it using load_inline:

softmax_cuda = load_inline(
    name="softmax_cuda",
    cpp_sources=softmax_source,
    functions=["softmax_cuda"],
    verbose=True,
)

Wait, but in the load_inline function, the 'cpp_sources' should be the C++ code, and the CUDA code can be in 'cuda_sources'. Wait, looking back at the example provided in the problem:

The example uses:

elementwise_add = load_inline(
    name="elementwise_add",
    cpp_sources=elementwise_add_cpp_source,
    cuda_sources=elementwise_add_source,
    functions=["elementwise_add_cuda"],
    ...
)

In that example, the CPP source is a header, and the CUDA source is the kernel.

In our case, the code includes both the kernel and the wrapper function in the same CUDA code. So perhaps the entire code is in 'cuda_sources', and the 'cpp_sources' can be an empty string or not needed.

Wait, the load_inline function requires 'cpp_sources' and 'cuda_sources' to be provided. So perhaps we can split the code into a header (cpp) and the CUDA code (cuda).

Alternatively, perhaps all the code can be in the 'cuda_sources', and the 'cpp_sources' is just the function declarations.

Wait, let's check the example given in the problem:

The example had:

elementwise_add_cpp_source = (
    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);"
)

elementwise_add_source (the CUDA code) contains the kernel and the elementwise_add_cuda function.

Therefore, in our case, the CPP source would be the declaration of the softmax_cuda function.

So:

softmax_cpp_source = """
#include <torch/extension.h>

torch::Tensor softmax_cuda(torch::Tensor input);
"""

softmax_cuda_source = the CUDA code above (the template kernel and the softmax_cuda function).

Wait, but in the code I wrote earlier, the 'softmax_cuda' function is already inside the CUDA code. Wait, no: in the code I wrote, the entire code is in the 'softmax_source' string. But to separate into CPP and CUDA sources:

The CPP source (header) declares the function:

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input);

}

The CUDA code (source) contains the implementation of the function and the kernel.

So, splitting the code:

softmax_cpp_source = """
#include <torch/extension.h>

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input);

}
"""

softmax_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void softmax_kernel(T* input, T* output, int dim) {
    // ... (same as before)
}

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input) {
    // ... (same as before)
}

}
"""

Wait, but in the CUDA code, we need to include the declaration of the softmax_cuda function.

Therefore, putting it all together, the Python code would be:

from torch.utils.cpp_extension import load_inline

softmax_cpp_source = """
#include <torch/extension.h>

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input);

}
"""

softmax_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void softmax_kernel(T* input, T* output, int dim) {
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    extern __shared__ T shared[];

    T local_sum = 0.0;

    // First pass: compute exponentials and accumulate sum
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        local_sum += exp_val;
    }

    // Write to shared memory
    shared[tid] = local_sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    T total_sum = shared[0];

    // Second pass: compute output
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        output[row * dim + i] = exp_val / total_sum;
    }
}

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input) {
    const int batch_size = input.size(0);
    const int dim = input.size(1);

    auto output = at::empty_like(input);

    const int threads_per_block = 256;
    const size_t shared_mem = threads_per_block * sizeof(float);

    dim3 blocks(batch_size);
    dim3 threads(threads_per_block);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "softmax_cuda", ([&] {
        softmax_kernel<scalar_t><<<blocks, threads, shared_mem>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            dim
        );
    }));

    return output;
}

}
"""

Then, load using:

softmax_ext = load_inline(
    name="softmax_ext",
    cpp_sources=softmax_cpp_source,
    cuda_sources=softmax_cuda_source,
    functions=["softmax_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"],
)

Wait, also note that in the kernel, we used 'exp(val)' which for float is fine, but the code is templated for any T (float or double). However, in the problem's code, the input is generated with torch.rand, which is float32, so it should work.

Now, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.softmax_cuda = softmax_ext  # Assuming the module is stored here?

Wait, no. The loaded module's function is accessible via the returned object. Let me recall from the example:

In the example, they did:

elementwise_add = load_inline(...)

and then in the ModelNew:

def forward(self, a, b):
    return self.elementwise_add.elementwise_add_cuda(a, b)

So in our case, after loading:

softmax_ext = load_inline(...)

Then in ModelNew:

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.softmax_cuda = softmax_ext

    def forward(self, x):
        return self.softmax_cuda.softmax_cuda(x)

Wait, but the function name is 'softmax_cuda', so:

self.softmax_cuda.softmax_cuda(x)

Alternatively, perhaps the function is directly accessible. Let me check the example again.

In the example, the loaded module (elementwise_add) has a function 'elementwise_add_cuda', so when called as self.elementwise_add.elementwise_add_cuda(a, b).

Similarly, our loaded module's function is 'softmax_cuda', so the code would be:

return self.softmax_cuda.softmax_cuda(x)

Yes.

Thus, putting it all together:

The final Python code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

softmax_cpp_source = """
#include <torch/extension.h>

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input);

}
"""

softmax_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void softmax_kernel(T* input, T* output, int dim) {
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    extern __shared__ T shared[];

    T local_sum = 0.0;

    // First pass: compute exponentials and accumulate sum
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        local_sum += exp_val;
    }

    // Write to shared memory
    shared[tid] = local_sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    T total_sum = shared[0];

    // Second pass: compute output
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        output[row * dim + i] = exp_val / total_sum;
    }
}

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input) {
    const int batch_size = input.size(0);
    const int dim = input.size(1);

    auto output = at::empty_like(input);

    const int threads_per_block = 256;
    const size_t shared_mem = threads_per_block * sizeof(float);

    dim3 blocks(batch_size);
    dim3 threads(threads_per_block);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "softmax_cuda", ([&] {
        softmax_kernel<scalar_t><<<blocks, threads, shared_mem>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            dim
        );
    }));

    return output;
}

}
"""

softmax_ext = load_inline(
    name="softmax_ext",
    cpp_sources=softmax_cpp_source,
    cuda_sources=softmax_cuda_source,
    functions=["softmax_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.softmax_cuda = softmax_ext

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.softmax_cuda.softmax_cuda(x)

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed

# The original batch_size and dim are defined in the original code, so we need to include them.
batch_size = 4096
dim = 393216
```

Wait, but in the problem's given code, the batch_size and dim are defined in the global scope. So in the new code, we need to include those definitions as well.

Thus, adding those lines at the top:

batch_size = 4096
dim = 393216

Wait, but in Python, if these are in the global scope, they'll be accessible in the get_inputs function.

Thus, the complete code is as above.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

softmax_cpp_source = """
#include <torch/extension.h>

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input);

}
"""

softmax_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void softmax_kernel(T* input, T* output, int dim) {
    int row = blockIdx.x;
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    extern __shared__ T shared[];

    T local_sum = 0.0;

    // First pass: compute exponentials and accumulate sum
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        local_sum += exp_val;
    }

    // Write to shared memory
    shared[tid] = local_sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    T total_sum = shared[0];

    // Second pass: compute output
    for (int i = tid; i < dim; i += num_threads) {
        T val = input[row * dim + i];
        T exp_val = exp(val);
        output[row * dim + i] = exp_val / total_sum;
    }
}

extern "C" {

torch::Tensor softmax_cuda(torch::Tensor input) {
    const int batch_size = input.size(0);
    const int dim = input.size(1);

    auto output = at::empty_like(input);

    const int threads_per_block = 256;
    const size_t shared_mem = threads_per_block * sizeof(float);

    dim3 blocks(batch_size);
    dim3 threads(threads_per_block);

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "softmax_cuda", ([&] {
        softmax_kernel<scalar_t><<<blocks, threads, shared_mem>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            dim
        );
    }));

    return output;
}

}
"""

softmax_ext = load_inline(
    name="softmax_ext",
    cpp_sources=softmax_cpp_source,
    cuda_sources=softmax_cuda_source,
    functions=["softmax_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.softmax_cuda = softmax_ext

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.softmax_cuda.softmax_cuda(x)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()
    return [x]

def get_init_inputs():
    return []
```