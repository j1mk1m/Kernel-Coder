The goal is to achieve a speedup over the original implementation (original implementation uses torch.matmul). 

Make sure that the input and output sizes match the original code. The optimized model must accept inputs of the same size and produce outputs of the same size as the original model. 

The problem is to implement a matrix vector multiplication (Ax = b) where A is a matrix of size M x K, x is a vector of size K, and b is a vector of size M. Here the dimensions are M = 2048 and K = 1048576. 

The original code uses torch.matmul which is a dense implementation. However, when K is very large (e.g., 1e6), the matrix vector product can be memory bandwidth bound. The goal is to write a custom kernel that performs the same computation as the original code but is faster. 

Possible directions to explore: 
    1. Reorganizing the computation to improve memory access patterns (e.g., tiling, loop ordering). 
    2. Exploiting shared memory to cache parts of the matrix A. 
    3. Avoiding redundant computations. 
    4. Using a block-wise approach to exploit coalesced memory access. 

You may also try to exploit sparsity in the matrix A, but given that the original problem states A is a dense matrix, this may not be applicable. 

You can also explore algorithmic changes such as using block-based matrix multiplication or reordering computations to maximize cache reuse.

You must ensure that your custom CUDA kernel is correct (produces the same output as the original code) and faster than the original code. 

To verify correctness, you can compute the output with both the original and custom kernels and check the difference. However, you do not need to include such testing code in your answer.



Okay, let's tackle this problem. The user wants to optimize matrix-vector multiplication where A is MxK (2048x1,048,576) and B is Kx1. The goal is to make this faster than PyTorch's matmul. 

First, I need to understand why the original might be slow. Since K is very large, memory bandwidth is a big factor. The standard matmul might not be optimized for such a large K. 

The key here is memory access patterns. In a matrix-vector multiply, each element of B is multiplied by a row of A, but since B is a vector, maybe we can process it more efficiently. Alternatively, transposing A might help? Wait, no, because in matrix multiplication, the matrix is MxK and the vector is Kx1. The result is sum(A[i,k] * B[k] for all k). 

The naive approach would be to loop over each row of A, multiply by B, and sum. But with K being 1e6, each row has a million elements, so doing this for each row (2048 times) could be slow. 

The problem is that accessing the matrix A sequentially might lead to poor cache performance. Since each row is accessed once per row, but with K being large, the matrix is stored in row-major, so each row is contiguous. So for each row, you can process the elements sequentially. Wait, but the B vector is K elements. So for each row, you have to go through all K elements of B? 

Alternatively, maybe reorganize the computation. Let me think of the matrix as columns. The vector B is a column vector. The result is a linear combination of the columns of A, weighted by the elements of B. So, each element of B scales a column of A, and all columns are summed. 

Wait, that's an interesting observation. The result is the sum over k of A[:,k] * B[k]. Because each column of A is a vector of M elements. So, if B is a vector of K elements, then each element B[k] scales the k-th column of A, and then all these are added together. 

Hmm, that might be a better way to compute it, especially if K is very large. Let me see. 

Suppose the matrix A is stored in row-major order. So each row is contiguous. To compute the result as the sum of scaled columns, we might have to access the columns of A, which are not contiguous in memory. That could be bad for cache. 

Alternatively, if we can process the computation in a way that maximizes memory access locality. 

Another thought: The standard matrix multiplication (GEMM) is optimized for when both matrices are large. Here, the second matrix is a vector, so maybe a custom kernel can exploit that structure. 

Let me think about the naive kernel first. The standard approach for the matrix-vector multiply would be for each row i in A, compute the dot product of row i with the vector B. 

The loop would look like:

for i in 0..M-1:
    result[i] = 0
    for k in 0..K-1:
        result[i] += A[i][k] * B[k]

But this is O(M*K), which is 2048*1e6 ~ 2e9 operations. On a GPU, this should be manageable, but the problem is memory access. 

In CUDA, each thread could handle a row. The threads can loop over the K elements. 

But K is 1e6, so each thread has to loop 1e6 times. That's a lot of iterations. Maybe we can tile the computation. 

Alternatively, since K is very large, we can process it in chunks. 

Wait, but with 1e6 elements, even with 1024 threads per block, each thread could handle a chunk of K / 1024 elements. 

Alternatively, using shared memory. Maybe break the K loop into chunks that fit in shared memory. 

Wait, but the problem is that for each row, we have to do K multiplications. If K is 1e6, then each thread (per row) has to process 1e6 elements. 

Hmm. Let's think of the kernel structure. 

Each thread could compute a partial sum for a row. But for K=1e6, even with 1024 threads, each would have to process ~ 1e3 elements. Wait, no, 1e6 / 1024 is ~976. So each thread would have to loop 976 times. 

But even that is a lot of loops. Maybe using a tiled approach where each thread block processes a block of rows, and the K loop is divided into chunks. 

Alternatively, since the B vector is a vector, maybe we can pre-load B into shared memory. 

Wait, here's an idea: 

Let me structure the computation so that each thread block is responsible for a block of rows. Suppose we have a block of, say, 32 rows. Then, each thread in the block can process a portion of the K elements. 

Alternatively, for each row, the kernel can have each thread in a block process a chunk of the K elements. 

Alternatively, the key is to make memory access coalesced. 

Let me think of the matrix as stored in row-major. So for a given row i, A[i][k] is contiguous. 

Suppose we launch a kernel where each thread is responsible for a row. 

Each thread would need to loop over all K elements. 

But with K=1e6, that's 1e6 iterations per thread. 

The problem is that this would lead to a lot of loops and might have high latency. 

Alternatively, can we parallelize over the K dimension as well? 

Hmm. Let me think of the problem differently. 

The result for row i is the sum of A[i][k] * B[k] for all k. 

Suppose we have multiple threads per row. Let's say each row is handled by a warp (32 threads). Each thread in the warp can handle a chunk of K elements. 

Wait, but the row has K elements. So for each row, split K into chunks. 

Suppose the warp divides the K elements into 32 chunks, each thread computes a partial sum for their chunk, then they reduce within the warp. 

That way, each thread in the warp processes K/32 elements. 

So for K=1e6, each thread would have to process ~3e4 elements. 

Still, that's a lot, but better than 1e6. 

Alternatively, even more parallelism. 

Alternatively, tile the K dimension into blocks that fit into shared memory. 

Wait, here's an approach inspired by matrix multiplication optimization:

Suppose we use a block of threads (like a block of 256 threads) and have each block process a block of rows. Let's say each block is responsible for a block of rows (e.g., 16 rows). Then, each thread in the block can process a chunk of the K elements. 

Wait, let's structure it this way:

- Each block handles a tile of rows (say, 16 rows). 
- The block's threads (e.g., 256 threads) process chunks of K elements. 

Wait, perhaps a better approach is to use a tiled kernel where each thread block is responsible for a block of rows and processes a chunk of the K elements. 

Alternatively, use a block of threads to compute a chunk of the K elements for a set of rows. 

Alternatively, think of the problem as each element of B[k] contributes to all rows. Wait, no, each element B[k] multiplies the k-th element of each row. 

Hmm. Let's consider the following: 

The result is sum_{k} (A[i][k] * B[k]). 

Suppose that for a given k, all rows can be processed in parallel. 

But since B[k] is a scalar, for each k, A's column k is a vector of M elements. So multiplying B[k] by that column and adding to the result for each row. 

Wait, that's the column-wise approach. So the result is the sum over all columns of (B[k] * column A's k-th column). 

This is equivalent to B being a vector, so each element of B scales the corresponding column of A, and all columns are summed together. 

This approach might be better because the columns are not contiguous in memory, but if we process in a way that accesses the columns efficiently. 

Wait, but in row-major storage, columns are not contiguous. So accessing column-wise would lead to non-coalesced memory access. 

Hmm, that might be worse. 

Alternatively, the row-wise approach is better because rows are contiguous. 

Let me think again. 

If we process row-wise:

Each row is a contiguous block. For each row, the computation is a dot product with B. 

But with K=1e6, each row's dot product requires 1e6 multiplications and additions. 

The problem here is that for each row, the loop over K elements is sequential, but with K so large, perhaps this can be vectorized or optimized for memory access. 

Let me think of the CUDA kernel structure. 

Suppose each thread handles one row. 

Each thread would have a loop over K elements. 

But with K=1e6, that's a loop of 1e6 iterations per thread. 

That might be too much, but maybe with unrolling or using vectorized instructions (e.g., using warp-level parallelism). 

Alternatively, using shared memory to cache parts of B or A. 

Wait, the B vector is of size K. Since K is 1e6, it's 4MB (floats). 

If we can load B into shared memory, but that's 4MB, which is way more than the shared memory available per block (typically 48KB or 96KB). So that's not feasible. 

Hmm, that's a problem. 

Alternative idea: Since B is a vector, each element of B is multiplied by a column of A. 

Wait, but to compute the sum over all columns, perhaps we can process the computation in a way where each thread handles a block of columns, contributing to all rows. 

Wait, but that might be better. 

Let me think of the following approach: 

The result is a vector of M elements. Each element (row) is the sum of A[i][k] * B[k] for all k. 

Suppose we divide the K elements into chunks. For each chunk, each row computes the partial sum for that chunk. 

But how to parallelize this. 

Alternatively, let's consider the following kernel structure:

Each block processes a set of rows. Let's say each block handles 16 rows. 

Each thread in the block processes a set of columns (k). 

Wait, here's a possible structure:

Let each thread handle a small chunk of K elements. 

Suppose the block has 256 threads. The total K elements are divided into 256 chunks, each thread handles K / 256 elements. 

Each thread computes the partial sum over their chunk for all rows in the block's rows. 

Wait, but with 16 rows, each thread would process (K/256) elements * 16 rows. 

Wait, perhaps that's manageable. 

Wait, let's think in terms of the algorithm. 

Suppose each block is responsible for a block of rows (like 16 rows). 

The block has 256 threads. 

Each thread is responsible for a range of columns (k_start to k_end). 

Each thread loops over their columns and for each column k in their range, they add A[i][k] * B[k] to the partial sum for each row i in the block's rows. 

Wait, but this requires each thread to loop over their columns and iterate over the rows. 

Alternatively, each thread handles a column and contributes to all rows. 

Wait, that might be better. 

Alternatively, let me structure it as follows:

- Each block processes a tile of rows (e.g., 16 rows).
- The block has 256 threads, each thread is responsible for a set of columns. 
- Each thread processes a chunk of columns. For each column in their chunk, they multiply B[k] with A[i][k] for all rows i in the block's rows, and accumulate the result for each row. 

Wait, this way, for each column k in their chunk, the thread does:

for i in 0..rows_per_block-1:
    result[i] += A[i][k] * B[k]

But this would require the rows to be stored in a contiguous block. 

Wait, but how to index into A. 

Alternatively, the block's rows are stored in a contiguous block. So the matrix A has rows in row-major order. 

Suppose the block is handling rows from row_start to row_start + rows_per_block -1. 

So for a given column k, the elements A[row][k] for row in the block's rows are spaced by M (since each row is M elements long). 

Wait, the address for A[i][k] would be (row_start + i)*K + k. 

So for a given k, the rows are spaced by K elements. 

Hmm, this could lead to non-coalesced memory access unless K is a multiple of the threads per block. 

Alternatively, perhaps this approach is not optimal. 

Let me think of another way. 

Suppose the kernel is designed so that each thread is responsible for a single element of the result. 

Each thread for row i:

- Initialize sum = 0.0
- For k from 0 to K-1:
    sum += A[i][k] * B[k]
- Write sum to result[i]

But with K=1e6, this is 1e6 operations per thread. 

The problem is that even with 1024 threads, each thread has to process 1e6 elements. 

Wait, but with 2048 rows, maybe we can have each thread handle two rows. Let's see: 

Total rows: 2048. 

If each thread handles two rows, then with 1024 threads, that's 2048 rows. 

But each thread would process 2 * K elements. 

Alternatively, using a grid of blocks. Let's say the grid has 64 blocks, each with 32 threads. 

Wait, perhaps a better approach is to use a tiled approach where each thread processes a tile of the K elements. 

Alternatively, using a shared memory to store a tile of A and B. 

Wait, here's an idea inspired by tiled matrix multiplication:

Divide the K dimension into tiles. 

Each block processes a tile of K elements (e.g., 512 elements per tile). 

The block's threads load a tile of A and B into shared memory, compute the partial sums for each row, and accumulate the result. 

But how to organize this. 

Suppose we have a tile size of T (e.g., 512). 

The total number of tiles is ceil(K / T). 

Each tile is processed by a block. 

Wait, but each tile corresponds to a block of K elements, and the computation for each row's contribution from that tile is done in parallel. 

Wait, here's the plan:

- The kernel is launched with a grid of blocks, each handling a tile of K (e.g., each block processes T=512 elements of K).
- Each block has a number of threads equal to the number of rows (2048), but that might be too much. Alternatively, split the rows into chunks. 

Alternatively, each block handles a tile of K elements and a set of rows. 

Hmm, this is getting complicated. 

Another approach: 

Let's use a kernel where each thread processes a single column k, and contributes to all rows. 

Wait, that would be 1e6 threads, which might be too many. 

Alternatively, each block of threads processes a range of columns. 

Suppose each block processes a range of columns (e.g., 1024 columns), and each thread in the block handles a column. 

Each thread would loop over all rows (2048) and add A[i][k] * B[k] to the result[i]. 

But this requires 2048 iterations per thread, which is manageable. 

Wait, that's better. 

Let me detail this:

Block size is 1024 threads. 

Each block processes 1024 columns (k values). 

The grid has ceil(K / 1024) blocks. 

Each thread in the block is assigned a column k = block_k_start + thread_idx. 

For each column k assigned to the thread, it loops over all rows i from 0 to M-1 (2048) and adds A[i][k] * B[k] to result[i]. 

Wait, but the problem here is that for each thread, the loop over M rows (2048) is small compared to the K loop. 

This approach might be better because each thread processes a column, and accesses the column elements in the matrix. But the matrix is stored in row-major, so the elements A[i][k] for a given k are spaced by K elements (since each row is length K). 

Wait, the address for A[i][k] is i*K + k. So for a fixed k, the rows are spaced by K bytes. 

This means that for a thread processing a column k, accessing A[i][k] for varying i would have a stride of K elements, which is not coalesced. 

This would be a problem because memory accesses would be all over the place, leading to poor performance. 

Hmm, that's a major issue. 

So the column-wise approach might have bad memory access patterns. 

Back to the row-wise approach. 

Each thread is assigned a row, and loops over all K elements. 

The memory access for row i is contiguous (since the row is stored contiguously), so when the thread accesses A[i][k] for k from 0 to K, this is a contiguous block. 

That would be good for memory access. 

The problem is that each thread has to process 1e6 elements, which could be slow. 

But maybe with CUDA's fast memory access and the loop being a simple addition and multiplication, it's manageable. 

Let me think about the kernel code. 

The kernel would look something like this:

__global__ void matvec(
    const float* A,
    const float* B,
    float* result,
    int M,
    int K) {

    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row >= M) return;

    float sum = 0.0f;
    for (int k = 0; k < K; ++k) {
        sum += A[row * K + k] * B[k];
    }
    result[row] = sum;
}

Wait, but K is 1e6, so each thread is looping 1e6 times. That's a loop with 1e6 iterations. 

In CUDA, loops with a large number of iterations can be slow because of the overhead of the loop control, but maybe the GPU's high throughput can handle it. 

The problem is that each thread has a loop with K iterations, which could lead to a lot of instructions. 

Wait, but with K=1e6, each thread would have to perform 1e6 multiplications and additions, which is 2e6 FLOPs. 

But the GPU has a lot of cores. Let's see:

Suppose the kernel is launched with 2048 threads (one per row). 

Each thread has 1e6 iterations. 

Total FLOPs: 2048 * 1e6 * 2 (since each iteration is a multiply and add) = 4.096e9 FLOPs. 

Assuming a GPU with 10 TFLOPS (single-precision), this would take ~0.4 seconds. 

But the original code using PyTorch's matmul might be faster? Or maybe not. 

Wait, but maybe the original code uses a highly optimized BLAS implementation, which is better than this naive kernel. 

Hmm. So perhaps this naive approach isn't better, but we need to optimize further. 

Let me think of ways to optimize this. 

First, unrolling the loop could help reduce loop overhead. 

Alternatively, using vectorized memory operations. 

Another idea: since B is a vector, we can load it into shared memory. But B is 1e6 elements, which is 4MB, but shared memory is limited. So not feasible. 

Hmm. 

Alternatively, since A and B are both read-only, we can use texture memory or other optimizations, but that might complicate things. 

Another idea: transpose A so that the columns are contiguous. 

Wait, if we transpose A to be K rows and M columns, then the matrix multiplication becomes B^T * A^T, resulting in a 1xM matrix (which is the transpose of the desired result). 

But transposing A would require K*M storage, which is 1e6 * 2048 = 2e9 elements (8GB for single-precision). That's a lot, and the user probably doesn't want to store the transposed matrix. 

Alternatively, maybe compute on the fly. 

Alternatively, process the computation in chunks where the columns are accessed in a coalesced way. 

Wait, let's try to think of the kernel again. 

If each thread handles a row, and loops over K elements, but K is large, perhaps using a block of threads to handle a row's K elements. 

Wait, here's another approach: 

Use a block of threads for each row. 

Suppose each row is handled by a block of 256 threads. 

The block's threads can divide the K elements into chunks. 

Each thread in the block processes a chunk of K / 256 elements. 

Then, they sum their partial sums using shared memory. 

This way, the per-row loop is split into chunks, and the summation is done with warp-level reductions. 

Let me structure this:

Each block processes a single row. 

The block has 256 threads. 

Each thread processes a chunk of K / 256 elements. 

Each thread computes a partial sum for their chunk. 

Then, the threads in the block perform a reduction to sum all partial sums into the final row's result. 

This way, each thread's loop over their chunk's elements is manageable. 

The code would look something like this:

__global__ void matvec_kernel(
    const float* A,
    const float* B,
    float* result,
    int K) {

    int row = blockIdx.x;

    __shared__ float partial_sums[256]; // Assuming block size 256

    int tid = threadIdx.x;

    float sum = 0.0f;

    // Each thread processes K / 256 elements
    int start = tid * (K / 256);
    int end = (tid + 1) * (K / 256);
    if (tid == 255) end = K; // Handle remainder

    for (int k = start; k < end; ++k) {
        sum += A[row * K + k] * B[k];
    }

    partial_sums[tid] = sum;
    __syncthreads();

    // Reduction within the block
    for (int s = 128; s > 0; s >>= 1) {
        if (tid < s) {
            partial_sums[tid] += partial_sums[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        result[row] = partial_sums[0];
    }
}

This way, each thread processes about 4096 elements (since 1e6 / 256 â‰ˆ 3906), which is better. 

The reduction step uses a binary reduction in shared memory. 

The block size would need to be 256 (since we have 256 partial sums). 

The grid size would be M (2048) blocks, each handling a row. 

This might be better, as the per-thread loop is shorter, and the reduction is efficient. 

But there's some overhead in the reduction. 

Also, the K might not be divisible by 256. Need to handle the remainder. 

Additionally, the shared memory usage here is 256 floats, which is acceptable. 

Another optimization could be using a larger block size (e.g., 512 threads), but then the K must be divided into more chunks. 

Alternatively, using a block size of 256, and each thread processes 1e6 / 256 ~ 3906 elements. 

But even that might be manageable. 

Another thing: the loop over K for each thread can be vectorized using CUDA's built-in vector types, like float4 or __ldg if L2 cache is used. 

Alternatively, using __ldg to read from global memory more efficiently. 

Wait, but in the current code, A is read as A[row*K +k], which is a contiguous block for a given row, so coalesced memory access for the row's elements. 

The elements A[row*K +k] are contiguous in memory for a fixed row. 

Thus, when a thread processes a chunk of k's, the memory accesses are contiguous, leading to good memory coalescing. 

This is a good property. 

So the above kernel might be a good approach. 

Let's see the steps again:

- For each row (each block handles a row), the block's threads split the K elements into chunks. 
- Each thread processes a chunk, accumulating their sum. 
- They write their partial sums to shared memory. 
- Then, a reduction in shared memory combines all partial sums into the final result. 

This reduces the number of iterations per thread from 1e6 to ~4000, which is better. 

The reduction step requires log2(blockSize) steps. 

This should be more efficient. 

Now, the next step is to write the CUDA code implementing this approach. 

Additionally, the block size must be a power of two. Let's choose 256. 

Wait, but what if K is not divisible by 256? For example, if K=1,048,576 (which is exactly divisible by 256?), since 1,048,576 = 1024*1024, and 1024 is divisible by 256. 

Wait, 256 * 4096 = 1,048,576. So yes, for K=1,048,576, each thread's chunk is exactly K / 256 = 4096. So no remainder. 

Great, so no need to handle remainder. 

Thus, the code can proceed as follows. 

The kernel will be launched with gridDim = M (2048) and blockDim = 256. 

But 2048 blocks with 256 threads each would require 2048 * 256 = ~524,288 threads. Which should be okay as the maximum number of threads per SM is higher (e.g., 2048). 

The total number of blocks might exceed the maximum (for example, if the GPU has a maximum grid size of 65535 per dimension, but 2048 is way under that). 

So this should be feasible. 

Another optimization: Using float4 loads for B. Since B is a vector, but in this kernel, each thread accesses B[k] for their chunk. 

Wait, in the above code, each thread is accessing B[k] for their chunk. 

But B is a vector of length K, stored contiguously. 

Each thread in the block is processing a range of k's. 

Thus, the B[k] accesses are contiguous for each thread's chunk. 

So using vectorized loads could help. 

Alternatively, since B is read-only, we can load it into shared memory, but that's 1e6 elements, which is 4MB. 

But shared memory is limited. 

Hmm, not feasible. 

Alternatively, use texture memory. 

Alternatively, use __ldg to read from L2 cache. 

Alternatively, leave it as is. 

The current approach might be okay. 

Now, coding this in CUDA. 

The kernel would be written as follows:

#include <cuda_runtime.h>

__global__ void matvec_kernel(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ result,
    int K) {

    const int row = blockIdx.x;
    const int tid = threadIdx.x;
    __shared__ float partial_sums[256];

    float sum = 0.0f;
    const int chunk_size = K / 256;

    // Each thread computes a chunk of K / 256 elements
    int start = tid * chunk_size;
    int end = (tid + 1) * chunk_size;

    for (int k = start; k < end; ++k) {
        sum += A[row * K + k] * B[k];
    }

    partial_sums[tid] = sum;
    __syncthreads();

    // Reduction step
    for (int s = 128; s > 0; s >>= 1) {
        if (tid < s) {
            partial_sums[tid] += partial_sums[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        result[row] = partial_sums[0];
    }
}

But wait, the chunk_size is K divided by 256. Since K is exactly divisible by 256, this is okay. 

Now, the driver function would need to launch this kernel. 

In the Python code, the dimensions are M=2048 and K=1048576. 

The kernel requires M blocks (each block for a row) and 256 threads per block. 

So in Python, the code would look like:

def matvec_cuda(A, B):
    M = A.shape[0]
    K = A.shape[1]
    result = torch.empty(M, 1, device='cuda', dtype=torch.float32)
    threads_per_block = 256
    blocks_per_grid = M
    matvec_kernel[blocks_per_grid, threads_per_block](A.data_ptr(), B.data_ptr(), result.data_ptr(), K)
    return result

Wait, but in the kernel code, the result is written as a 1D array, but the output should be Mx1. 

Wait, the kernel code currently writes to a 1D array. 

The Python code should handle the result as a 1D array, but the user expects a Mx1 tensor. 

Alternatively, adjust the kernel to write to a Mx1 array. 

In the kernel, the result is a 1D array, so the Python code can create a Mx1 tensor by reshaping. 

Alternatively, the kernel can index as result[row], and the output is a 1D tensor which can be reshaped. 

Thus, in Python, the result would be:

result = torch.cuda.FloatTensor(M).zero_()
matvec_kernel(..., result.data_ptr(), ...)
result = result.view(M, 1)

This should work. 

Now, putting this into the code structure required. 

The user's original code has:

class Model(nn.Module):
    def forward(self, A, B):
        return torch.matmul(A, B)

The new code should have a ModelNew class that uses the custom CUDA kernel. 

The code will involve defining the CUDA kernel inline in Python using torch.utils.cpp_extension.load_inline. 

Following the example given, the CUDA code needs to be in strings. 

The CUDA kernel code must be in a string, and the function to call it must be in another string. 

So putting it all together:

First, write the CUDA source code as a string. 

matvec_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matvec_kernel(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ result,
    int K) {

    const int row = blockIdx.x;
    const int tid = threadIdx.x;
    __shared__ float partial_sums[256];

    float sum = 0.0f;
    const int chunk_size = K / 256;

    // Each thread computes a chunk of K / 256 elements
    int start = tid * chunk_size;
    int end = (tid + 1) * chunk_size;

    for (int k = start; k < end; ++k) {
        sum += A[row * K + k] * B[k];
    }

    partial_sums[tid] = sum;
    __syncthreads();

    // Reduction step
    for (int s = 128; s > 0; s >>= 1) {
        if (tid < s) {
            partial_sums[tid] += partial_sums[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        result[row] = partial_sums[0];
    }
}

torch::Tensor matvec_cuda(torch::Tensor A, torch::Tensor B) {
    const int M = A.size(0);
    const int K = A.size(1);
    auto result = torch::empty({M}, A.options());

    const int threads_per_block = 256;
    const dim3 blocks(M);
    const dim3 threads(threads_per_block);

    matvec_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), result.data_ptr<float>(), K);

    return result.view({M, 1});
}
"""

Then, the header:

matvec_h_source = """
torch::Tensor matvec_cuda(torch::Tensor A, torch::Tensor B);
"""

Then, compiling this using load_inline. 

elementwise_add = load_inline( ... )

Wait, the user example used elementwise_add as the variable name. Here, the function is named matvec_cuda. 

Thus, the code would be:

from torch.utils.cpp_extension import load_inline

matvec_cuda_mod = load_inline(
    name="matvec_cuda",
    cpp_sources=matvec_h_source,
    cuda_sources=matvec_source,
    functions=["matvec_cuda"],
    verbose=True,
)

Then, in the ModelNew class, we can call this function. 

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.matvec_cuda = matvec_cuda_mod

    def forward(self, A, B):
        return self.matvec_cuda.matvec_cuda(A, B)

Wait, need to make sure the function is correctly imported. 

Alternatively, in the forward function, we can directly call the loaded function. 

Wait, the load_inline returns a module which has the functions. 

Thus, in the ModelNew's forward, we can do:

return matvec_cuda_mod.matvec_cuda(A,B)

Thus, the code should work. 

Now, let me check the parameters:

The user's original code has A.shape (M, K), B.shape (K,1). 

The kernel expects A to be MxK and B to be K elements (since B is Kx1, we can treat it as a 1D array of length K). 

Thus, in the Python code, the B tensor is passed as a Kx1, but in the kernel, it's accessed as B[k], which is okay because B[k] is B[k][0]. 

Wait, but in the kernel code, B is passed as a pointer to a 1D array. 

In the forward function of the ModelNew, the B tensor is Kx1. So in the kernel code, when we read B[k], it's the same as B[k][0], because the storage is contiguous. 

Wait, in PyTorch, a tensor of shape (K,1) is stored as a contiguous array of K elements (since it's a column vector). 

Wait, no. The storage of a (K,1) tensor is K rows of 1 element each, so the total size is K elements. 

Thus, the data pointer can be treated as a 1D array of length K. 

So accessing B[k] is correct. 

Therefore, the code should work. 

Testing for correctness is not required, but the user mentioned that the output must match the original. 

Another thing to note: The kernel's result is a Mx1 tensor. 

The kernel returns a tensor of shape (M,), but then .view({M,1}) is applied, which is correct. 

So the final code should be as follows. 

Now, checking for possible errors:

In the CUDA kernel, the chunk_size is K/256. Since K is 1048576, which is divisible by 256, this is okay. 

If K wasn't divisible, we would have to handle the remainder, but in this problem's case, it's okay. 

The block size is 256, and the grid size is M=2048. 

The shared memory allocation is 256 floats, which is acceptable. 

The reduction loop in the kernel: the loop steps from 128 down to 1, each time halving s. This correctly performs a reduction. 

The __syncthreads() after each step is necessary to ensure synchronization. 

Another optimization could be using warp-level reductions to make it faster, but the current approach should work. 

Thus, the code should be correct and faster than the original. 

Now, writing this all into the required format. 

The final code would look like this in Python with the CUDA kernel inline.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matvec_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matvec_kernel(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ result,
    int K) {

    const int row = blockIdx.x;
    const int tid = threadIdx.x;
    __shared__ float partial_sums[256];

    float sum = 0.0f;
    const int chunk_size = K / 256;

    int start = tid * chunk_size;
    int end = (tid + 1) * chunk_size;

    for (int k = start; k < end; ++k) {
        sum += A[row * K + k] * B[k];
    }

    partial_sums[tid] = sum;
    __syncthreads();

    for (int s = 128; s > 0; s >>= 1) {
        if (tid < s) {
            partial_sums[tid] += partial_sums[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        result[row] = partial_sums[0];
    }
}

torch::Tensor matvec_cuda(torch::Tensor A, torch::Tensor B) {
    const int M = A.size(0);
    const int K = A.size(1);
    auto result = torch::empty({M}, A.options());

    const int threads_per_block = 256;
    const dim3 blocks(M);
    const dim3 threads(threads_per_block);

    matvec_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), result.data_ptr<float>(), K);

    return result.view({M, 1});
}
"""

matvec_h_source = """
torch::Tensor matvec_cuda(torch::Tensor A, torch::Tensor B);
"""

matvec_cuda_mod = load_inline(
    name="matvec_cuda",
    cpp_sources=matvec_h_source,
    cuda_sources=matvec_source,
    functions=["matvec_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return matvec_cuda_mod.matvec_cuda(A, B)
```