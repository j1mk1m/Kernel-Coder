2025-11-21 15:49:26,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:26,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:26,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:26,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:26,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:26,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:26,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:26,101 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:49:52,776 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:49:52,777 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 26.68s
2025-11-21 15:49:52,777 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 26.68s
2025-11-21 15:49:52,777 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:50:06,380 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:50:06,380 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 40.28s
2025-11-21 15:50:06,380 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 40.28s
2025-11-21 15:50:06,382 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:50:32,318 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:50:32,318 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 66.22s
2025-11-21 15:50:32,318 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 66.22s
2025-11-21 15:50:32,319 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:52:48,590 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:52:48,591 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 162.21s
2025-11-21 15:52:48,592 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 162.21s
2025-11-21 15:52:48,592 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:53:55,576 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:53:55,576 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 66.98s
2025-11-21 15:53:55,576 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 66.98s
2025-11-21 15:53:55,578 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:54:13,028 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:54:13,028 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 286.93s
2025-11-21 15:54:13,029 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 286.93s
2025-11-21 15:54:13,030 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:55:19,112 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:55:19,112 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 353.02s
2025-11-21 15:55:19,113 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 353.02s
2025-11-21 15:55:19,115 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:55:40,702 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:55:40,702 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 374.61s
2025-11-21 15:55:40,702 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 374.61s
2025-11-21 15:55:40,704 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:57:24,258 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:57:24,258 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 208.68s
2025-11-21 15:57:24,259 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 208.68s
2025-11-21 15:57:24,261 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:57:25,374 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:57:25,374 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 479.28s
2025-11-21 15:57:25,374 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 479.28s
2025-11-21 15:57:25,376 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 15:58:43,582 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 15:58:43,582 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 557.49s
2025-11-21 15:58:43,582 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 557.49s
2025-11-21 15:58:43,584 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:01:38,179 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:01:38,179 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 253.92s
2025-11-21 16:01:38,179 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 253.92s
2025-11-21 16:01:38,180 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:03:25,767 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:03:25,767 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 486.65s
2025-11-21 16:03:25,767 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 486.65s
2025-11-21 16:03:25,768 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:03:58,202 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:03:58,202 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 497.50s
2025-11-21 16:03:58,202 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 497.50s
2025-11-21 16:03:58,204 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:08:20,045 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:08:20,045 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 576.46s
2025-11-21 16:08:20,046 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 576.46s
2025-11-21 16:08:20,047 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:08:23,553 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:08:23,553 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 405.37s
2025-11-21 16:08:23,553 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 405.37s
2025-11-21 16:08:23,555 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:08:34,443 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:08:34,443 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 669.07s
2025-11-21 16:08:34,443 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 669.07s
2025-11-21 16:08:34,445 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:08:58,960 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:08:58,961 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1146.18s
2025-11-21 16:08:58,961 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1146.18s
2025-11-21 16:08:58,963 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:09:22,655 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:09:22,655 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 59.10s
2025-11-21 16:09:22,655 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 59.10s
2025-11-21 16:09:22,657 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:09:37,969 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:09:37,970 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 77.92s
2025-11-21 16:09:37,970 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 77.92s
2025-11-21 16:09:37,972 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:10:21,041 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:10:21,041 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 58.38s
2025-11-21 16:10:21,041 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 58.38s
2025-11-21 16:10:21,043 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:10:28,533 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:10:28,533 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 975.50s
2025-11-21 16:10:28,533 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 975.50s
2025-11-21 16:10:28,534 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:11:55,334 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:11:55,335 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 477.13s
2025-11-21 16:11:55,335 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 477.13s
2025-11-21 16:11:55,335 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:14:25,867 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:14:25,868 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 326.90s
2025-11-21 16:14:25,868 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 326.90s
2025-11-21 16:14:25,868 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:15:12,818 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:15:12,818 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 284.28s
2025-11-21 16:15:12,818 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 284.28s
2025-11-21 16:15:12,820 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:16:04,546 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:16:04,547 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 51.73s
2025-11-21 16:16:04,547 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 51.73s
2025-11-21 16:16:04,548 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:18:05,092 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:18:05,092 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 570.65s
2025-11-21 16:18:05,092 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 570.65s
2025-11-21 16:18:05,094 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:19:01,121 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:19:01,121 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 56.03s
2025-11-21 16:19:01,121 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 56.03s
2025-11-21 16:19:01,123 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:20:33,928 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.61s
2025-11-21 16:20:33,928 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-21 16:20:33,928 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:21:09,877 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:21:09,877 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 35.95s
2025-11-21 16:21:09,877 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 35.95s
2025-11-21 16:21:09,879 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:21:35,852 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:21:35,852 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 674.81s
2025-11-21 16:21:35,852 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 674.81s
2025-11-21 16:21:35,854 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:22:06,462 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:22:06,462 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 361.91s
2025-11-21 16:22:06,462 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 361.91s
2025-11-21 16:22:06,464 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:22:18,376 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:22:18,377 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 472.51s
2025-11-21 16:22:18,377 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 472.51s
2025-11-21 16:22:18,379 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:23:48,698 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:23:48,699 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 132.84s
2025-11-21 16:23:48,699 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 132.84s
2025-11-21 16:23:48,700 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:27:28,484 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:27:28,485 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 507.36s
2025-11-21 16:27:28,485 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 507.36s
2025-11-21 16:27:28,487 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:28:11,565 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:28:11,565 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 353.19s
2025-11-21 16:28:11,565 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 353.19s
2025-11-21 16:28:11,567 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:28:53,978 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:28:53,978 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 42.41s
2025-11-21 16:28:53,979 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 42.41s
2025-11-21 16:28:53,979 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:29:43,320 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:29:43,320 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1067.98s
2025-11-21 16:29:43,320 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1067.98s
2025-11-21 16:29:43,322 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:30:35,030 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:30:35,030 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1629.26s
2025-11-21 16:30:35,030 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1629.26s
2025-11-21 16:30:35,032 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:35:15,476 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:35:15,477 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 381.50s
2025-11-21 16:35:15,477 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 381.50s
2025-11-21 16:35:15,478 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:35:33,181 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:35:33,181 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 704.48s
2025-11-21 16:35:33,181 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 704.48s
2025-11-21 16:35:33,183 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:36:44,617 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:36:44,617 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 71.43s
2025-11-21 16:36:44,618 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 71.43s
2025-11-21 16:36:44,619 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:38:01,863 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:38:01,863 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 77.24s
2025-11-21 16:38:01,863 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 77.24s
2025-11-21 16:38:01,865 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:39:39,596 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.62s
2025-11-21 16:39:39,597 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-21 16:39:39,597 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:41:24,721 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:41:24,722 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 369.24s
2025-11-21 16:41:24,722 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 369.24s
2025-11-21 16:41:24,724 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:41:30,667 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:41:30,667 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1164.20s
2025-11-21 16:41:30,667 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1164.20s
2025-11-21 16:41:30,669 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:47:19,245 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:47:19,246 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 354.52s
2025-11-21 16:47:19,247 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 354.52s
2025-11-21 16:47:19,249 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:47:32,185 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:47:32,186 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1068.86s
2025-11-21 16:47:32,186 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1068.86s
2025-11-21 16:47:32,187 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:47:57,189 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:47:57,189 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1042.16s
2025-11-21 16:47:57,190 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1042.16s
2025-11-21 16:47:57,191 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:48:00,473 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:48:00,474 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 41.22s
2025-11-21 16:48:00,474 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 41.22s
2025-11-21 16:48:00,474 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:48:05,814 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:48:05,814 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1615.93s
2025-11-21 16:48:05,814 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1615.93s
2025-11-21 16:48:05,815 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:49:03,616 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:49:03,616 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1295.13s
2025-11-21 16:49:03,616 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1295.13s
2025-11-21 16:49:03,617 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:49:33,870 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:49:33,871 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 88.05s
2025-11-21 16:49:33,871 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 88.05s
2025-11-21 16:49:33,872 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:50:56,873 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:50:56,873 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 775.01s
2025-11-21 16:50:56,873 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 775.01s
2025-11-21 16:50:56,875 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:55:11,338 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:55:11,338 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 254.46s
2025-11-21 16:55:11,338 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 254.46s
2025-11-21 16:55:11,340 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:55:21,182 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:55:21,182 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 440.71s
2025-11-21 16:55:21,183 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 440.71s
2025-11-21 16:55:21,184 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:55:39,434 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:55:39,434 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 487.25s
2025-11-21 16:55:39,434 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 487.25s
2025-11-21 16:55:39,436 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:55:53,441 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:55:53,444 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 42.10s
2025-11-21 16:55:53,444 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 42.10s
2025-11-21 16:55:53,466 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:56:04,890 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:56:04,891 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 391.02s
2025-11-21 16:56:04,891 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 391.02s
2025-11-21 16:56:04,893 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:57:56,576 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:57:56,576 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 599.38s
2025-11-21 16:57:56,576 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 599.38s
2025-11-21 16:57:56,577 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:59:02,720 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:59:02,721 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 599.10s
2025-11-21 16:59:02,721 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 599.10s
2025-11-21 16:59:02,722 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 16:59:27,163 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 16:59:27,164 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 245.98s
2025-11-21 16:59:27,164 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 245.98s
2025-11-21 16:59:27,165 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:00:25,526 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:00:25,527 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1134.86s
2025-11-21 17:00:25,527 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1134.86s
2025-11-21 17:00:25,529 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:00:28,150 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:00:28,150 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 263.26s
2025-11-21 17:00:28,150 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 263.26s
2025-11-21 17:00:28,152 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:01:21,059 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:01:21,059 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 55.53s
2025-11-21 17:01:21,060 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 55.53s
2025-11-21 17:01:21,061 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:03:15,630 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:03:15,631 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 456.19s
2025-11-21 17:03:15,631 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 456.19s
2025-11-21 17:03:15,632 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:03:16,912 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:03:16,912 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 168.76s
2025-11-21 17:03:16,913 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 168.76s
2025-11-21 17:03:16,914 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:03:43,257 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:03:43,257 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 26.34s
2025-11-21 17:03:43,257 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 26.34s
2025-11-21 17:03:43,259 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:07:10,931 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:07:10,932 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 207.67s
2025-11-21 17:07:10,932 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 207.67s
2025-11-21 17:07:10,933 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:07:14,404 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:07:14,404 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 491.68s
2025-11-21 17:07:14,404 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 491.68s
2025-11-21 17:07:14,405 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:07:44,980 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:07:44,980 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 711.51s
2025-11-21 17:07:44,980 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 711.51s
2025-11-21 17:07:44,982 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:08:41,079 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:08:41,079 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1741.48s
2025-11-21 17:08:41,079 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1741.48s
2025-11-21 17:08:41,081 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:09:32,606 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:09:32,607 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 51.53s
2025-11-21 17:09:32,607 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 51.53s
2025-11-21 17:09:32,608 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:09:33,030 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:09:33,031 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 142.10s
2025-11-21 17:09:33,031 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 142.10s
2025-11-21 17:09:33,032 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:09:59,739 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:09:59,739 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 26.71s
2025-11-21 17:09:59,739 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 26.71s
2025-11-21 17:09:59,741 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:10:28,150 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:10:28,151 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 28.41s
2025-11-21 17:10:28,151 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 28.41s
2025-11-21 17:10:28,152 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:11:10,417 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:11:10,418 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 474.78s
2025-11-21 17:11:10,418 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 474.78s
2025-11-21 17:11:10,420 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:11:23,036 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:11:23,037 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 218.05s
2025-11-21 17:11:23,037 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 218.05s
2025-11-21 17:11:23,038 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:12:09,875 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:12:09,875 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 59.45s
2025-11-21 17:12:09,875 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 59.45s
2025-11-21 17:12:09,877 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:13:40,942 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:13:40,943 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 192.79s
2025-11-21 17:13:40,943 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 192.79s
2025-11-21 17:13:40,944 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:14:26,193 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:14:26,194 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 45.25s
2025-11-21 17:14:26,194 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 45.25s
2025-11-21 17:14:26,195 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:14:31,796 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:14:31,796 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 437.39s
2025-11-21 17:14:31,796 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 437.39s
2025-11-21 17:14:31,798 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:15:09,945 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:15:09,946 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 226.91s
2025-11-21 17:15:09,946 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 226.91s
2025-11-21 17:15:09,947 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:15:10,847 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:15:10,847 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 943.68s
2025-11-21 17:15:10,847 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 943.68s
2025-11-21 17:15:10,849 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:15:58,172 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:15:58,173 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 48.22s
2025-11-21 17:15:58,173 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 48.22s
2025-11-21 17:15:58,174 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:18:27,908 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:18:27,908 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 378.03s
2025-11-21 17:18:27,908 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 378.03s
2025-11-21 17:18:27,910 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:21:44,473 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:21:44,473 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 346.30s
2025-11-21 17:21:44,473 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 346.30s
2025-11-21 17:21:44,475 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:22:14,074 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:22:14,074 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 29.60s
2025-11-21 17:22:14,075 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 29.60s
2025-11-21 17:22:14,076 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:22:43,708 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:22:43,708 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 29.63s
2025-11-21 17:22:43,708 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 29.63s
2025-11-21 17:22:43,710 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:24:13,282 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:24:13,282 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 345.37s
2025-11-21 17:24:13,282 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 345.37s
2025-11-21 17:24:13,283 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:24:38,882 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:24:38,883 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1397.82s
2025-11-21 17:24:38,883 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1397.82s
2025-11-21 17:24:38,884 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:25:09,271 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:25:09,271 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 145.56s
2025-11-21 17:25:09,271 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 145.56s
2025-11-21 17:25:09,273 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:25:34,168 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:25:34,169 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 24.89s
2025-11-21 17:25:34,169 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 24.89s
2025-11-21 17:25:34,171 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:25:56,595 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:25:56,595 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 684.80s
2025-11-21 17:25:56,595 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 684.80s
2025-11-21 17:25:56,597 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:27:58,149 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.57s
2025-11-21 17:27:58,149 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-21 17:27:58,150 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:28:24,801 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:28:24,801 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 148.20s
2025-11-21 17:28:24,801 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 148.20s
2025-11-21 17:28:24,803 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-21 17:28:39,977 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-21 17:28:39,977 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 185.81s
2025-11-21 17:28:39,977 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 185.81s
2025-11-21 17:28:39,979 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,862 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,878 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,883 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,890 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,892 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,895 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,898 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:04,911 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:39,620 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:51:39,621 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 34.72s
2025-11-22 15:51:39,621 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 34.72s
2025-11-22 15:51:39,622 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:51:49,827 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:51:49,827 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 44.93s
2025-11-22 15:51:49,827 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 44.93s
2025-11-22 15:51:49,828 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:53:59,525 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:53:59,525 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 174.61s
2025-11-22 15:53:59,525 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 174.61s
2025-11-22 15:53:59,526 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:56:15,641 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:56:15,641 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 310.78s
2025-11-22 15:56:15,641 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 310.78s
2025-11-22 15:56:15,651 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:56:23,265 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:56:23,265 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 143.74s
2025-11-22 15:56:23,265 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 143.74s
2025-11-22 15:56:23,266 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:57:06,929 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:57:06,929 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 43.66s
2025-11-22 15:57:06,929 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 43.66s
2025-11-22 15:57:06,930 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:57:43,806 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:57:43,806 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 36.87s
2025-11-22 15:57:43,806 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 36.87s
2025-11-22 15:57:43,807 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:58:13,822 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:58:13,822 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 30.01s
2025-11-22 15:58:13,822 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 30.01s
2025-11-22 15:58:13,823 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:58:34,390 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:58:34,390 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 404.56s
2025-11-22 15:58:34,390 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 404.56s
2025-11-22 15:58:34,391 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:58:59,827 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:58:59,827 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 46.00s
2025-11-22 15:58:59,827 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 46.00s
2025-11-22 15:58:59,828 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 15:59:34,197 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 15:59:34,197 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 34.37s
2025-11-22 15:59:34,197 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 34.37s
2025-11-22 15:59:34,198 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:00:22,916 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:00:22,916 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 48.72s
2025-11-22 16:00:22,916 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 48.72s
2025-11-22 16:00:22,917 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:00:47,930 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:00:47,930 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 583.05s
2025-11-22 16:00:47,930 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 583.05s
2025-11-22 16:00:47,931 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:03:32,103 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:03:32,103 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 436.45s
2025-11-22 16:03:32,104 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 436.45s
2025-11-22 16:03:32,105 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:04:38,093 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:04:38,093 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 255.18s
2025-11-22 16:04:38,093 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 255.18s
2025-11-22 16:04:38,095 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:06:46,612 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:06:46,612 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 941.72s
2025-11-22 16:06:46,612 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 941.72s
2025-11-22 16:06:46,613 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:07:01,549 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:07:01,550 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 956.67s
2025-11-22 16:07:01,550 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 956.67s
2025-11-22 16:07:01,552 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:08:03,782 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:08:03,782 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 569.39s
2025-11-22 16:08:03,782 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 569.39s
2025-11-22 16:08:03,784 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:08:22,413 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:08:22,413 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 80.86s
2025-11-22 16:08:22,413 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 80.86s
2025-11-22 16:08:22,414 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:08:47,904 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:08:47,904 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 249.81s
2025-11-22 16:08:47,904 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 249.81s
2025-11-22 16:08:47,905 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:09:22,236 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:09:22,236 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 59.82s
2025-11-22 16:09:22,236 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 59.82s
2025-11-22 16:09:22,237 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:09:22,375 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:09:22,375 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1097.48s
2025-11-22 16:09:22,375 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1097.48s
2025-11-22 16:09:22,376 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:09:52,571 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:09:52,571 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 30.19s
2025-11-22 16:09:52,571 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 30.19s
2025-11-22 16:09:52,572 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:10:30,096 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:10:30,096 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 582.16s
2025-11-22 16:10:30,096 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 582.16s
2025-11-22 16:10:30,098 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:11:38,016 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:11:38,017 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1198.39s
2025-11-22 16:11:38,017 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1198.39s
2025-11-22 16:11:38,018 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:11:54,833 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:11:54,834 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 308.22s
2025-11-22 16:11:54,834 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 308.22s
2025-11-22 16:11:54,835 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:12:40,204 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:12:40,204 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 45.37s
2025-11-22 16:12:40,205 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 45.37s
2025-11-22 16:12:40,205 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:13:07,706 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:13:07,706 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 259.80s
2025-11-22 16:13:07,706 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 259.80s
2025-11-22 16:13:07,707 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:16:30,363 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:16:30,363 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 230.16s
2025-11-22 16:16:30,363 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 230.16s
2025-11-22 16:16:30,364 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:16:45,872 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:16:45,872 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 218.16s
2025-11-22 16:16:45,872 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 218.16s
2025-11-22 16:16:45,873 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:19:12,152 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:19:12,152 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 668.37s
2025-11-22 16:19:12,152 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 668.37s
2025-11-22 16:19:12,153 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:19:14,230 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:19:14,230 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 561.66s
2025-11-22 16:19:14,230 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 561.66s
2025-11-22 16:19:14,231 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:19:41,755 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:19:41,755 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 29.60s
2025-11-22 16:19:41,755 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 29.60s
2025-11-22 16:19:41,756 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:19:52,354 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:19:52,354 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 186.48s
2025-11-22 16:19:52,354 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 186.48s
2025-11-22 16:19:52,355 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:20:29,622 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:20:29,622 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1017.52s
2025-11-22 16:20:29,622 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1017.52s
2025-11-22 16:20:29,623 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:21:41,903 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:21:41,903 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 311.54s
2025-11-22 16:21:41,903 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 311.54s
2025-11-22 16:21:41,904 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:25:23,681 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:25:23,681 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 341.92s
2025-11-22 16:25:23,681 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 341.92s
2025-11-22 16:25:23,683 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:25:41,165 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:25:41,166 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 239.26s
2025-11-22 16:25:41,166 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 239.26s
2025-11-22 16:25:41,166 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:26:28,975 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:26:28,975 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1026.74s
2025-11-22 16:26:28,975 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1026.74s
2025-11-22 16:26:28,976 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:27:12,022 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:27:12,022 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1001.92s
2025-11-22 16:27:12,022 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1001.92s
2025-11-22 16:27:12,023 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:27:25,141 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:27:25,141 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 56.16s
2025-11-22 16:27:25,142 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 56.16s
2025-11-22 16:27:25,142 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:27:39,117 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:27:39,118 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 429.49s
2025-11-22 16:27:39,118 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 429.49s
2025-11-22 16:27:39,119 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:28:05,538 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:28:05,538 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 161.85s
2025-11-22 16:28:05,538 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 161.85s
2025-11-22 16:28:05,539 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:28:29,077 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:28:29,077 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 167.91s
2025-11-22 16:28:29,077 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 167.91s
2025-11-22 16:28:29,078 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:28:34,122 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:28:34,122 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 559.89s
2025-11-22 16:28:34,122 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 559.89s
2025-11-22 16:28:34,123 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:29:31,646 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:29:31,646 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 62.57s
2025-11-22 16:29:31,646 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 62.57s
2025-11-22 16:29:31,647 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:29:36,607 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:29:36,607 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1078.59s
2025-11-22 16:29:36,607 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1078.59s
2025-11-22 16:29:36,609 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:30:24,980 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:30:24,980 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 53.33s
2025-11-22 16:30:24,980 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 53.33s
2025-11-22 16:30:24,982 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:30:46,525 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:30:46,526 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 654.17s
2025-11-22 16:30:46,526 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 654.17s
2025-11-22 16:30:46,527 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:33:03,225 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:33:03,225 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 136.70s
2025-11-22 16:33:03,225 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 136.70s
2025-11-22 16:33:03,226 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:33:30,051 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:33:30,051 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 295.93s
2025-11-22 16:33:30,052 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 295.93s
2025-11-22 16:33:30,052 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:33:55,378 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:33:55,378 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 52.15s
2025-11-22 16:33:55,378 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 52.15s
2025-11-22 16:33:55,379 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:34:54,085 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:34:54,086 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 434.97s
2025-11-22 16:34:54,086 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 434.97s
2025-11-22 16:34:54,087 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:35:03,193 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:35:03,193 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 417.65s
2025-11-22 16:35:03,193 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 417.65s
2025-11-22 16:35:03,194 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:35:29,566 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:35:29,566 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 484.42s
2025-11-22 16:35:29,566 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 484.42s
2025-11-22 16:35:29,567 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:35:44,386 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:35:44,386 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 109.01s
2025-11-22 16:35:44,386 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 109.01s
2025-11-22 16:35:44,387 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:35:52,649 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:35:52,650 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 520.63s
2025-11-22 16:35:52,650 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 520.63s
2025-11-22 16:35:52,651 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:36:12,810 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:36:12,810 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 28.42s
2025-11-22 16:36:12,810 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 28.42s
2025-11-22 16:36:12,811 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:37:04,253 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:37:04,253 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 121.06s
2025-11-22 16:37:04,253 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 121.06s
2025-11-22 16:37:04,254 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:40:30,335 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:40:30,336 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 420.28s
2025-11-22 16:40:30,336 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 420.28s
2025-11-22 16:40:30,337 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:43:13,455 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:43:13,456 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 369.20s
2025-11-22 16:43:13,456 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 369.20s
2025-11-22 16:43:13,457 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:43:24,643 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:43:24,644 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 475.07s
2025-11-22 16:43:24,644 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 475.07s
2025-11-22 16:43:24,645 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:43:24,707 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:43:24,707 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 779.72s
2025-11-22 16:43:24,707 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 779.72s
2025-11-22 16:43:24,708 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:43:43,723 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:43:43,723 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 471.07s
2025-11-22 16:43:43,723 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 471.07s
2025-11-22 16:43:43,725 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:44:16,646 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:44:16,646 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 562.56s
2025-11-22 16:44:16,646 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 562.56s
2025-11-22 16:44:16,647 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:44:24,446 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:44:24,447 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 59.80s
2025-11-22 16:44:24,447 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 59.80s
2025-11-22 16:44:24,447 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:44:40,951 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:44:40,951 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 57.23s
2025-11-22 16:44:40,951 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 57.23s
2025-11-22 16:44:40,952 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:45:21,243 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:45:21,243 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 64.59s
2025-11-22 16:45:21,243 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 64.59s
2025-11-22 16:45:21,244 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:45:43,155 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:45:43,155 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 149.70s
2025-11-22 16:45:43,155 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 149.70s
2025-11-22 16:45:43,156 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:46:20,578 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:46:20,578 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 37.42s
2025-11-22 16:46:20,579 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 37.42s
2025-11-22 16:46:20,579 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:47:18,731 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:47:18,732 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 58.15s
2025-11-22 16:47:18,732 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 58.15s
2025-11-22 16:47:18,733 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:48:36,019 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:48:36,019 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 77.29s
2025-11-22 16:48:36,019 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 77.29s
2025-11-22 16:48:36,020 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:50:03,123 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:50:03,123 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 87.10s
2025-11-22 16:50:03,123 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 87.10s
2025-11-22 16:50:03,124 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:50:08,378 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:50:08,378 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 578.04s
2025-11-22 16:50:08,378 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 578.04s
2025-11-22 16:50:08,379 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:50:41,308 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:50:41,308 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 436.60s
2025-11-22 16:50:41,308 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 436.60s
2025-11-22 16:50:41,309 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:51:25,184 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:51:25,185 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 404.23s
2025-11-22 16:51:25,185 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 404.23s
2025-11-22 16:51:25,186 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:51:26,747 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:51:26,747 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 45.44s
2025-11-22 16:51:26,748 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 45.44s
2025-11-22 16:51:26,748 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:52:14,625 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:52:14,625 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 49.44s
2025-11-22 16:52:14,625 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 49.44s
2025-11-22 16:52:14,626 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:56:37,159 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:56:37,160 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 675.91s
2025-11-22 16:56:37,160 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 675.91s
2025-11-22 16:56:37,161 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:58:35,889 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:58:35,889 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1343.08s
2025-11-22 16:58:35,889 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1343.08s
2025-11-22 16:58:35,890 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:59:38,385 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.78s
2025-11-22 16:59:38,385 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-22 16:59:38,385 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 16:59:43,152 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 16:59:43,152 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 574.77s
2025-11-22 16:59:43,153 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 574.77s
2025-11-22 16:59:43,153 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:00:01,318 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:00:01,319 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 85.43s
2025-11-22 17:00:01,319 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 85.43s
2025-11-22 17:00:01,319 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:00:20,379 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:00:20,379 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 485.75s
2025-11-22 17:00:20,379 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 485.75s
2025-11-22 17:00:20,380 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:02:22,644 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:02:22,644 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 345.48s
2025-11-22 17:02:22,644 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 345.48s
2025-11-22 17:02:22,646 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:02:34,876 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:02:34,876 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 751.75s
2025-11-22 17:02:34,876 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 751.75s
2025-11-22 17:02:34,877 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:03:14,894 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:03:14,894 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 193.57s
2025-11-22 17:03:14,894 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 193.57s
2025-11-22 17:03:14,895 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:03:55,516 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:03:55,516 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 80.64s
2025-11-22 17:03:55,516 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 80.64s
2025-11-22 17:03:55,518 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:04:48,151 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:04:48,151 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 52.63s
2025-11-22 17:04:48,151 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 52.63s
2025-11-22 17:04:48,152 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:08:04,639 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:08:04,640 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 501.48s
2025-11-22 17:08:04,640 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 501.48s
2025-11-22 17:08:04,641 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:08:37,493 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:08:37,493 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 374.85s
2025-11-22 17:08:37,493 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 374.85s
2025-11-22 17:08:37,495 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:08:39,741 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:08:39,741 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 541.35s
2025-11-22 17:08:39,741 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 541.35s
2025-11-22 17:08:39,742 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:08:58,749 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:08:58,749 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 54.11s
2025-11-22 17:08:58,749 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 54.11s
2025-11-22 17:08:58,750 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:09:23,238 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:09:23,238 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1076.49s
2025-11-22 17:09:23,238 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1076.49s
2025-11-22 17:09:23,239 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:10:02,727 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:10:02,727 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 582.35s
2025-11-22 17:10:02,727 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 582.35s
2025-11-22 17:10:02,729 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:10:20,780 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:10:20,780 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 332.63s
2025-11-22 17:10:20,780 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 332.63s
2025-11-22 17:10:20,782 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:11:29,662 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:11:29,662 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1625.21s
2025-11-22 17:11:29,662 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1625.21s
2025-11-22 17:11:29,663 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:11:41,265 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:11:41,265 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 183.77s
2025-11-22 17:11:41,265 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 183.77s
2025-11-22 17:11:41,266 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:12:27,384 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:12:27,384 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 144.65s
2025-11-22 17:12:27,384 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 144.65s
2025-11-22 17:12:27,386 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:14:10,596 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:14:10,596 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 229.81s
2025-11-22 17:14:10,596 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 229.81s
2025-11-22 17:14:10,598 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:14:33,860 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:14:33,860 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 172.59s
2025-11-22 17:14:33,860 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 172.59s
2025-11-22 17:14:33,861 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:15:28,880 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:15:28,880 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 365.64s
2025-11-22 17:15:28,880 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 365.64s
2025-11-22 17:15:28,881 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:15:56,782 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:15:56,783 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 437.04s
2025-11-22 17:15:56,783 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 437.04s
2025-11-22 17:15:56,784 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:17:37,254 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:17:37,254 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 206.65s
2025-11-22 17:17:37,254 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 206.65s
2025-11-22 17:17:37,255 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:17:58,980 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:17:58,980 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 205.12s
2025-11-22 17:17:58,980 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 205.12s
2025-11-22 17:17:58,981 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:18:18,398 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:18:18,398 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 19.42s
2025-11-22 17:18:18,398 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 19.42s
2025-11-22 17:18:18,399 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:19:45,018 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:19:45,018 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 495.35s
2025-11-22 17:19:45,018 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 495.35s
2025-11-22 17:19:45,019 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:19:55,733 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:19:55,733 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 138.48s
2025-11-22 17:19:55,733 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 138.48s
2025-11-22 17:19:55,734 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:21:29,043 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:21:29,043 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 93.31s
2025-11-22 17:21:29,043 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 93.31s
2025-11-22 17:21:29,044 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:21:36,098 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:21:36,098 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 548.71s
2025-11-22 17:21:36,098 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 548.71s
2025-11-22 17:21:36,099 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:24:38,101 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:24:38,101 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 521.32s
2025-11-22 17:24:38,101 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 521.32s
2025-11-22 17:24:38,103 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:24:44,054 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:24:44,054 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 195.01s
2025-11-22 17:24:44,054 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 195.01s
2025-11-22 17:24:44,055 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:25:29,307 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:25:29,308 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 45.25s
2025-11-22 17:25:29,308 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 45.25s
2025-11-22 17:25:29,309 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:25:54,870 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:25:54,871 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1016.12s
2025-11-22 17:25:54,871 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1016.12s
2025-11-22 17:25:54,873 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:26:06,774 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:26:06,774 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 381.75s
2025-11-22 17:26:06,774 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 381.75s
2025-11-22 17:26:06,775 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:26:39,658 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:26:39,658 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 70.35s
2025-11-22 17:26:39,658 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 70.35s
2025-11-22 17:26:39,659 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:26:56,782 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:26:56,782 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1421.89s
2025-11-22 17:26:56,782 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1421.89s
2025-11-22 17:26:56,783 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:28:40,177 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:28:40,178 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 120.52s
2025-11-22 17:28:40,178 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 120.52s
2025-11-22 17:28:40,179 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:30:48,537 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:30:48,538 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 370.43s
2025-11-22 17:30:48,538 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 370.43s
2025-11-22 17:30:48,540 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:31:15,266 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:31:15,266 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 579.17s
2025-11-22 17:31:15,266 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 579.17s
2025-11-22 17:31:15,267 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:31:51,007 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:31:51,007 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 982.12s
2025-11-22 17:31:51,007 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 982.12s
2025-11-22 17:31:51,008 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:32:06,432 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:32:06,432 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 309.65s
2025-11-22 17:32:06,432 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 309.65s
2025-11-22 17:32:06,433 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:35:50,472 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:35:50,472 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 595.60s
2025-11-22 17:35:50,472 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 595.60s
2025-11-22 17:35:50,473 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:37:52,040 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:37:52,041 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 423.50s
2025-11-22 17:37:52,041 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 423.50s
2025-11-22 17:37:52,043 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:38:44,296 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:38:44,297 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 397.86s
2025-11-22 17:38:44,297 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 397.86s
2025-11-22 17:38:44,298 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:39:25,494 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:39:25,494 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 41.20s
2025-11-22 17:39:25,494 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 41.20s
2025-11-22 17:39:25,496 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:39:41,066 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:39:41,066 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 660.89s
2025-11-22 17:39:41,066 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 660.89s
2025-11-22 17:39:41,067 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:39:54,420 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:39:54,421 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 483.41s
2025-11-22 17:39:54,421 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 483.41s
2025-11-22 17:39:54,421 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:41:08,255 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:41:08,255 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 73.83s
2025-11-22 17:41:08,255 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 73.83s
2025-11-22 17:41:08,256 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:42:08,155 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:42:08,155 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 652.89s
2025-11-22 17:42:08,155 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 652.89s
2025-11-22 17:42:08,157 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:44:23,270 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:44:23,271 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 282.20s
2025-11-22 17:44:23,271 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 282.20s
2025-11-22 17:44:23,273 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:44:48,053 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:44:48,053 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1589.65s
2025-11-22 17:44:48,053 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1589.65s
2025-11-22 17:44:48,054 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:44:55,065 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:44:55,065 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 31.79s
2025-11-22 17:44:55,065 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 31.79s
2025-11-22 17:44:55,066 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:45:30,669 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:45:30,669 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 580.19s
2025-11-22 17:45:30,669 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 580.19s
2025-11-22 17:45:30,671 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:45:41,382 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:45:41,382 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 375.88s
2025-11-22 17:45:41,382 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 375.88s
2025-11-22 17:45:41,383 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:46:37,696 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:46:37,697 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1230.92s
2025-11-22 17:46:37,697 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1230.92s
2025-11-22 17:46:37,698 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:47:31,911 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:47:31,911 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 383.65s
2025-11-22 17:47:31,911 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 383.65s
2025-11-22 17:47:31,912 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:47:38,088 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:47:38,088 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 586.04s
2025-11-22 17:47:38,089 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 586.04s
2025-11-22 17:47:38,090 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:47:58,828 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:47:58,829 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 350.67s
2025-11-22 17:47:58,829 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 350.67s
2025-11-22 17:47:58,829 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:48:30,666 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:48:30,667 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 222.61s
2025-11-22 17:48:30,667 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 222.61s
2025-11-22 17:48:30,667 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:48:54,398 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:48:54,398 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 193.01s
2025-11-22 17:48:54,399 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 193.01s
2025-11-22 17:48:54,400 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:49:22,391 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:49:22,391 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 51.72s
2025-11-22 17:49:22,391 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 51.72s
2025-11-22 17:49:22,392 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:51:48,857 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:51:48,858 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 230.03s
2025-11-22 17:51:48,858 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 230.03s
2025-11-22 17:51:48,859 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:53:18,793 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:53:18,794 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 340.70s
2025-11-22 17:53:18,794 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 340.70s
2025-11-22 17:53:18,795 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:54:17,477 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:54:17,477 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 323.08s
2025-11-22 17:54:17,477 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 323.08s
2025-11-22 17:54:17,478 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:55:02,433 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:55:02,433 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 450.52s
2025-11-22 17:55:02,434 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 450.52s
2025-11-22 17:55:02,435 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:55:17,218 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:55:17,218 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 622.15s
2025-11-22 17:55:17,218 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 622.15s
2025-11-22 17:55:17,219 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:56:15,540 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:56:15,540 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 577.84s
2025-11-22 17:56:15,540 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 577.84s
2025-11-22 17:56:15,541 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:57:07,424 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:57:07,425 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 465.03s
2025-11-22 17:57:07,425 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 465.03s
2025-11-22 17:57:07,426 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:57:29,184 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:57:29,184 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 191.70s
2025-11-22 17:57:29,184 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 191.70s
2025-11-22 17:57:29,185 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:58:29,523 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:58:29,523 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 192.30s
2025-11-22 17:58:29,524 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 192.30s
2025-11-22 17:58:29,525 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:58:29,696 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:58:29,696 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 60.51s
2025-11-22 17:58:29,696 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 60.51s
2025-11-22 17:58:29,696 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:59:17,139 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:59:17,139 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 358.34s
2025-11-22 17:59:17,139 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 358.34s
2025-11-22 17:59:17,140 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 17:59:53,755 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 17:59:53,756 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 36.61s
2025-11-22 17:59:53,756 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 36.61s
2025-11-22 17:59:53,757 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:00:33,697 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:00:33,697 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 524.84s
2025-11-22 18:00:33,697 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 524.84s
2025-11-22 18:00:33,698 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:00:45,712 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:00:45,712 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 136.19s
2025-11-22 18:00:45,712 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 136.19s
2025-11-22 18:00:45,713 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:00:47,455 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:00:47,455 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 916.78s
2025-11-22 18:00:47,455 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 916.78s
2025-11-22 18:00:47,456 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:01:10,591 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:01:10,591 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 23.13s
2025-11-22 18:01:10,591 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 23.13s
2025-11-22 18:01:10,592 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:01:52,474 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:01:52,474 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 410.04s
2025-11-22 18:01:52,474 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 410.04s
2025-11-22 18:01:52,475 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:01:53,457 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:01:53,457 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 42.86s
2025-11-22 18:01:53,457 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 42.86s
2025-11-22 18:01:53,458 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:02:30,256 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:02:30,256 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 322.83s
2025-11-22 18:02:30,256 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 322.83s
2025-11-22 18:02:30,257 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:03:25,170 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:03:25,170 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 159.46s
2025-11-22 18:03:25,171 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 159.46s
2025-11-22 18:03:25,171 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:07:24,453 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:07:24,453 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 239.28s
2025-11-22 18:07:24,453 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 239.28s
2025-11-22 18:07:24,454 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:07:53,221 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:07:53,221 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 28.77s
2025-11-22 18:07:53,221 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 28.77s
2025-11-22 18:07:53,222 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:07:56,383 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:07:56,383 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 363.91s
2025-11-22 18:07:56,383 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 363.91s
2025-11-22 18:07:56,384 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:08:18,837 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:08:18,838 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 348.58s
2025-11-22 18:08:18,838 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 348.58s
2025-11-22 18:08:18,839 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:08:23,939 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:08:23,939 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 470.24s
2025-11-22 18:08:23,939 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 470.24s
2025-11-22 18:08:23,940 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:09:29,696 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:09:29,697 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 456.24s
2025-11-22 18:09:29,697 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 456.24s
2025-11-22 18:09:29,698 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:13:16,904 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:13:16,905 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 887.21s
2025-11-22 18:13:16,905 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 887.21s
2025-11-22 18:13:16,906 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:13:52,744 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:13:52,744 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 333.90s
2025-11-22 18:13:52,744 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 333.90s
2025-11-22 18:13:52,745 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:14:11,167 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:14:11,167 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 374.78s
2025-11-22 18:14:11,167 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 374.78s
2025-11-22 18:14:11,168 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:15:55,511 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:15:55,511 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 961.75s
2025-11-22 18:15:55,511 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 961.75s
2025-11-22 18:15:55,512 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:16:35,500 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:16:35,501 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 491.56s
2025-11-22 18:16:35,501 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 491.56s
2025-11-22 18:16:35,502 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:17:08,135 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:17:08,135 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 72.62s
2025-11-22 18:17:08,135 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 72.62s
2025-11-22 18:17:08,136 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:17:11,197 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:17:11,197 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 234.29s
2025-11-22 18:17:11,197 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 234.29s
2025-11-22 18:17:11,198 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:20:25,287 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:20:25,287 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 392.54s
2025-11-22 18:20:25,287 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 392.54s
2025-11-22 18:20:25,288 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:20:38,899 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:20:38,899 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 765.68s
2025-11-22 18:20:38,899 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 765.68s
2025-11-22 18:20:38,900 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:21:18,467 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:21:18,467 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 247.27s
2025-11-22 18:21:18,467 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 247.27s
2025-11-22 18:21:18,468 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:21:31,933 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:21:31,933 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 66.64s
2025-11-22 18:21:31,933 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 66.64s
2025-11-22 18:21:31,934 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:21:37,480 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:21:37,481 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 19.01s
2025-11-22 18:21:37,481 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 19.01s
2025-11-22 18:21:37,481 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:22:08,338 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:22:08,338 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 30.86s
2025-11-22 18:22:08,338 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 30.86s
2025-11-22 18:22:08,339 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:22:35,522 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:22:35,522 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 360.02s
2025-11-22 18:22:35,522 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 360.02s
2025-11-22 18:22:35,523 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:22:56,472 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:22:56,472 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 348.33s
2025-11-22 18:22:56,472 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 348.33s
2025-11-22 18:22:56,473 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:23:46,932 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:23:46,933 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 50.46s
2025-11-22 18:23:46,933 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 50.46s
2025-11-22 18:23:46,933 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:24:05,596 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:24:05,596 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1670.05s
2025-11-22 18:24:05,596 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1670.05s
2025-11-22 18:24:05,597 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:26:09,444 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:26:09,445 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 999.75s
2025-11-22 18:26:09,445 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 999.75s
2025-11-22 18:26:09,446 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:26:22,626 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:26:22,627 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 137.03s
2025-11-22 18:26:22,627 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 137.03s
2025-11-22 18:26:22,627 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:27:29,968 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:27:29,969 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 358.03s
2025-11-22 18:27:29,969 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 358.03s
2025-11-22 18:27:29,970 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:28:27,169 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:28:27,169 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 468.27s
2025-11-22 18:28:27,169 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 468.27s
2025-11-22 18:28:27,170 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:29:35,560 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:29:35,560 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 192.93s
2025-11-22 18:29:35,560 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 192.93s
2025-11-22 18:29:35,561 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:29:59,842 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:29:59,842 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 948.67s
2025-11-22 18:29:59,842 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 948.67s
2025-11-22 18:29:59,843 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:30:09,885 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:30:09,885 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 382.95s
2025-11-22 18:30:09,885 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 382.95s
2025-11-22 18:30:09,886 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:30:12,059 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:30:12,059 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 456.53s
2025-11-22 18:30:12,059 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 456.53s
2025-11-22 18:30:12,060 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:30:40,378 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:30:40,378 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 28.32s
2025-11-22 18:30:40,378 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 28.32s
2025-11-22 18:30:40,379 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:31:01,197 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:31:01,197 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 61.35s
2025-11-22 18:31:01,197 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 61.35s
2025-11-22 18:31:01,197 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:32:45,841 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:32:45,841 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 190.28s
2025-11-22 18:32:45,841 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 190.28s
2025-11-22 18:32:45,842 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:33:24,414 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:33:24,415 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 354.44s
2025-11-22 18:33:24,415 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 354.44s
2025-11-22 18:33:24,416 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:34:57,574 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:34:57,574 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 131.73s
2025-11-22 18:34:57,574 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 131.73s
2025-11-22 18:34:57,576 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:35:51,154 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:35:51,154 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 822.81s
2025-11-22 18:35:51,154 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 822.81s
2025-11-22 18:35:51,155 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:36:03,293 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:36:03,293 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 65.72s
2025-11-22 18:36:03,293 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 65.72s
2025-11-22 18:36:03,294 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:36:53,241 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:36:53,241 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 352.04s
2025-11-22 18:36:53,241 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 352.04s
2025-11-22 18:36:53,242 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:36:54,136 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:36:54,136 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 404.25s
2025-11-22 18:36:54,136 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 404.25s
2025-11-22 18:36:54,137 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:37:21,805 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:37:21,805 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 534.63s
2025-11-22 18:37:21,805 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 534.63s
2025-11-22 18:37:21,806 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:38:22,953 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:38:22,953 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 139.66s
2025-11-22 18:38:22,953 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 139.66s
2025-11-22 18:38:22,954 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:39:18,314 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:39:18,314 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 55.36s
2025-11-22 18:39:18,314 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 55.36s
2025-11-22 18:39:18,315 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:41:08,128 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:41:08,129 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 463.71s
2025-11-22 18:41:08,129 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 463.71s
2025-11-22 18:41:08,130 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:41:30,906 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:41:30,906 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 650.53s
2025-11-22 18:41:30,906 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 650.53s
2025-11-22 18:41:30,906 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:43:03,746 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:43:03,747 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1014.30s
2025-11-22 18:43:03,747 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1014.30s
2025-11-22 18:43:03,749 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:43:11,221 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:43:11,222 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 440.06s
2025-11-22 18:43:11,222 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 440.06s
2025-11-22 18:43:11,223 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:43:30,461 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:43:30,461 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 142.33s
2025-11-22 18:43:30,461 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 142.33s
2025-11-22 18:43:30,461 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:43:31,499 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:43:31,499 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 397.36s
2025-11-22 18:43:31,499 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 397.36s
2025-11-22 18:43:31,500 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:45:25,948 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:45:25,948 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 484.14s
2025-11-22 18:45:25,948 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 484.14s
2025-11-22 18:45:25,949 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:47:44,074 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:47:44,075 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 650.83s
2025-11-22 18:47:44,075 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 650.83s
2025-11-22 18:47:44,076 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:50:53,046 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:50:53,047 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 441.54s
2025-11-22 18:50:53,047 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 441.54s
2025-11-22 18:50:53,049 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:51:19,812 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:51:19,812 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 488.59s
2025-11-22 18:51:19,812 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 488.59s
2025-11-22 18:51:19,813 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:51:32,597 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:51:32,597 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 39.55s
2025-11-22 18:51:32,598 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 39.55s
2025-11-22 18:51:32,599 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:52:39,767 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:52:39,768 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 433.82s
2025-11-22 18:52:39,768 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 433.82s
2025-11-22 18:52:39,769 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 18:54:27,915 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 18:54:27,915 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 403.84s
2025-11-22 18:54:27,915 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 403.84s
2025-11-22 18:54:27,918 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:00:39,291 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:00:39,292 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1148.38s
2025-11-22 19:00:39,292 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1148.38s
2025-11-22 19:00:39,294 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:00:39,484 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:00:39,484 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 371.57s
2025-11-22 19:00:39,484 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 371.57s
2025-11-22 19:00:39,485 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:03:53,866 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:03:53,866 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 741.27s
2025-11-22 19:03:53,866 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 741.27s
2025-11-22 19:03:53,867 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:06:36,871 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:06:36,871 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 357.58s
2025-11-22 19:06:36,872 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 357.58s
2025-11-22 19:06:36,873 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:06:40,219 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:06:40,219 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 920.40s
2025-11-22 19:06:40,220 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 920.40s
2025-11-22 19:06:40,221 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:07:51,088 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:07:51,088 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 431.60s
2025-11-22 19:07:51,088 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 431.60s
2025-11-22 19:07:51,089 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:08:32,503 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:08:32,503 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 952.73s
2025-11-22 19:08:32,503 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 952.73s
2025-11-22 19:08:32,504 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:09:19,941 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.63s
2025-11-22 19:09:19,942 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-22 19:09:19,942 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:10:03,998 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:10:03,998 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1593.54s
2025-11-22 19:10:03,998 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1593.54s
2025-11-22 19:10:03,999 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:10:21,346 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:10:21,346 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1637.60s
2025-11-22 19:10:21,346 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1637.60s
2025-11-22 19:10:21,347 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:10:31,698 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:10:31,699 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 397.83s
2025-11-22 19:10:31,699 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 397.83s
2025-11-22 19:10:31,699 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:13:33,213 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:13:33,213 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 416.34s
2025-11-22 19:13:33,213 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 416.34s
2025-11-22 19:13:33,217 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:13:46,612 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:13:46,612 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 355.52s
2025-11-22 19:13:46,612 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 355.52s
2025-11-22 19:13:46,614 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:14:15,178 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:14:15,178 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 342.67s
2025-11-22 19:14:15,178 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 342.67s
2025-11-22 19:14:15,180 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:18:08,031 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:18:08,031 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 466.68s
2025-11-22 19:18:08,031 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 466.68s
2025-11-22 19:18:08,033 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:18:28,141 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:18:28,141 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 476.44s
2025-11-22 19:18:28,141 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 476.44s
2025-11-22 19:18:28,142 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:18:38,563 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:18:38,563 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 558.62s
2025-11-22 19:18:38,563 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 558.62s
2025-11-22 19:18:38,564 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:19:03,430 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:19:03,430 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 316.82s
2025-11-22 19:19:03,430 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 316.82s
2025-11-22 19:19:03,431 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:19:48,913 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:19:48,913 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 45.48s
2025-11-22 19:19:48,913 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 45.48s
2025-11-22 19:19:48,914 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:20:03,251 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:20:03,251 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 95.11s
2025-11-22 19:20:03,251 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 95.11s
2025-11-22 19:20:03,253 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:21:46,007 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:21:46,008 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 450.83s
2025-11-22 19:21:46,008 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 450.83s
2025-11-22 19:21:46,010 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:22:22,495 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:22:22,495 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 942.27s
2025-11-22 19:22:22,495 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 942.27s
2025-11-22 19:22:22,497 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:24:40,212 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:24:40,212 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 666.99s
2025-11-22 19:24:40,212 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 666.99s
2025-11-22 19:24:40,214 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:25:13,547 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:25:13,547 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 394.98s
2025-11-22 19:25:13,547 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 394.98s
2025-11-22 19:25:13,549 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:25:51,783 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:25:51,783 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 348.53s
2025-11-22 19:25:51,783 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 348.53s
2025-11-22 19:25:51,785 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:26:37,274 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:26:37,275 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 408.36s
2025-11-22 19:26:37,275 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 408.36s
2025-11-22 19:26:37,275 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:27:43,655 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:27:43,655 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 575.62s
2025-11-22 19:27:43,655 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 575.62s
2025-11-22 19:27:43,657 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:27:55,363 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:27:55,363 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1071.36s
2025-11-22 19:27:55,363 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1071.36s
2025-11-22 19:27:55,364 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:28:23,443 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:28:23,443 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 39.78s
2025-11-22 19:28:23,443 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 39.78s
2025-11-22 19:28:23,444 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:29:58,693 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:29:58,694 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 456.20s
2025-11-22 19:29:58,694 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 456.20s
2025-11-22 19:29:58,696 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:31:56,729 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:31:56,729 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 319.45s
2025-11-22 19:31:56,729 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 319.45s
2025-11-22 19:31:56,731 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:33:05,364 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:33:05,365 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 471.81s
2025-11-22 19:33:05,365 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 471.81s
2025-11-22 19:33:05,366 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:34:37,507 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:34:37,508 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 597.29s
2025-11-22 19:34:37,508 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 597.29s
2025-11-22 19:34:37,510 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:35:09,517 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:35:09,517 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 434.15s
2025-11-22 19:35:09,517 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 434.15s
2025-11-22 19:35:09,518 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:38:06,875 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:38:06,876 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 980.86s
2025-11-22 19:38:06,876 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 980.86s
2025-11-22 19:38:06,878 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:39:54,284 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:39:54,284 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 316.77s
2025-11-22 19:39:54,284 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 316.77s
2025-11-22 19:39:54,286 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:42:44,473 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:42:44,473 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 647.74s
2025-11-22 19:42:44,473 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 647.74s
2025-11-22 19:42:44,475 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:44:17,686 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:44:17,686 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1105.90s
2025-11-22 19:44:17,686 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1105.90s
2025-11-22 19:44:17,688 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:45:18,480 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:45:18,480 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1015.03s
2025-11-22 19:45:18,480 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1015.03s
2025-11-22 19:45:18,482 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:46:19,308 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:46:19,309 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 492.43s
2025-11-22 19:46:19,309 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 492.43s
2025-11-22 19:46:19,311 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:47:30,893 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:47:30,893 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 456.61s
2025-11-22 19:47:30,893 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 456.61s
2025-11-22 19:47:30,895 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:48:10,589 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:48:10,589 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1091.89s
2025-11-22 19:48:10,589 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1091.89s
2025-11-22 19:48:10,591 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:48:28,992 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:48:28,992 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 344.52s
2025-11-22 19:48:28,992 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 344.52s
2025-11-22 19:48:28,993 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:52:26,182 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:52:26,182 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1036.66s
2025-11-22 19:52:26,183 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1036.66s
2025-11-22 19:52:26,184 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:52:40,889 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:52:40,889 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 309.99s
2025-11-22 19:52:40,889 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 309.99s
2025-11-22 19:52:40,890 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:52:49,901 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:52:49,901 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 260.91s
2025-11-22 19:52:49,901 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 260.91s
2025-11-22 19:52:49,902 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:52:51,546 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:52:51,546 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 392.23s
2025-11-22 19:52:51,546 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 392.23s
2025-11-22 19:52:51,547 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:53:19,605 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:53:19,605 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 53.42s
2025-11-22 19:53:19,605 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 53.42s
2025-11-22 19:53:19,606 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:53:59,119 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:53:59,120 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 581.43s
2025-11-22 19:53:59,120 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 581.43s
2025-11-22 19:53:59,121 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:54:46,399 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:54:46,399 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 395.81s
2025-11-22 19:54:46,399 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 395.81s
2025-11-22 19:54:46,401 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:56:07,049 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:56:07,049 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 197.15s
2025-11-22 19:56:07,049 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 197.15s
2025-11-22 19:56:07,051 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:59:38,771 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:59:38,772 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 339.65s
2025-11-22 19:59:38,772 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 339.65s
2025-11-22 19:59:38,774 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 19:59:56,743 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 19:59:56,744 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 397.14s
2025-11-22 19:59:56,744 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 397.14s
2025-11-22 19:59:56,745 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:00:40,297 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:00:40,297 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 61.52s
2025-11-22 20:00:40,297 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 61.52s
2025-11-22 20:00:40,298 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:01:40,203 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:01:40,203 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1714.84s
2025-11-22 20:01:40,203 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1714.84s
2025-11-22 20:01:40,205 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:02:48,068 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:02:48,069 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 596.52s
2025-11-22 20:02:48,069 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 596.52s
2025-11-22 20:02:48,070 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:04:26,765 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:04:26,765 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 705.87s
2025-11-22 20:04:26,766 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 705.87s
2025-11-22 20:04:26,767 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:05:30,583 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:05:30,583 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 230.38s
2025-11-22 20:05:30,583 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 230.38s
2025-11-22 20:05:30,585 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:06:40,138 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:06:40,138 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1281.66s
2025-11-22 20:06:40,138 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1281.66s
2025-11-22 20:06:40,139 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:07:20,614 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:07:20,614 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 400.31s
2025-11-22 20:07:20,614 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 400.31s
2025-11-22 20:07:20,615 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:08:29,186 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:08:29,186 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 68.57s
2025-11-22 20:08:29,186 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 68.57s
2025-11-22 20:08:29,189 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:10:28,425 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:10:28,425 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 942.02s
2025-11-22 20:10:28,425 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 942.02s
2025-11-22 20:10:28,427 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:10:30,864 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:10:30,865 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 462.79s
2025-11-22 20:10:30,865 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 462.79s
2025-11-22 20:10:30,865 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:11:56,967 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:11:56,967 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 949.91s
2025-11-22 20:11:56,967 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 949.91s
2025-11-22 20:11:56,969 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:13:08,516 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:13:08,517 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 521.75s
2025-11-22 20:13:08,517 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 521.75s
2025-11-22 20:13:08,519 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:13:43,512 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:13:43,512 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 492.93s
2025-11-22 20:13:43,513 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 492.93s
2025-11-22 20:13:43,514 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:18:40,652 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:18:40,652 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 492.22s
2025-11-22 20:18:40,652 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 492.22s
2025-11-22 20:18:40,654 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:19:15,894 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:19:15,894 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1159.15s
2025-11-22 20:19:15,894 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1159.15s
2025-11-22 20:19:15,895 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:19:23,663 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:19:23,663 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 43.01s
2025-11-22 20:19:23,664 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 43.01s
2025-11-22 20:19:23,664 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:20:23,484 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:20:23,484 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 434.96s
2025-11-22 20:20:23,484 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 434.96s
2025-11-22 20:20:23,485 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:21:15,503 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:21:15,503 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 451.99s
2025-11-22 20:21:15,503 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 451.99s
2025-11-22 20:21:15,504 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:22:00,537 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:22:00,537 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 811.35s
2025-11-22 20:22:00,537 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 811.35s
2025-11-22 20:22:00,538 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:27:52,090 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:27:52,090 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 508.42s
2025-11-22 20:27:52,090 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 508.42s
2025-11-22 20:27:52,092 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:28:26,137 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:28:26,137 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 550.24s
2025-11-22 20:28:26,137 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 550.24s
2025-11-22 20:28:26,139 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:28:31,949 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:28:31,949 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 488.46s
2025-11-22 20:28:31,949 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 488.46s
2025-11-22 20:28:31,950 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:28:37,394 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:28:37,395 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 441.89s
2025-11-22 20:28:37,395 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 441.89s
2025-11-22 20:28:37,395 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:28:38,872 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:28:38,872 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1088.01s
2025-11-22 20:28:38,872 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1088.01s
2025-11-22 20:28:38,874 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:29:04,271 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:29:04,271 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 25.40s
2025-11-22 20:29:04,271 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 25.40s
2025-11-22 20:29:04,272 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:31:32,310 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:31:32,310 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 571.77s
2025-11-22 20:31:32,310 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 571.77s
2025-11-22 20:31:32,312 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:34:31,939 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:34:31,940 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 359.99s
2025-11-22 20:34:31,940 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 359.99s
2025-11-22 20:34:31,942 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:35:02,435 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:35:02,435 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 358.16s
2025-11-22 20:35:02,435 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 358.16s
2025-11-22 20:35:02,436 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:36:20,621 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:36:20,621 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 78.18s
2025-11-22 20:36:20,621 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 78.18s
2025-11-22 20:36:20,623 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:36:26,084 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:36:26,084 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 479.94s
2025-11-22 20:36:26,084 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 479.94s
2025-11-22 20:36:26,085 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:36:41,874 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.73s
2025-11-22 20:36:41,874 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-22 20:36:41,875 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:37:32,590 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:37:32,591 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 50.71s
2025-11-22 20:37:32,591 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 50.71s
2025-11-22 20:37:32,592 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:38:16,747 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:38:16,747 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 579.35s
2025-11-22 20:38:16,747 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 579.35s
2025-11-22 20:38:16,748 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:38:48,321 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:38:48,321 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 436.01s
2025-11-22 20:38:48,321 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 436.01s
2025-11-22 20:38:48,322 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:39:31,834 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:39:31,834 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1654.86s
2025-11-22 20:39:31,834 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1654.86s
2025-11-22 20:39:31,836 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:42:06,752 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:42:06,753 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 346.13s
2025-11-22 20:42:06,753 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 346.13s
2025-11-22 20:42:06,755 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:42:23,876 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:42:23,876 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 471.93s
2025-11-22 20:42:23,876 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 471.93s
2025-11-22 20:42:23,877 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:44:48,413 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:44:48,413 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1016.32s
2025-11-22 20:44:48,413 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1016.32s
2025-11-22 20:44:48,415 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:46:01,555 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:46:01,555 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 508.96s
2025-11-22 20:46:01,555 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 508.96s
2025-11-22 20:46:01,556 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:47:56,566 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:47:56,567 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 115.01s
2025-11-22 20:47:56,567 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 115.01s
2025-11-22 20:47:56,569 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:49:55,646 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:49:55,646 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 468.89s
2025-11-22 20:49:55,646 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 468.89s
2025-11-22 20:49:55,648 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:51:40,164 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:51:40,164 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 411.75s
2025-11-22 20:51:40,164 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 411.75s
2025-11-22 20:51:40,166 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:53:15,250 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:53:15,250 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1009.16s
2025-11-22 20:53:15,250 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1009.16s
2025-11-22 20:53:15,252 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:53:24,965 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:53:24,965 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 661.09s
2025-11-22 20:53:24,965 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 661.09s
2025-11-22 20:53:24,966 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:57:54,417 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:57:54,417 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1177.67s
2025-11-22 20:57:54,418 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1177.67s
2025-11-22 20:57:54,420 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:59:00,308 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:59:00,308 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1168.47s
2025-11-22 20:59:00,308 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1168.47s
2025-11-22 20:59:00,309 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:59:07,583 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:59:07,583 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 551.93s
2025-11-22 20:59:07,583 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 551.93s
2025-11-22 20:59:07,585 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 20:59:28,784 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 20:59:28,784 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 692.21s
2025-11-22 20:59:28,784 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 692.21s
2025-11-22 20:59:28,785 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:02:59,941 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:02:59,941 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1451.62s
2025-11-22 21:02:59,941 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1451.62s
2025-11-22 21:02:59,943 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:03:42,406 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:03:42,406 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 42.46s
2025-11-22 21:03:42,406 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 42.46s
2025-11-22 21:03:42,407 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:05:42,208 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:05:42,208 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 394.62s
2025-11-22 21:05:42,208 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 394.62s
2025-11-22 21:05:42,210 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:07:28,568 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:07:28,568 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 574.15s
2025-11-22 21:07:28,568 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 574.15s
2025-11-22 21:07:28,570 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:08:19,521 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:08:19,521 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 894.55s
2025-11-22 21:08:19,521 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 894.55s
2025-11-22 21:08:19,522 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:08:56,753 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:08:56,753 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1036.59s
2025-11-22 21:08:56,753 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1036.59s
2025-11-22 21:08:56,754 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:09:11,706 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:09:11,706 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 582.92s
2025-11-22 21:09:11,706 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 582.92s
2025-11-22 21:09:11,707 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:09:19,102 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:09:19,102 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 59.58s
2025-11-22 21:09:19,103 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 59.58s
2025-11-22 21:09:19,103 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:10:16,996 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:10:16,996 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1021.74s
2025-11-22 21:10:16,996 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1021.74s
2025-11-22 21:10:16,997 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:10:26,847 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:10:26,848 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 404.44s
2025-11-22 21:10:26,848 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 404.44s
2025-11-22 21:10:26,849 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:11:17,795 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:11:17,795 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 126.09s
2025-11-22 21:11:17,795 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 126.09s
2025-11-22 21:11:17,797 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:13:28,876 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:13:28,876 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 466.67s
2025-11-22 21:13:28,877 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 466.67s
2025-11-22 21:13:28,878 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:13:35,842 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:13:35,842 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 367.27s
2025-11-22 21:13:35,842 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 367.27s
2025-11-22 21:13:35,843 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:14:30,636 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:14:30,636 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 54.79s
2025-11-22 21:14:30,636 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 54.79s
2025-11-22 21:14:30,639 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:14:37,139 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:14:37,139 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 318.03s
2025-11-22 21:14:37,139 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 318.03s
2025-11-22 21:14:37,140 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:16:07,114 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:16:07,115 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 430.36s
2025-11-22 21:16:07,115 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 430.36s
2025-11-22 21:16:07,117 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:16:55,554 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:16:55,554 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 48.44s
2025-11-22 21:16:55,554 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 48.44s
2025-11-22 21:16:55,555 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:18:30,071 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:18:30,071 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1169.76s
2025-11-22 21:18:30,071 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1169.76s
2025-11-22 21:18:30,072 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:19:14,270 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:19:14,270 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 537.27s
2025-11-22 21:19:14,270 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 537.27s
2025-11-22 21:19:14,272 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:21:51,211 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:21:51,211 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 434.07s
2025-11-22 21:21:51,211 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 434.07s
2025-11-22 21:21:51,213 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:22:20,611 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:22:20,611 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 531.73s
2025-11-22 21:22:20,611 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 531.73s
2025-11-22 21:22:20,612 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:23:03,873 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:23:03,874 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 368.32s
2025-11-22 21:23:03,874 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 368.32s
2025-11-22 21:23:03,875 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:23:20,657 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:23:20,657 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 290.58s
2025-11-22 21:23:20,657 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 290.58s
2025-11-22 21:23:20,658 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:23:28,688 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:23:28,688 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 538.05s
2025-11-22 21:23:28,688 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 538.05s
2025-11-22 21:23:28,689 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:28:04,742 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:28:04,742 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 530.47s
2025-11-22 21:28:04,742 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 530.47s
2025-11-22 21:28:04,744 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:28:59,867 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:28:59,867 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1113.02s
2025-11-22 21:28:59,867 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1113.02s
2025-11-22 21:28:59,869 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:29:22,817 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:29:22,818 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1085.02s
2025-11-22 21:29:22,818 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1085.02s
2025-11-22 21:29:22,819 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:32:05,508 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:32:05,508 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 516.82s
2025-11-22 21:32:05,509 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 516.82s
2025-11-22 21:32:05,511 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:33:33,086 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:33:33,086 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 672.47s
2025-11-22 21:33:33,086 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 672.47s
2025-11-22 21:33:33,087 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:36:43,602 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:36:43,602 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 892.39s
2025-11-22 21:36:43,602 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 892.39s
2025-11-22 21:36:43,604 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:36:46,460 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:36:46,461 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 521.71s
2025-11-22 21:36:46,461 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 521.71s
2025-11-22 21:36:46,461 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:37:29,433 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:37:29,433 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 486.61s
2025-11-22 21:37:29,433 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 486.61s
2025-11-22 21:37:29,434 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:38:48,132 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:38:48,132 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 78.70s
2025-11-22 21:38:48,132 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 78.70s
2025-11-22 21:38:48,134 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:39:40,535 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:39:40,538 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 455.02s
2025-11-22 21:39:40,538 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 455.02s
2025-11-22 21:39:40,540 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:40:37,020 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:40:37,021 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1053.14s
2025-11-22 21:40:37,021 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1053.14s
2025-11-22 21:40:37,023 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:41:13,580 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:41:13,580 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 460.49s
2025-11-22 21:41:13,580 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 460.49s
2025-11-22 21:41:13,581 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:42:21,844 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:42:21,844 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 104.82s
2025-11-22 21:42:21,844 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 104.82s
2025-11-22 21:42:21,846 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:45:01,783 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:45:01,783 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 498.18s
2025-11-22 21:45:01,783 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 498.18s
2025-11-22 21:45:01,785 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:45:05,802 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:45:05,802 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 377.67s
2025-11-22 21:45:05,802 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 377.67s
2025-11-22 21:45:05,803 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:45:21,335 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:45:21,335 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 340.79s
2025-11-22 21:45:21,335 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 340.79s
2025-11-22 21:45:21,337 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:45:31,037 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:45:31,037 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 991.17s
2025-11-22 21:45:31,037 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 991.17s
2025-11-22 21:45:31,038 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:48:37,933 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:48:37,933 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 444.35s
2025-11-22 21:48:37,934 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 444.35s
2025-11-22 21:48:37,936 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:51:05,366 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:51:05,366 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 359.56s
2025-11-22 21:51:05,367 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 359.56s
2025-11-22 21:51:05,368 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:51:33,666 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:51:33,666 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 372.33s
2025-11-22 21:51:33,666 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 372.33s
2025-11-22 21:51:33,668 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:51:41,852 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:51:41,852 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1701.19s
2025-11-22 21:51:41,852 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1701.19s
2025-11-22 21:51:41,853 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:52:02,819 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:52:02,819 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 57.45s
2025-11-22 21:52:02,819 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 57.45s
2025-11-22 21:52:02,820 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:52:10,998 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:52:10,998 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 589.15s
2025-11-22 21:52:10,998 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 589.15s
2025-11-22 21:52:11,000 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:53:14,698 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:53:14,698 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 492.91s
2025-11-22 21:53:14,698 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 492.91s
2025-11-22 21:53:14,700 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:54:16,108 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:54:16,108 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 338.17s
2025-11-22 21:54:16,108 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 338.17s
2025-11-22 21:54:16,110 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:56:38,856 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:56:38,857 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1192.39s
2025-11-22 21:56:38,857 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1192.39s
2025-11-22 21:56:38,859 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 21:57:38,665 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 21:57:38,665 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 727.63s
2025-11-22 21:57:38,665 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 727.63s
2025-11-22 21:57:38,667 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:05:37,645 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:05:37,645 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 478.98s
2025-11-22 22:05:37,645 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 478.98s
2025-11-22 22:05:37,648 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:06:19,807 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:06:19,807 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 580.95s
2025-11-22 22:06:19,807 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 580.95s
2025-11-22 22:06:19,809 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:10:14,925 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:10:14,926 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1113.07s
2025-11-22 22:10:14,926 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1113.07s
2025-11-22 22:10:14,928 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:10:34,802 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:10:34,802 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1111.98s
2025-11-22 22:10:34,802 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1111.98s
2025-11-22 22:10:34,803 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:10:50,661 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:10:50,662 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1156.99s
2025-11-22 22:10:50,662 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1156.99s
2025-11-22 22:10:50,663 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:11:15,319 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:11:15,319 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1144.32s
2025-11-22 22:11:15,319 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1144.32s
2025-11-22 22:11:15,320 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:13:15,728 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:13:15,729 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 458.08s
2025-11-22 22:13:15,729 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 458.08s
2025-11-22 22:13:15,731 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:13:22,791 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:13:22,792 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1146.68s
2025-11-22 22:13:22,792 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1146.68s
2025-11-22 22:13:22,793 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:16:51,905 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:16:51,906 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 396.98s
2025-11-22 22:16:51,906 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 396.98s
2025-11-22 22:16:51,908 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:17:13,489 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:17:13,489 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 398.68s
2025-11-22 22:17:13,489 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 398.68s
2025-11-22 22:17:13,490 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:18:05,435 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:18:05,435 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 410.11s
2025-11-22 22:18:05,436 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 410.11s
2025-11-22 22:18:05,437 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:18:08,651 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:18:08,651 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 708.84s
2025-11-22 22:18:08,651 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 708.84s
2025-11-22 22:18:08,652 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:19:31,904 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:19:31,904 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 521.24s
2025-11-22 22:19:31,904 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 521.24s
2025-11-22 22:19:31,906 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:23:16,336 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.64s
2025-11-22 22:23:16,336 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-22 22:23:16,337 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:24:31,978 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:24:31,978 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 438.49s
2025-11-22 22:24:31,979 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 438.49s
2025-11-22 22:24:31,980 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:25:47,809 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:25:47,809 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 535.90s
2025-11-22 22:25:47,809 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 535.90s
2025-11-22 22:25:47,810 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:26:19,951 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:26:19,951 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 408.04s
2025-11-22 22:26:19,951 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 408.04s
2025-11-22 22:26:19,952 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:26:20,971 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:26:20,972 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 492.32s
2025-11-22 22:26:20,972 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 492.32s
2025-11-22 22:26:20,973 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:26:49,521 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:26:49,521 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 524.08s
2025-11-22 22:26:49,521 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 524.08s
2025-11-22 22:26:49,523 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:28:39,045 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:28:39,045 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 923.31s
2025-11-22 22:28:39,045 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 923.31s
2025-11-22 22:28:39,046 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:29:58,929 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:29:58,929 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 326.95s
2025-11-22 22:29:58,929 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 326.95s
2025-11-22 22:29:58,930 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:30:31,661 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:30:31,661 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 250.69s
2025-11-22 22:30:31,661 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 250.69s
2025-11-22 22:30:31,662 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:33:39,847 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:33:39,847 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 472.04s
2025-11-22 22:33:39,847 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 472.04s
2025-11-22 22:33:39,850 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:33:51,409 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:33:51,409 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 421.88s
2025-11-22 22:33:51,409 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 421.88s
2025-11-22 22:33:51,410 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:34:29,986 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:34:29,986 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 50.13s
2025-11-22 22:34:29,986 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 50.13s
2025-11-22 22:34:29,986 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:34:45,866 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:34:45,866 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 689.53s
2025-11-22 22:34:45,866 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 689.53s
2025-11-22 22:34:45,867 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:36:27,245 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:36:27,245 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 355.58s
2025-11-22 22:36:27,245 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 355.58s
2025-11-22 22:36:27,246 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:41:32,485 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:41:32,485 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 406.62s
2025-11-22 22:41:32,485 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 406.62s
2025-11-22 22:41:32,486 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:43:24,446 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.65s
2025-11-22 22:43:24,446 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-22 22:43:24,447 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:45:22,675 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:45:22,675 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1003.63s
2025-11-22 22:45:22,675 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1003.63s
2025-11-22 22:45:22,676 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:45:44,404 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:45:44,404 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 945.47s
2025-11-22 22:45:44,404 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 945.47s
2025-11-22 22:45:44,405 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:48:44,832 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:48:44,832 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 854.84s
2025-11-22 22:48:44,832 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 854.84s
2025-11-22 22:48:44,834 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:49:50,765 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:49:50,765 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 268.09s
2025-11-22 22:49:50,765 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 268.09s
2025-11-22 22:49:50,767 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:50:03,119 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:50:03,119 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 78.28s
2025-11-22 22:50:03,119 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 78.28s
2025-11-22 22:50:03,120 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:51:02,738 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:51:02,739 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 570.25s
2025-11-22 22:51:02,739 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 570.25s
2025-11-22 22:51:02,741 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:55:19,499 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:55:19,499 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1132.25s
2025-11-22 22:55:19,499 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1132.25s
2025-11-22 22:55:19,501 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:56:21,698 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.75s
2025-11-22 22:56:21,698 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-22 22:56:21,699 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:56:28,728 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:56:28,729 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1357.32s
2025-11-22 22:56:28,729 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1357.32s
2025-11-22 22:56:28,729 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 22:59:47,543 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 22:59:47,543 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 524.80s
2025-11-22 22:59:47,543 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 524.80s
2025-11-22 22:59:47,546 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:00:40,520 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:00:40,520 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 649.75s
2025-11-22 23:00:40,520 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 649.75s
2025-11-22 23:00:40,522 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:01:25,524 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:01:25,524 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1081.08s
2025-11-22 23:01:25,524 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1081.08s
2025-11-22 23:01:25,526 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:03:33,987 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:03:33,987 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 494.48s
2025-11-22 23:03:33,987 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 494.48s
2025-11-22 23:03:33,989 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:04:10,809 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:04:10,809 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1106.40s
2025-11-22 23:04:10,809 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1106.40s
2025-11-22 23:04:10,810 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:07:54,597 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:07:54,597 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 389.07s
2025-11-22 23:07:54,598 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 389.07s
2025-11-22 23:07:54,600 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:07:55,502 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:07:55,502 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1072.38s
2025-11-22 23:07:55,502 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1072.38s
2025-11-22 23:07:55,503 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:10:28,723 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:10:28,723 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 588.20s
2025-11-22 23:10:28,723 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 588.20s
2025-11-22 23:10:28,725 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:13:04,201 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:13:04,201 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 995.47s
2025-11-22 23:13:04,201 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 995.47s
2025-11-22 23:13:04,203 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:17:51,572 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:17:51,572 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 596.97s
2025-11-22 23:17:51,572 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 596.97s
2025-11-22 23:17:51,574 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:18:38,916 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:18:38,916 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1131.37s
2025-11-22 23:18:38,916 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1131.37s
2025-11-22 23:18:38,918 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:20:21,411 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:20:21,411 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1007.42s
2025-11-22 23:20:21,411 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1007.42s
2025-11-22 23:20:21,413 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:21:30,802 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:21:30,802 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1039.99s
2025-11-22 23:21:30,802 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1039.99s
2025-11-22 23:21:30,804 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:21:30,872 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:21:30,872 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 662.15s
2025-11-22 23:21:30,872 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 662.15s
2025-11-22 23:21:30,873 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:22:34,346 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:22:34,346 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 63.54s
2025-11-22 23:22:34,346 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 63.54s
2025-11-22 23:22:34,348 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:22:58,207 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:22:58,207 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1596.51s
2025-11-22 23:22:58,207 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1596.51s
2025-11-22 23:22:58,208 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:23:24,877 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:23:24,877 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 50.53s
2025-11-22 23:23:24,877 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 50.53s
2025-11-22 23:23:24,878 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:25:08,019 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:25:08,020 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1032.52s
2025-11-22 23:25:08,020 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1032.52s
2025-11-22 23:25:08,022 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:26:34,739 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:26:34,739 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 475.82s
2025-11-22 23:26:34,739 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 475.82s
2025-11-22 23:26:34,742 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:27:02,629 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:27:02,629 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 551.05s
2025-11-22 23:27:02,629 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 551.05s
2025-11-22 23:27:02,631 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:30:06,187 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:30:06,187 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 584.77s
2025-11-22 23:30:06,187 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 584.77s
2025-11-22 23:30:06,190 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:30:24,185 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:30:24,186 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 445.98s
2025-11-22 23:30:24,186 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 445.98s
2025-11-22 23:30:24,187 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:30:27,403 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:30:27,404 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 536.53s
2025-11-22 23:30:27,404 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 536.53s
2025-11-22 23:30:27,405 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:31:22,556 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:31:22,556 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 76.36s
2025-11-22 23:31:22,556 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 76.36s
2025-11-22 23:31:22,559 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:31:35,669 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:31:35,669 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 490.79s
2025-11-22 23:31:35,669 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 490.79s
2025-11-22 23:31:35,670 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:33:14,643 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:33:14,643 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 486.62s
2025-11-22 23:33:14,643 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 486.62s
2025-11-22 23:33:14,645 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:35:53,897 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:35:53,897 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 531.27s
2025-11-22 23:35:53,897 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 531.27s
2025-11-22 23:35:53,899 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:37:27,118 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:37:27,118 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 351.45s
2025-11-22 23:37:27,118 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 351.45s
2025-11-22 23:37:27,119 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:39:09,780 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:39:09,780 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 467.22s
2025-11-22 23:39:09,781 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 467.22s
2025-11-22 23:39:09,782 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:39:39,920 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:39:39,921 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 552.51s
2025-11-22 23:39:39,921 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 552.51s
2025-11-22 23:39:39,922 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:40:44,989 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:40:44,990 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1660.78s
2025-11-22 23:40:44,990 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1660.78s
2025-11-22 23:40:44,992 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:42:27,127 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:42:27,127 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 552.48s
2025-11-22 23:42:27,127 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 552.48s
2025-11-22 23:42:27,128 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:43:04,602 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:43:04,602 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 430.70s
2025-11-22 23:43:04,602 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 430.70s
2025-11-22 23:43:04,603 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:46:11,594 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:46:11,595 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1176.85s
2025-11-22 23:46:11,595 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1176.85s
2025-11-22 23:46:11,597 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:46:14,892 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:46:14,892 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 527.77s
2025-11-22 23:46:14,892 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 527.77s
2025-11-22 23:46:14,894 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:47:46,451 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:47:46,451 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1042.26s
2025-11-22 23:47:46,451 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1042.26s
2025-11-22 23:47:46,453 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:51:11,854 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:51:11,854 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 691.93s
2025-11-22 23:51:11,854 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 691.93s
2025-11-22 23:51:11,856 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:52:10,497 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:52:10,497 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 583.37s
2025-11-22 23:52:10,497 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 583.37s
2025-11-22 23:52:10,498 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:55:14,178 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:55:14,178 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 964.40s
2025-11-22 23:55:14,178 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 964.40s
2025-11-22 23:55:14,180 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:55:15,708 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:55:15,708 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 544.11s
2025-11-22 23:55:15,708 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 544.11s
2025-11-22 23:55:15,709 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:56:53,258 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:56:53,258 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 546.80s
2025-11-22 23:56:53,258 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 546.80s
2025-11-22 23:56:53,260 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-22 23:58:33,340 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-22 23:58:33,341 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 382.84s
2025-11-22 23:58:33,341 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 382.84s
2025-11-22 23:58:33,343 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:02:35,989 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:02:35,989 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1171.38s
2025-11-23 00:02:35,989 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1171.38s
2025-11-23 00:02:35,992 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:03:27,178 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:03:27,179 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 493.00s
2025-11-23 00:03:27,179 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 493.00s
2025-11-23 00:03:27,181 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:03:40,988 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:03:40,989 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 505.28s
2025-11-23 00:03:40,989 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 505.28s
2025-11-23 00:03:40,990 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:04:20,821 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:04:20,821 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 39.83s
2025-11-23 00:04:20,821 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 39.83s
2025-11-23 00:04:20,823 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:07:31,942 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:07:31,942 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1277.05s
2025-11-23 00:07:31,942 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1277.05s
2025-11-23 00:07:31,946 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:07:51,018 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:07:51,018 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 557.67s
2025-11-23 00:07:51,018 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 557.67s
2025-11-23 00:07:51,019 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:10:46,792 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.80s
2025-11-23 00:10:46,792 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 00:10:46,793 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:10:47,426 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:10:47,426 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1175.57s
2025-11-23 00:10:47,426 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1175.57s
2025-11-23 00:10:47,427 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:10:57,790 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:10:57,790 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 450.61s
2025-11-23 00:10:57,790 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 450.61s
2025-11-23 00:10:57,791 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:11:50,608 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:11:50,608 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 554.61s
2025-11-23 00:11:50,608 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 554.61s
2025-11-23 00:11:50,611 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:12:23,402 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:12:23,402 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 482.58s
2025-11-23 00:12:23,402 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 482.58s
2025-11-23 00:12:23,404 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:15:28,757 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:15:28,757 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 457.74s
2025-11-23 00:15:28,757 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 457.74s
2025-11-23 00:15:28,759 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:15:29,063 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:15:29,063 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1115.80s
2025-11-23 00:15:29,063 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1115.80s
2025-11-23 00:15:29,064 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:16:39,979 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:16:39,980 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 548.03s
2025-11-23 00:16:39,980 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 548.03s
2025-11-23 00:16:39,982 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:18:49,980 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:18:49,980 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 472.19s
2025-11-23 00:18:49,980 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 472.19s
2025-11-23 00:18:49,983 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:19:16,863 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:19:16,864 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 446.25s
2025-11-23 00:19:16,864 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 446.25s
2025-11-23 00:19:16,866 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:20:27,426 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:20:27,426 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 580.63s
2025-11-23 00:20:27,426 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 580.63s
2025-11-23 00:20:27,428 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:20:36,250 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:20:36,250 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 492.84s
2025-11-23 00:20:36,250 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 492.84s
2025-11-23 00:20:36,251 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:21:48,530 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:21:48,530 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 308.55s
2025-11-23 00:21:48,530 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 308.55s
2025-11-23 00:21:48,532 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:22:16,141 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:22:16,141 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 688.71s
2025-11-23 00:22:16,141 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 688.71s
2025-11-23 00:22:16,142 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:24:12,033 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:24:12,033 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 522.97s
2025-11-23 00:24:12,033 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 522.97s
2025-11-23 00:24:12,035 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:26:58,511 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:26:58,512 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 391.08s
2025-11-23 00:26:58,512 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 391.08s
2025-11-23 00:26:58,514 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:27:44,872 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:27:44,872 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 508.01s
2025-11-23 00:27:44,872 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 508.01s
2025-11-23 00:27:44,875 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:28:04,009 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:28:04,009 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 375.48s
2025-11-23 00:28:04,009 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 375.48s
2025-11-23 00:28:04,010 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:28:48,827 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:28:48,827 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 598.84s
2025-11-23 00:28:48,827 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 598.84s
2025-11-23 00:28:48,829 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:32:38,279 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:32:38,280 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 506.24s
2025-11-23 00:32:38,280 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 506.24s
2025-11-23 00:32:38,282 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:35:22,227 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:35:22,227 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1193.47s
2025-11-23 00:35:22,227 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1193.47s
2025-11-23 00:35:22,229 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:36:01,708 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:36:01,709 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 496.83s
2025-11-23 00:36:01,709 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 496.83s
2025-11-23 00:36:01,711 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:36:57,206 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:36:57,206 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 55.49s
2025-11-23 00:36:57,206 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 55.49s
2025-11-23 00:36:57,208 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:39:39,422 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:39:39,423 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1143.17s
2025-11-23 00:39:39,423 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1143.17s
2025-11-23 00:39:39,425 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:40:41,828 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:40:41,828 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 319.60s
2025-11-23 00:40:41,828 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 319.60s
2025-11-23 00:40:41,829 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:41:31,954 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:41:31,954 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 533.67s
2025-11-23 00:41:31,954 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 533.67s
2025-11-23 00:41:32,007 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:43:30,016 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:43:30,017 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 991.50s
2025-11-23 00:43:30,017 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 991.50s
2025-11-23 00:43:30,019 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:43:35,787 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:43:35,787 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1279.64s
2025-11-23 00:43:35,787 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1279.64s
2025-11-23 00:43:35,788 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:44:47,649 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:44:47,650 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 71.86s
2025-11-23 00:44:47,650 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 71.86s
2025-11-23 00:44:47,651 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:45:23,834 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:45:23,835 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1039.82s
2025-11-23 00:45:23,835 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1039.82s
2025-11-23 00:45:23,836 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:46:51,788 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:46:51,788 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1082.96s
2025-11-23 00:46:51,788 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1082.96s
2025-11-23 00:46:51,789 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:49:48,544 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:49:48,545 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 496.54s
2025-11-23 00:49:48,545 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 496.54s
2025-11-23 00:49:48,547 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:52:06,527 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:52:06,527 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 909.32s
2025-11-23 00:52:06,527 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 909.32s
2025-11-23 00:52:06,528 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:52:20,639 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:52:20,639 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 416.80s
2025-11-23 00:52:20,639 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 416.80s
2025-11-23 00:52:20,640 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:54:29,576 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:54:29,577 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 581.92s
2025-11-23 00:54:29,577 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 581.92s
2025-11-23 00:54:29,578 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:54:53,329 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:54:53,329 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 683.31s
2025-11-23 00:54:53,329 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 683.31s
2025-11-23 00:54:53,330 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:55:47,696 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:55:47,696 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 207.05s
2025-11-23 00:55:47,696 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 207.05s
2025-11-23 00:55:47,697 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:56:32,796 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:56:32,796 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 404.25s
2025-11-23 00:56:32,796 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 404.25s
2025-11-23 00:56:32,797 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:56:38,079 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:56:38,079 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 586.29s
2025-11-23 00:56:38,079 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 586.29s
2025-11-23 00:56:38,080 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:57:32,979 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:57:32,980 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 54.90s
2025-11-23 00:57:32,980 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 54.90s
2025-11-23 00:57:32,981 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 00:57:55,468 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 00:57:55,468 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 82.67s
2025-11-23 00:57:55,469 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 82.67s
2025-11-23 00:57:55,469 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:01:57,318 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:01:57,319 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1275.49s
2025-11-23 01:01:57,319 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1275.49s
2025-11-23 01:01:57,321 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:02:58,399 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:02:58,400 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 508.82s
2025-11-23 01:02:58,400 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 508.82s
2025-11-23 01:02:58,401 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:04:01,862 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:04:01,862 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 388.88s
2025-11-23 01:04:01,862 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 388.88s
2025-11-23 01:04:01,864 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:04:30,777 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:04:30,777 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 577.45s
2025-11-23 01:04:30,778 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 577.45s
2025-11-23 01:04:30,778 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:05:10,090 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:05:10,090 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 562.39s
2025-11-23 01:05:10,090 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 562.39s
2025-11-23 01:05:10,091 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:05:35,226 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:05:35,226 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 93.36s
2025-11-23 01:05:35,227 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 93.36s
2025-11-23 01:05:35,227 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:09:41,013 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.59s
2025-11-23 01:09:41,013 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 01:09:41,014 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:10:28,200 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:10:28,200 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1101.67s
2025-11-23 01:10:28,200 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1101.67s
2025-11-23 01:10:28,201 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:12:33,588 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:12:33,588 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 418.36s
2025-11-23 01:12:33,588 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 418.36s
2025-11-23 01:12:33,590 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:13:01,294 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:13:01,294 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 471.20s
2025-11-23 01:13:01,294 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 471.20s
2025-11-23 01:13:01,295 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:13:05,979 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:13:05,979 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 668.66s
2025-11-23 01:13:05,979 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 668.66s
2025-11-23 01:13:05,980 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:16:54,253 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:16:54,253 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1138.78s
2025-11-23 01:16:54,253 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1138.78s
2025-11-23 01:16:54,255 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:19:21,118 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:19:21,118 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 407.53s
2025-11-23 01:19:21,119 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 407.53s
2025-11-23 01:19:21,121 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:21:55,597 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:21:55,597 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 534.30s
2025-11-23 01:21:55,597 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 534.30s
2025-11-23 01:21:55,600 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:24:16,108 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:24:16,108 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 670.13s
2025-11-23 01:24:16,108 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 670.13s
2025-11-23 01:24:16,110 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:25:36,149 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:25:36,149 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1265.37s
2025-11-23 01:25:36,149 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1265.37s
2025-11-23 01:25:36,150 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:28:20,334 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:28:20,334 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1119.32s
2025-11-23 01:28:20,334 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1119.32s
2025-11-23 01:28:20,336 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:28:39,765 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:28:39,765 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1091.56s
2025-11-23 01:28:39,765 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1091.56s
2025-11-23 01:28:39,766 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:29:42,527 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:29:42,528 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 466.93s
2025-11-23 01:29:42,528 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 466.93s
2025-11-23 01:29:42,529 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:30:40,711 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:30:40,711 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 679.59s
2025-11-23 01:30:40,711 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 679.59s
2025-11-23 01:30:40,712 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:30:48,094 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:30:48,095 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1669.69s
2025-11-23 01:30:48,095 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1669.69s
2025-11-23 01:30:48,096 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:32:56,988 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:32:56,988 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 128.89s
2025-11-23 01:32:56,989 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 128.89s
2025-11-23 01:32:56,990 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:35:34,218 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:35:34,218 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 433.88s
2025-11-23 01:35:34,218 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 433.88s
2025-11-23 01:35:34,219 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:36:27,027 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:36:27,027 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1172.77s
2025-11-23 01:36:27,027 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1172.77s
2025-11-23 01:36:27,028 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:44:20,948 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:44:20,948 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 473.92s
2025-11-23 01:44:20,949 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 473.92s
2025-11-23 01:44:20,950 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:44:29,414 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:44:29,414 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 535.19s
2025-11-23 01:44:29,415 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 535.19s
2025-11-23 01:44:29,416 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:45:36,847 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:45:36,847 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 896.13s
2025-11-23 01:45:36,847 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 896.13s
2025-11-23 01:45:36,848 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:45:46,402 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:45:46,402 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 963.87s
2025-11-23 01:45:46,403 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 963.87s
2025-11-23 01:45:46,403 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:46:15,350 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:46:15,350 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1055.58s
2025-11-23 01:46:15,350 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1055.58s
2025-11-23 01:46:15,351 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:48:19,869 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:48:19,869 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 153.46s
2025-11-23 01:48:19,869 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 153.46s
2025-11-23 01:48:19,870 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:51:09,050 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:51:09,051 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 408.10s
2025-11-23 01:51:09,051 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 408.10s
2025-11-23 01:51:09,052 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:54:01,261 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:54:01,262 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 504.41s
2025-11-23 01:54:01,262 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 504.41s
2025-11-23 01:54:01,263 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:54:17,678 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.57s
2025-11-23 01:54:17,678 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 01:54:17,679 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:55:37,628 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.48s
2025-11-23 01:55:37,628 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 01:55:37,629 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 01:55:44,579 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 01:55:44,579 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 569.23s
2025-11-23 01:55:44,579 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 569.23s
2025-11-23 01:55:44,581 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:00:26,256 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:00:26,256 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 557.20s
2025-11-23 02:00:26,256 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 557.20s
2025-11-23 02:00:26,258 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:01:18,213 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:01:18,213 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 420.53s
2025-11-23 02:01:18,213 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 420.53s
2025-11-23 02:01:18,215 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:02:23,866 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:02:23,867 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1766.88s
2025-11-23 02:02:23,867 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1766.88s
2025-11-23 02:02:23,869 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:02:50,412 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:02:50,412 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1100.99s
2025-11-23 02:02:50,412 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1100.99s
2025-11-23 02:02:50,413 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:07:19,239 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:07:19,240 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 694.66s
2025-11-23 02:07:19,240 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 694.66s
2025-11-23 02:07:19,242 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:08:15,886 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:08:15,886 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1196.01s
2025-11-23 02:08:15,886 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1196.01s
2025-11-23 02:08:15,887 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:10:27,953 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:10:27,953 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 549.74s
2025-11-23 02:10:27,953 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 549.74s
2025-11-23 02:10:27,954 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:10:28,123 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:10:28,124 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 457.71s
2025-11-23 02:10:28,124 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 457.71s
2025-11-23 02:10:28,125 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:12:03,873 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:12:03,873 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 697.61s
2025-11-23 02:12:03,873 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 697.61s
2025-11-23 02:12:03,875 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:13:45,127 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:13:45,127 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1087.50s
2025-11-23 02:13:45,127 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1087.50s
2025-11-23 02:13:45,129 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:16:45,090 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:16:45,091 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 565.85s
2025-11-23 02:16:45,091 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 565.85s
2025-11-23 02:16:45,093 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:17:40,848 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:17:40,849 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 432.72s
2025-11-23 02:17:40,849 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 432.72s
2025-11-23 02:17:40,850 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:17:43,855 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:17:43,855 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 567.97s
2025-11-23 02:17:43,855 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 567.97s
2025-11-23 02:17:43,856 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:17:52,609 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:17:52,609 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 348.73s
2025-11-23 02:17:52,609 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 348.73s
2025-11-23 02:17:52,611 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:20:17,777 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:20:17,777 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 589.82s
2025-11-23 02:20:17,777 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 589.82s
2025-11-23 02:20:17,780 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:24:02,827 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.56s
2025-11-23 02:24:02,828 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 02:24:02,829 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:27:16,517 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:27:16,518 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 418.74s
2025-11-23 02:27:16,518 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 418.74s
2025-11-23 02:27:16,520 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:27:29,250 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:27:29,250 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 588.40s
2025-11-23 02:27:29,250 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 588.40s
2025-11-23 02:27:29,251 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:28:32,740 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:28:32,741 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 63.49s
2025-11-23 02:28:32,741 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 63.49s
2025-11-23 02:28:32,742 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:30:27,246 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:30:27,246 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 754.63s
2025-11-23 02:30:27,247 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 754.63s
2025-11-23 02:30:27,248 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:30:40,551 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:30:40,552 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1696.68s
2025-11-23 02:30:40,552 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1696.68s
2025-11-23 02:30:40,553 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:32:04,242 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:32:04,243 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1099.11s
2025-11-23 02:32:04,243 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1099.11s
2025-11-23 02:32:04,244 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:33:13,337 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:33:13,337 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 356.82s
2025-11-23 02:33:13,337 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 356.82s
2025-11-23 02:33:13,338 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:35:36,666 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:35:36,666 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 423.92s
2025-11-23 02:35:36,666 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 423.92s
2025-11-23 02:35:36,668 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:38:26,360 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:38:26,360 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 479.11s
2025-11-23 02:38:26,360 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 479.11s
2025-11-23 02:38:26,363 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:40:43,619 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:40:43,619 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 450.28s
2025-11-23 02:40:43,620 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 450.28s
2025-11-23 02:40:43,622 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:41:25,531 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:41:25,532 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1042.70s
2025-11-23 02:41:25,532 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1042.70s
2025-11-23 02:41:25,535 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:43:00,957 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:43:00,958 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 444.29s
2025-11-23 02:43:00,958 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 444.29s
2025-11-23 02:43:00,960 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:43:30,441 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:43:30,441 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 29.48s
2025-11-23 02:43:30,441 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 29.48s
2025-11-23 02:43:30,489 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:45:09,144 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:45:09,144 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1645.29s
2025-11-23 02:45:09,144 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1645.29s
2025-11-23 02:45:09,146 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:45:15,014 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:45:15,014 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 104.52s
2025-11-23 02:45:15,014 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 104.52s
2025-11-23 02:45:15,015 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:45:31,981 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:45:31,981 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 425.62s
2025-11-23 02:45:31,981 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 425.62s
2025-11-23 02:45:31,982 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:46:05,337 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:46:05,337 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 50.32s
2025-11-23 02:46:05,337 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 50.32s
2025-11-23 02:46:05,338 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:46:46,724 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.63s
2025-11-23 02:46:46,724 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 02:46:46,725 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:49:04,421 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:49:04,422 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 179.08s
2025-11-23 02:49:04,422 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 179.08s
2025-11-23 02:49:04,424 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:49:54,775 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:49:54,776 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 551.15s
2025-11-23 02:49:54,776 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 551.15s
2025-11-23 02:49:54,777 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:50:11,291 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:50:11,291 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 66.87s
2025-11-23 02:50:11,291 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 66.87s
2025-11-23 02:50:11,292 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:50:41,838 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:50:41,839 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 30.55s
2025-11-23 02:50:41,839 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 30.55s
2025-11-23 02:50:41,840 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:54:14,549 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:54:14,549 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 545.40s
2025-11-23 02:54:14,549 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 545.40s
2025-11-23 02:54:14,552 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:58:38,180 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:58:38,180 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 523.40s
2025-11-23 02:58:38,180 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 523.40s
2025-11-23 02:58:38,183 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 02:59:58,792 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 02:59:58,793 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1758.24s
2025-11-23 02:59:58,793 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1758.24s
2025-11-23 02:59:58,795 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:01:35,373 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:01:35,374 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 653.53s
2025-11-23 03:01:35,374 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 653.53s
2025-11-23 03:01:35,376 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:02:05,659 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.41s
2025-11-23 03:02:05,659 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 03:02:05,659 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:02:26,767 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:02:26,767 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 492.21s
2025-11-23 03:02:26,767 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 492.21s
2025-11-23 03:02:26,769 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:03:17,835 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:03:17,836 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 51.07s
2025-11-23 03:03:17,836 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 51.07s
2025-11-23 03:03:17,837 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:08:14,271 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:08:14,271 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 398.89s
2025-11-23 03:08:14,271 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 398.89s
2025-11-23 03:08:14,274 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:09:42,995 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:09:42,995 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 385.16s
2025-11-23 03:09:42,995 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 385.16s
2025-11-23 03:09:42,996 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:10:05,466 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:10:05,467 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 111.19s
2025-11-23 03:10:05,467 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 111.19s
2025-11-23 03:10:05,467 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:10:24,729 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:10:24,730 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1739.19s
2025-11-23 03:10:24,730 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1739.19s
2025-11-23 03:10:24,731 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:13:01,366 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:13:01,367 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1649.38s
2025-11-23 03:13:01,367 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1649.38s
2025-11-23 03:13:01,369 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:13:18,006 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:13:18,006 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 672.35s
2025-11-23 03:13:18,006 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 672.35s
2025-11-23 03:13:18,007 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:13:26,317 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:13:26,317 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 807.52s
2025-11-23 03:13:26,317 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 807.52s
2025-11-23 03:13:26,318 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:15:16,630 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:15:16,630 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 998.45s
2025-11-23 03:15:16,630 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 998.45s
2025-11-23 03:15:16,632 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:16:19,643 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:16:19,643 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 63.01s
2025-11-23 03:16:19,643 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 63.01s
2025-11-23 03:16:19,645 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:16:48,399 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.67s
2025-11-23 03:16:48,399 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 03:16:48,399 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:17:22,377 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:17:22,378 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 417.65s
2025-11-23 03:17:22,378 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 417.65s
2025-11-23 03:17:22,379 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:17:56,361 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:17:56,361 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 493.36s
2025-11-23 03:17:56,361 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 493.36s
2025-11-23 03:17:56,362 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:18:03,157 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:18:03,157 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 276.84s
2025-11-23 03:18:03,158 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 276.84s
2025-11-23 03:18:03,159 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:18:30,714 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:18:30,714 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 312.71s
2025-11-23 03:18:30,714 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 312.71s
2025-11-23 03:18:30,716 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:23:40,617 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:23:40,618 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 440.97s
2025-11-23 03:23:40,618 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 440.97s
2025-11-23 03:23:40,620 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:23:42,781 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:23:42,781 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 312.06s
2025-11-23 03:23:42,781 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 312.06s
2025-11-23 03:23:42,782 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:24:28,056 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:24:28,056 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 47.43s
2025-11-23 03:24:28,056 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 47.43s
2025-11-23 03:24:28,057 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:25:31,359 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:25:31,359 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 925.89s
2025-11-23 03:25:31,359 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 925.89s
2025-11-23 03:25:31,360 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:26:07,272 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:26:07,272 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 490.91s
2025-11-23 03:26:07,272 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 490.91s
2025-11-23 03:26:07,274 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:26:23,377 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:26:23,377 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 52.02s
2025-11-23 03:26:23,377 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 52.02s
2025-11-23 03:26:23,379 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:26:32,844 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:26:32,845 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 584.44s
2025-11-23 03:26:32,845 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 584.44s
2025-11-23 03:30:14,779 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:30:14,779 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1033.41s
2025-11-23 03:30:14,780 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1033.41s
2025-11-23 03:30:14,782 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:30:58,949 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:30:58,949 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 436.17s
2025-11-23 03:30:58,949 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 436.17s
2025-11-23 03:30:58,950 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:31:39,844 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:31:39,845 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 85.06s
2025-11-23 03:31:39,845 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 85.06s
2025-11-23 03:31:39,846 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:33:54,020 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:33:54,020 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 565.96s
2025-11-23 03:33:54,020 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 565.96s
2025-11-23 03:33:54,023 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:34:28,450 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:34:28,451 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 985.29s
2025-11-23 03:34:28,451 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 985.29s
2025-11-23 03:34:28,452 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:38:21,560 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:38:21,561 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 233.11s
2025-11-23 03:38:21,561 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 233.11s
2025-11-23 03:38:21,563 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:41:02,167 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:41:02,168 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 878.79s
2025-11-23 03:41:02,168 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 878.79s
2025-11-23 03:41:02,170 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:41:39,930 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:41:39,930 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 465.91s
2025-11-23 03:41:39,930 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 465.91s
2025-11-23 03:41:39,931 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:41:51,793 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:41:51,793 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 49.62s
2025-11-23 03:41:51,794 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 49.62s
2025-11-23 03:41:51,794 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:43:26,576 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:43:26,576 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 706.73s
2025-11-23 03:43:26,576 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 706.73s
2025-11-23 03:43:26,577 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:43:41,073 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:43:41,073 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 109.28s
2025-11-23 03:43:41,073 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 109.28s
2025-11-23 03:43:41,074 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:44:20,662 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:44:20,662 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 359.10s
2025-11-23 03:44:20,662 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 359.10s
2025-11-23 03:44:20,664 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:45:14,904 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:45:14,905 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 108.33s
2025-11-23 03:45:14,905 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 108.33s
2025-11-23 03:46:46,215 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:46:46,215 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 306.28s
2025-11-23 03:46:46,215 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 306.28s
2025-11-23 03:46:46,217 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:47:23,811 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.43s
2025-11-23 03:47:23,812 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 03:47:23,813 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:47:28,815 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:47:28,815 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 227.74s
2025-11-23 03:47:28,815 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 227.74s
2025-11-23 03:47:28,816 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:48:16,756 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:48:16,756 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 52.94s
2025-11-23 03:48:16,756 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 52.94s
2025-11-23 03:48:16,758 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:48:19,288 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:48:19,288 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 50.47s
2025-11-23 03:48:19,288 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 50.47s
2025-11-23 03:48:19,289 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:49:05,912 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:49:05,912 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 49.15s
2025-11-23 03:49:05,912 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 49.15s
2025-11-23 03:49:05,913 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:49:43,205 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:49:43,205 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 322.54s
2025-11-23 03:49:43,205 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 322.54s
2025-11-23 03:49:43,206 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:52:08,184 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:52:08,184 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1269.23s
2025-11-23 03:52:08,184 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1269.23s
2025-11-23 03:52:08,186 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:52:41,714 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:52:41,714 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 355.50s
2025-11-23 03:52:41,714 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 355.50s
2025-11-23 03:52:41,715 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:52:56,449 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:52:56,449 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1609.17s
2025-11-23 03:52:56,449 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1609.17s
2025-11-23 03:52:56,451 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:53:34,749 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:53:34,749 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 38.30s
2025-11-23 03:53:34,750 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 38.30s
2025-11-23 03:53:34,750 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:54:52,037 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:54:52,037 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 308.83s
2025-11-23 03:54:52,037 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 308.83s
2025-11-23 03:54:52,039 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:55:45,221 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:55:45,221 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 445.93s
2025-11-23 03:55:45,221 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 445.93s
2025-11-23 03:55:45,223 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:57:14,928 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:57:14,928 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 489.01s
2025-11-23 03:57:14,928 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 489.01s
2025-11-23 03:57:14,931 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 03:58:34,811 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 03:58:34,811 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 386.62s
2025-11-23 03:58:34,811 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 386.62s
2025-11-23 03:58:34,814 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:01:58,881 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:01:58,881 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 504.13s
2025-11-23 04:01:58,881 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 504.13s
2025-11-23 04:01:58,884 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:02:16,324 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:02:16,324 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 391.10s
2025-11-23 04:02:16,324 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 391.10s
2025-11-23 04:02:16,325 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:02:58,367 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:02:58,367 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 42.04s
2025-11-23 04:02:58,367 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 42.04s
2025-11-23 04:02:58,368 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:03:43,716 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:03:43,716 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 308.90s
2025-11-23 04:03:43,716 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 308.90s
2025-11-23 04:03:43,718 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:03:49,580 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:03:49,580 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 394.65s
2025-11-23 04:03:49,580 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 394.65s
2025-11-23 04:03:49,581 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:03:54,539 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:03:54,540 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 56.17s
2025-11-23 04:03:54,540 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 56.17s
2025-11-23 04:03:54,540 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:06:56,988 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:06:56,988 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 298.10s
2025-11-23 04:06:56,988 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 298.10s
2025-11-23 04:06:56,990 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:11:59,863 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:11:59,863 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 485.32s
2025-11-23 04:11:59,864 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 485.32s
2025-11-23 04:11:59,865 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:12:15,286 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:12:15,286 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1173.57s
2025-11-23 04:12:15,287 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1173.57s
2025-11-23 04:12:15,288 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:12:36,358 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:12:36,358 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 526.78s
2025-11-23 04:12:36,358 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 526.78s
2025-11-23 04:12:36,360 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:12:56,563 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:12:56,563 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 359.57s
2025-11-23 04:12:56,563 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 359.57s
2025-11-23 04:12:56,564 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:17:46,180 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:17:46,180 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 309.82s
2025-11-23 04:17:46,181 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 309.82s
2025-11-23 04:17:46,182 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:18:43,319 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:18:43,319 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 403.45s
2025-11-23 04:18:43,320 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 403.45s
2025-11-23 04:18:43,320 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:20:15,931 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:20:15,932 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1523.89s
2025-11-23 04:20:15,932 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1523.89s
2025-11-23 04:20:15,933 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:20:31,394 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:20:31,395 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 454.83s
2025-11-23 04:20:31,395 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 454.83s
2025-11-23 04:20:31,396 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:21:50,742 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:21:50,742 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1087.02s
2025-11-23 04:21:50,742 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1087.02s
2025-11-23 04:21:50,743 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:27:02,916 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:27:02,916 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 499.59s
2025-11-23 04:27:02,916 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 499.59s
2025-11-23 04:27:02,918 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:27:23,894 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:27:23,894 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 577.71s
2025-11-23 04:27:23,894 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 577.71s
2025-11-23 04:27:23,896 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:28:30,348 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:28:30,348 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 66.45s
2025-11-23 04:28:30,348 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 66.45s
2025-11-23 04:28:30,350 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:30:47,338 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:30:47,338 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 631.40s
2025-11-23 04:30:47,338 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 631.40s
2025-11-23 04:30:47,339 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:31:58,484 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:31:58,485 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 71.14s
2025-11-23 04:31:58,485 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 71.14s
2025-11-23 04:31:58,486 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:33:18,436 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:33:18,436 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 288.09s
2025-11-23 04:33:18,437 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 288.09s
2025-11-23 04:33:18,437 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:34:23,591 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:34:23,591 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 65.15s
2025-11-23 04:34:23,591 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 65.15s
2025-11-23 04:34:23,592 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:35:00,819 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:35:00,819 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 790.07s
2025-11-23 04:35:00,819 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 790.07s
2025-11-23 04:35:00,820 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:35:18,269 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:35:18,269 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 886.87s
2025-11-23 04:35:18,269 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 886.87s
2025-11-23 04:35:18,270 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:36:42,012 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:36:42,012 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 579.09s
2025-11-23 04:36:42,012 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 579.09s
2025-11-23 04:36:42,014 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:39:48,189 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:39:48,190 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 324.60s
2025-11-23 04:39:48,190 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 324.60s
2025-11-23 04:39:48,191 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:40:27,231 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:40:27,232 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 326.41s
2025-11-23 04:40:27,232 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 326.41s
2025-11-23 04:40:27,233 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:40:39,593 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:40:39,593 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1704.30s
2025-11-23 04:40:39,593 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1704.30s
2025-11-23 04:40:39,594 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:41:17,827 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:41:17,828 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 38.23s
2025-11-23 04:41:17,828 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 38.23s
2025-11-23 04:41:17,829 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:41:44,963 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:41:44,963 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 77.73s
2025-11-23 04:41:44,964 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 77.73s
2025-11-23 04:41:44,964 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:44:48,179 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:44:48,179 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 486.16s
2025-11-23 04:44:48,179 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 486.16s
2025-11-23 04:46:03,284 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:46:03,284 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 258.32s
2025-11-23 04:46:03,284 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 258.32s
2025-11-23 04:46:03,285 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:46:34,898 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:46:34,898 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 876.41s
2025-11-23 04:46:34,898 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 876.41s
2025-11-23 04:46:34,899 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:46:50,987 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:46:50,987 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 422.79s
2025-11-23 04:46:50,987 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 422.79s
2025-11-23 04:46:50,988 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:48:58,774 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:48:58,775 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 460.94s
2025-11-23 04:48:58,775 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 460.94s
2025-11-23 04:48:58,776 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:49:36,413 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:49:36,413 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 37.64s
2025-11-23 04:49:36,413 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 37.64s
2025-11-23 04:49:36,414 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:50:42,445 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:50:42,445 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 247.54s
2025-11-23 04:50:42,445 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 247.54s
2025-11-23 04:50:42,446 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:51:14,353 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:51:14,353 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 956.08s
2025-11-23 04:51:14,354 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 956.08s
2025-11-23 04:51:14,354 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 04:53:43,206 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:53:43,206 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 412.22s
2025-11-23 04:53:43,206 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 412.22s
2025-11-23 04:56:56,129 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 04:56:56,129 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 373.68s
2025-11-23 04:56:56,129 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 373.68s
2025-11-23 04:56:56,130 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:01:06,627 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:01:06,627 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 690.21s
2025-11-23 05:01:06,627 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 690.21s
2025-11-23 05:01:06,628 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:01:09,657 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:01:09,657 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 253.53s
2025-11-23 05:01:09,657 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 253.53s
2025-11-23 05:01:09,658 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:03:21,988 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:03:21,988 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 135.36s
2025-11-23 05:03:21,988 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 135.36s
2025-11-23 05:03:21,989 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:04:16,266 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:04:16,266 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1092.98s
2025-11-23 05:04:16,266 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1092.98s
2025-11-23 05:04:16,268 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:04:41,017 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:04:41,017 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 211.36s
2025-11-23 05:04:41,017 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 211.36s
2025-11-23 05:04:41,019 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:06:43,366 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:06:43,366 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 929.01s
2025-11-23 05:06:43,366 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 929.01s
2025-11-23 05:06:43,367 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:10:19,192 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:10:19,193 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 338.17s
2025-11-23 05:10:19,193 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 338.17s
2025-11-23 05:10:19,194 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:10:42,662 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:10:42,662 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 386.39s
2025-11-23 05:10:42,662 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 386.39s
2025-11-23 05:10:42,663 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:10:48,154 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:10:48,154 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 28.96s
2025-11-23 05:10:48,154 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 28.96s
2025-11-23 05:10:48,155 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:13:40,776 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:13:40,776 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 618.79s
2025-11-23 05:13:40,776 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 618.79s
2025-11-23 05:17:25,525 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:17:25,525 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 402.86s
2025-11-23 05:17:25,525 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 402.86s
2025-11-23 05:17:25,527 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:18:07,639 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:18:07,639 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 439.48s
2025-11-23 05:18:07,640 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 439.48s
2025-11-23 05:18:07,641 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:21:35,280 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:21:35,281 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 249.75s
2025-11-23 05:21:35,281 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 249.75s
2025-11-23 05:21:35,282 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:24:19,643 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:24:19,643 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1056.27s
2025-11-23 05:24:19,643 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1056.27s
2025-11-23 05:24:19,644 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:25:48,040 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:25:48,040 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 460.40s
2025-11-23 05:25:48,040 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 460.40s
2025-11-23 05:25:48,042 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:25:55,625 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:25:55,625 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 260.34s
2025-11-23 05:25:55,625 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 260.34s
2025-11-23 05:25:55,626 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:32:15,212 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:32:15,212 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 387.17s
2025-11-23 05:32:15,212 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 387.17s
2025-11-23 05:32:15,214 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:36:36,576 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:36:36,577 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 640.95s
2025-11-23 05:36:36,577 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 640.95s
2025-11-23 05:36:36,578 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:38:18,727 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:38:18,727 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 839.08s
2025-11-23 05:38:18,727 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 839.08s
2025-11-23 05:38:18,728 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:39:54,659 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:39:54,660 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 198.08s
2025-11-23 05:39:54,660 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 198.08s
2025-11-23 05:39:54,661 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:45:31,698 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:45:31,698 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 337.04s
2025-11-23 05:45:31,698 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 337.04s
2025-11-23 05:45:31,700 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:45:45,755 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:45:45,755 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 447.03s
2025-11-23 05:45:45,755 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 447.03s
2025-11-23 05:45:45,757 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:52:58,358 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:52:58,358 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 432.60s
2025-11-23 05:52:58,359 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 432.60s
2025-11-23 05:52:58,360 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 05:53:36,589 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 05:53:36,590 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 38.23s
2025-11-23 05:53:36,590 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 38.23s
2025-11-23 05:53:36,590 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:00:09,980 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:00:09,981 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 878.28s
2025-11-23 06:00:09,981 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 878.28s
2025-11-23 06:00:09,982 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:01:12,033 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:01:12,033 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1736.82s
2025-11-23 06:01:12,033 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1736.82s
2025-11-23 06:01:12,034 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:04:10,767 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:04:10,767 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 240.78s
2025-11-23 06:04:10,767 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 240.78s
2025-11-23 06:04:10,768 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:08:09,413 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:08:09,413 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 417.37s
2025-11-23 06:08:09,413 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 417.37s
2025-11-23 06:08:09,414 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:09:06,864 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:09:06,865 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 57.45s
2025-11-23 06:09:06,865 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 57.45s
2025-11-23 06:09:06,865 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:09:42,698 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:09:42,698 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 331.93s
2025-11-23 06:09:42,698 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 331.93s
2025-11-23 06:09:42,699 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:15:03,341 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:15:03,341 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 320.64s
2025-11-23 06:15:03,341 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 320.64s
2025-11-23 06:15:03,342 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:16:44,305 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:16:44,305 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 457.44s
2025-11-23 06:16:44,305 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 457.44s
2025-11-23 06:16:44,306 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:19:22,189 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:19:22,189 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 258.85s
2025-11-23 06:19:22,190 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 258.85s
2025-11-23 06:23:38,199 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.61s
2025-11-23 06:23:38,199 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-23 06:23:38,200 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:27:46,969 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:27:46,969 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 662.66s
2025-11-23 06:27:46,970 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 662.66s
2025-11-23 06:41:47,038 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:41:47,039 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1088.84s
2025-11-23 06:41:47,039 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1088.84s
2025-11-23 06:41:47,042 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:45:17,716 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:45:17,717 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 210.67s
2025-11-23 06:45:17,717 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 210.67s
2025-11-23 06:45:17,718 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:46:41,325 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:46:41,325 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 83.61s
2025-11-23 06:46:41,325 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 83.61s
2025-11-23 06:46:41,326 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: kernel_generation
2025-11-23 06:52:00,222 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-23 06:52:00,223 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 318.89s
2025-11-23 06:52:00,223 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 318.89s
2025-11-25 01:30:38,694 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 01:53:06,102 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 01:53:06,102 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 1347.41s
2025-11-25 01:53:06,102 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 1347.41s
2025-11-25 01:53:06,103 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 01:53:21,130 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 01:53:21,130 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 15.03s
2025-11-25 01:53:21,130 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 15.03s
2025-11-25 01:53:21,131 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 02:04:43,008 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 02:04:43,009 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 681.88s
2025-11-25 02:04:43,011 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 681.88s
2025-11-25 02:04:43,013 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 02:14:44,964 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 02:14:44,965 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 601.95s
2025-11-25 02:14:44,965 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 601.95s
2025-11-25 02:14:44,967 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 02:17:10,983 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 02:17:10,984 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 146.02s
2025-11-25 02:17:10,984 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 146.02s
2025-11-25 02:17:10,986 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 02:47:12,597 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.61s
2025-11-25 02:47:12,598 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-25 08:54:37,964 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 08:54:40,906 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 08:54:40,906 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 2.94s
2025-11-25 08:54:40,906 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 2.94s
2025-11-25 08:54:40,907 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 08:58:16,887 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 08:58:16,888 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 215.98s
2025-11-25 08:58:16,888 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 215.98s
2025-11-25 08:58:16,889 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 08:58:16,982 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your request has 8321 input tokens. Please reduce the length of the input messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}}, response_time: 0.09s
2025-11-25 08:58:16,982 - llm_utils.client - ERROR - Unexpected error in LLM text completion: ContextWindowExceededError: litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your request has 8321 input tokens. Please reduce the length of the input messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-25 10:31:25,201 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:31:25,447 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your request has 8321 input tokens. Please reduce the length of the input messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}}, response_time: 0.25s
2025-11-25 10:31:25,447 - llm_utils.client - ERROR - Unexpected error in LLM text completion: ContextWindowExceededError: litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: Hosted_vllmException - Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your request has 8321 input tokens. Please reduce the length of the input messages.", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-25 10:34:23,259 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:37:25,881 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:37:25,881 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 182.62s
2025-11-25 10:37:25,881 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 182.62s
2025-11-25 10:37:25,882 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:39:33,261 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:39:33,261 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 127.38s
2025-11-25 10:39:33,261 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 127.38s
2025-11-25 10:39:33,262 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:41:58,226 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:41:58,226 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 144.96s
2025-11-25 10:41:58,226 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 144.96s
2025-11-25 10:41:58,228 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:42:01,744 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:42:01,744 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 3.52s
2025-11-25 10:42:01,744 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 3.52s
2025-11-25 10:42:01,746 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:43:21,732 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:43:21,733 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 79.99s
2025-11-25 10:43:21,733 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 79.99s
2025-11-25 10:43:21,734 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:43:25,098 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:43:25,099 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 3.36s
2025-11-25 10:43:25,099 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 3.36s
2025-11-25 10:43:25,100 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:46:13,609 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:46:13,609 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 168.51s
2025-11-25 10:46:13,609 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 168.51s
2025-11-25 10:46:13,610 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:46:18,903 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:46:18,904 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 5.29s
2025-11-25 10:46:18,904 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 5.29s
2025-11-25 10:46:18,905 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:49:26,240 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:49:26,241 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 187.33s
2025-11-25 10:49:26,241 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 187.33s
2025-11-25 10:49:26,242 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:52:22,693 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:52:22,694 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 176.45s
2025-11-25 10:52:22,694 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 176.45s
2025-11-25 10:52:22,695 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:55:29,045 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:55:29,045 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 186.35s
2025-11-25 10:55:29,045 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 186.35s
2025-11-25 10:55:29,046 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:56:04,650 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:56:04,651 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 35.60s
2025-11-25 10:56:04,651 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 35.60s
2025-11-25 10:56:04,652 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:58:14,228 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:58:14,228 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 129.58s
2025-11-25 10:58:14,228 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 129.58s
2025-11-25 10:58:14,229 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 10:58:24,278 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 10:58:24,279 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 10.05s
2025-11-25 10:58:24,279 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 10.05s
2025-11-25 10:58:24,280 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 11:01:34,375 - llm_utils.client - WARNING - Failed to calculate token costs for hosted_vllm/Qwen/QwQ-32B: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/QwQ-32B
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-11-25 11:01:34,375 - llm_utils.tracker - INFO - Tracked successful usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - tokens: 0, cost: $0.0000, response_time: 190.09s
2025-11-25 11:01:34,375 - llm_utils.client - INFO - LLM text completion successful: hosted_vllm/Qwen/QwQ-32B - tokens: 0, cost: $0.0000, response_time: 190.09s
2025-11-25 11:01:34,377 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
2025-11-25 11:31:35,996 - llm_utils.tracker - ERROR - Tracked failed usage: hosted_vllm/Qwen/QwQ-32B (hosted_vllm) - error: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out., response_time: 1801.62s
2025-11-25 11:31:35,996 - llm_utils.client - ERROR - Unexpected error in LLM text completion: Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
2025-11-25 12:06:59,103 - llm_utils.client - INFO - Making LLM text completion to hosted_vllm/Qwen/QwQ-32B (hosted_vllm) with tag: rule_extraction
