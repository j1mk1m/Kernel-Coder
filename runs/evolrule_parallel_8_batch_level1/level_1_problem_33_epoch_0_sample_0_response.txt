The code should follow these rules:

- The code should be compatible with PyTorch, using the torch.utils.cpp_extension.load_inline function to compile custom CUDA kernels.

- You may create any number of kernels, but you must implement at least one custom CUDA kernel.

- Your kernel must handle inputs with any dimensions (i.e., *any batch size, num_features, dim1, dim2, etc.).

- Your kernel should not use any PyTorch CUDA functions (like cublas, cub, etc.) in the kernel. You can use low-level CUDA runtime APIs.

- You can use any PyTorch functions outside the CUDA kernels (e.g., to compute mean and variance).

- The forward pass should be optimized, and the backward pass can be left unoptimized (i.e., you can let PyTorch handle the backward automatically). However, your forward pass must be correct and fast.

- The new ModelNew class should be functionally identical to the original Model class (i.e., produce the same outputs given the same inputs), except for possible minor numerical differences due to computation order.

- The kernel should be named with a suffix of `_kernel` and the launcher function should be named with a suffix of `_cuda`.

- You can use any of PyTorch's helper functions (e.g., `AT_DISPATCH_ALL_TYPES`, `TORCH_CHECK`, `cudaStreamDefault`, etc.)

- You can assume that the input tensor is contiguous and in CUDA memory (so no need to check for pinned memory or non-contiguous tensors; however, you can use `cudaMalloc`/`cudaMemcpy` if needed).

- Please inline the CUDA code in Python using the `load_inline` function as in the example.

- Please use the same input/output tensor shapes and data types as the original model.

Okay, I need to optimize the BatchNorm2d layer in PyTorch by replacing its forward pass with a custom CUDA kernel. Let me think through how to approach this.

First, the BatchNorm2d's forward pass involves several steps: computing the mean and variance of the input, then normalizing the input using these statistics, scaling by gamma, adding beta. The standard implementation uses mean and variance over the batch and spatial dimensions.

But the problem says to implement a custom CUDA kernel for the forward pass. Since the user wants at least one custom kernel, I need to implement the computation steps in CUDA.

Wait, but how does the standard BatchNorm work? The forward pass for training is:

1. Compute mean over the batch and spatial dimensions (for each channel)
2. Compute variance (unbiased estimator with Bessel's correction if track_running_stats)
3. Normalize the input using these mean and var
4. Scale by gamma and add beta.

But in the forward pass, the batch statistics are used, and the running stats are updated.

But the problem says the new ModelNew should be functionally identical except for minor numerical differences. So the custom kernel must compute exactly the same steps as the original.

Wait, but the original code uses nn.BatchNorm2d, which in training mode computes mean and variance over the batch and spatial dimensions. The kernel must do the same.

However, implementing the mean and variance computation in CUDA might be tricky because they require reductions across multiple dimensions. For a 4D tensor (N, C, H, W), the mean and variance are computed per channel (C), so for each channel c, compute the mean over N, H, W dimensions.

Hmm, to compute the mean for each channel, I need to sum all elements in each channel and divide by the number of elements (N*H*W). Similarly for variance.

The problem is that for a custom kernel, doing reductions like sum across all elements except the channel dimension is not straightforward. Maybe the best approach is to compute the mean and variance first using PyTorch's functions (since they can be faster) and then perform the normalization in a custom kernel. But the problem allows using PyTorch functions outside the kernels except for low-level CUDA functions inside kernels.

Wait, the problem says:

- "Your kernel must handle inputs with any dimensions (i.e., *any batch size, num_features, dim1, dim2, etc.)."

- "You can use any PyTorch functions outside the CUDA kernels (e.g., to compute mean and variance)."

Ah, so the plan is:

1. Compute mean and variance using PyTorch's functions (since those are optimized, and we can use them outside the kernel).

2. Then, the actual normalization, scaling, and shifting can be done in a custom CUDA kernel.

This way, the kernel doesn't have to do the reduction steps, which are more complex. Let's proceed this way.

So the steps would be:

In the forward pass:

- Compute mean and variance using PyTorch's functions (since they are allowed outside the kernel). For example, for the input x of shape (N, C, H, W):

mean = x.mean([0, 2, 3], keepdim=True)

var = x.var([0, 2, 3], unbiased=False, keepdim=True)

Wait, PyTorch's BatchNorm uses unbiased=False (i.e., variance is computed with Bessel's correction only when track_running_stats is on? Wait, actually, the variance in training is computed with unbiased=True (using N-1), but in the denominator, but PyTorch's var function has unbiased=True by default, so maybe to replicate the behavior, the variance should be computed with unbiased=False? Wait, the formula for variance in BatchNorm is: var = E[(x - mean)^2], so if unbiased=False, then it uses N (the size of the sample) as the denominator. Wait, actually, when computing the unbiased variance estimator, you use N-1, but for the batch norm, the denominator is N*H*W for each channel. Let me check:

In PyTorch's BatchNorm2d, the variance is computed with unbiased=True (since the default in var is unbiased=True, which uses N-1). Wait, let me confirm. The documentation says: variance is computed as var = (x - mean(x)).pow(2).mean() * N/(N-1) if unbiased is True. Wait no, actually, the formula is variance = sum((x_i - mean)^2) / (N - unbiased), where unbiased is 1 if unbiased=True, 0 otherwise. So for the variance in BatchNorm, since the running variance is computed with unbiased=False (because it's an unbiased estimator when using the population variance), but during training, the mini-batch variance uses unbiased=True (N-1 in denominator). Wait, perhaps I should check the actual implementation.

Alternatively, perhaps the problem allows us to use PyTorch's functions to compute mean and variance, so that we can just call torch.mean and torch.var, then pass those into the kernel.

But let's proceed.

Once we have the mean and variance, then the normalization is:

x_hat = (x - mean) / sqrt(var + eps)

Then the output is gamma * x_hat + beta.

The gamma and beta are the learnable parameters of BatchNorm2d. So in the Model class, self.bn has parameters weight (gamma) and bias (beta), and eps.

The custom kernel can take as inputs x, mean, var, gamma, beta, and eps, and compute the normalized result.

Wait, but in the custom ModelNew, how do we get the parameters? Since we are replacing the BatchNorm2d layer with our own implementation, we need to create the parameters ourselves.

Wait, the original Model uses a nn.BatchNorm2d layer. So in ModelNew, instead of using nn.BatchNorm2d, we can implement the forward pass ourselves, including the parameters.

Therefore, in ModelNew, we need to define the parameters gamma (weight) and beta (bias), and eps. The running mean and variance are tracked, but since the problem says the forward pass is to be optimized and the backward can be left to PyTorch, perhaps we can ignore the running stats for now? Wait, no. The original model's forward pass in training mode uses the batch statistics, and updates the running stats. Since the problem states that the new model must be functionally identical except for minor numerical differences, we need to compute the running mean and variance as well.

Wait, but the problem says "Your kernel must handle inputs with any dimensions". The running mean and variance are per-channel and are updated during training. To compute them, we can use PyTorton's mean function over the batch and spatial dimensions, similar to the batch mean.

Therefore, the steps for the forward pass in ModelNew would be:

1. Compute the mean and variance over the batch and spatial dimensions (for each channel).

2. Normalize the input using these statistics, apply gamma and beta.

3. Update the running_mean and running_var attributes if in training mode and the module is tracking running stats.

However, implementing all of this in a custom kernel might be complex, but perhaps we can separate the computation into parts.

The problem requires that the forward pass is optimized. Let's see:

The core computation (the normalization and scaling) can be done in a single kernel. The mean and variance computation can be done via PyTorch's functions, which are already optimized, so no need to reimplement those in CUDA.

Therefore, the plan is:

- In the ModelNew's forward pass:

   a. Compute mean and variance using PyTorch's functions (allowed).

   b. Use a custom CUDA kernel to perform the normalization (x - mean)/sqrt(var + eps), then multiply by gamma and add beta.

   c. Update the running mean and var as needed (using PyTorch's functions).

Therefore, the custom kernel will handle the element-wise operations after the mean/var are computed.

Now, writing the CUDA kernel.

The kernel must take the input tensor x, mean, var, gamma, beta, and eps, and output the result.

Wait, the parameters gamma and beta are the weight and bias of the BatchNorm layer. The mean and var are computed for each channel.

So for a tensor of shape (N, C, H, W), the mean and var are of shape (1, C, 1, 1). So each element in the channel c is subtracted by mean[0, c, 0, 0], divided by sqrt(var[0, c, 0, 0] + eps), then multiplied by gamma[c], then add beta[c].

The kernel needs to process each element of the input tensor, applying these operations.

The kernel can be structured as follows:

Each thread processes one element of the input. The thread index can be computed as a 1D index, then mapped to 4D indices (n, c, h, w). Alternatively, since the mean and var are per-channel, the c index is sufficient.

Wait, but the mean and var are broadcastable. So for each element (n, c, h, w), the mean is at [0, c, 0, 0], so the c index is needed. Therefore, the kernel can compute the channel of the current element and perform the computation.

The kernel code:

__global__ void batchnorm_forward_kernel(
    const float* x_data, const float* mean_data, const float* var_data,
    const float* gamma_data, const float* beta_data, float eps,
    float* y_data, int N, int C, int H, int W) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= N * C * H * W) return;

    // Compute 4D indices
    int w = index % W;
    int h = (index / W) % H;
    int c = (index / (W * H)) % C;
    int n = index / (C * W * H);

    float x_val = x_data[index];
    float mean_val = mean_data[c]; // because mean is (1, C, 1, 1), so flattened, the c-th element is mean_data[c]
    float var_val = var_data[c];
    float gamma_val = gamma_data[c];
    float beta_val = beta_data[c];

    float norm = (x_val - mean_val) / sqrt(var_val + eps);
    y_data[index] = gamma_val * norm + beta_val;
}

Wait, but the mean and var tensors are stored in a 4D tensor, but when flattened, their data is contiguous. Wait, the mean tensor shape is (1, C, 1, 1). So the data is stored as a 1D array where the elements are ordered as [mean_0, mean_1, ..., mean_{C-1}]. Wait, no. Let me see:

In C order (row-major), for a tensor with shape (1, C, 1, 1), the strides would be such that the first dimension (size 1) doesn't contribute, then the second dimension (C), then the third and fourth (each 1). Therefore, the data for mean is stored as a contiguous array where the elements are ordered as mean[0,0,0,0], mean[0,1,0,0], ..., mean[0,C-1,0,0]. Therefore, for channel c, mean_data[c] gives the correct value. Similarly for var.

Therefore, in the kernel, the mean_val can be accessed as mean_data[c], since the tensor is contiguous and the channel is c.

This way, the kernel can efficiently compute the normalized value.

The launcher function will need to take the tensors, extract their data pointers, and compute the grid and block dimensions.

Now, the launcher function:

torch::Tensor batchnorm_forward_cuda(
    torch::Tensor x, torch::Tensor mean, torch::Tensor var,
    torch::Tensor gamma, torch::Tensor beta, float eps) {

    // Check dimensions and devices
    TORCH_CHECK(x.is_cuda(), "x must be on CUDA");
    TORCH_CHECK(mean.is_cuda(), "mean must be on CUDA");
    TORCH_CHECK(var.is_cuda(), "var must be on CUDA");
    TORCH_CHECK(gamma.is_cuda(), "gamma must be on CUDA");
    TORCH_CHECK(beta.is_cuda(), "beta must be on CUDA");
    TORCH_CHECK(mean.sizes() == torch::IntArrayRef({1, x.size(1), 1, 1}), "mean has incorrect shape");
    TORCH_CHECK(var.sizes() == mean.sizes(), "var has incorrect shape");
    TORCH_CHECK(gamma.sizes() == torch::IntArrayRef({x.size(1)}), "gamma has incorrect shape");
    TORCH_CHECK(beta.sizes() == gamma.sizes(), "beta has incorrect shape");

    auto output = torch::empty_like(x);

    int N = x.size(0);
    int C = x.size(1);
    int H = x.size(2);
    int W = x.size(3);
    int total_elements = N * C * H * W;

    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;

    batchnorm_forward_kernel<<<grid_size, block_size, 0, at::cuda::getCurrentCUDAStream()>>>(
        x.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        eps,
        output.data_ptr<float>(),
        N, C, H, W
    );

    return output;
}

Wait, but the CUDA kernel must be in the source code. Let's structure the CUDA code accordingly.

Now, the ModelNew class needs to have the parameters gamma and beta, and eps. The original Model has a BatchNorm2d layer, so in ModelNew, we can replicate the parameters by defining:

class ModelNew(nn.Module):
    def __init__(self, num_features):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.eps = 1e-5  # same as default in BatchNorm2d

        # Also need to track running_mean and running_var, and momentum
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.momentum = 0.1  # default value

Wait, but the original Model uses the default parameters of BatchNorm2d, which include momentum=0.1, eps=1e-5, track_running_stats=True, etc. So to be functionally identical, ModelNew must replicate all these parameters.

Therefore, in the __init__ method of ModelNew, we need to initialize the parameters as:

    self.weight = nn.Parameter(torch.ones(num_features))
    self.bias = nn.Parameter(torch.zeros(num_features))
    self.register_buffer('running_mean', torch.zeros(num_features))
    self.register_buffer('running_var', torch.ones(num_features))
    self.eps = 1e-5
    self.momentum = 0.1

Then, in the forward function:

def forward(self, x):
    # Compute mean and variance over the batch and spatial dimensions
    # For a 4D tensor (N, C, H, W), the dims to reduce are 0, 2, 3
    mean = x.mean([0, 2, 3], keepdim=True)
    # variance with unbiased=False (since the default var in PyTorch uses unbiased=True, but BatchNorm uses Bessel's correction only when track_running_stats is on?)
    # Wait, in PyTorch's BatchNorm2d, the variance is computed with unbiased=True (using N-1 in denominator), so we need to set unbiased=True here.
    var = x.var([0, 2, 3], unbiased=True, keepdim=True)

    # compute the normalized output using the custom CUDA kernel
    y = batchnorm_forward_cuda(x, mean, var, self.weight, self.bias, self.eps)

    # update the running statistics if in training mode
    if self.training:
        with torch.no_grad():
            # Update running_mean and running_var using momentum
            exponential_average_factor = 0.0
            if self.momentum is None:  # use cumulative moving average
                exponential_average_factor = 1.0 / float(self.num_batches_tracked + 1)
            else:  # use exponential moving average
                exponential_average_factor = self.momentum

            self.running_mean = self.running_mean * (1 - exponential_average_factor) + mean.squeeze() * exponential_average_factor
            self.running_var = self.running_var * (1 - exponential_average_factor) + var.squeeze() * exponential_average_factor

    return y

Wait, but the problem says that the backward can be left unoptimized. However, the parameters (weight and bias) are learnable, so their gradients must be computed correctly. Since the custom kernel is used only for the forward pass, the backward will be handled by autograd, which requires that the forward's computation is differentiable. Since the custom kernel is written correctly (the operations are differentiable), autograd should automatically compute the gradients through the kernel.

However, the custom kernel must be written in such a way that the computation graph is properly tracked. Since we are using PyTorch tensors and the kernel is a function that can be wrapped with autograd, but to do that, we need to register the backward function, but since we are not implementing the backward, perhaps we can let PyTorch compute it via the forward's operations.

Wait, but the problem states that the backward can be left unoptimized, so we can rely on PyTorch's automatic differentiation. However, for that to work, the custom CUDA kernel must have its operations recorded in the autograd graph. Therefore, the kernel must be wrapped in a way that it's part of the autograd function.

Ah, right! The problem says that the backward can be left unoptimized, so perhaps we can let PyTorch compute the backward automatically. However, the custom kernel needs to be wrapped in a way that the gradients are computed. To do this, we can use a custom autograd.Function that calls the CUDA kernel. Alternatively, we can use a wrapper function that uses the kernel and allows gradients to flow.

Wait, but when using the kernel directly like in the example, the autograd graph will not track the computation through the kernel unless we explicitly create a Function for it. Because otherwise, the kernel is treated as a pure function, and autograd has no way to know how to compute the gradients. So this is a critical point.

Therefore, the kernel must be wrapped in a PyTorch extension that provides the backward pass, or alternatively, the kernel's computation must be expressed in terms of differentiable operations that PyTorch can track.

Hmm, this complicates things because if we don't implement the backward, then PyTorch won't be able to compute gradients through the kernel, leading to errors. So, this is a problem.

Wait, but the problem says: "the backward pass can be left unoptimized (i.e., you can let PyTorch handle the backward automatically). However, your forward pass must be correct and fast."

Wait, perhaps the user is okay with not optimizing the backward, but the forward must be correct. However, the backward must still work. So, the custom kernel's computation must be differentiable.

Therefore, the way to proceed is to wrap the kernel in a PyTorch Function that can compute the forward and allow the backward to be computed by PyTorch's autograd. But the problem allows us to leave the backward unoptimized, so perhaps we can just let PyTorch compute the gradients through the forward's mathematical operations (even though they are done in a kernel), but for that, the kernel must be considered as a function whose gradient is computed via autograd's engine.

Alternatively, perhaps the kernel is written in a way that the operations are recorded in the autograd graph. Wait, but when you call a CUDA kernel, it's a pure function and not part of the computational graph. So, in order for autograd to track it, we need to create a Function that wraps the kernel and registers the inputs and outputs so that gradients can be computed.

Therefore, to properly integrate the kernel into the autograd system, we need to:

1. Define a custom autograd.Function that implements the forward pass using the kernel, and the backward pass (which can be left unoptimized).

2. The backward function can be implemented using PyTorch's built-in functions or left to autograd.

Alternatively, we can implement the forward as a separate function and let PyTorch's autograd track it via the inputs.

Wait, but the kernel is a pure function. Therefore, the only way for autograd to track it is to have the kernel be part of a Function that records the inputs and outputs, so that gradients can be computed.

Therefore, the approach would be:

Implement the forward in a custom Function where the CUDA kernel is the forward, and the backward is implemented via PyTorch's autograd (i.e., the backward is not optimized, but functional).

Therefore, the CUDA kernel's code must be part of a Function's forward method.

So, modifying the previous code:

The custom CUDA kernel is part of a PyTorch extension. The function batchnorm_forward_cuda is a function that can be called, but to integrate into autograd, we need to wrap it.

Alternatively, perhaps the code can be written using the extension, and the forward function uses the kernel, and the backward is handled via autograd's implicit gradients.

Wait, but in the example provided earlier (the elementwise_add example), they just call the CUDA function directly in the forward, but that might not be sufficient for autograd. Wait, actually, in the example, they are adding two tensors using a custom kernel, but the autograd would not track that operation unless the kernel is part of a Function with proper gradient hooks.

Hmm, perhaps the example is incomplete, but in practice, to make the kernel's computation part of the autograd graph, we need to use a Function.

Therefore, in this case, the correct approach is:

Create a custom PyTorch Function (subclassing torch.autograd.Function) that implements the forward using the CUDA kernel, and the backward using PyTorch's autograd.

But the problem says that the backward can be left unoptimized, so perhaps the backward can be implemented via the autograd's automatic differentiation.

Therefore, the steps:

1. Define the CUDA kernel and its launcher as before.

2. Create a Function that calls the CUDA launcher in its forward method, and let the backward be handled by autograd.

Wait, but how does PyTorch know how to compute the gradients? The CUDA kernel's computation must be represented in a way that allows PyTorch to compute gradients. Since the kernel is a custom operation, the backward must be defined manually, unless the kernel can be expressed in terms of existing PyTorch operations that are differentiable.

Alternatively, perhaps the kernel is written in such a way that the operations are all element-wise and can be differentiated using the autograd's built-in rules, even though they are in a kernel. But I think that's not possible unless the Function is properly registered with gradients.

Hmm, this is getting complicated. Let me think again.

The problem states that the forward pass must be correct and fast, and the backward can be left unoptimized. It also says that the new model must be functionally identical except for minor numerical differences. Therefore, the backward should still compute the gradients correctly, even if not optimized. Hence, the custom kernel's forward must be part of the computational graph so that gradients can be computed.

Therefore, the correct approach is to wrap the CUDA kernel in a Function that provides the forward, and allows the backward to be computed by PyTorch. To do that, the backward can be left as PyTorch's automatic differentiation, which will compute the gradients based on the mathematical operations performed in the kernel.

To enable this, the Function's forward must return the output tensor, and the backward must be implemented (even if it's a placeholder that lets autograd compute it). Alternatively, the Function's forward can be written in terms of differentiable operations so that autograd can track them.

Wait, but the CUDA kernel is a custom operation. So unless the Function's forward is implemented using existing PyTorch functions (like mean, var, etc.), the autograd won't know how to compute gradients. Therefore, this suggests that the kernel's computation must be expressed in terms of differentiable PyTorch operations, but that would negate the need for the kernel.

Alternatively, perhaps the Function's forward can call the CUDA kernel, and then the backward can be implemented via PyTorch's autograd, but that requires that the kernel's computation is recorded in the graph somehow.

Hmm, perhaps the only way to make this work is to have the kernel's operations be part of the computational graph. To do that, the Function's forward must return tensors that have the necessary gradient information. Since the kernel is a custom operation, we must define the backward in the Function's backward method, even if it's a naive implementation.

Alternatively, we can use the autograd.Function's backward function to compute the gradients using PyTorch's functions, which may not be as optimized, but still correct.

This complicates the code, but is necessary for the model to work properly.

Alternatively, maybe the problem allows us to ignore the backward pass's optimization and just have the forward pass correct. Wait, the problem says:

"Your forward pass must be correct and fast. The backward pass can be left unoptimized (i.e., you can let PyTorch handle the backward automatically). However, your forward pass must be correct and fast."

So, the backward can be left as is, but the forward must be correct. However, the backward must still compute the gradients correctly. So the forward must be part of the autograd graph.

Therefore, the only way is to have the CUDA kernel's forward be part of the autograd graph, which requires the use of a Function.

Therefore, here's the plan:

Implement the forward in a custom autograd.Function, where the forward calls the CUDA kernel. The backward can be implemented using PyTorch's autograd, which requires that the Function's forward is written in a way that the operations are differentiable. Alternatively, the backward can be left unimplemented (so that autograd will throw an error), but that's not acceptable.

Alternatively, we can let the Function's backward be automatically computed by autograd. To do this, the Function's forward must be written using PyTorch operations that are differentiable, but the kernel is a custom operation. So perhaps the Function's forward is written using the kernel's computation, but the autograd can't compute the gradient unless we provide the backward.

This is a problem. Therefore, perhaps the better approach is to write the forward using PyTorch operations except for the element-wise computation, and replace only that part with a CUDA kernel.

Wait, perhaps the core computation (the per-element calculation) can be done in the kernel, while the mean and variance are computed via PyTorch's functions, which are tracked in the autograd graph.

Wait, let me think:

Suppose the Function's forward is:

def forward(ctx, x, mean, var, gamma, beta, eps):
    # compute the output using the CUDA kernel
    output = batchnorm_forward_cuda(x, mean, var, gamma, beta, eps)
    ctx.save_for_backward(x, mean, var, gamma, beta, output)
    return output

Then, in the backward:

def backward(ctx, grad_output):
    # compute the gradients w.r. to x, mean, var, gamma, beta
    # this would require implementing the gradients manually, which is time-consuming

Alternatively, maybe the backward can be computed via autograd by expressing the computation in terms of PyTorch functions, but that would require not using the kernel. So this is a dilemma.

Alternatively, perhaps the problem allows us to omit the backward pass's implementation and just focus on the forward, assuming that the user will not need gradients. But the original model's BatchNorm does compute gradients, so the new model must as well.

Hmm, this is a critical issue. Without a backward function, the model won't be trainable. Therefore, the Function must implement the backward, even if it's a naive implementation.

Alternatively, maybe the CUDA kernel's computation can be expressed in a way that autograd can automatically differentiate through it. However, since the kernel is a custom operation, this requires that the Function's forward is registered with a backward.

Given the time constraints, perhaps the problem expects us to proceed with writing the forward pass with the kernel, and just not worry about the backward's implementation, assuming that the problem allows it as the backward is left unoptimized. But in practice, without a backward, the code would fail.

Alternatively, perhaps the Function's backward can be left to be automatically computed via the kernel's mathematical expression, but that requires that the kernel is written in terms of differentiable operations that are tracked.

Alternatively, perhaps the problem is designed in such a way that the backward can be handled by the autograd system even without a custom backward function, provided the forward is written in terms of PyTorch's functions.

Wait, in the example given, the elementwise_add example uses a custom kernel for addition. If you have a = a + b, then the gradient would be straightforward. The example's kernel is just adding two tensors. The autograd would need to know that the output is a + b. But the example's code just calls the kernel directly in the forward, and the autograd would have no record of that operation. Therefore, that example might not actually work for training, unless there is some other mechanism.

Wait, that's a problem. The example provided might not actually work for training because the autograd graph doesn't track the kernel's computation. So perhaps the example is incomplete or assumes that the kernel is part of a Function.

Therefore, in the correct code, the custom kernel must be part of an autograd.Function to allow gradients to be computed.

Given that, I need to adjust the code accordingly.

Let me structure this properly.

First, the CUDA kernel and its launcher are defined as before. Then, a custom Function is defined that uses the kernel in the forward, and computes the backward via PyTorch's autograd or a manual implementation.

Alternatively, perhaps the backward can be left unimplemented, but the problem states that the forward must be correct and the backward can be left unoptimized. Therefore, the backward can be left to autograd, but that requires the kernel's computation to be expressed in terms of differentiable operations.

Alternatively, perhaps the Function's backward can call PyTorch's built-in operations to compute the gradients. For example:

class BatchNormFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, mean, var, gamma, beta, eps):
        # call the CUDA kernel
        output = batchnorm_forward_cuda(x, mean, var, gamma, beta, eps)
        ctx.save_for_backward(x, mean, var, gamma, beta, output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        # compute the gradients with respect to x, gamma, beta
        # (mean and var are intermediate values, so their gradients are summed into the inputs)
        # this requires a lot of code, but perhaps we can let PyTorch compute it automatically

        # Alternatively, we can recompute the gradients using PyTorch's functions

        x, mean, var, gamma, beta, output = ctx.saved_tensors

        # compute dL/dx, dL/dgamma, dL/dbeta

        # The backward for batch norm is known, but implementing it would take time.

        # The gradients are as follows:

        # Let's denote:
        # y = (x - mean) / sqrt(var + eps) * gamma + beta
        # So output = y

        # The gradient w.r. to x is (grad_output * gamma) / sqrt(var + eps)

        # The gradient w.r. to gamma is sum over all but channel dim of (x - mean)/sqrt(var + eps) * grad_output

        # The gradient w.r. to beta is sum over all but channel dim of grad_output

        # The gradients w.r. to mean and var are more complex, but since mean and var are intermediate variables computed from x,
        # the chain rule would propagate the gradients through them back to x.

        # To compute the gradients properly, we need to compute:

        # Let's denote:
        # m = x.shape[0] * x.shape[2] * x.shape[3] (the number of elements per channel)

        # Then:

        # grad_gamma = sum over (output - mean) / sqrt(var + eps) * grad_output
        # grad_beta = sum over grad_output
        # grad_x = (gamma / sqrt(var + eps)) * (grad_output - (grad_gamma / m) - (grad_beta / m) * (output - mean)/sqrt(var + eps))

        # This is getting complex. To compute this correctly would take a lot of code.

        # Alternatively, use PyTorch's autograd to compute the gradients, but that would require re-doing the computation in a way that autograd can track.

        # This is getting too involved for the current problem, but perhaps the problem expects us to proceed without the backward implementation.

        # For the sake of time, perhaps we can proceed by leaving the backward to be computed via PyTorch's autograd by using the Function's forward as a pure function that uses PyTorch operations, but that would negate the need for the kernel.

        # Alternatively, perhaps the problem allows us to proceed with the forward and ignore the backward (even though it's required), but the problem states that the backward can be left unoptimized, implying that it should work but not be optimized.

        # Given the time constraints, perhaps I should proceed with writing the Function and its forward, and leave the backward as a TODO, but the problem requires the code to be functional.

        # Alternatively, perhaps the problem assumes that the kernel's forward is part of the computational graph by using the Function properly.

        # Given that, I'll proceed with the Function and the forward, and the backward will be left as a simple pass-through (which would be incorrect but to see if the code can be written).

        # Alternatively, perhaps the problem expects that the kernel's output is directly returned as a tensor that autograd can track.

        # Wait, perhaps the Function's forward is written using the kernel, and the backward can be computed by autograd by treating the kernel's output as a function of its inputs. Since the kernel is a mathematical function, the autograd could theoretically compute the gradients via the saved variables and the mathematical expressions.

        # However, in practice, the Function must have a backward method. So, I have to write it.

        # Let's proceed with the Function and implement the backward manually.

        # Let me try to code it.

        # First, save the necessary variables.

        # Let me compute the necessary gradients.

        # The backward function receives grad_output (the gradient w.r. to the output), and must return the gradients w.r. to each input of the forward function.

        # The inputs to the forward are x, mean, var, gamma, beta, eps (but eps is a scalar, so no gradient needed).

        # The outputs are output.

        # The gradients are:

        # dL/dx, dL/dmean, dL/dvar, dL/dgamma, dL/dbeta, dL/deps (which is zero)

        # But mean and var are computed from x, so their gradients will contribute to the gradient of x.

        # To compute the gradients properly, we need to apply the chain rule through mean and var.

        # This is quite involved. Let me see.

        # Let me denote:

        # x: input tensor

        # mean: computed as mean = x.mean(axes)

        # var: computed as var = x.var(axes)

        # output = (x - mean)/sqrt(var + eps) * gamma + beta

        # Let me denote:

        # inv_std = 1 / sqrt(var + eps)

        # x_hat = (x - mean) * inv_std

        # output = gamma * x_hat + beta

        # Then:

        # dL/dx = dL/doutput * doutput/dx

        # doutput/dx = gamma * inv_std - (gamma * inv_std^3) * (x - mean) * (d/dx (sum(x)/m)) ??

        # Alternatively, better to compute the gradients step by step.

        # Let's denote m = number of elements per channel (for a 4D tensor, m = N*H*W)

        # The derivative of mean with respect to x is 1/m for each element.

        # The derivative of var with respect to x is 2*(x - mean)/m ?

        # This is getting too complex. Given time constraints, perhaps the problem expects us to focus on the forward and assume the backward is handled by the autograd.

        # Alternatively, perhaps the backward can be left unimplemented, but that would cause an error. So, perhaps the problem expects us to proceed with the forward and not the backward, but the code must work.

        # Alternatively, maybe the kernel's computation can be expressed as a series of PyTorch operations, so that autograd can track them, and the kernel is just a fast implementation of those operations.

        # For example, the element-wise computation can be done in the kernel, but mean and var are computed via PyTorch's functions, which are tracked in the graph. The rest is element-wise operations that can be tracked.

        # Wait, in the forward function of the model, the mean and variance are computed using PyTorch's mean and var functions, which are differentiable. Then, the kernel is used to compute the element-wise operations, which are a combination of sub, sqrt, div, mul, add. So perhaps if those operations are done via PyTorch functions, then autograd can track them, but using the kernel would prevent that.

        # Therefore, to make the kernel's computation part of the autograd graph, the kernel must be wrapped in a Function whose forward is the kernel, and the backward is computed via PyTorch's autograd by expressing the operations in terms of differentiable functions.

        # This is a contradiction unless the kernel's operations are expressed in a way that autograd can track, which would require using PyTorch functions, not a kernel.

        # Hmm, I'm stuck here. Given the time, perhaps I should proceed with the code as per the forward, and assume that the backward is handled via autograd by the Function's forward, even if it requires implementing the backward.

        # Let's proceed step by step.

First, define the CUDA kernel and its launcher as before.

Then, create a custom Function:

class BatchNormFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, mean, var, gamma, beta, eps):
        output = batchnorm_forward_cuda(x, mean, var, gamma, beta, eps)
        ctx.save_for_backward(x, mean, var, gamma, beta, output)
        ctx.eps = eps
        return output

    @staticmethod
    def backward(ctx, grad_output):
        # Compute the gradients
        x, mean, var, gamma, beta, output = ctx.saved_tensors
        eps = ctx.eps

        # Compute the gradients for x, gamma, beta
        # We need to compute dL/dx, dL/dgamma, dL/dbeta

        # First compute the necessary intermediate variables
        N, C, H, W = x.size()
        m = N * H * W  # number of elements per channel

        inv_var_eps = 1.0 / (var + eps)
        inv_std = torch.rsqrt(var + eps)

        x_mu = x - mean
        x_hat = x_mu * inv_std

        # Gradients w.r. to gamma and beta
        dgamma = torch.sum(grad_output * x_hat, dim=[0, 2, 3], keepdim=True).squeeze()
        dbeta = torch.sum(grad_output, dim=[0, 2, 3], keepdim=True).squeeze()

        # Gradient w.r. to x
        dgamma_reshaped = dgamma.view(1, C, 1, 1)
        dbeta_reshaped = dbeta.view(1, C, 1, 1)

        d_x_hat = grad_output * gamma.view(1, C, 1, 1)
        d_inv_std = torch.sum(d_x_hat * x_mu, dim=[0, 2, 3], keepdim=True)
        d_inv_std = d_inv_std * inv_var_eps  # derivative of inv_std w.r. to var ?

        # Wait, this is getting too complex. Let me recall the standard backward formula for batch norm.

        # According to the batch norm paper, the gradients are:

        # dx_hat = grad_output * gamma
        # dgamma = sum(grad_output * x_hat)
        # dbeta = sum(grad_output)
        # dx = dx_hat / std + ... terms from the mean and variance.

        # The full formula is:

        # Let me look up the derivation:

        # From the batch norm paper's appendix:

        # The gradient with respect to x is computed as:

        # dx = (gamma / std) * (grad_output - mean(grad_output) - x_hat * mean(grad_output * x_hat))

        # So:

        dx_hat = grad_output * gamma.view(1, C, 1, 1)
        dgamma = (x_hat * grad_output).sum([0, 2, 3], keepdim=True).squeeze()
        dbeta = grad_output.sum([0, 2, 3], keepdim=True).squeeze()

        # Compute the mean of dx_hat and dx_hat * x_hat
        mean_dx_hat = dx_hat.mean((0, 2, 3), keepdim=True)
        mean_dxhat_xhat = (dx_hat * x_hat).mean((0, 2, 3), keepdim=True)

        # Compute dx
        dx = (dx_hat - mean_dx_hat - x_hat * mean_dxhat_xhat) / torch.sqrt(var + eps)

        return dx, None, None, dgamma, dbeta, None

        # The None's are for the gradients of mean, var, and eps (which are not learnable).

This is an approximate implementation of the backward pass based on the batch norm's gradient formulas. This may not be completely accurate, but it's a start.

Now, putting it all together:

The CUDA kernel and launcher are as before, but wrapped in the Function.

The ModelNew class uses this Function.

The full code would look like this:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for BatchNorm forward
batchnorm_forward_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void batchnorm_forward_kernel(
    const scalar_t* x_data,
    const scalar_t* mean_data,
    const scalar_t* var_data,
    const scalar_t* gamma_data,
    const scalar_t* beta_data,
    scalar_t eps,
    scalar_t* y_data,
    int N, int C, int H, int W
) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= N * C * H * W) return;

    // Compute 4D indices
    int w = index % W;
    int h = (index / W) % H;
    int c = (index / (W * H)) % C;
    int n = index / (C * W * H);

    scalar_t x_val = x_data[index];
    scalar_t mean_val = mean_data[c];
    scalar_t var_val = var_data[c];
    scalar_t gamma_val = gamma_data[c];
    scalar_t beta_val = beta_data[c];

    scalar_t inv_std = 1.0 / sqrt(var_val + eps);
    scalar_t norm = (x_val - mean_val) * inv_std;
    y_data[index] = gamma_val * norm + beta_val;
}

std::tuple<torch::Tensor> batchnorm_forward_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor gamma,
    torch::Tensor beta,
    double eps
) {
    auto output = torch::empty_like(x);

    const int threads = 256;
    const int elements = x.numel();
    const int blocks = (elements + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(x.type(), "batchnorm_forward_cuda", ([&] {
        batchnorm_forward_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(
            x.data<scalar_t>(),
            mean.data<scalar_t>(),
            var.data<scalar_t>(),
            gamma.data<scalar_t>(),
            beta.data<scalar_t>(),
            eps,
            output.data<scalar_t>(),
            x.size(0), x.size(1), x.size(2), x.size(3)
        );
    }));

    return output;
}
"""

batchnorm_forward_cpp_source = R"""
torch::Tensor batchnorm_forward_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor gamma,
    torch::Tensor beta,
    double eps
);
"""

# Compile the CUDA code
batchnorm_forward = load_inline(
    name="batchnorm_forward",
    cpp_sources=batchnorm_forward_cpp_source,
    cuda_sources=batchnorm_forward_source,
    functions=["batchnorm_forward_cuda"],
    verbose=True,
)

class BatchNormFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, mean, var, gamma, beta, eps):
        output = batchnorm_forward.batchnorm_forward_cuda(x, mean, var, gamma, beta, eps)
        ctx.save_for_backward(x, mean, var, gamma, beta, output)
        ctx.eps = eps
        return output

    @staticmethod
    def backward(ctx, grad_output):
        x, mean, var, gamma, beta, output = ctx.saved_tensors
        eps = ctx.eps

        N, C, H, W = x.size()
        m = N * H * W

        # Compute intermediate values
        x_mu = x - mean
        inv_var_eps = 1.0 / (var + eps)
        inv_std = torch.rsqrt(var + eps)
        x_hat = x_mu * inv_std.view(1, C, 1, 1)

        # Compute gradients for gamma and beta
        dgamma = (x_hat * grad_output).sum(dim=[0, 2, 3], keepdim=True).squeeze()
        dbeta = grad_output.sum(dim=[0, 2, 3], keepdim=True).squeeze()

        # Compute dx_hat
        dx_hat = grad_output * gamma.view(1, C, 1, 1)

        # Compute mean of dx_hat and dx_hat * x_hat
        mean_dx_hat = dx_hat.mean(dim=(0, 2, 3), keepdim=True)
        mean_dx_hat_x_hat = (dx_hat * x_hat).mean(dim=(0, 2, 3), keepdim=True)

        # Compute dx
        dx = (dx_hat - mean_dx_hat - x_hat * mean_dx_hat_x_hat) / inv_std.view(1, C, 1, 1)

        return dx, None, None, dgamma, dbeta, None

class ModelNew(nn.Module):
    def __init__(self, num_features):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.eps = 1e-5
        self.momentum = 0.1  # default momentum

    def forward(self, x):
        if self.training:
            # Compute mean and variance
            mean = x.mean([0, 2, 3], keepdim=True)
            var = x.var([0, 2, 3], unbiased=True, keepdim=True)

            # Update running statistics
            with torch.no_grad():
                exponential_average_factor = self.momentum
                self.running_mean = self.running_mean * (1 - exponential_average_factor) + mean.squeeze() * exponential_average_factor
                self.running_var = self.running_var * (1 - exponential_average_factor) + var.squeeze() * exponential_average_factor
        else:
            mean = self.running_mean.view(1, -1, 1, 1)
            var = self.running_var.view(1, -1, 1, 1)

        # Apply batch norm using the custom function
        out = BatchNormFunction.apply(x, mean, var, self.weight, self.bias, self.eps)
        return out

```

Wait, but in the forward function of ModelNew, when in evaluation mode, the code uses the running mean and var. The original BatchNorm2d uses the running stats during evaluation, so this is correct.

Also, the parameters (weight and bias) are learnable, so they are parameters.

Testing the code for correctness:

- The CUDA kernel computes (x - mean)/sqrt(var + eps) * gamma + beta.

- The Function's forward calls the kernel.

- The backward computes gradients for x, gamma, beta, and returns them.

Potential issues:

- The CUDA kernel's template must handle all floating point types (float and double), which is handled by AT_DISPATCH_FLOATING_TYPES.

- The backward implementation may have errors in the gradient formulas.

But given the time constraints, this is the best I can do.

Final code in code blocks:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

batchnorm_forward_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void batchnorm_forward_kernel(
    const scalar_t* x_data,
    const scalar_t* mean_data,
    const scalar_t* var_data,
    const scalar_t* gamma_data,
    const scalar_t* beta_data,
    scalar_t eps,
    scalar_t* y_data,
    int N, int C, int H, int W
) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= N * C * H * W) return;

    int w = index % W;
    int h = (index / W) % H;
    int c = (index / (W * H)) % C;
    int n = index / (C * W * H);

    scalar_t x_val = x_data[index];
    scalar_t mean_val = mean_data[c];
    scalar_t var_val = var_data[c];
    scalar_t gamma_val = gamma_data[c];
    scalar_t beta_val = beta_data[c];

    scalar_t inv_std = 1.0 / sqrt(var_val + eps);
    scalar_t norm = (x_val - mean_val) * inv_std;
    y_data[index] = gamma_val * norm + beta_val;
}

std::tuple<torch::Tensor> batchnorm_forward_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor gamma,
    torch::Tensor beta,
    double eps
) {
    auto output = torch::empty_like(x);

    const int threads = 256;
    const int elements = x.numel();
    const int blocks = (elements + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(x.type(), "batchnorm_forward_cuda", ([&] {
        batchnorm_forward_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(
            x.data<scalar_t>(),
            mean.data<scalar_t>(),
            var.data<scalar_t>(),
            gamma.data<scalar_t>(),
            beta.data<scalar_t>(),
            eps,
            output.data<scalar_t>(),
            x.size(0), x.size(1), x.size(2), x.size(3)
        );
    }));

    return output;
}
"""

batchnorm_forward_cpp_source = """
torch::Tensor batchnorm_forward_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor gamma,
    torch::Tensor beta,
    double eps
);
"""

batchnorm_forward = load_inline(
    name="batchnorm_forward",
    cpp_sources=batchnorm_forward_cpp_source,
    cuda_sources=batchnorm_forward_source,
    functions=["batchnorm_forward_cuda"],
    verbose=True,
)

class BatchNormFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, mean, var, gamma, beta, eps):
        output = batchnorm_forward.batchnorm_forward_cuda(x, mean, var, gamma, beta, eps)
        ctx.save_for_backward(x, mean, var, gamma, beta)
        ctx.eps = eps
        return output

    @staticmethod
    def backward(ctx, grad_output):
        x, mean, var, gamma, beta = ctx.saved_tensors
        eps = ctx.eps

        N, C, H, W = x.size()
        m = N * H * W

        x_mu = x - mean
        inv_std = 1.0 / torch.sqrt(var + eps)
        x_hat = x_mu * inv_std.view(1, C, 1, 1)

        # Compute gradients for gamma and beta
        dgamma = (x_hat * grad_output).sum([0, 2, 3], keepdim=True).squeeze()
        dbeta = grad_output.sum([0, 2, 3], keepdim=True).squeeze()

        # Compute dx_hat
        dx_hat = grad_output * gamma.view(1, C, 1, 1)

        # Compute mean of dx_hat and dx_hat * x_hat
        mean_dx_hat = dx_hat.mean(dim=(0, 2, 3), keepdim=True)
        mean_dx_hat_x_hat = (dx_hat * x_hat).mean(dim=(0, 2, 3), keepdim=True)

        # Compute dx
        dx = (dx_hat - mean_dx_hat - x_hat * mean_dx_hat_x_hat) * inv_std.view(1, C, 1, 1)

        return dx, None, None, dgamma, dbeta, None

class ModelNew(nn.Module):
    def __init__(self, num_features):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.eps = 1e-5
        self.momentum = 0.1  # default momentum

    def forward(self, x):
        if self.training:
            mean = x.mean([0, 2, 3], keepdim=True)
            var = x.var([0, 2, 3], unbiased=True, keepdim=True)

            with torch.no_grad():
                exponential_average_factor = self.momentum
                self.running_mean = self.running_mean * (1 - exponential_average_factor) + mean.squeeze() * exponential_average_factor
                self.running_var = self.running_var * (1 - exponential_average_factor) + var.squeeze() * exponential_average_factor
        else:
            mean = self.running_mean.view(1, -1, 1, 1)
            var = self.running_var.view(1, -1, 1, 1)

        return BatchNormFunction.apply(x, mean, var, self.weight, self.bias, self.eps)
```

Wait, in the backward, the saved tensors include x, mean, var, gamma, beta. But in the forward, the output is also saved? Wait in the initial code:

In the forward:

ctx.save_for_backward(x, mean, var, gamma, beta, output)

But in the current code, it's:

ctx.save_for_backward(x, mean, var, gamma, beta)

Wait, the output isn't saved, but in the backward, when computing dgamma and dbeta, we need grad_output multiplied by x_hat and summed. Since x_hat is (x - mean)/sqrt(var + eps), which can be computed from the saved variables (x, mean, var), so it's okay.

Therefore, the backward can proceed without needing the output.

The code should now be correct.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

batchnorm_forward_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void batchnorm_forward_kernel(
    const scalar_t* x_data,
    const scalar_t* mean_data,
    const scalar_t* var_data,
    const scalar_t* gamma_data,
    const scalar_t* beta_data,
    scalar_t eps,
    scalar_t* y_data,
    int N, int C, int H, int W
) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= N * C * H * W) return;

    int w = index % W;
    int h = (index / W) % H;
    int c = (index / (W * H)) % C;
    int n = index / (C * W * H);

    scalar_t x_val = x_data[index];
    scalar_t mean_val = mean_data[c];
    scalar_t var_val = var_data[c];
    scalar_t gamma_val = gamma_data[c];
    scalar_t beta_val = beta_data[c];

    scalar_t inv_std = 1.0 / sqrt(var_val + eps);
    scalar_t norm = (x_val - mean_val) * inv_std;
    y_data[index] = gamma_val * norm + beta_val;
}

std::tuple<torch::Tensor> batchnorm_forward_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor gamma,
    torch::Tensor beta,
    double eps
) {
    auto output = torch::empty_like(x);

    const int threads = 256;
    const int elements = x.numel();
    const int blocks = (elements + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(x.type(), "batchnorm_forward_cuda", ([&] {
        batchnorm_forward_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(
            x.data<scalar_t>(),
            mean.data<scalar_t>(),
            var.data<scalar_t>(),
            gamma.data<scalar_t>(),
            beta.data<scalar_t>(),
            eps,
            output.data<scalar_t>(),
            x.size(0), x.size(1), x.size(2), x.size(3)
        );
    }));

    return output;
}
"""

batchnorm_forward_cpp_source = """
torch::Tensor batchnorm_forward_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor gamma,
    torch::Tensor beta,
    double eps
);
"""

batchnorm_forward = load_inline(
    name="batchnorm_forward",
    cpp_sources=batchnorm_forward_cpp_source,
    cuda_sources=batchnorm_forward_source,
    functions=["batchnorm_forward_cuda"],
    verbose=True,
)

class BatchNormFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, mean, var, gamma, beta, eps):
        output = batchnorm_forward.batchnorm_forward_cuda(x, mean, var, gamma, beta, eps)
        ctx.save_for_backward(x, mean, var, gamma, beta)
        ctx.eps = eps
        return output

    @staticmethod
    def backward(ctx, grad_output):
        x, mean, var, gamma, beta = ctx.saved_tensors
        eps = ctx.eps

        N, C, H, W = x.size()
        m = N * H * W

        x_mu = x - mean
        inv_std = 1.0 / torch.sqrt(var + eps)
        x_hat = x_mu * inv_std.view(1, C, 1, 1)

        # Compute gradients for gamma and beta
        dgamma = (x_hat * grad_output).sum([0, 2, 3], keepdim=True).squeeze()
        dbeta = grad_output.sum([0, 2, 3], keepdim=True).squeeze()

        # Compute dx_hat
        dx_hat = grad_output * gamma.view(1, C, 1, 1)

        # Compute mean of dx_hat and dx_hat * x_hat
        mean_dx_hat = dx_hat.mean(dim=(0, 2, 3), keepdim=True)
        mean_dx_hat_x_hat = (dx_hat * x_hat).mean(dim=(0, 2, 3), keepdim=True)

        # Compute dx
        dx = (dx_hat - mean_dx_hat - x_hat * mean_dx_hat_x_hat) * inv_std.view(1, C, 1, 1)

        return dx, None, None, dgamma, dbeta, None

class ModelNew(nn.Module):
    def __init__(self, num_features):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.eps = 1e-5
        self.momentum = 0.1  # default momentum

    def forward(self, x):
        if self.training:
            mean = x.mean([0, 2, 3], keepdim=True)
            var = x.var([0, 2, 3], unbiased=True, keepdim=True)

            with torch.no_grad():
                exponential_average_factor = self.momentum
                self.running_mean = self.running_mean * (1 - exponential_average_factor) + mean.squeeze() * exponential_average_factor
                self.running_var = self.running_var * (1 - exponential_average_factor) + var.squeeze() * exponential_average_factor
        else:
            mean = self.running_mean.view(1, -1, 1, 1)
            var = self.running_var.view(1, -1, 1, 1)

        return BatchNormFunction.apply(x, mean, var, self.weight, self.bias, self.eps)
```