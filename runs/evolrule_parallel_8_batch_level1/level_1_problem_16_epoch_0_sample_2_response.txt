The code is written in Python with PyTorch and you are to replace the matmul operator with your own custom CUDA implementation. You are free to choose any optimization strategy. The output code must be a complete Python module that can be saved to a .py file and run. The code must not have any syntax errors.


Okay, let me tackle this problem. The user wants me to replace the torch.matmul in their model with a custom CUDA kernel. The original model does a matrix multiplication of A transposed and B. The dimensions given are M=1024*2, K=4096*2, N=2048*2. So the input tensors are A (K,M) and B (K,N). The output is (M, N) since it's A.T (M,K) multiplied by B (K,N).

First, I need to write a CUDA kernel for matrix multiplication. The standard approach is to use a 2D grid and block structure to handle matrix elements. Each thread can compute a single element of the output matrix. Alternatively, there's the tiled approach for better cache utilization, but since the problem allows any optimization strategy, maybe starting with a straightforward implementation is better for simplicity and ensuring correctness.

Wait, but the input tensors are both 2D. The matrix multiplication here is (M,K) * (K,N), resulting in (M,N). The CUDA kernel needs to compute each element C[i][j] = sum(A[i][k] * B[k][j]) for k from 0 to K-1.

Hmm, the kernel needs to loop over all i, j elements and compute their sum. To parallelize this, each thread can handle a specific (i,j) pair. However, for large matrices, this might be inefficient because each thread would have to loop over K elements, which could be 8192 (since K is 4096*2). That could be slow if K is large. Alternatively, using a tiled approach where each block handles a tile of the matrix and threads within the block compute multiple elements, but that's more complex.

Alternatively, maybe using a blocked algorithm where each thread block computes a block of the output matrix, and each thread handles a part of the computation. Let me think about the dimensions.

Alternatively, perhaps using shared memory for tiles to improve memory access. But since the user's example used a simple kernel, maybe starting with a straightforward implementation is better, even if it's not the most optimized. But for a better performance, a tiled approach might be necessary.

Wait, the user's example used an element-wise addition kernel with a 1D grid. For matrix multiplication, a 2D grid might be better. Let me sketch the kernel.

The kernel function would take pointers to A, B, and C. The kernel would have each thread compute one element of C. But for large K, each thread would need to do K multiplications and additions. The problem is that if K is 8192, then each thread would have to loop 8192 times, which might be too slow. So perhaps this approach isn't efficient.

Alternatively, using a thread per row and column. Let's see: the output matrix has M rows and N columns. So for each element (i,j), thread (i,j) can compute the sum. But the loop over K would be the inner loop. The problem is that K is large (8192), so the loop might be slow. To reduce this, maybe we can parallelize the K loop across threads. Wait, perhaps a better way is to use a grid where each thread is responsible for a block of elements and uses a tiled approach to load into shared memory.

Alternatively, perhaps using the standard matrix multiplication implementation found in CUDA examples. Let me recall the tiled matrix multiplication.

In the tiled approach, each block is responsible for a tile of the output matrix. The block size might be 16x16, and each thread computes one element in the tile. But actually, each thread would handle a portion of the tiles. Wait, maybe the standard tiled approach uses a block for a tile of MxN and each thread handles a part. Hmm, perhaps I should look up a standard CUDA kernel for matrix multiplication.

Alternatively, let me think of the dimensions. Let me note that the output is M=2048, K=8192, N=4096. Wait, wait, original variables: M is 1024*2=2048, K is 4096*2=8192, N is 2048*2=4096. Wait, in the forward function, the code is torch.matmul(A.T, B). So A is K,M (since the input is A of shape (K, M)), so A.T is (M, K). Multiplying with B (K, N) gives (M, N). So the resulting matrix has M rows and N columns, so each element is computed over K elements.

The problem is that the naive approach (each thread computes one output element) would have each thread looping K times. Since K is 8192, this would be 8192 iterations per thread, which is a lot and might be slow. So maybe a better approach is to use a tiled approach where each thread processes multiple elements and uses shared memory to cache the tiles.

Alternatively, let me think of the standard matrix multiply kernel. For example, using a grid where each block is responsible for a block of the output matrix. Let's say each block handles a tile of 16x16. Each block would load a tile of A and a tile of B into shared memory and compute the product.

Alternatively, here's a possible approach:

The kernel will have each thread compute a single element of the output matrix, but the loop over K is done in chunks, using shared memory to cache tiles of A and B to reduce global memory accesses.

Alternatively, perhaps the best way to proceed is to use the standard tiled matrix multiplication kernel.

Let me recall that in the tiled approach, the matrix is divided into tiles, and each block processes a tile of the output matrix. The tiles are of size, say, 16x16. Each thread in the block handles a small portion of the tile. The steps are:

1. Each block is assigned a tile of the output matrix (blockDim.x and blockDim.y could be 16x16, but maybe 16x16 threads per block).

Wait, perhaps the block is responsible for a tile of size (blockDim.x, blockDim.y). Each thread in the block will compute a part of the tile. The kernel would loop over the tiles of A and B. For example, each block computes a tile of C (tile_A_rows x tile_B_cols), and loops over the tiles of A's columns (or rows?) to accumulate the product.

Alternatively, here's a standard kernel structure:

The kernel has each thread compute a single element of the output matrix, but using a tiling approach to load into shared memory. Let me see:

The kernel would have each block handle a block of the matrix. The block size can be, say, 16x16 threads. The output matrix is divided into tiles of size 16x16. Each thread in the block will process a single element in the tile. To compute that element, the thread will need to access the corresponding rows and columns from A and B, but using shared memory to cache the tiles of A and B.

The steps would be:

1. Each block is assigned a tile of the output matrix (each thread in the block handles a position in the tile).

2. The block loops over the tiles of the inner dimension (the K dimension, which is 8192 here). For each tile, the block loads the corresponding tile from A and B into shared memory.

3. Each thread in the block computes its part of the tile by multiplying the elements from the A and B tiles and accumulating the sum.

4. After all tiles are processed, the result is written to global memory.

This reduces the number of global memory accesses and uses shared memory for caching, which is faster.

So, let me outline the code structure.

First, define the kernel function. Let's say the tile size is 16x16. The block dimensions would be (blockDim.x, blockDim.y) as 16x16. The grid dimensions would be ceil(M / blockDim.x) x ceil(N / blockDim.y). Wait, perhaps the block is 16x16 threads, and each thread computes one element in the tile. The tile size would be 16x16. The inner loop over the tiles of K (the 8192 dimension) would require dividing K into chunks of tile_size, say 16.

Wait, perhaps the tile size for the inner dimension (K) is also 16. Let me think in terms of variables:

Tile size: Let's use TILE_SIZE = 16. The shared memory arrays for A and B would be of size TILE_SIZE x TILE_SIZE. Wait, but actually, the tiles for A and B would have different dimensions. For A, which is M x K (after transpose, actually A is K rows and M columns? Wait, original A is (K, M), so A.T is (M, K). So when we do A.T (M rows, K columns) multiplied by B (K rows, N columns), the resulting matrix is M rows x N columns. So each element C[i][j] = sum_{k=0}^{K-1} (A.T[i][k] * B[k][j]) = sum_{k} A[k][i] * B[k][j].

Wait, actually, A is (K, M), so A.T is (M, K). B is (K, N). So the multiplication is (M, K) * (K, N) → (M, N).

The element C[i][j] = sum_{k=0 to K-1} A.T[i][k] * B[k][j].

Which is equivalent to A[k][i] * B[k][j].

Hmm. The tiles for A and B would need to be arranged such that they can be multiplied. The standard approach would be to tile A and B in a way that allows their products to be accumulated into the output tile.

The tile size for the inner dimension (K) is important. Let's choose a tile size, say TILE_SIZE = 16.

The kernel would have a grid of blocks, each block handling a tile of the output matrix. The block has threads arranged as a 2D grid (TILE_SIZE x TILE_SIZE), but perhaps the threads are arranged as a 1D array? Wait, maybe the block is 1D but each thread is responsible for a row and column. Alternatively, better to use a 2D block.

Wait, perhaps the block is a 2D grid of threads, each handling a single element in the tile. The block dimensions could be (TILE_SIZE, TILE_SIZE). The grid is (ceil(M / TILE_SIZE), ceil(N / TILE_SIZE)).

Each block processes a tile of size TILE_SIZE x TILE_SIZE in the output matrix. The block loops over the tiles of the K dimension (the inner loop) to accumulate the products.

For each tile of K (the inner dimension), the block loads a tile of A and a tile of B into shared memory. The A tile would be of size TILE_SIZE rows (from A's row dimension, which is K) and TILE_SIZE columns (which is the column dimension of A, which is M). Wait, perhaps I need to think of A as (M rows, K columns) (since A.T is M x K), and B as (K rows, N columns).

Wait, the tile for A would be a block of rows from A's rows (M) and columns (K). Wait, actually, the A tile is a block of rows and columns. Wait, perhaps the tile for A would be a block of rows (tile in K direction?), but I'm getting confused.

Let me try to structure this step by step.

The output tile is a block of the output matrix of size TILE_SIZE x TILE_SIZE. The output tile is at position (blockIdx.x*TILE_SIZE, blockIdx.y*TILE_SIZE).

Each tile of the inner dimension K is processed in loops. Let's say the inner dimension is divided into tiles of size TILE_SIZE, so the number of tiles is (K + TILE_SIZE -1)/TILE_SIZE.

For each such tile, the block loads a tile of A's columns (since A is M rows, K columns) corresponding to the current tile of K. The A tile would be of size (TILE_SIZE columns), but the rows are all of the block's output rows (the current tile in M direction). Wait, perhaps the A tile is a submatrix of A that is (M_rows x TILE_SIZE columns). Wait, maybe it's better to think:

The A matrix is (M rows, K columns). The B matrix is (K rows, N columns).

The output C is (M rows, N columns).

Each output tile is a submatrix C_rr to C_rr + TILE_SIZE rows, C_cc to C_cc + TILE_SIZE columns (where rr and cc are the starting indices based on the block's position).

To compute this tile, we need to loop over all tiles of the K dimension. For each tile of K (say starting at kk), we load a TILE_SIZE x TILE_SIZE block from A and B into shared memory.

Specifically, the A tile would be a submatrix of A starting at column kk, of size M x TILE_SIZE. But since each block is handling a block of M rows (the output rows), perhaps the A tile for the current block is the current rows (block's rows) and the current tile columns (kk to kk + TILE_SIZE -1).

Wait, maybe this is getting too complicated. Let me look for a standard CUDA kernel example.

Alternatively, perhaps using the approach from the CUDA Samples, specifically matrixMul. Let me recall that matrixMul uses a tiled approach.

Looking up the code from the matrixMul example:

The kernel is:

__global__ void
matrixMulKernel(float* C, float* A, float* B, int wA, int wB)
{
    // Block index
    int bx = blockIdx.x;
    int by = blockIdx.y;

    // Thread index
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // Each thread computes one element of the block sub-matrix
    float Csub = 0.0;

    // Thread shared memory
    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    // Loop over the tiles required to compute the block sub-matrix
    for (int m = 0; m < (wA-1)/TILE_WIDTH+1; ++m)
    {
        // Collaborative loading of As and Bs tiles into shared memory
        // Load the As tile into shared memory
        if ((m*TILE_WIDTH + tx) < wA && by*TILE_WIDTH + ty < wA) {
            As[ty][tx] = A[by * wA * TILE_WIDTH + m*TILE_WIDTH + tx + ty * wA];
        }
        else {
            As[ty][tx] = 0.0;
        }

        // Load the Bs tile into shared memory
        if ((m*TILE_WIDTH + ty) < wB && bx*TILE_WIDTH + tx < wB) {
            Bs[ty][tx] = B[bx * wB * TILE_WIDTH + m*TILE_WIDTH + ty + tx * wB];
        }
        else {
            Bs[ty][tx] = 0.0;
        }

        __syncthreads();

        // Multiply the two tiles together and accumulate into Csub
        for (int k = 0; k < TILE_WIDTH; ++k)
        {
            Csub += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    // Write the block sub-matrix to device memory
    // Each thread writes one element
    if (by*TILE_WIDTH + ty < wA && bx*TILE_WIDTH + tx < wB)
    {
        int row = by*TILE_WIDTH + ty;
        int col = bx*TILE_WIDTH + tx;
        C[row * wB + col] = Csub;
    }
}

Wait, perhaps this code is for when the matrices are square and the dimensions are multiples of the tile size. But the idea is to loop over the tiles of the inner dimension (wA), load the tiles into shared memory, perform the multiplication, accumulate, and then write out.

In our case, the matrices are A (M rows x K columns) and B (K rows x N columns). The output is M x N. So the equivalent parameters would be:

wA (width of A) is K (columns of A), and wB is N (columns of B). Wait, maybe need to adjust parameters accordingly.

Alternatively, let me adjust the kernel for our dimensions.

In our case, A is (M rows, K columns), B is (K rows, N columns). The output is (M rows, N columns).

The tile size is TILE_WIDTH, say 16.

The block dimensions are (TILE_WIDTH, TILE_WIDTH), so each block has 16x16 threads.

The grid is (ceil(N / TILE_WIDTH), ceil(M / TILE_WIDTH)), but perhaps the block indices are arranged such that each block handles a tile of the output matrix. Let me think:

The block (bx, by) is responsible for the tile starting at row = by * TILE_WIDTH and column = bx * TILE_WIDTH.

Each thread (tx, ty) within the block computes the element at (by*TILE_WIDTH + ty, bx*TILE_WIDTH + tx).

The inner loop over the tiles of K. The number of tiles needed is (K + TILE_WIDTH -1)/TILE_WIDTH.

For each such tile m, the block loads a TILE_WIDTH x TILE_WIDTH tile from A and B.

The A tile corresponds to rows (by*TILE_WIDTH + ty) and columns (m*TILE_WIDTH + tx). Wait, no, perhaps the A tile is from column m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH -1, and all rows in the block's output rows. Wait, maybe I need to think in terms of the matrix multiplication formula.

Alternatively, the As shared memory holds a tile of A's columns. Since A has dimensions (M rows, K columns), the tile for A would be a block of columns from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH -1, and all rows in the block's rows. But since each block is handling a tile of the output, the rows in the A tile are the same as the rows in the output tile. Wait, perhaps the rows are the same. So the A tile for the current block would be the rows corresponding to the block's rows (ty + by*TILE_WIDTH) and columns in the current tile (m*TILE_WIDTH + tx). Hmm, this is getting confusing.

Alternatively, in the standard kernel, the A matrix has dimensions wA columns (columns of A). The code in the matrixMul example has As[ty][tx] = A[by*TILE_WIDTH + ty + m*TILE_WIDTH + tx * wA], but I might need to adjust for the dimensions.

Alternatively, let me try to write the kernel for our case.

First, define constants:

#define TILE_WIDTH 16

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {

    // Block and thread indices
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // Compute the global indices for the output element
    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;

    // Each thread computes a partial sum for the output element (row, col)
    float C_value = 0.0f;

    // Shared memory for tiles of A and B
    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    // Number of tiles needed to cover the K dimension
    for (int m = 0; m < (K + TILE_WIDTH - 1)/TILE_WIDTH; m++) {
        // Load the tile of A into shared memory
        // A is M x K (since A.T is M rows, K columns)
        // The current tile of A is columns from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH -1
        // The row is row (since each thread is responsible for a specific row)
        int a_col = m*TILE_WIDTH + tx;
        int a_row = row;

        if (a_col < K && a_row < M) {
            As[ty][tx] = A[a_row * K + a_col]; // since A is stored in row-major, row a_row has elements K in length
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load the tile of B into shared memory
        // B is K rows x N columns. The current tile is rows m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH -1
        // column is col (since each thread is responsible for a specific column)
        int b_row = m*TILE_WIDTH + ty;
        int b_col = col;
        if (b_row < K && b_col < N) {
            Bs[ty][tx] = B[b_row * N + b_col]; // B is stored as row-major, so each row has N elements
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // Multiply the tiles and accumulate
        for (int k = 0; k < TILE_WIDTH; k++) {
            C_value += As[ty][k] * Bs[k][tx]; // Wait, Bs is indexed as [k][tx]?
            // Wait, Bs is stored as [row][col], so Bs[ty][tx] is row b_row and column b_col. Hmm, maybe the Bs indexing is wrong here.

        }

        __syncthreads();
    }

    // Write the result to global memory
    if (row < M && col < N) {
        C[row * N + col] = C_value;
    }
}

Wait, there might be some mistakes here. Let me think again about the indices for A and B.

Wait, the A matrix is M rows x K columns. The A's elements are stored in row-major order, so A[i][j] is at A[i*K + j].

When loading into As (the shared memory), for the current tile m, the columns are from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1. The rows for A's tile would be the current block's rows (row = by*TILE_WIDTH + ty), but each thread in the block is responsible for a specific row and column in the output. Wait, perhaps I'm mixing up the indices here.

Alternatively, each thread in the block's tile is responsible for a specific row and column in the block's output tile. The A and B tiles are loaded such that each thread can contribute to the multiplication.

Wait, perhaps the As and Bs should be arranged such that each thread in the block handles a position in the A and B tiles. Let me see:

The As tile is a block of A's columns starting at m*TILE_WIDTH. Each thread (ty, tx) in the block would load the element A[row][a_col], where row = by*TILE_WIDTH + ty (the global row for the current output element), and a_col = m*TILE_WIDTH + tx (the column in A). 

Wait, but As is a 2D array of size [TILE_WIDTH][TILE_WIDTH]. So As[ty][tx] would be the element at row (by*TILE_WIDTH + ty) and column (m*TILE_WIDTH + tx).

Similarly, for B's Bs tile, the rows are m*TILE_WIDTH + ty and columns are col (bx*TILE_WIDTH + tx). So Bs[ty][tx] would be B's element at row (m*TILE_WIDTH + ty) and column (bx*TILE_WIDTH + tx).

Wait, maybe the Bs is loaded with the current column in the output's column (col = bx*TILE_WIDTH + tx) and the row is the current tile's row.

Wait, the Bs is a tile from the B matrix. Since B is K rows by N columns, each tile in the B matrix's rows would be m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1. The columns for Bs would be the current output column's column (col). 

Hmm, perhaps the Bs tile is loaded as follows:

For each B's row in the current tile (m*TILE_WIDTH + ty), and column is col (bx*TILE_WIDTH + tx). Wait, perhaps the Bs should be loaded such that the Bs matrix is (TILE_WIDTH rows x TILE_WIDTH columns?), but the tile for B is columns fixed to the output column's current tile and rows from the current tile m.

Alternatively, perhaps I should structure the Bs as:

Bs is a tile of B's rows from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1 and columns from the current output column's block? Not sure.

This is getting a bit tangled. Let me try to rework the indices.

Let me think of the A and B tiles as follows:

The A tile for this iteration m is a submatrix of A that covers all rows in the current block's output rows (which are by*TILE_WIDTH to by*TILE_WIDTH + TILE_WIDTH-1), and columns m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1.

The B tile is a submatrix of B covering rows m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1, and all columns in the current block's output columns (bx*TILE_WIDTH to bx*TILE_WIDTH + TILE_WIDTH-1).

Wait, perhaps the B tile's columns are the current output's column block (col), so the columns for B would be the output's columns (bx's block), but that might not be necessary. Alternatively, the B tile's columns are the entire range, but that's not manageable. Hmm, this is getting too confusing. Let me refer back to the matrixMul example.

In the matrixMul example, the kernel loops over tiles of the inner dimension (wA, which is the number of columns of A, which is the same as the rows of B). The tiles are of size TILE_WIDTH. For each tile m, the A tile is a block of columns m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1, and all rows. The B tile is a block of rows m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1, and all columns.

Wait, that way, the multiplication between the A tile (rows x TILE_WIDTH) and B tile (TILE_WIDTH x columns) can be done in shared memory.

Ah, right. So the A tile is a block of columns (current tile m) and all rows, and the B tile is a block of rows (current tile m) and all columns. This allows the multiplication between A's tile columns and B's tile rows, resulting in a TILE_WIDTH x TILE_WIDTH product that can be accumulated into the output block.

Wait, perhaps in our case, the A is M rows x K columns, B is K rows x N columns. The inner dimension is K. The tiles are in steps of TILE_WIDTH along the K dimension.

So for each m, the A tile is:

A's columns from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1, and all rows (so size M rows x TILE_WIDTH columns).

The B tile is B's rows from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1, and all columns (so size TILE_WIDTH rows x N columns).

Then, the multiplication of the A tile (M rows x TILE_WIDTH) and B tile (TILE_WIDTH x N columns) gives a M x N matrix, which contributes to the final output.

But in the kernel, each block handles a block of the output matrix (TILE_WIDTH x TILE_WIDTH). So for each thread in the block, the thread computes a single element in their block's output tile. The multiplication between the A and B tiles can be done in shared memory.

Thus, the A tile and B tile are loaded into shared memory. The A tile for a thread would be the portion of the A tile corresponding to the block's rows and the current tile's columns. Similarly, the B tile for the thread would be the portion corresponding to the current tile's rows and the block's columns.

Wait, the block's output tile is at (by*TILE_WIDTH, bx*TILE_WIDTH). The A tile for the current m is:

For the A tile (columns m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH -1), and rows in the block's output rows (row = by*TILE_WIDTH + ty). So each thread (ty, tx) in the block would load A[row][a_col], where a_col is m*TILE_WIDTH + tx. 

The B tile for the current m is rows m*TILE_WIDTH + ty, and columns in the block's output columns (col = bx*TILE_WIDTH + tx). So Bs[ty][tx] would be B[b_row][col], where b_row = m*TILE_WIDTH + ty.

Wait, this way, the As and Bs tiles are loaded into shared memory such that As has the rows of the block's output and the current tile's columns, and Bs has the current tile's rows and the columns of the block's output.

Then, when multiplying, each element C(row, col) is the sum over k of A(row, k) * B(k, col). The tiles split the k dimension into chunks of TILE_WIDTH. For each chunk m, the As has columns m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1, so the elements A[row][k] for k in this range. The Bs has rows k in this range, so B[k][col].

Thus, the multiplication between the As and Bs tiles for each chunk m is:

For each thread in the block's tile (row, col):

C_value += sum_{k in current tile} A[row][k] * B[k][col]

This is done by multiplying the As[ty][tx] (which is A[row][k = m*TILE_WIDTH + tx]) and Bs[tx][ty] (Wait, perhaps Bs is stored such that Bs[k's row, col's column? Maybe Bs is arranged as [k][col]?

Wait, perhaps the Bs tile is stored as B's rows in the current tile and columns in the block's output. So Bs[ty][tx] would be B[m*TILE_WIDTH + ty][bx*TILE_WIDTH + tx].

Wait, let's think of the Bs as:

Bs is a submatrix of B with rows from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH -1, and columns from the block's column start to block's column end (bx*TILE_WIDTH to bx*TILE_WIDTH + TILE_WIDTH-1). So the Bs tile's dimensions are TILE_WIDTH rows x TILE_WIDTH columns.

Each thread in the block can be responsible for a particular element in the Bs tile. For example, thread (ty, tx) would load the element at row (m*TILE_WIDTH + ty) and column (bx*TILE_WIDTH + tx). 

Similarly, the A's tile is columns from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1, and rows from the block's row start to block's row end (by*TILE_WIDTH + ty). So the A's tile has dimensions TILE_WIDTH rows x TILE_WIDTH columns (since the rows are limited by the block's TILE_WIDTH rows). 

Wait, perhaps the A's tile is the block's rows (by*TILE_WIDTH to by*TILE_WIDTH + TILE_WIDTH-1) and columns from the current tile's columns (m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1). So the A tile has size TILE_WIDTH rows x TILE_WIDTH columns.

Thus, each thread (ty, tx) in the block can load the A element at row (by*TILE_WIDTH + ty), column (m*TILE_WIDTH + tx).

Putting this together, the As and Bs tiles are both of size TILE_WIDTH x TILE_WIDTH. Then, to compute the contribution to C[row][col], the thread can loop over the TILE_WIDTH elements in the current tile and accumulate the product.

The multiplication would involve for each k in 0..TILE_WIDTH-1:

C_value += As[ty][k] * Bs[k][tx]

Wait, because As is A's rows (row) and columns (m*TILE_WIDTH + k), so As[ty][k] is A[row][m*TILE_WIDTH +k]. 

Bs[k][tx] is B[m*TILE_WIDTH +k][col], where col is bx*TILE_WIDTH + tx.

Thus, the product As[ty][k] * Bs[k][tx] is A[row][m*TILE_WIDTH +k] * B[m*TILE_WIDTH +k][col], which is the contribution to C[row][col] for that k in the current tile.

Summing over all k in the tile and all tiles gives the total.

So the code for the kernel should be structured as follows:

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    // Define tile size
    const int TILE_WIDTH = 16;

    // Block and thread indices
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // Global indices
    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;

    // Each thread's partial sum
    float C_value = 0.0f;

    // Shared memory for tiles
    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    // Loop over tiles of K
    for (int m = 0; m < (K + TILE_WIDTH - 1) / TILE_WIDTH; m++) {
        // Load tile of A into shared memory
        int a_row = row;
        int a_col = m*TILE_WIDTH + tx;
        if (a_col < K && a_row < M) {
            As[ty][tx] = A[a_row * K + a_col];
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load tile of B into shared memory
        int b_row = m*TILE_WIDTH + ty;
        int b_col = col;
        if (b_row < K && b_col < N) {
            Bs[ty][tx] = B[b_row * N + b_col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // Multiply tiles and accumulate
        for (int k = 0; k < TILE_WIDTH; k++) {
            C_value += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    // Write the result
    if (row < M && col < N) {
        C[row * N + col] = C_value;
    }
}

Wait, but in the B loading part, b_col is col (bx*TILE_WIDTH + tx). The Bs is stored as [ty][tx], so for thread (ty, tx), Bs[ty][tx] is B[b_row][b_col]. So that's correct.

The Bs is a block of rows from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH -1 and columns from col's starting (bx*TILE_WIDTH) to bx*TILE_WIDTH + TILE_WIDTH-1. Each thread in the block is loading a particular element of this Bs block.

Wait, but in the Bs loading, the b_col is the column of the output, which is fixed for each thread (since col = bx*TILE_WIDTH + tx). So for each thread in the block, the column in B is fixed as col. However, the Bs tile is a block of rows (m's current tile) and columns (the current block's columns). But since each thread in the block is only responsible for their own column (col), the Bs tile is actually a vertical strip of columns corresponding to the block's columns.

Hmm, perhaps this approach is not correct because the Bs tile is not a square block but a vertical strip. This might cause inefficiency because Bs is loaded into a shared memory of size TILE_WIDTH x TILE_WIDTH, but the actual data might not fit unless the block's column tile is TILE_WIDTH columns. 

Wait, the Bs tile is loaded as follows:

Each thread (ty, tx) in the block loads B[b_row][b_col], where b_row is m*TILE_WIDTH + ty and b_col is col (bx*TILE_WIDTH + tx).

Wait, but for a given m and block, each thread (ty, tx) is loading a different b_col. Since the block is handling a TILE_WIDTH x TILE_WIDTH block of the output, each thread's tx corresponds to the column offset within the block. So the b_col is fixed as the column of the output tile. But for the Bs tile, each thread is loading a different column (based on tx) but the same row (m*TILE_WIDTH + ty). 

This means that the Bs tile is actually a set of rows from m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1 and columns from the block's starting column to block's starting column + TILE_WIDTH-1.

But to load this into the shared memory Bs, which is [TILE_WIDTH][TILE_WIDTH], each thread (ty, tx) would be responsible for row ty and column tx in the shared memory's Bs. So:

Bs[ty][tx] = B[b_row][b_col]

This way, the Bs shared memory contains the submatrix of B from rows m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1 and columns bx*TILE_WIDTH to bx*TILE_WIDTH + TILE_WIDTH-1. Each thread in the block loads their own element in this submatrix.

Similarly, the As shared memory contains the submatrix of A from rows by*TILE_WIDTH to by*TILE_WIDTH + TILE_WIDTH-1 and columns m*TILE_WIDTH to m*TILE_WIDTH + TILE_WIDTH-1.

This way, the two tiles can be multiplied.

Thus, the kernel should work as intended.

Now, the grid and block dimensions need to be set correctly.

The block dimensions are (TILE_WIDTH, TILE_WIDTH) since each block has 16x16 threads.

The grid dimensions are (ceil(N / TILE_WIDTH), ceil(M / TILE_WIDTH)). Because each block covers a block of the output matrix's columns (bx) and rows (by).

Wait, the block index x is for columns, so the number of blocks in x is ceil(N / TILE_WIDTH), and in y it's ceil(M / TILE_WIDTH).

Yes.

Now, in the Python code, we need to compile this kernel using torch.utils.cpp_extension.load_inline.

First, define the kernel code as a string.

Wait, but the TILE_WIDTH is a #define in the CUDA code, so it should be part of the source.

Putting this all together, here's the plan:

In the Python code:

- Define the CUDA kernel source code with the tiled matrix multiplication.

- Compile it using load_inline.

- The forward function will call this kernel.

Now, the dimensions for the problem are M = 2048 (since 1024*2), K=8192 (4096*2), N=4096 (2048*2). 

Wait, the problem says:

M = 1024 * 2 → 2048

K = 4096 * 2 → 8192

N = 2048 * 2 → 4096.

Wait, no, the code given in the problem has:

def get_inputs():
    A = torch.rand(K, M)
    B = torch.rand(K, N)
    return [A, B]

Wait, the forward function says the input A is (M, K) after transposing (since it's A.T multiplied by B). Wait, the forward function's inputs are A and B of shapes (K, M) and (K, N). So A has shape (K, M), so A.T is (M, K). Then multiplied by B (K, N) gives (M, N).

Thus, in the kernel, the dimensions are:

M rows in A.T → M = 2048,

K columns in A.T → K = 8192,

B has K rows (8192) and N columns (4096).

So the output is M x N → 2048x4096.

Now, in the CUDA kernel, the parameters are:

A is (M rows, K columns) → A is passed as a pointer to its data, and M is the number of rows, K columns.

Wait, in the kernel, the parameters are:

matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N)

So in the kernel, A is stored as rows-major. So A's element at (row, col) is at A[row * K + col].

Similarly, B is stored as rows-major. B has K rows, N columns, so element (row, col) is at B[row * N + col].

The output C is stored as M rows x N columns, so C[row * N + col].

Now, the Python wrapper function needs to handle the kernel launch.

The Python code for the kernel:

elementwise_add_source in the example was a simple kernel, but here it's more complex. So the CUDA source will include the kernel and a wrapper function.

The wrapper function would take tensors A and B, compute the output, launch the kernel.

Let me write the CUDA source code as a string:

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define TILE_WIDTH 16

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;

    float C_value = 0.0f;

    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    for (int m = 0; m < (K + TILE_WIDTH - 1) / TILE_WIDTH; m++) {
        // Load As tile
        int a_col = m * TILE_WIDTH + tx;
        int a_row = row;
        if (a_col < K && a_row < M) {
            As[ty][tx] = A[a_row * K + a_col];
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load Bs tile
        int b_row = m * TILE_WIDTH + ty;
        int b_col = col;
        if (b_row < K && b_col < N) {
            Bs[ty][tx] = B[b_row * N + b_col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // Multiply and accumulate
        for (int k = 0; k < TILE_WIDTH; ++k) {
            C_value += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        C[row * N + col] = C_value;
    }
}

torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    // Get dimensions
    int M = A.size(0); // A is (M, K)
    int K = A.size(1);
    int N = B.size(1);

    // Output tensor
    auto C = torch::empty({M, N}, A.options());

    // Calculate grid and block dimensions
    dim3 threads(TILE_WIDTH, TILE_WIDTH);
    dim3 blocks(
        (N + TILE_WIDTH - 1) / TILE_WIDTH,
        (M + TILE_WIDTH - 1) / TILE_WIDTH
    );

    // Launch kernel
    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);

    return C;
}
"""

Then the header file:

matmul_cuda_h = """
torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B);
"""

Wait, in the code above, the A and B tensors are passed to the wrapper function. The wrapper function checks their sizes and launches the kernel.

Wait, in the problem's original code, the Model's forward function takes A and B as inputs, which are (K, M) and (K, N). But the code's forward function does A.T * B. So when using the custom kernel, we need to transpose A first?

Wait, the forward function in the original model is:

return torch.matmul(A.T, B)

In the custom kernel, the input A is (K, M), so A.T is (M, K). So in the kernel, the A tensor is the transposed version, but in the kernel, the parameters are expecting A to be M rows x K columns (since M = number of rows, K columns).

Thus, in the wrapper function, when passing A to the kernel, we should pass A.T's data. Wait, no, because in the kernel, the code uses A as if it's already transposed.

Wait, the forward function in the new model's class would need to call the custom_matmul_cuda function with A transposed.

Wait, the original forward function is:

def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
    return torch.matmul(A.T, B)

So in the new model's forward, we need to compute A.T @ B. The custom_matmul_cuda function should take A.T and B as inputs.

Thus, in the Python code for the ModelNew class, the forward would be:

def forward(self, A, B):
    return self.custom_matmul(A.T, B)

Wait, but in the wrapper function's code, the inputs are A and B. So the wrapper function assumes that A is already the transposed version. So in the code for the ModelNew class's forward:

    def forward(self, A, B):
        return self.custom_matmul(A.t(), B)

Thus, the kernel is written to handle the case where A is M x K (the transposed A).

Now, the CUDA kernel's parameters are:

M is the number of rows of A (which is the transposed A's rows, so original A's columns: M = 2048).

K is the number of columns of A (transposed A's columns, which is original A's rows: K = 8192).

N is the columns of B (4096).

Thus, the kernel should work correctly.

Now, compiling this in PyTorch using load_inline.

The code would look like:

from torch.utils.cpp_extension import load_inline

matmul_cuda_source = [the source above]

matmul_cuda_h = [the header]

Then,

custom_matmul = load_inline(
    name="custom_matmul",
    cpp_sources=matmul_cuda_h,
    cuda_sources=matmul_cuda_source,
    functions=["custom_matmul_cuda"],
    verbose=True,
)

Then in the ModelNew class, we have:

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.custom_matmul = custom_matmul

    def forward(self, A, B):
        return self.custom_matmul.custom_matmul_cuda(A.t(), B)

Wait, but in the CUDA code, the wrapper function expects A and B as the transposed and the B as passed. So the code is correct.

Now, checking the parameters for the kernel launch.

The grid is blocks(ceil(N / 16), ceil(M / 16)), since each block handles a tile of 16x16 in the output.

The threads are (16,16).

Now, the code should work.

Now, let's check for possible errors.

In the kernel, the Bs tile's column is col (bx*TILE_WIDTH + tx). The Bs is stored as Bs[ty][tx], which corresponds to row m*TILE_WIDTH + ty and column col. 

The multiplication loop in the kernel's shared memory is:

for (int k = 0; k < TILE_WIDTH; ++k) {
    C_value += As[ty][k] * Bs[k][tx];
}

This is correct because:

As[ty][k] is the element from A's tile row ty (which corresponds to the global row row = by*TILE_WIDTH + ty) and column m*TILE_WIDTH +k.

Bs[k][tx] is from B's tile row m*TILE_WIDTH +k and column tx's global column (bx*TILE_WIDTH + tx).

Thus, the product is (A's element) * (B's element) for that k in the current tile.

Summing over all k in the tile gives the contribution to C_value for the current tile.

Then, after looping over all tiles, the C_value holds the total sum over all K elements.

Thus, the kernel should compute the correct result.

Now, the final code.

Putting it all together in the Python code:

import torch
import torch.nn as nn

from torch.utils.cpp_extension import load_inline

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define TILE_WIDTH 16

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;

    float C_value = 0.0f;

    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    for (int m = 0; m < (K + TILE_WIDTH - 1) / TILE_WIDTH; m++) {
        // Load tile of A into shared memory
        int a_col = m * TILE_WIDTH + tx;
        int a_row = row;
        if (a_col < K && a_row < M) {
            As[ty][tx] = A[a_row * K + a_col];
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load tile of B into shared memory
        int b_row = m * TILE_WIDTH + ty;
        int b_col = col;
        if (b_row < K && b_col < N) {
            Bs[ty][tx] = B[b_row * N + b_col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // Multiply and accumulate
        for (int k = 0; k < TILE_WIDTH; ++k) {
            C_value += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        C[row * N + col] = C_value;
    }
}

torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0); // M rows in A
    int K = A.size(1);
    int N = B.size(1);

    auto C = torch::empty({M, N}, A.options());

    dim3 threads(TILE_WIDTH, TILE_WIDTH);
    dim3 blocks(
        (N + TILE_WIDTH - 1) / TILE_WIDTH,
        (M + TILE_WIDTH - 1) / TILE_WIDTH
    );

    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);

    return C;
}
"""

matmul_cuda_h = """
torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B);
"""

custom_matmul = load_inline(
    name="custom_matmul",
    cpp_sources=matmul_cuda_h,
    cuda_sources=matmul_cuda_source,
    functions=["custom_matmul_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.custom_matmul = custom_matmul

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.custom_matmul.custom_matmul_cuda(A.t(), B)

def get_inputs():
    K = 4096 * 2
    M = 1024 * 2
    N = 2048 * 2
    A = torch.rand(K, M).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

def get_init_inputs():
    return []

Wait, but in the get_inputs() function, the original code uses global variables M, K, N. However, in the provided code, the user's code had:

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

def get_inputs():
    A = torch.rand(K, M)
    B = torch.rand(K, N)
    return [A, B]

But in the optimized code, we need to ensure that the inputs are on the GPU (cuda()), so in the get_inputs function of the new code, I added .cuda().

Wait, but in the problem's instructions, the user provided get_inputs() as part of the original code, and the new code should output the complete module. So in the new code, the get_inputs function must be included, and the tensors should be on the GPU. The original code's get_inputs had tensors on CPU, but in the example given by the user (the first one), the inputs were on CUDA, so perhaps the new code's get_inputs should also return CUDA tensors.

Thus, in the generated code, the get_inputs function is adjusted to use .cuda(), and the ModelNew's forward function should also ensure that the inputs are on the correct device. But the user's original code's get_inputs didn't have .cuda(), so maybe the problem expects the model to handle inputs on CPU, but in practice, for CUDA kernels, they need to be on GPU. Hence, it's better to generate code that moves tensors to CUDA.

Wait, the user's example in the first problem had:

def get_inputs():
    a = torch.randn(1, 128).cuda()
    b = torch.randn(1, 128).cuda()
    return [a, b]

So similarly, the new code should generate get_inputs() that returns CUDA tensors.

Thus, the get_inputs function in the generated code would be:

def get_inputs():
    K = 4096 * 2
    M = 1024 * 2
    N = 2048 * 2
    A = torch.rand(K, M).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

Wait, but the original code's get_inputs uses global variables M, K, N. So the new code should define those variables as well.

Thus, adding the lines:

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

at the top of the code.

So the complete code would be:

```python
import torch
import torch.nn as nn

from torch.utils.cpp_extension import load_inline

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define TILE_WIDTH 16

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;

    float C_value = 0.0f;

    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    for (int m = 0; m < (K + TILE_WIDTH - 1) / TILE_WIDTH; m++) {
        // Load tile of A into shared memory
        int a_col = m * TILE_WIDTH + tx;
        int a_row = row;
        if (a_col < K && a_row < M) {
            As[ty][tx] = A[a_row * K + a_col];
        } else {
            As[ty][tx] = 0.0f;
        }

        // Load tile of B into shared memory
        int b_row = m * TILE_WIDTH + ty;
        int b_col = col;
        if (b_row < K && b_col < N) {
            Bs[ty][tx] = B[b_row * N + b_col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // Multiply and accumulate
        for (int k = 0; k < TILE_WIDTH; ++k) {
            C_value += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        C[row * N + col] = C_value;
    }
}

torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M_val = A.size(0); // M rows in A
    int K_val = A.size(1);
    int N_val = B.size(1);

    auto C = torch::empty({M_val, N_val}, A.options());

    dim3 threads(TILE_WIDTH, TILE_WIDTH);
    dim3 blocks(
        (N_val + TILE_WIDTH - 1) / TILE_WIDTH,
        (M_val + TILE_WIDTH - 1) / TILE_WIDTH
    );

    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M_val, K_val, N_val);

    return C;
}
"""

matmul_cuda_h = """
torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B);
"""

custom_matmul = load_inline(
    name="custom_matmul",
    cpp_sources=matmul_cuda_h,
    cuda_sources=matmul_cuda_source,
    functions=["custom_matmul_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.custom_matmul = custom_matmul

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.custom_matmul.custom_matmul_cuda(A.t(), B)

def get_inputs():
    A = torch.rand(K, M).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

def get_init_inputs():
    return []
```

Wait, but in the custom_matmul_cuda function, the variables M, K, N are passed as parameters. The M_val, K_val, N_val are derived from the input tensors. However, in the kernel, these parameters are used to ensure that the accesses are within bounds.

This should work. However, there might be an issue with the thread indices when the dimensions are not multiples of TILE_WIDTH. The code uses ceil division in the grid calculation, which is correct.

Testing this code would require compiling it, but given that the user requires a complete code block, this should be the correct approach.
</think>

```python
import torch
import torch.nn as nn

from torch.utils.cpp_extension import load_inline

M = 1024 * 2
K = 4096 * 2
N = 2048 * 2

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define TILE_WIDTH 16

__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;

    float C_value = 0.0f;

    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    for (int m = 0; m < (K + TILE_WIDTH - 1) / TILE_WIDTH; m++) {
        int a_col = m * TILE_WIDTH + tx;
        int a_row = row;
        if (a_col < K && a_row < M) {
            As[ty][tx] = A[a_row * K + a_col];
        } else {
            As[ty][tx] = 0.0f;
        }

        int b_row = m * TILE_WIDTH + ty;
        int b_col = col;
        if (b_row < K && b_col < N) {
            Bs[ty][tx] = B[b_row * N + b_col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        for (int k = 0; k < TILE_WIDTH; ++k) {
            C_value += As[ty][k] * Bs[k][tx];
        }

        __syncthreads();
    }

    if (row < M && col < N) {
        C[row * N + col] = C_value;
    }
}

torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M_val = A.size(0);
    int K_val = A.size(1);
    int N_val = B.size(1);

    auto C = torch::empty({M_val, N_val}, A.options());

    dim3 threads(TILE_WIDTH, TILE_WIDTH);
    dim3 blocks(
        (N_val + TILE_WIDTH - 1) / TILE_WIDTH,
        (M_val + TILE_WIDTH - 1) / TILE_WIDTH
    );

    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M_val, K_val, N_val);

    return C;
}
"""

matmul_cuda_h = """
torch::Tensor custom_matmul_cuda(torch::Tensor A, torch::Tensor B);
"""

custom_matmul = load_inline(
    name="custom_matmul",
    cpp_sources=matmul_cuda_h,
    cuda_sources=matmul_cuda_source,
    functions=["custom_matmul_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.custom_matmul = custom_matmul

    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.custom_matmul.custom_matmul_cuda(A.t(), B)

def get_inputs():
    A = torch.rand(K, M).cuda()
    B = torch.rand(K, N).cuda()
    return [A, B]

def get_init_inputs():
    return []
```