First, analyze the given architecture and identify which operators can be optimized with custom CUDA kernels. Then, write the optimized ModelNew class. Here are the steps I need you to follow:

1. Analyze the given architecture. What are the operators that can be replaced with custom CUDA kernels for better performance? Justify your choice.
2. Write the custom CUDA kernel code to replace those operators.
3. Integrate the custom CUDA kernels into the ModelNew class.
4. Ensure that the code is compatible with PyTorch and can be compiled and run without errors.
5. Avoid unnecessary changes; only the necessary parts should be modified.

Now, proceed step by step to optimize the given architecture.




Okay, let's tackle this problem. The user wants to optimize the given PyTorch model that uses torch.argmin with a custom CUDA kernel. The original model's forward pass is just a single call to torch.argmin along a specified dimension. The task is to replace that with a custom CUDA kernel for potential speedups.

First, I need to analyze why torch.argmin might be a candidate for optimization. The built-in function is already optimized, but perhaps in specific cases, especially with large tensors, a custom kernel could be faster. The argmin operation involves finding the index of the minimum value along a dimension. For a 3D tensor of size (batch_size, dim1, dim2), when dim=1 (the middle dimension here), the kernel has to process each batch and each element along dim2, comparing values to find the minimum indices.

The steps I need to follow are:

1. **Analyze the Operators**: The main operator here is torch.argmin. Since it's a single function, replacing it with a custom kernel is straightforward. The goal is to implement the argmin logic in CUDA to leverage parallelism.

2. **Write the CUDA Kernel**: The kernel needs to process each element along the specified dimension. For each batch and each position in the remaining dimensions (dim2 in this case), find the minimum value's index along dim1. Since dim is fixed here (dim=1), the kernel can be specialized for that dimension.

   The CUDA approach would involve threads processing elements. Since the operation is per batch and per dim2 element, we can structure the kernel such that each thread handles a single (batch, dim2) position. For each such position, the thread iterates over the dim1 elements to find the index of the minimum value.

3. **Integration into ModelNew**: The custom kernel must be compiled and called within the PyTorch model. Using load_inline as in the example is the way to go.

4. **Ensure Compatibility**: The kernel must handle the input tensor correctly, output the right tensor (int64?), and be compatible with PyTorch's autograd system. However, since argmin is not differentiable, the kernel doesn't need to handle gradients.

Potential challenges:

- **Memory Management**: The output tensor should be pre-allocated. The kernel must write to it correctly.

- **Thread and Block Configuration**: Choosing the right grid and block sizes. Since each thread can handle a single (batch, dim2) position, the number of threads needed is batch_size * dim2. But CUDA has limits on grid dimensions, so need to structure blocks appropriately.

Wait, the input tensor dimensions are (batch_size, dim1, dim2). The output is (batch_size, dim2), since we're taking argmin along dim1. So for each batch and dim2 index, we process dim1 elements. So the total number of elements to process is batch_size * dim2. Each thread can handle one such element.

The CUDA kernel can be designed as follows:

- The kernel function takes pointers to the input and output tensors, along with the dimensions.

- Each thread is responsible for one (batch, dim2) position. The thread's index can be computed as blockIdx.x * blockDim.x + threadIdx.x. But since the total number of elements is batch_size * dim2, the grid and block sizes need to cover that.

Wait, but in CUDA, the maximum grid dimensions are limited (like 65535 per dimension). So if batch_size * dim2 is very large (like 128*4095=524,288), we can use a 1D grid. Let's see: 524,288 threads. The maximum block size is 1024. So the number of blocks would be ceil(524288 / 1024) = 512 blocks, which is acceptable.

The kernel steps would be:

For each thread:

- Compute the batch and dim2 index from the thread index.

- Iterate over the dim1 elements (from 0 to dim1-1) for that position, track the minimum value and its index.

- Write the index to the output tensor.

Wait, but this is O(dim1) per thread. For dim1=4096, that's 4096 iterations per thread. That might be slow. Alternatively, can we parallelize this further?

Hmm, but in this case, since each (batch, dim2) position requires a sequential scan along dim1, it's a reduction over the dim1 dimension. Maybe using shared memory and parallel reduction techniques could help. But for simplicity, since the problem requires a custom kernel and the example in the question uses a straightforward kernel, maybe the initial approach is acceptable. However, for better performance, a parallel reduction might be better.

Alternatively, if dim1 is 4096, iterating in a loop for each thread might be manageable on the GPU, especially since 4096 is a power of two. Let's proceed with the straightforward approach first, then see.

The kernel code would look something like:

__global__ void argmin_kernel(const float* input, int* output, int batch_size, int dim1, int dim2) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * dim2) return;

    int batch = idx / dim2;
    int d2 = idx % dim2;

    float min_val = input[batch * dim1 * dim2 + 0 * dim2 + d2]; // first element of dim1
    int min_idx = 0;

    for (int d1 = 1; d1 < dim1; ++d1) {
        float current_val = input[batch * dim1 * dim2 + d1 * dim2 + d2];
        if (current_val < min_val) {
            min_val = current_val;
            min_idx = d1;
        }
    }

    output[batch * dim2 + d2] = min_idx;
}

Wait, the input tensor is stored in row-major order. So the input's memory layout is:

input[batch][d1][d2] corresponds to linear index batch * dim1*dim2 + d1 * dim2 + d2. So the indexing is correct.

The output is of size (batch_size, dim2). So the output's linear index is batch * dim2 + d2.

This approach is straightforward but might have high latency for dim1=4096, but perhaps the overhead of kernel launch is negligible compared to the computation.

Now, the wrapper function:

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    // Since the model is fixed to dim=1, but maybe allow general dim? Or since in the problem, the dim is fixed, perhaps hardcode it.

Wait, in the given model, the dim is a parameter passed during initialization, but in the code provided, the get_init_inputs returns [dim], so during initialization, the dim is passed. However, in the original model, the forward function uses self.dim. So the kernel needs to take that into account. Wait, in the problem's given code, the Model class has a __init__ with dim, and the forward uses self.dim. So the custom kernel should also take the dim as an argument.

Wait, but in the problem's code, the Model is initialized with dim=1. So in the kernel, perhaps we can hardcode the dim as 1, but that's not general. However, in the example given by the user (the first example with add), the kernel didn't take any parameters except tensors. So in this case, since the model's dim is fixed at initialization, maybe the kernel can be specialized for that dim. But the problem requires the optimized code to be compatible with the original architecture, which may have different dim values. Wait, but in the code given, the Model is initialized with dim=1, but the code should work for any dim. Wait, the original Model's __init__ takes dim as an argument, so the optimized ModelNew should also take dim as an argument, and the kernel must accept it.

Hmm, this complicates things. Because the kernel's code needs to know the dimension along which to compute the argmin. But in CUDA kernels, the dimension is a compile-time constant? No, variables can be passed as arguments. So the kernel can take the dim as an argument. Wait, but in the kernel function, the dim is part of the logic. For example, if dim is 0, then the loop would iterate over batches, etc. So perhaps the kernel needs to handle arbitrary dimensions. But that adds complexity.

Wait the problem's example code for the original model has dim=1, but the user's code allows any dim. So the optimized kernel must handle arbitrary dimensions, but how?

Alternatively, maybe the user's example's dim is fixed, but the problem requires the kernel to handle the dim passed to the model. Therefore, the kernel must take the dimension as an argument. However, in CUDA, the kernel can have parameters. So the kernel code would have to handle the dimension dynamically.

Hmm, this complicates the kernel code. For example, if the dimension is 0 (batch dimension), then for each element in the other dimensions, we need to find the min along the batch. But since the problem's example is for dim=1, perhaps in the optimized code, the kernel can take the dimension as an input parameter, and the code can be written to handle different dims.

But this requires more code. Alternatively, maybe in the given problem, the dim is fixed to 1, so we can hardcode it. Wait, looking at the problem's code:

In the given architecture, the Model is initialized with dim=1. The get_init_inputs() returns [dim], so during initialization, the dim is passed. The forward function uses self.dim.

The user's task is to optimize the given architecture. The optimized code must take the same parameters. So the ModelNew must have a dim parameter, and the kernel must accept that dim when called.

Therefore, the kernel must take the dimension as an argument.

This complicates the kernel's code. Let's think of how to structure it.

Suppose the kernel function has dim as an argument. So the kernel code can be written with the dimension as a parameter.

Wait, in CUDA, kernel parameters can be variables. So the kernel can take the dimension as an argument. Let's see:

The kernel function signature would be:

__global__ void argmin_kernel(const float* input, int* output, int batch_size, int dim_size, int other_dim, int dim) {

Wait, perhaps I need to compute the strides. The input tensor's dimensions are (B, D1, D2). Suppose dim is the dimension along which to compute the argmin. So for example, if dim=1 (the middle dimension), then the loop is over D1 elements for each position in B and D2. The strides would be different depending on the dimension.

Alternatively, the kernel can be designed to take the dimensions of the input tensor and the dimension to reduce over. The code would have to compute the indices accordingly.

This requires more complex indexing. Let me think of the general case.

Suppose the input is a 3D tensor with dimensions (B, D1, D2). The dim to reduce is dim. The output has dimensions (B, D1 if dim !=1 else D2, etc.) Wait, no. The output is of size (B, D2) if dim=1, because the dimension is reduced. Wait, the output of argmin over dim=1 (size D1) would be (B, D2), yes.

In general, the output's shape is the input's shape without the dim dimension.

But the kernel's code must handle any dim. Let's see:

Suppose the input has shape (B, D0, D1, D2, ...) depending on the number of dimensions, but in the given problem, the input is 3D (since get_inputs returns a 3D tensor: x = torch.rand(batch_size, dim1, dim2)), so the input is 3D.

Therefore, the kernel can be designed for 3D tensors. The dim can be 0,1, or 2. For each case, the code would loop over the elements of the dim dimension.

Let me think of the kernel's code with dim as an argument:

The kernel needs to process each element of the output tensor. The output tensor has dimensions (B, D1, D2) without the dim dimension. For example:

- If dim=0 (batch dimension), output is (D1, D2)

Wait no, original input is (B, D1, D2). So reducing along dim=0 gives (D1, D2). Wait, no. The argmin over dim=0 (the first dimension) would have shape (D1, D2), because each position in D1 and D2 is reduced along B. The output would be (D1, D2). Wait, no: the batch dimension is first. Let me think again.

If input is (B, D1, D2), then:

- argmin along dim=0 (batch) → output shape (D1, D2)

- argmin along dim=1 (middle) → output (B, D2)

- argmin along dim=2 (last) → output (B, D1)

Thus, the output has a shape that depends on the input's shape and the dim. The kernel needs to handle these cases.

To make the kernel general, it's better to compute the indices based on the input's dimensions and the reduction dimension.

However, writing a kernel that can handle any dimension dynamically might be more complex. For the problem's case, perhaps the user expects the kernel to be specialized for dim=1, since in the given code, the Model is initialized with dim=1. But the problem says the optimized code should be a replacement for the original Model, which uses a dim parameter. Therefore, the kernel must accept the dim parameter.

Let me proceed by assuming the input is 3D and dim can be 0,1, or 2. The kernel must handle this.

First, the kernel function:

The parameters would be:

- input: pointer to float data

- output: pointer to int (assuming torch.argmin returns int64, which is 64-bit, so maybe using int64_t instead of int)

- batch_size: B

- dim_size: the size of the dimension being reduced (e.g., if dim=1, then dim_size is D1)

- other_dim1: the size of the first other dimension (if dim is 0, then other_dim1 is D1, other_dim2 is D2)

Wait, perhaps it's better to pass all three dimensions (B, D1, D2) along with the dim parameter.

Alternatively, the kernel can compute the strides. Let me think of the kernel's code:

Each thread is responsible for an output element. Let's say the output has a total of N elements. The number of threads is equal to N.

The output's index can be mapped to the input indices depending on the dim.

Let me denote the output's indices as:

For the case of dim=1 (original problem's case), the output is (B, D2). So each output element is at position (b, d2).

The corresponding input elements are (b, d1, d2) for all d1 in 0..D1-1. The thread for (b, d2) needs to find the d1 that gives the minimum value.

In general, for a given output index, the input indices depend on the dim.

Alternatively, the kernel can compute the input indices based on the thread's position and the dim.

Let me think of the kernel's code as follows:

Each thread processes one output element. The output element's position is determined by its linear index (thread index). Let’s suppose the output has a total of (B * D1 * D2) / (dim_size) elements (since we reduce over dim). Wait, no: the output is the product of the other two dimensions. For example, if dim=1, then output elements are B*D2.

The kernel function:

__global__ void argmin_kernel(
    const float* input,
    int64_t* output,
    int batch_size, int dim1_size, int dim2_size,
    int reduction_dim,
    int input_stride0, int input_stride1, int input_stride2
) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (out_idx >= (batch_size * dim1_size * dim2_size) / dim1_size) { // Wait, depends on reduction dim
        return;
    }

Hmm, perhaps the strides approach is better. The input tensor's strides can be computed based on the dimensions and reduction dimension. Alternatively, the kernel can compute the strides dynamically. Alternatively, pass the strides as parameters.

Alternatively, since the input is a 3D tensor, we can compute the strides as follows:

Assuming input is 3D (B, D1, D2), the strides are:

- stride0 (along batch) = D1 * D2

- stride1 (along D1) = D2

- stride2 (along D2) = 1

But depending on the reduction dimension, the strides change.

Alternatively, the kernel can compute the input index for a given output index and the reduction dimension.

Let me try to structure this step by step.

Suppose the output is 3D reduced along dim:

For example, if reduction_dim is 1 (the D1 dimension), then for each output index (b, d2), the input indices are (b, 0, d2), (b,1,d2), ..., (b, D1-1, d2).

The thread's output index can be expressed as:

If reduction_dim is 0:

The output is (D1, D2). The output index is (d1, d2), and the thread's linear index is d1*D2 + d2.

The input indices are (0, d1, d2), (1, d1, d2), ..., (B-1, d1, d2).

If reduction_dim is 1 (original case):

Output is (B, D2). Output index (b, d2), linear index b*D2 + d2.

Input indices: (b, 0, d2), (b,1,d2), ..., (b, D1-1, d2).

If reduction_dim is 2:

Output is (B, D1). Output index (b, d1). Linear index is b*D1 + d1.

Input indices: (b, d1, 0), (b,d1,1), ..., (b, d1, D2-1).

Thus, the kernel can handle different reduction dimensions by using conditional logic based on reduction_dim.

The kernel code outline:

for each thread:

compute the output's linear index (out_idx) from thread index.

based on reduction_dim, compute the input indices that are not reduced, then iterate over the reduced dimension.

But the code needs to compute the starting input index and the step for each element in the reduced dimension.

Alternatively, using the reduction_dim, we can compute the strides.

Let me think of the input index as a 3D coordinate (i0, i1, i2), where the coordinates correspond to the three dimensions. The reduction is over one of these dimensions.

For example, reduction over dim=1 (the middle dimension):

The output index is (b, d2) → so i0 = b, i2 = d2. The loop is over i1 from 0 to D1-1.

The input index for each i1 is (b, i1, d2).

The linear index in input is: i0 * (D1*D2) + i1 * D2 + i2.

Wait, the linear index can be computed as:

input_linear = i0 * (D1 * D2) + i1 * D2 + i2.

Yes, since the first dimension has stride D1*D2 (each batch is a D1x D2 matrix), the second has stride D2 (each row in D1 has D2 elements), third has 1.

So, for the case of reduction over dim=1:

The output's (b, d2) corresponds to varying i0 = b, i2 = d2, and varying i1.

The starting input index is:

start = b * (D1 * D2) + 0 * D2 + d2 → = b * D1*D2 + d2.

Then, each step in the loop over i1 increments the input index by D2 (since each step increases i1 by 1, which contributes D2 to the linear index).

Thus, the loop over the reduction dimension can be written as:

float min_val = input[start];

int min_idx = 0;

for (int i = 1; i < dim_size; i++) {

    float current_val = input[start + i * stride_step];

    // where stride_step is the step for each increment of the reduction dimension.

    if (current_val < min_val) {

        min_val = current_val;

        min_idx = i;

    }

}

Then, the output[out_idx] = min_idx.

The stride_step depends on the reduction dimension.

If the reduction is over dimension 0 (stride_step would be D1*D2 → each step increases i0 by 1 → input index increases by D1*D2).

Wait, let's recast:

The stride_step for the reduction dimension is the stride of that dimension. For example:

- If reduction dimension is 0 (batch), then the stride is D1 * D2 → each step increases the batch index by 1 → the linear index increases by that stride.

- If reduction dimension is 1 (middle), the stride is D2 → each step increases the middle index by 1 → linear index increases by D2.

- If reduction dimension is 2 (last), the stride is 1 → each step increases the last index by 1 → linear index increases by 1.

Thus, the stride_step is the stride of the reduction dimension.

Therefore, the kernel can compute the start and stride_step based on the reduction dimension.

To compute the start and stride_step:

First, we need to know which indices are being held constant and which are varying.

The start is the linear index of the first element in the reduction dimension.

The stride_step is the step between elements along the reduction dimension.

To compute start and stride_step:

Let's define the output's indices as follows:

Let the output have dimensions:

For reduction_dim=0:

output shape is (D1, D2).

An output index is (d1, d2) → mapped to linear index (d1 * D2 + d2).

The non-reduction dimensions are D1 and D2. Wait no: the reduction is over the first dimension (size B), so the non-reduction dimensions are the second and third (D1 and D2). So the output's indices are (d1, d2). Thus, the input coordinates for the first element (reduction_dim=0) would be (0, d1, d2). The starting index is (0, d1, d2) → linear index = 0 * (D1*D2) + d1 * D2 + d2 → but wait, that's not correct. Wait, the reduction is over the first dimension (B), so for output (d1, d2), the reduction is over all batches (i0 from 0 to B-1), but the other indices are d1 (second dimension) and d2 (third). So the first element of the reduction sequence is (0, d1, d2), next (1, d1, d2), etc.

Thus, the start is:

start = 0 * (D1*D2) + d1 * D2 + d2 → but this depends on the output indices.

Wait, the output's indices are (d1, d2), so how do we get those from the output's linear index?

The linear index of the output (out_idx) is d1 * D2 + d2.

Thus, for reduction_dim=0:

d1 = out_idx / D2

d2 = out_idx % D2

The start is the first element in the reduction dimension (i0=0), so the input's linear index is:

input_start = (0) * (D1*D2) + (d1) * D2 + (d2) → = d1*D2 + d2.

Wait, but the input's first dimension is B. So the first term is i0*(D1*D2). For i0=0, it's zero. So yes.

The stride_step for reduction_dim=0 is the stride of the first dimension, which is D1*D2.

Thus, the loop for reduction_dim=0 would start at d1*D2 + d2, and each step increases by D1*D2 (to go to next batch).

Similarly for other dimensions:

reduction_dim=1 (middle):

Output index is (b, d2) → linear index is b * D2 + d2.

The input's non-reduction dimensions are b and d2. The reduction dimension is i1 (middle), which goes from 0 to D1-1.

The start input index is:

b * (D1*D2) + 0 * D2 + d2 → b*(D1*D2) + d2.

The stride_step is D2 (stride of the second dimension).

reduction_dim=2 (last):

Output index is (b, d1). linear index is b*D1 + d1.

The non-reduction dimensions are b and d1. The reduction dimension is the third dimension (i2 from 0 to D2-1).

The start is:

b*(D1*D2) + d1*D2 + 0 → b*D1*D2 + d1*D2.

The stride_step is 1 (stride of the third dimension).

Thus, the start and stride_step can be computed based on the reduction dimension and the output's indices.

Thus, in the kernel:

The thread's out_idx is the linear index of the output.

Based on reduction_dim, we compute the coordinates of the non-reduction dimensions, then compute the start and stride_step.

This requires branching in the kernel code based on the reduction dimension. Since the reduction dimension is a parameter to the kernel, we can use if-else statements.

The kernel code outline:

__global__ void argmin_kernel(
    const float* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (out_idx >= (batch_size * dim1_size * dim2_size) / dim_size) { // Wait, need to compute the output's total elements.

    // First, compute the output's total elements.

    int output_size;

    if (reduction_dim == 0) {
        output_size = dim1_size * dim2_size;
    } else if (reduction_dim == 1) {
        output_size = batch_size * dim2_size;
    } else { // reduction_dim ==2
        output_size = batch_size * dim1_size;
    }

    if (out_idx >= output_size) return;

    // Now compute the start and stride_step based on reduction_dim.

    int start;
    int stride_step;

    if (reduction_dim ==0) {

        // reduction over first dimension (batch)
        // output is (dim1_size, dim2_size)
        // out_idx corresponds to (d1, d2) where d1 = out_idx / dim2_size, d2 = out_idx % dim2_size

        int d1 = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;

        start = d1 * dim2_size + d2; // since i0 starts at 0, the first element is 0*(dim1*dim2) + d1*dim2 +d2 ?

        Wait, no. Let me re-calculate:

        For reduction_dim=0, the first dimension is batch (size B). The non-reduction dimensions are dim1 and dim2 (indices d1 and d2).

        The input start is the linear index of (i0=0, d1, d2):

        input_start = 0 * (dim1_size * dim2_size) + d1 * dim2_size + d2.

        So start = d1 * dim2_size + d2.

        The stride_step for reduction_dim=0 is the stride of the first dimension (batch), which is dim1_size * dim2_size.

    } else if (reduction_dim ==1) {

        // reduction over dim1 (middle)

        // output is (batch_size, dim2_size)
        // out_idx = b * dim2_size + d2.

        int b = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;

        start = b * (dim1_size * dim2_size) + d2; // because for i1=0 (second dimension), the index is b * (D1*D2) + 0 * D2 + d2 → D2 is dim2_size.

        Wait, yes. The second dimension's stride is dim2_size (since each step in dim1 increases the index by D2).

        So start = b*(D1*D2) + 0 * D2 + d2 → which is b*(D1*D2) + d2.

        stride_step is the stride of the second dimension → D2.

    } else { // reduction_dim ==2

        // reduction over third dimension (dim2)

        // output is (batch_size, dim1_size)
        // out_idx = b * dim1_size + d1.

        int b = out_idx / dim1_size;
        int d1 = out_idx % dim1_size;

        start = b * (dim1_size * dim2_size) + d1 * dim2_size; // because for i2=0 (third dimension), the index is b*(D1*D2) + d1 * D2 + 0 → the starting point is b*D1*D2 + d1*D2.

        The stride_step is the stride of the third dimension (1).

    }

    // Now compute the minimum over the reduction dimension.

    int min_idx = 0;
    float min_val = input[start];

    for (int i=1; i<dim_size; i++) { // dim_size is the size of the reduction dimension (e.g., for reduction over dim=0, it's batch_size)

        float current_val = input[start + i * stride_step];

        if (current_val < min_val) {
            min_val = current_val;
            min_idx = i;
        }
    }

    // Write the result to output.
    output[out_idx] = min_idx;
}

Wait, but the dim_size is the size of the reduction dimension. For example, if reduction_dim is 0, then the size is batch_size. If reduction_dim is 1, the size is dim1_size (D1), etc.

Therefore, the kernel needs to also take the size of the reduction dimension as an argument. So the parameters should include:

- dim_size: the size of the dimension being reduced.

Wait, that complicates things further. How do we get the dim_size?

The dim_size is the size of the input's dimension corresponding to reduction_dim. For example:

if reduction_dim is 0 → dim_size = batch_size,

if reduction_dim is 1 → dim_size = dim1_size,

if reduction_dim is 2 → dim_size = dim2_size.

Thus, the kernel can compute dim_size based on reduction_dim and the input's dimensions (batch_size, dim1_size, dim2_size).

So the code inside the kernel can do:

int dim_size;
if (reduction_dim ==0) dim_size = batch_size;
else if (reduction_dim ==1) dim_size = dim1_size;
else dim_size = dim2_size;

Then, the loop is for (int i=1; i<dim_size; i++).

This is manageable.

Now, putting all together, the kernel function will need the following parameters:

- input: input tensor data.

- output: output data.

- batch_size, dim1_size, dim2_size: the sizes of the input's dimensions.

- reduction_dim: the dimension to reduce over.

Thus, the kernel function's parameters are:

__global__ void argmin_kernel(
    const float* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
)

Now, the wrapper function in Python must pass these parameters correctly.

The wrapper function in Python would be something like:

def argmin_cuda(input, dim):

    # Get the input dimensions.

    batch_size, dim1, dim2 = input.shape

    # Get the reduction dimension.

    reduction_dim = dim

    # Compute the output tensor.

    output = torch.empty(...)

    # Launch kernel.

    block_size = 256

    num_blocks = (output_size + block_size -1 ) // block_size

    argmin_kernel<<<num_blocks, block_size>>>(input.data_ptr(), output.data_ptr(), batch_size, dim1, dim2, reduction_dim)

    return output.

Wait, but how to compute the output's shape and create it?

The output's shape depends on the input's shape and reduction_dim. For example, if input is (B, D1, D2), and dim is reduction_dim:

- If reduction_dim is 0 → output is (D1, D2).

- If 1 → (B, D2).

- If 2 → (B, D1).

Thus, in the wrapper function:

def argmin_cuda(input: torch.Tensor, dim: int) -> torch.Tensor:

    batch_size, dim1, dim2 = input.size()

    if dim ==0:

        output_size = dim1 * dim2

        output_shape = (dim1, dim2)

    elif dim ==1:

        output_size = batch_size * dim2

        output_shape = (batch_size, dim2)

    else: # dim ==2

        output_size = batch_size * dim1

        output_shape = (batch_size, dim1)

    output = torch.empty(output_shape, dtype=torch.int64, device=input.device)

    block_size = 256

    num_blocks = (output_size + block_size -1) // block_size

    argmin_kernel[...]

    return output

Now, the CUDA kernel code:

Putting it all together:

The kernel's code:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void argmin_kernel(
    const float* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (out_idx >= (batch_size * dim1_size * dim2_size) / ...) { // no, better to compute based on reduction_dim.

    // Compute the total output size based on reduction_dim:

    int output_size;
    if (reduction_dim ==0) {
        output_size = dim1_size * dim2_size;
    } else if (reduction_dim ==1) {
        output_size = batch_size * dim2_size;
    } else { // reduction_dim ==2
        output_size = batch_size * dim1_size;
    }

    if (out_idx >= output_size) return;

    // Compute start and stride_step:

    int dim_size;
    if (reduction_dim ==0) dim_size = batch_size;
    else if (reduction_dim ==1) dim_size = dim1_size;
    else dim_size = dim2_size;

    int start;
    int stride_step;

    if (reduction_dim ==0) {
        // Reduction over batch (dim0)
        int d1 = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = d1 * dim2_size + d2; // initial index when batch=0
        stride_step = dim1_size * dim2_size; // stride of batch (dim0)
    } else if (reduction_dim ==1) {
        // Reduction over dim1 (middle)
        int b = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = b * (dim1_size * dim2_size) + d2; // i1=0: b*(D1*D2) + 0*D2 + d2 → D2 is dim2_size
        stride_step = dim2_size; // stride of dim1 (each step increases i1 by 1 → D2)
    } else {
        // Reduction over dim2 (third)
        int b = out_idx / dim1_size;
        int d1 = out_idx % dim1_size;
        start = b * (dim1_size * dim2_size) + d1 * dim2_size; // i2=0 → D2=dim2_size → d1*dim2_size +0
        stride_step = 1; // stride of dim2
    }

    // Initialize min_val and min_idx
    int min_idx = 0;
    float min_val = input[start];

    for (int i = 1; i < dim_size; ++i) {
        float current_val = input[start + i * stride_step];
        if (current_val < min_val) {
            min_val = current_val;
            min_idx = i;
        }
    }

    output[out_idx] = min_idx;
}

The wrapper function in C++:

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    // Check that input is 3D
    auto input_shape = input.sizes();
    TORCH_CHECK(input.dim() == 3, "Input must be 3D tensor");
    int batch_size = input_shape[0];
    int dim1_size = input_shape[1];
    int dim2_size = input_shape[2];

    int output_size;
    torch::IntArrayRef output_shape;
    if (dim == 0) {
        output_size = dim1_size * dim2_size;
        output_shape = {dim1_size, dim2_size};
    } else if (dim == 1) {
        output_size = batch_size * dim2_size;
        output_shape = {batch_size, dim2_size};
    } else { // dim ==2
        output_size = batch_size * dim1_size;
        output_shape = {batch_size, dim1_size};
    }

    auto output = torch::empty(output_shape, torch::dtype(torch::kInt64).device(input.device()));

    const int block_size = 256;
    int num_blocks = (output_size + block_size - 1) / block_size;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "argmin_cuda", ([&] {
        argmin_kernel<<<num_blocks, block_size>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<int64_t>(),
            batch_size,
            dim1_size,
            dim2_size,
            dim
        );
    }));

    cudaDeviceSynchronize(); // To handle any errors?

    return output;
}

Wait, but in the kernel, the input is a float tensor. The code uses const float* input, which is correct if the input is float. But if the input is double, then this would fail. The wrapper uses AT_DISPATCH_FLOATING_TYPES to handle different dtypes.

However, in the problem's given code, the input is generated by torch.rand, which is float32. So perhaps the kernel can assume float, but for generality, the code should handle other types. But since the example uses float, maybe it's okay.

Now, integrating this into the ModelNew:

The ModelNew class will use the custom kernel instead of torch.argmin.

The code steps:

First, load the CUDA kernel using load_inline.

The CUDA code needs to be written as a string. The kernel and wrapper must be in the CUDA sources.

The Python code would look like:

from torch.utils.cpp_extension import load_inline

argmin_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void argmin_kernel(
    const float* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    // The kernel code as above
    // ... (paste the code here)
}

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    // The wrapper code as above
    // ... (paste the code here)
}
"""

argmin_cpp_source = "torch::Tensor argmin_cuda(torch::Tensor input, int dim);"

Then, load the extension:

argmin_cuda = load_inline(
    name="argmin_cuda",
    cpp_sources=argmin_cpp_source,
    cuda_sources=argmin_source,
    functions=["argmin_cuda"],
    verbose=True
)

Then, in the ModelNew:

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return argmin_cuda.argmin_cuda(x, self.dim)

Wait, but the argmin_cuda function is in the module returned by load_inline. So the code would be:

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return self.argmin_cuda(x, self.dim)

Wait, the argmin_cuda function is a member of the loaded module. So the code would need to have the module as an attribute. Let me see:

The example in the user's first example had:

elementwise_add = load_inline(...)

Then, in the forward:

return self.elementwise_add.elementwise_add_cuda(a, b)

So similarly, in this case:

argmin_cuda_mod = load_inline(...)

class ModelNew(...):

    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.argmin_cuda = argmin_cuda_mod

    def forward(self, x):
        return self.argmin_cuda.argmin_cuda(x, self.dim)

Alternatively, the function can be imported directly. But the syntax would be:

The loaded module (argmin_cuda_mod) has the function argmin_cuda, so when you do:

return argmin_cuda_mod.argmin_cuda(x, self.dim)

But in the class, the function needs to be accessible. Hence, storing the module as an attribute.

Putting it all together:

Now, the complete code:

The code must be written in Python, with the CUDA kernel code as a string.

Now, let me write this step by step.

First, the CUDA kernel code:

The kernel code from before:

The kernel is __global__ void argmin_kernel with the parameters. Then the wrapper function in C++.

Wait, the wrapper function's code in the C++ part:

The code I wrote earlier for the C++ wrapper function uses AT_DISPATCH_FLOATING_TYPES. But in the CUDA sources, the code must be written in C++.

Wait, the CUDA code must be in the cuda_sources string. The kernel is in the .cu file (cuda_sources), and the wrapper is in the .cpp file (cpp_sources). But in the case of load_inline, the cuda_sources and cpp_sources can be provided as strings.

Wait, the code for the kernel and the wrapper are both part of the CUDA extension. The wrapper is part of the C++ code.

Thus, the full code for the CUDA part:

The header includes:

#include <torch/extension.h>
#include <cuda_runtime.h>

Then the kernel and the wrapper function.

So putting all together in a string:

argmin_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void argmin_kernel(
    const float* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;

    int output_size;
    if (reduction_dim ==0) {
        output_size = dim1_size * dim2_size;
    } else if (reduction_dim ==1) {
        output_size = batch_size * dim2_size;
    } else { // reduction_dim ==2
        output_size = batch_size * dim1_size;
    }

    if (out_idx >= output_size) return;

    int dim_size;
    if (reduction_dim ==0) dim_size = batch_size;
    else if (reduction_dim ==1) dim_size = dim1_size;
    else dim_size = dim2_size;

    int start, stride_step;

    if (reduction_dim ==0) {
        int d1 = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = d1 * dim2_size + d2;
        stride_step = dim1_size * dim2_size;
    } else if (reduction_dim ==1) {
        int b = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = b * (dim1_size * dim2_size) + d2;
        stride_step = dim2_size;
    } else {
        int b = out_idx / dim1_size;
        int d1 = out_idx % dim1_size;
        start = b * (dim1_size * dim2_size) + d1 * dim2_size;
        stride_step = 1;
    }

    int min_idx = 0;
    float min_val = input[start];

    for (int i = 1; i < dim_size; ++i) {
        float current_val = input[start + i * stride_step];
        if (current_val < min_val) {
            min_val = current_val;
            min_idx = i;
        }
    }

    output[out_idx] = min_idx;
}

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    auto input_shape = input.sizes();
    if (input.dim() != 3) {
        throw std::invalid_argument("Input must be a 3D tensor");
    }
    int batch_size = input_shape[0];
    int dim1_size = input_shape[1];
    int dim2_size = input_shape[2];

    int output_size;
    torch::IntArrayRef output_shape;
    if (dim == 0) {
        output_size = dim1_size * dim2_size;
        output_shape = {dim1_size, dim2_size};
    } else if (dim == 1) {
        output_size = batch_size * dim2_size;
        output_shape = {batch_size, dim2_size};
    } else {
        output_size = batch_size * dim1_size;
        output_shape = {batch_size, dim1_size};
    }

    auto output = torch::empty(output_shape, torch::dtype(torch::kInt64).device(input.device()));

    const int block_size = 256;
    int num_blocks = (output_size + block_size - 1) / block_size;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "argmin_cuda", ([&] {
        argmin_kernel<<<num_blocks, block_size>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<int64_t>(),
            batch_size,
            dim1_size,
            dim2_size,
            dim
        );
    }));

    cudaDeviceSynchronize();
    return output;
}
"""

Then, the header for the C++ part is:

argmin_cpp_source = """
torch::Tensor argmin_cuda(torch::Tensor input, int dim);
"""

Then, loading the module:

argmin_cuda_mod = load_inline(
    name="argmin_cuda",
    cpp_sources=argmin_cpp_source,
    cuda_sources=argmin_source,
    functions=["argmin_cuda"],
    verbose=True,
)

Then the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return argmin_cuda_mod.argmin_cuda(x, self.dim)

Wait, but the load_inline returns a module object. So in the __init__, the module can be stored as an attribute.

Alternatively, the forward function can access the module through a class attribute:

But in the code above, argmin_cuda_mod is a variable in the global scope, so the forward function can reference it directly. However, for encapsulation, it's better to have it as an attribute.

Alternatively, the code can be structured as follows:

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.argmin_cuda = load_inline(...)  # but this would recompile every time, which is not good.

No, better to load once outside.

So the code would look like:

The complete code:

```python
import torch
import torch.nn as nn

from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for argmin
argmin_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void argmin_kernel(
    const float* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;

    int output_size;
    if (reduction_dim ==0) {
        output_size = dim1_size * dim2_size;
    } else if (reduction_dim ==1) {
        output_size = batch_size * dim2_size;
    } else { // reduction_dim ==2
        output_size = batch_size * dim1_size;
    }

    if (out_idx >= output_size) return;

    int dim_size;
    if (reduction_dim ==0) dim_size = batch_size;
    else if (reduction_dim ==1) dim_size = dim1_size;
    else dim_size = dim2_size;

    int start, stride_step;

    if (reduction_dim ==0) {
        int d1 = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = d1 * dim2_size + d2;
        stride_step = dim1_size * dim2_size;
    } else if (reduction_dim ==1) {
        int b = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = b * (dim1_size * dim2_size) + d2;
        stride_step = dim2_size;
    } else {
        int b = out_idx / dim1_size;
        int d1 = out_idx % dim1_size;
        start = b * (dim1_size * dim2_size) + d1 * dim2_size;
        stride_step = 1;
    }

    int min_idx = 0;
    float min_val = input[start];

    for (int i = 1; i < dim_size; ++i) {
        float current_val = input[start + i * stride_step];
        if (current_val < min_val) {
            min_val = current_val;
            min_idx = i;
        }
    }

    output[out_idx] = min_idx;
}

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    auto input_shape = input.sizes();
    if (input.dim() != 3) {
        throw std::invalid_argument("Input must be a 3D tensor");
    }
    int batch_size = input_shape[0];
    int dim1_size = input_shape[1];
    int dim2_size = input_shape[2];

    int output_size;
    torch::IntArrayRef output_shape;
    if (dim == 0) {
        output_size = dim1_size * dim2_size;
        output_shape = {dim1_size, dim2_size};
    } else if (dim == 1) {
        output_size = batch_size * dim2_size;
        output_shape = {batch_size, dim2_size};
    } else {
        output_size = batch_size * dim1_size;
        output_shape = {batch_size, dim1_size};
    }

    auto output = torch::empty(output_shape, torch::dtype(torch::kInt64).device(input.device()));

    const int block_size = 256;
    int num_blocks = (output_size + block_size - 1) / block_size;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "argmin_cuda", ([&] {
        argmin_kernel<<<num_blocks, block_size>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<int64_t>(),
            batch_size,
            dim1_size,
            dim2_size,
            dim
        );
    }));

    cudaDeviceSynchronize();
    return output;
}
"""

argmin_cpp_source = """
torch::Tensor argmin_cuda(torch::Tensor input, int dim);
"""

# Compile the inline CUDA code
argmin_cuda_mod = load_inline(
    name="argmin_cuda",
    cpp_sources=argmin_cpp_source,
    cuda_sources=argmin_source,
    functions=["argmin_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return argmin_cuda_mod.argmin_cuda(x, self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095
dim = 1

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2).cuda()
    return [x]

def get_init_inputs():
    return [dim]
```

Wait, but in the original code, the get_inputs function was returning tensors on CPU, but in the example, the user's example used .cuda(). The problem's original code has get_inputs returning tensors on CPU (because it says "randomly generate input tensors based on the model architecture"). But in the problem's given code:

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

Thus, to be compatible, the new code should have the inputs generated on CPU. But the kernel expects the input to be on CUDA. Wait, but the custom kernel must be called with tensors on CUDA.

Thus, in the forward function of ModelNew, the input x must be on the same device as the kernel. Since the kernel is CUDA, the input must be on CUDA. Therefore, the get_inputs function should return CUDA tensors.

But the original code's get_inputs returns CPU tensors. To make it compatible, perhaps the user expects that the input is moved to CUDA in the forward pass. However, in PyTorch, modules typically expect inputs to be on the correct device.

Alternatively, the get_inputs function in the original code may have been intended to produce CPU tensors, but the model expects them on CUDA. So in the optimized code, the get_inputs should generate CUDA tensors.

Looking at the problem's given code for the example (the first one), the get_inputs there uses .cuda().

In the problem's given Model code, the get_inputs returns tensors on CPU (without .cuda()), but the example in the question's first part had the get_inputs returning CUDA tensors. The user might have intended that in the new code, the inputs are on CUDA.

Thus, I will adjust the get_inputs in the new code to return CUDA tensors.

Hence, in the provided code above, the get_inputs function is modified to include .cuda().

The rest of the code should be correct. The kernel is designed to handle any reduction dimension (0,1,2) for a 3D tensor. The ModelNew uses the custom kernel instead of torch.argmin.

Now, verifying correctness:

- The kernel correctly computes the argmin along the specified dimension.

- The wrapper function creates the output tensor with the correct shape and dtype (int64).

- The kernel launch parameters are correct.

Potential issues:

- The kernel's block and grid sizes must be chosen properly. The code uses 256 threads per block, which is standard. The number of blocks is computed as (output_size + block_size-1)//block_size, which is correct.

- The loop over the reduction dimension's elements (for i in 1 to dim_size) must be correct. For large dim_size (like 4096), this loop will execute 4095 iterations per thread, which could be slow. However, this is inherent in the problem, as each thread must process the entire dimension to find the minimum. There's no way around it unless using a parallel reduction, which would be more complex.

- The code assumes the input is float32. If the input is of a different type (e.g., double), the kernel would fail. The AT_DISPATCH_FLOATING_TYPES macro in the wrapper handles this by dispatching to the appropriate type. However, the kernel uses float*, which may not match. So the kernel needs to be modified to use a template or handle all types.

Wait, this is a problem! The kernel code currently uses float*, but the wrapper uses AT_DISPATCH_FLOATING_TYPES which dispatches to scalar_t (the input's type). So the kernel should be a template.

This is a critical mistake. The kernel must be able to handle different data types.

To fix this, the kernel needs to be a template function, and the wrapper must dispatch to it based on the input type.

This complicates the code.

Let me adjust the kernel to be templated.

The kernel function should be a template:

template <typename scalar_t>
__global__ void argmin_kernel(
    const scalar_t* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    // The same code as before, but using scalar_t instead of float
}

Then, in the wrapper function, the kernel launch would be:

AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "argmin_cuda", ([&] {
    argmin_kernel<scalar_t><<<num_blocks, block_size>>>(
        input.data_ptr<scalar_t>(),
        output.data_ptr<int64_t>(),
        batch_size,
        dim1_size,
        dim2_size,
        dim
    );
}));

The kernel code now needs to be templated.

Thus, updating the kernel code:

In the CUDA source string:

template <typename scalar_t>
__global__ void argmin_kernel(
    const scalar_t* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    // same code as before, with scalar_t instead of float
}

Then, in the wrapper function, the kernel is called as argmin_kernel<scalar_t>.

This is necessary to handle different input types.

Thus, correcting the kernel code:

The revised kernel code in the CUDA source string:

template <typename scalar_t>
__global__ void argmin_kernel(
    const scalar_t* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;

    int output_size;
    if (reduction_dim ==0) {
        output_size = dim1_size * dim2_size;
    } else if (reduction_dim ==1) {
        output_size = batch_size * dim2_size;
    } else { // reduction_dim ==2
        output_size = batch_size * dim1_size;
    }

    if (out_idx >= output_size) return;

    int dim_size;
    if (reduction_dim ==0) dim_size = batch_size;
    else if (reduction_dim ==1) dim_size = dim1_size;
    else dim_size = dim2_size;

    int start, stride_step;

    if (reduction_dim ==0) {
        int d1 = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = d1 * dim2_size + d2;
        stride_step = dim1_size * dim2_size;
    } else if (reduction_dim ==1) {
        int b = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = b * (dim1_size * dim2_size) + d2;
        stride_step = dim2_size;
    } else {
        int b = out_idx / dim1_size;
        int d1 = out_idx % dim1_size;
        start = b * (dim1_size * dim2_size) + d1 * dim2_size;
        stride_step = 1;
    }

    int min_idx = 0;
    scalar_t min_val = input[start];

    for (int i = 1; i < dim_size; ++i) {
        scalar_t current_val = input[start + i * stride_step];
        if (current_val < min_val) {
            min_val = current_val;
            min_idx = i;
        }
    }

    output[out_idx] = min_idx;
}

And in the wrapper function:

AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "argmin_cuda", ([&] {
    argmin_kernel<scalar_t><<<num_blocks, block_size>>>(
        input.data_ptr<scalar_t>(),
        output.data_ptr<int64_t>(),
        batch_size,
        dim1_size,
        dim2_size,
        dim
    );
}));

This now handles different floating point types.

Thus, the corrected code includes these changes.

Now, with these changes, the kernel should handle float or double inputs.

Finally, the complete code is as written above with the corrected kernel and wrapper.

Another thing to check: the output tensor's device must match the input's device.

In the wrapper function:

output = torch::empty(output_shape, torch::dtype(torch::kInt64).device(input.device()));

This correctly places the output on the same device as the input.

Thus, the code should be correct.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for argmin
argmin_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void argmin_kernel(
    const scalar_t* input,
    int64_t* output,
    int batch_size,
    int dim1_size,
    int dim2_size,
    int reduction_dim
) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;

    int output_size;
    if (reduction_dim == 0) {
        output_size = dim1_size * dim2_size;
    } else if (reduction_dim == 1) {
        output_size = batch_size * dim2_size;
    } else { // reduction_dim == 2
        output_size = batch_size * dim1_size;
    }

    if (out_idx >= output_size) return;

    int dim_size;
    if (reduction_dim == 0) dim_size = batch_size;
    else if (reduction_dim == 1) dim_size = dim1_size;
    else dim_size = dim2_size;

    int start, stride_step;

    if (reduction_dim == 0) {
        int d1 = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = d1 * dim2_size + d2;
        stride_step = dim1_size * dim2_size;
    } else if (reduction_dim == 1) {
        int b = out_idx / dim2_size;
        int d2 = out_idx % dim2_size;
        start = b * (dim1_size * dim2_size) + d2;
        stride_step = dim2_size;
    } else {
        int b = out_idx / dim1_size;
        int d1 = out_idx % dim1_size;
        start = b * (dim1_size * dim2_size) + d1 * dim2_size;
        stride_step = 1;
    }

    int min_idx = 0;
    scalar_t min_val = input[start];

    for (int i = 1; i < dim_size; ++i) {
        scalar_t current_val = input[start + i * stride_step];
        if (current_val < min_val) {
            min_val = current_val;
            min_idx = i;
        }
    }

    output[out_idx] = min_idx;
}

torch::Tensor argmin_cuda(torch::Tensor input, int dim) {
    auto input_shape = input.sizes();
    if (input.dim() != 3) {
        throw std::invalid_argument("Input must be a 3D tensor");
    }
    int batch_size = input_shape[0];
    int dim1_size = input_shape[1];
    int dim2_size = input_shape[2];

    int output_size;
    torch::IntArrayRef output_shape;
    if (dim == 0) {
        output_size = dim1_size * dim2_size;
        output_shape = {dim1_size, dim2_size};
    } else if (dim == 1) {
        output_size = batch_size * dim2_size;
        output_shape = {batch_size, dim2_size};
    } else {
        output_size = batch_size * dim1_size;
        output_shape = {batch_size, dim1_size};
    }

    auto output = torch::empty(output_shape, torch::dtype(torch::kInt64).device(input.device()));

    const int block_size = 256;
    int num_blocks = (output_size + block_size - 1) / block_size;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "argmin_cuda", ([&] {
        argmin_kernel<scalar_t><<<num_blocks, block_size>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<int64_t>(),
            batch_size,
            dim1_size,
            dim2_size,
            dim
        );
    }));

    cudaDeviceSynchronize();
    return output;
}
"""

argmin_cpp_source = """
torch::Tensor argmin_cuda(torch::Tensor input, int dim);
"""

# Compile the inline CUDA code
argmin_cuda_mod = load_inline(
    name="argmin_cuda",
    cpp_sources=argmin_cpp_source,
    cuda_sources=argmin_source,
    functions=["argmin_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return argmin_cuda_mod.argmin_cuda(x, self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095
dim = 1

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2).cuda()
    return [x]

def get_init_inputs():
    return [dim]
```