The goal is to achieve maximum possible speedup, so focus on the operator that has the most potential for speedup. For the architecture given, the operator to focus on is the torch.softmax operator. The rest of the code can be left as is.

You must use the torch.utils.cpp_extension.load_inline method to embed your CUDA code directly in the Python file. You may define helper functions or other kernels as needed.

You must ensure that the new architecture ModelNew is functionally equivalent to the original Model. The outputs must be the same when given the same inputs.

Make sure that all the inputs and outputs are contiguous and on the same device as the original code. You may assume that inputs are on CUDA device.

You can use any PyTorch internal functions or CUDA libraries as needed, but do not use third-party libraries.

**Important:** The input tensors to the model may have different shapes. Your kernel must handle tensors of arbitrary shape, but the batch dimension (first dimension) is always batch_size and the second dimension is num_features (dim). So for example, the input could be (batch_size, dim) or (batch_size, num_features, ...), but in the given case, the input is (batch_size, dim). The kernel must be able to handle any shape as long as the batch is the first dimension and features are along the second dimension. However, in the given problem, the input is 2D, so you can specialize for that.

Wait, the problem says "the input could be (batch_size, dim) or (batch_size, num_features, ...), but in the given case, the input is (batch_size, dim)". But in the given architecture, the input is 2D. So I can just handle 2D case? Or must I handle arbitrary dimensions?

Looking at the original forward function's docstring: It says the input is shape (batch_size, num_features), so the kernel can be written for 2D tensors. However, the problem mentions that inputs may have different shapes, so maybe it's better to handle arbitrary dimensions. Wait, the problem says "the input could be (batch_size, dim) or (batch_size, num_features, ...), but in the given case, the input is (batch_size, dim)". So perhaps the kernel should handle arbitrary shapes where the batch is first and features are along the remaining dimensions? Or perhaps the problem expects us to handle only the given 2D case. The user might have made a mistake in the problem statement, but given the original architecture's forward function specifies 2D input, maybe just handle that. Let me check the original code: the get_inputs() returns x of shape (batch_size, dim), so 2D. The problem says "input could be" but in the given case it's 2D, so perhaps we can focus on 2D for maximum speed. Let me proceed under that assumption.

Now, the goal is to replace torch.softmax with a custom CUDA kernel for maximum speed. The standard implementation of torch.softmax involves three steps: subtracting the max for numerical stability, exponentiating, then dividing by the sum.

The standard implementation for softmax can be slow because of the reduce operations (summing over features). The key is to optimize this reduce step.

Possible optimizations for softmax:

1. Use shared memory to reduce memory access for the sum.

2. Use warp-level parallelism for the reduction.

3. Avoid atomic operations by using a parallel reduction algorithm.

4. Combine the exponentiation and summation steps to reduce the number of memory transactions.

The standard approach for a CUDA kernel for softmax:

For a 2D input (batch_size, dim), each batch element is independent. So each thread can process one element of the batch. Wait, no: the softmax is applied along the feature dimension (dim=1). So for each batch element, we have to compute the softmax over its dim features.

Thus, the kernel can be structured such that each block handles one batch element. The threads in the block process each element of the feature dimension. But for reduction (summing over features), we need to perform a reduction over the features of each batch element.

The steps for one batch element:

- Find the max value along the features (to prevent overflow)

- Subtract the max from all elements, exponentiate, sum the exponents, then divide each by the sum.

Thus, the kernel can be structured as:

For each batch element:

1. Compute max_val for the features.

2. Compute exp(x_i - max_val) for each feature.

3. Sum all exp terms to get sum_exp.

4. Compute output = exp(x_i - max_val) / sum_exp.

The problem is the reduction step (step 3), which is O(N) where N is the number of features. For N=393k, that's a lot. So parallelizing the reduction is key.

Approach for CUDA kernel:

- Each batch element is processed by a block.

- Within a block, threads can cooperate to compute the max and the sum.

Let me think in terms of threads and blocks.

Suppose for each batch element (there are 4096 batches):

We launch a block for each batch element. The number of blocks is 4096.

Each block has enough threads to cover the features. For 393216 features, that's 393k elements. So if each block has, say, 256 threads, then the number of warps is 393216 / 256 ≈ 1536 warps per block. Wait, no, each thread would process one element? Or perhaps use a tiled approach.

Wait, perhaps better to use a block per batch element. Let me think of the block processing the features of a single batch element.

Each block has Nthreads threads. Let's say Nthreads = 256.

Each thread in the block is responsible for a chunk of the features. For example, each thread handles (feature_count / Nthreads) elements.

First, compute max_val:

Each thread computes the max of its chunk, then perform a block-wide reduction to get the overall max.

Similarly, compute the exponents and accumulate the sum.

The steps would be:

For each batch element (block):

1. Each thread loads a chunk of the input features, computes the local max, and stores it in shared memory.

2. Perform a block reduction to compute the global max for that batch element.

3. Each thread then computes the exponent of (x_i - max_val), stores in shared memory.

4. Perform a block reduction to compute the sum of exponents.

5. Each thread then divides the exponent by the sum and writes the result to output.

This approach uses shared memory for intermediate storage and reduces the number of global memory accesses.

The key is implementing the reductions efficiently.

Let's outline the CUDA kernel steps.

First, the kernel signature:

__global__ void softmax_kernel(const float* input, float* output, int batch_size, int features) {

    // Each block handles one batch element.
    int batch_idx = blockIdx.x;

    // Each thread in the block processes a portion of the features.
    int tid = threadIdx.x;

    // Load the input for this batch into shared memory for processing.
    // But since features can be large (393k), shared memory might not be enough.

Wait, 393216 elements * 4 bytes per float is about 1.5MB, which exceeds the shared memory capacity (which is 48 or 96KB per SM, depending on the GPU). Thus, using shared memory for the entire feature vector is not feasible.

Alternative approach:

Use a block per batch element, and within the block, use a reduction approach where threads compute partial max and partial sum without using shared memory (but with registers and block-wide synchronization).

Wait, for the max computation:

Each thread can compute the max over a subset of features. To compute the global max, we need to combine all these local maxima.

Similarly for the sum.

Here's an approach:

For the max:

- Each thread processes a chunk of features, computes the local max.

- Use a parallel reduction within the block to compute the global max.

- Similarly for the sum after exponentiation.

The algorithm steps for a single batch element (per block):

1. Compute max_val:

   a. Each thread loads a chunk of features, computes the max of its chunk.

   b. Use block-wide reduction to find the overall max_val.

2. Compute exponentials and accumulate sum:

   a. Each thread computes exp(x_i - max_val) for its chunk of features.

   b. Sum all exponents across the block to get sum_exp.

3. Compute the output elements by dividing each exponential by sum_exp.

To handle the chunks, the number of threads per block must be chosen such that the total number of elements per chunk per thread is manageable.

Let me consider using 256 threads per block. For 393216 features:

Each thread would process 393216 / 256 ≈ 1536 elements.

But this requires a lot of computation per thread, but with parallelism, it might be manageable.

Alternatively, use more threads, but that might not be feasible.

Alternatively, use a tiled approach where each thread processes one element, but that would require 393k threads, which is way too many (max threads per block is 1024 on some GPUs).

Thus, the first approach with 256 threads per block, each handling ~1500 elements, seems feasible.

Let me structure the code:

First, the kernel:

Each block corresponds to a batch element. The block index blockIdx.x is the batch index.

The number of blocks is batch_size (4096).

Each block has blockDim.x threads, say 256.

Each thread processes (features / blockDim.x) elements.

The features must be divisible by blockDim.x, but since the problem says arbitrary shapes, perhaps we can handle it with modulo.

First, for the max computation:

In the block, each thread loads its chunk, computes local max.

Then, perform a block reduction to get the global max.

Then, compute exponents and accumulate sum.

But the exponent and sum steps need to be done in the same pass?

Alternatively:

First compute max, then compute exponents and sum.

But the sum needs the exponents, so that's necessary.

Now, let's outline the steps in code.

First, shared memory for the max reduction. Since the max is a single value per block, perhaps we can use a register to hold the max, but need synchronization.

Wait, here's a possible approach using a block-wide reduction:

For the max:

Each thread loads their chunk's elements, computes the max of their chunk. They then write this to shared memory.

Then, perform a reduction over the shared memory to get the block's max.

Wait, but shared memory might not be necessary for the first step. Alternatively, use a register for each thread's local max, then use a reduction kernel.

Alternatively, here's a standard parallel reduction approach:

Each thread has a local max.

The threads then perform a reduction in steps, pairing threads to compute the max between two values.

This requires multiple synchronization steps.

The same approach can be used for the sum.

Let me think of code for the max reduction:

Inside the block (for a single batch):

// Compute local max for each thread's chunk.

float local_max = -INFINITY;

int start = tid * (features / blockDim.x);

int end = (tid + 1) * (features / blockDim.x);

for (int i = start; i < end; ++i) {

   float val = input[batch_idx * features + i];

   if (val > local_max) {

       local_max = val;

   }

}

// Now perform block reduction to get global max.

// Use a parallel reduction in shared memory.

float *sdata = shared memory array.

sdata[threadIdx.x] = local_max;

__syncthreads();

// then reduce sdata to get max.

But the problem is that the block size could be 256, so the shared memory needed is 256 floats (1KB), which is manageable.

Wait, let's think in code:

First, the block has blockDim.x threads.

Each thread computes its local max, then writes to shared memory.

Then, we perform a reduction in shared memory:

for (int s=blockDim.x/2; s>0; s>>=1) {

    if (threadIdx.x < s) {

        sdata[threadIdx.x] = max(sdata[threadIdx.x], sdata[threadIdx.x + s]);

    }

    __syncthreads();

}

Then, the global max is sdata[0].

Similarly for the sum.

Wait, for the sum:

After computing the exponents, each thread would compute the sum of their exponents in their chunk.

Then perform a block reduction for the sum.

So, the steps are:

Per batch element (per block):

1. Compute max_val.

2. Compute exponents and their local sum.

3. Compute global sum.

4. Write the results.

Now, putting this together.

First, the kernel:

__global__ void softmax_kernel(const float *input, float *output, int batch_size, int features) {

    extern __shared__ float shared_data[];

    // Each block is a batch element.

    int batch_idx = blockIdx.x;

    // Each thread processes a chunk.

    int tid = threadIdx.x;

    // Compute max.

    float local_max = -FLT_MAX;

    // Each thread handles a chunk of features.

    int chunk_size = (features + blockDim.x - 1) / blockDim.x;

    for (int i = tid; i < features; i += blockDim.x) {

        float val = input[batch_idx * features + i];

        if (val > local_max) {

            local_max = val;

        }

    }

    // Write to shared memory.

    shared_data[threadIdx.x] = local_max;

    __syncthreads();

    // Reduce to find max.

    for (int s = blockDim.x/2; s > 0; s >>=1) {

        if (threadIdx.x < s) {

            if (shared_data[threadIdx.x] < shared_data[threadIdx.x + s]) {

                shared_data[threadIdx.x] = shared_data[threadIdx.x + s];

            }

        }

        __syncthreads();

    }

    float max_val = shared_data[0];

    __syncthreads();

    // Now compute exponents and accumulate sum.

    // Reset shared data for sum.

    shared_data[threadIdx.x] = 0.0f;

    __syncthreads();

    // Each thread computes their chunk's exponent sum.

    for (int i = tid; i < features; i += blockDim.x) {

        float exp_val = expf(input[batch_idx * features + i] - max_val);

        shared_data[threadIdx.x] += exp_val;

    }

    __syncthreads();

    // Reduce sum.

    for (int s = blockDim.x/2; s > 0; s >>=1) {

        if (threadIdx.x < s) {

            shared_data[threadIdx.x] += shared_data[threadIdx.x + s];

        }

        __syncthreads();

    }

    float sum_exp = shared_data[0];

    __syncthreads();

    // Now compute the output.

    for (int i = tid; i < features; i += blockDim.x) {

        float exp_val = expf(input[batch_idx * features + i] - max_val);

        output[batch_idx * features + i] = exp_val / sum_exp;

    }

}

Wait, but in the above code, the exponent is computed twice: once for the sum and once for the output. To save computation, we can compute the exponent once and store it in shared memory, but that may require more shared memory.

Alternatively, compute the exponent once per element, but store it in a register for the first pass (sum), and then reuse it in the output step.

Wait, let's see:

In the sum calculation step:

Each thread computes their chunk's exponents and sums them into shared memory.

But they can also store the exponents in a register (array) to reuse for the output step.

Wait, but for a large chunk (e.g., 1500 elements per thread), storing in a register array may not be feasible.

Alternatively, compute the exponents again in the output step. That's 2x computation but may be manageable.

Alternatively, the first loop (for the max) could be combined with the exponentiation and summing, but that might complicate things.

Alternatively, let's reorganize the code to compute the exponent once.

Here's a revised plan:

Per batch (block):

1. Compute max_val (as before).

2. Each thread computes their chunk's exponents and stores them in registers (or shared memory if possible), then compute the sum.

But storing exponents in shared memory may require too much memory. For 393k elements, even storing in shared memory per block would require 393KB per block, which is not feasible.

Hence, recomputing the exponent is necessary.

Thus, the code above is acceptable, but with some redundant computation.

Alternatively, the exponent can be precomputed in a separate step.

Wait, the code as written has two passes over the data for exponents:

First, in the sum phase, each thread computes the exponent for their chunk's elements and sums them.

Second, in the output phase, each thread recomputes the exponent and divides by the sum.

This is redundant, so let's try to eliminate that.

Alternative approach:

In the sum phase, compute the exponent and store it in a temporary array. However, since we don't have global memory for that, perhaps use shared memory for a partial storage, but it's not straightforward.

Alternatively, compute the exponents and store them in registers. For each thread's chunk, the thread can store the exponents in an array in registers, but with 1500 elements per thread, that's 1500 floats, which is 6KB per thread, which is way too much (registers are limited to ~2KB per thread).

Thus, that's not feasible.

So, the code as written is the way to go, with the redundant computation. Alternatively, optimize by computing the exponent once and storing in a register per element. But that may not be possible.

Alternatively, use a single loop for both the exponent and sum:

Wait, in the sum phase, each thread can compute the exponent and accumulate the sum, but without storing the exponent. Then, when computing the output, they have to recompute the exponent. So, there is some redundancy, but it's necessary.

Now, considering that expf is a relatively fast operation (since it's a single-precision exponential), the extra computation might be acceptable for the sake of code simplicity.

Thus, proceeding with the original code.

Now, the shared memory usage in the kernel must be declared.

The shared memory for the max and sum steps.

The total shared memory needed is blockDim.x * sizeof(float) * 2 (for the max and sum phases), but actually, the same shared memory can be reused between phases.

Wait in the code above, after computing the max, the shared_data is reused for the sum.

Yes, the code first uses shared_data for max, then reuses it for the sum by resetting it to zero.

Thus, the total shared memory required is blockDim.x floats, which for 256 threads is 256 * 4 = 1KB, which is acceptable.

Now, in the kernel call:

The shared memory is allocated by the user. So, when launching the kernel, we need to specify the shared memory size per block.

The required shared memory is blockDim.x * sizeof(float).

In the code above, the kernel requires:

extern __shared__ float shared_data[];

The size is passed as an argument when launching the kernel.

Thus, when calling the kernel, we need to allocate blockDim.x * sizeof(float) bytes.

Now, the kernel function signature must include the shared memory parameter.

Wait, the kernel is declared as:

__global__ void softmax_kernel(const float *input, float *output, int batch_size, int features) {

But to use extern shared memory, the kernel launch must specify the amount of shared memory per block.

The kernel call would be:

softmax_kernel<<<num_blocks, threads_per_block, shared_mem_size>>>(input, output, ...);

Now, in the Python code, when defining the CUDA source.

Putting this all together.

Now, the Python code:

First, define the CUDA kernel code.

The kernel will be in a string.

We also need to handle input and output tensors.

The kernel will process batches in blocks, so the number of blocks is batch_size (4096).

The number of threads per block is chosen as 256 (a typical value, but could be tuned).

Thus, in code:

elementwise_add_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void softmax_kernel(const float* input, float* output, int batch_size, int features) {
    extern __shared__ float shared_data[];

    int batch_idx = blockIdx.x;
    int tid = threadIdx.x;

    float local_max = -FLT_MAX;

    // Compute max for each thread's chunk
    for (int i = tid; i < features; i += blockDim.x) {
        float val = input[batch_idx * features + i];
        if (val > local_max) {
            local_max = val;
        }
    }

    // Write to shared memory for max reduction
    shared_data[threadIdx.x] = local_max;
    __syncthreads();

    // Reduce to find max_val
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            if (shared_data[threadIdx.x] < shared_data[threadIdx.x + s]) {
                shared_data[threadIdx.x] = shared_data[threadIdx.x + s];
            }
        }
        __syncthreads();
    }

    float max_val = shared_data[0];
    __syncthreads();

    // Reset shared memory for sum calculation
    shared_data[threadIdx.x] = 0.0f;
    __syncthreads();

    // Compute exponents and accumulate sum
    for (int i = tid; i < features; i += blockDim.x) {
        float exp_val = expf(input[batch_idx * features + i] - max_val);
        shared_data[threadIdx.x] += exp_val;
    }

    __syncthreads();

    // Reduce to find sum_exp
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared_data[threadIdx.x] += shared_data[threadIdx.x + s];
        }
        __syncthreads();
    }

    float sum_exp = shared_data[0];
    __syncthreads();

    // Compute output values
    for (int i = tid; i < features; i += blockDim.x) {
        float exp_val = expf(input[batch_idx * features + i] - max_val);
        output[batch_idx * features + i] = exp_val / sum_exp;
    }
}

torch::Tensor softmax_cuda(torch::Tensor input) {
    const int batch_size = input.size(0);
    const int features = input.size(1);
    const int threads_per_block = 256;
    const int blocks = batch_size;

    // Output tensor
    auto output = torch::empty_like(input);

    // Calculate shared memory size
    int shared_mem_size = threads_per_block * sizeof(float);

    // Launch kernel
    softmax_kernel<<<blocks, threads_per_block, shared_mem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        features
    );

    return output;
}
"""

Wait, but in the kernel code, when using shared memory, the size is specified when launching the kernel.

Yes, the third argument to <<<...>>> is the shared memory size.

Now, in the Python code:

We need to define the CUDA sources and the wrapper function.

Wait, the code above defines the kernel and a wrapper function softmax_cuda, which is called from Python.

Thus, the Python code would load this inline CUDA code.

Now, in the ModelNew class, replace the forward method with a call to softmax_cuda.

Thus, the code would be:

from torch.utils.cpp_extension import load_inline

softmax_source = ... (the above CUDA code)

softmax_cpp_source = "torch::Tensor softmax_cuda(torch::Tensor input);"

Then load the CUDA module:

softmax = load_inline(...)

class ModelNew(nn.Module):

    def __init__(self):
        super().__init__()
        self.softmax_cuda = softmax

    def forward(self, x):
        return self.softmax_cuda.softmax_cuda(x)

But need to ensure that the input is on the GPU.

Wait, in the original code, the inputs are generated with get_inputs() which returns tensors on the CPU. But in the problem statement, it says "you may assume that inputs are on CUDA device".

Wait, the original get_inputs() function in the problem is:

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

Which creates tensors on CPU. However, the problem says "you may assume that inputs are on CUDA device".

Thus, in the new code, we can assume x is on CUDA.

Therefore, in the CUDA code, the input must be on CUDA.

Thus, the kernel code is correct.

Now, check for possible errors.

First, in the kernel:

The input is stored as a contiguous tensor? PyTorch tensors may be non-contiguous, but the code assumes that the data is stored in a contiguous block.

Thus, the code should ensure that input is contiguous. The wrapper function should check that input is contiguous and on CUDA.

In the wrapper function, we can add:

assert input.is_cuda and input.is_contiguous(), "Input must be contiguous CUDA tensor"

Thus, modifying the wrapper function:

torch::Tensor softmax_cuda(torch::Tensor input) {
    CHECK_CUDA(input);
    CHECK_CONTIGUOUS(input);

    // ... rest as before

}

But in the CUDA code, need to include macros for that.

Alternatively, in the code:

if (!input.is_contiguous() || !input.is_cuda()) {
    AT_ERROR("Input must be contiguous CUDA tensor");
}

Wait, using PyTorch's error checking.

Thus, adding:

if (!input.is_contiguous()) {
    AT_ERROR("Input must be contiguous");
}
if (!input.device().is_cuda()) {
    AT_ERROR("Input must be on CUDA");
}

So, in the code:

torch::Tensor softmax_cuda(torch::Tensor input) {
    if (!input.is_contiguous()) {
        AT_ERROR("Input must be contiguous");
    }
    if (!input.device().is_cuda()) {
        AT_ERROR("Input must be on CUDA");
    }

    const int batch_size = input.size(0);
    const int features = input.size(1);
    const int threads_per_block = 256;
    const int blocks = batch_size;

    auto output = torch::empty_like(input);

    int shared_mem_size = threads_per_block * sizeof(float);

    softmax_kernel<<<blocks, threads_per_block, shared_mem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        features
    );

    return output;
}

Also, need to include <ATen/ATen.h> and other headers.

Wait, the kernel uses torch::Tensor, so the headers are already included.

Wait, in the CUDA code, the kernel is in a .cu file, but in the inline code, the headers must be properly included.

The code above includes <torch/extension.h>, which should be sufficient.

Now, check the loop in the kernel for features:

In the kernel loops:

for (int i = tid; i < features; i += blockDim.x) {

This correctly iterates over all features in the current batch element.

The output is written to the correct location: output[batch_idx * features + i]

Assuming that the tensors are 2D, with batch_size and features as the two dimensions, the storage is contiguous, so this addressing is correct.

Now, testing the code:

Suppose batch_size is 4096 and features is 393216.

Each block has 256 threads.

The number of threads per block is 256, so each thread handles 393216 / 256 ≈ 1536 elements per thread.

In the first loop (max computation), each thread loops 1536 times.

The reduction steps are O(log2(256)) steps, which is manageable.

The same for the sum reduction.

The final loop also loops 1536 times.

The total number of operations per thread is:

For max: 1536 comparisons.

For exponents in sum and output: 1536 * 2 exponentials, plus the division.

The exponentials are the main computational cost, but since they are computed in parallel across all threads, it should be faster than the PyTorch implementation which might be less optimized.

Potential improvements:

- Use warp-level reduction for better performance.

- Tune the number of threads per block (256 might not be optimal).

- Use a larger block size (but limited by the maximum threads per block, which is 1024 on many GPUs).

- Handle the case where features is not divisible by the number of threads.

In the current code, when features is not divisible by blockDim.x, the for loop (i += blockDim.x) ensures that all elements are processed. For example, if features=5 and blockDim.x=3, then threads 0,1,2 will process i=0,1,2; then threads 0,1,2 will process i=3,4,5 (but 5 is beyond features, so only thread 0 and 1 will process 3 and 4, and thread 2 will process i=5 which is beyond, so their loop stops at i < features.

Thus, the code handles arbitrary feature dimensions.

Now, compiling this code with load_inline should work.

Thus, the final Python code with the CUDA kernel is as follows.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

softmax_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void softmax_kernel(const float* input, float* output, int batch_size, int features) {
    extern __shared__ float shared_data[];

    int batch_idx = blockIdx.x;
    int tid = threadIdx.x;

    float local_max = -FLT_MAX;

    // Compute max for each thread's chunk
    for (int i = tid; i < features; i += blockDim.x) {
        float val = input[batch_idx * features + i];
        if (val > local_max) {
            local_max = val;
        }
    }

    // Write to shared memory for max reduction
    shared_data[threadIdx.x] = local_max;
    __syncthreads();

    // Reduce to find max_val
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            if (shared_data[threadIdx.x] < shared_data[threadIdx.x + s]) {
                shared_data[threadIdx.x] = shared_data[threadIdx.x + s];
            }
        }
        __syncthreads();
    }

    float max_val = shared_data[0];
    __syncthreads();

    // Reset shared memory for sum calculation
    shared_data[threadIdx.x] = 0.0f;
    __syncthreads();

    // Compute exponents and accumulate sum
    for (int i = tid; i < features; i += blockDim.x) {
        float exp_val = expf(input[batch_idx * features + i] - max_val);
        shared_data[threadIdx.x] += exp_val;
    }

    __syncthreads();

    // Reduce to find sum_exp
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared_data[threadIdx.x] += shared_data[threadIdx.x + s];
        }
        __syncthreads();
    }

    float sum_exp = shared_data[0];
    __syncthreads();

    // Compute output values
    for (int i = tid; i < features; i += blockDim.x) {
        float exp_val = expf(input[batch_idx * features + i] - max_val);
        output[batch_idx * features + i] = exp_val / sum_exp;
    }
}

torch::Tensor softmax_cuda(torch::Tensor input) {
    if (!input.is_contiguous()) {
        AT_ERROR("Input must be contiguous");
    }
    if (!input.device().is_cuda()) {
        AT_ERROR("Input must be on CUDA");
    }

    const int batch_size = input.size(0);
    const int features = input.size(1);
    const int threads_per_block = 256;
    const int blocks = batch_size;

    auto output = torch::empty_like(input);

    int shared_mem_size = threads_per_block * sizeof(float);

    softmax_kernel<<<blocks, threads_per_block, shared_mem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        features
    );

    return output;
}
"""

softmax_cpp_source = """
torch::Tensor softmax_cuda(torch::Tensor input);
"""

softmax = load_inline(
    name="softmax_cuda",
    cpp_sources=softmax_cpp_source,
    cuda_sources=softmax_source,
    functions=["softmax_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.softmax_cuda = softmax

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.softmax_cuda.softmax_cuda(x)
```

This code replaces the PyTorch `torch.softmax` with a custom CUDA kernel optimized for speed. The kernel processes each batch element in a separate CUDA block, using block-level parallel reductions for computing the maximum value and sum of exponentials efficiently. This approach minimizes global memory access and leverages shared memory for intermediate results, leading to faster execution compared to the default implementation.