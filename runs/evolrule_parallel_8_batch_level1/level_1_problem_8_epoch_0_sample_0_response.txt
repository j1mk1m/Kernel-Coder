The matrix dimensions are M=8205, K=2949, N=5921. The problem is to compute C = A * B with these irregular matrix dimensions. The goal is to maximize the speed of the matrix multiplication on a GPU. You can choose to implement a custom CUDA kernel for matrix multiplication and/or make other optimizations. The input tensors A and B are dense and contiguous. 

You are allowed to write the code in either Python with inline CUDA extensions or in a separate .cu file, but given the previous example, it's better to stick with the Python with inline CUDA extensions style. 

Your solution should achieve a higher GFLOPS than PyTorch's native matmul implementation. Please note that the problem requires you to write a custom CUDA kernel for the matrix multiplication. 

When writing your code, pay attention to the following:

- The dimensions are irregular and not divisible by common block sizes, so thread block dimensions should be chosen carefully to avoid unused threads. 
- Use shared memory to exploit spatial locality and reduce global memory accesses. 
- Implement proper memory coalescing for both input and output accesses.
- Use proper thread and block indexing to handle irregular matrix dimensions. 
- You may choose to pad the matrices, but this is optional. However, be cautious of the overhead introduced by padding.

Additionally, you may want to optimize for the given specific dimensions (M=8205, K=2949, N=5921), but the code should still handle arbitrary dimensions. 

The code must be compatible with PyTorch's autograd system (i.e., the kernel must be differentiable, unless you implement a backward kernel as well). 

The code must be able to be run on a GPU. 

The code must be written in the style of the provided example, using load_inline for the CUDA kernel. 

The code must be correct. The output tensor should match the result of PyTorch's matmul. 

The code must be as fast as possible. 

You should not use any existing libraries like cuBLAS, but instead implement the matrix multiplication from scratch.

I will compile and test your code on a GPU, and if it does not work or is slower than PyTorch's implementation, I will deduct points. So make sure your code is correct and faster.




Alright, I need to optimize the matrix multiplication for the given dimensions (8205x2949 and 2949x5921) using a custom CUDA kernel. Let's think through the steps required.

First, the standard approach for matrix multiplication on CUDA uses shared memory to reduce global memory accesses. Since the matrices are irregular, I'll have to manage the thread blocks and indexing carefully. 

I'll start by determining the thread block dimensions. The example uses a block size of 256 for element-wise operations, but for matrix multiplication, 2D blocks are more common. Maybe a 16x16 block size? Let me check the dimensions: 

The matrix C has dimensions MxN = 8205x5921. If I use 16x16 blocks, then the grid will be ceil(8205/16) by ceil(5921/16). But 8205 divided by 16 is 512.8125, so 513 blocks in rows. Similarly for columns. That should work, but maybe tuning the block size could help.

Wait, maybe using a tiled approach where each block computes a tile of the matrix. Let me recall the tiled matrix multiplication algorithm. Each thread block computes a tile of size TBxTB, and each thread handles a single element in the tile. The shared memory tiles are loaded from global memory, and the threads compute the dot product between rows and columns of the tiles.

The standard tiling approach uses block dimensions of (BLOCK_SIZE, BLOCK_SIZE), and each thread handles one element of the tile. The shared memory tiles for A and B would be of size (BLOCK_SIZE + 1) x (BLOCK_SIZE + 1) to avoid bank conflicts? Or maybe just BLOCK_SIZE x K_BLOCK for the columns? Hmm, need to think.

Let me outline the plan:

1. Choose a block size, say 16x16. Maybe 32x8? Need to balance between shared memory usage and coalescing.

Wait, the key points from the problem: dimensions are irregular, so the block dimensions should divide into M, K, N as much as possible? Not sure. Maybe just pick a block size that's a power of two for efficiency, even if it doesn't divide the matrix dimensions.

Alternatively, use a block size of 16x16 for the tiles. Each block is responsible for a block of the output matrix C. The shared memory for A and B would be blocks of size BLOCK_SIZE x K_BLOCK? Wait, the K dimension is 2949. Hmm, maybe use a tiled approach where each block processes a tile of the output matrix, and each thread computes a small part of the tile.

Wait, here's a standard tiled approach:

Each block computes a tile of the output matrix. Let's say the block dimensions are (BLOCK_SIZE, BLOCK_SIZE). Each block is responsible for a tile of size BLOCK_SIZE x BLOCK_SIZE in the output matrix. The shared memory tiles for A and B will be of size BLOCK_SIZE x TILE_WIDTH, where TILE_WIDTH is the number of tiles along the K dimension.

Wait, actually, the standard tiled matrix multiplication uses a tile size in both dimensions. Let me recall the canonical implementation:

The output matrix is divided into tiles of size BLOCK_SIZE x BLOCK_SIZE. Each block processes one tile. The K dimension is processed in chunks of TILE_WIDTH, which is the tile size along that dimension. So, each block loads a block of A (BLOCK_SIZE x TILE_WIDTH) and B (TILE_WIDTH x BLOCK_SIZE), multiplies them, and accumulates into the output tile.

In this case, the TILE_WIDTH can be equal to the block size. Let's choose BLOCK_SIZE as 16, so each block handles a 16x16 tile. The TILE_WIDTH is also 16, so each step processes 16 elements of K.

But the problem is that the K dimension is 2949, which isn't a multiple of 16. So, the loop over the tiles along K will have to handle that. The number of tiles along K is ceil(K / TILE_WIDTH).

This approach requires that the shared memory can hold the tiles. For each block, we need shared memory arrays for A and B tiles. The size would be (BLOCK_SIZE + 1) * (TILE_WIDTH + 1) for each, but actually, the A tile is BLOCK_SIZE rows x TILE_WIDTH columns, and the B tile is TILE_WIDTH rows x BLOCK_SIZE columns.

Wait, the A tile is [block_size][tile_width], and the B tile is [tile_width][block_size]. So, the size for A's shared memory is (BLOCK_SIZE + 1)*TILE_WIDTH, and similarly for B. The +1 is to avoid bank conflicts, but maybe not necessary here. Let's see.

The kernel's main loop will iterate over each tile along the K dimension. For each tile, the block loads the A and B tiles into shared memory, then computes the dot products. 

Now, for the thread indices: each thread in the block handles one element in the output tile. So, each thread (tx, ty) in the block computes the element at (blockStartRow + ty, blockStartCol + tx) in the output matrix.

The key steps:

1. Determine the block's position in the output matrix. The block's starting row is blockIdx.y * BLOCK_SIZE, and starting column is blockIdx.x * BLOCK_SIZE.

2. Each thread loads a portion of A and B into shared memory. The shared memory tiles for A and B are loaded from global memory in a way that coalesces accesses.

3. The tile computation is done by each thread in the block, looping over all tiles along the K dimension.

Now, choosing the block size. Let me pick a block size of 16x16. The shared memory per block would be:

For A: 16 rows x 16 (tile_width) columns? Wait, if the tile width is 16, then each A tile is 16 x 16, and each B tile is 16 x 16. Wait, no, if TILE_WIDTH is 16, then the A tile is BLOCK_SIZE x TILE_WIDTH (16x16) and B is TILE_WIDTH x BLOCK_SIZE (16x16). So the shared memory for A is 16*16 = 256 elements, same for B. That's manageable.

But the K dimension is 2949, so the number of tiles along K would be ceil(2949 / 16) = 185 (since 16*184=2944, so 185 tiles). So each block has to loop 185 times, which might be a bit slow. Maybe a larger block size would reduce the number of tiles? Let's see: if block size is 32, then the number of tiles would be ceil(2949 / 32) = 93, which is better. So maybe a block size of 32 is better. However, the shared memory usage would be 32*32*2 = 2048 elements, which is 2048*4 bytes = 8KB, which is under the 48KB limit per SM. So that's okay.

Wait, 32x32 block would mean each shared memory tile is 32x32? Or 32xTILE_WIDTH. Wait, the tile width is the same as the block size. Let me re-calculate.

Wait, the block size is the size of the tile in the output matrix. The TILE_WIDTH is the size along the K dimension. So if the block size is 32, then each A tile is 32 rows x TILE_WIDTH columns, and B is TILE_WIDTH rows x 32 columns. So for block size 32 and tile width 32, the shared memory would be 32*32 for A and 32*32 for B, totaling 2048 elements (8KB). That's acceptable. So maybe 32 is better.

But the problem is that the K dimension is 2949. Let me see 2949 divided by 32: 2949 /32 = 92.15625. So 93 iterations. That's manageable.

Alternatively, maybe 16 is better. Let's pick 16 for simplicity first.

Proceeding with block size 16.

Now, the code structure:

The CUDA kernel function will be written in C++ with __global__ qualifier. The kernel will take pointers to A, B, C, and the dimensions.

Wait, the function signature would be something like:

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N)

But actually, since the dimensions are fixed (M=8205, K=2949, N=5921), but the code should handle arbitrary dimensions. Hmm, the problem says the code should handle arbitrary dimensions, so the kernel must be general.

So, the kernel must be flexible. But for performance, we can choose block and grid dimensions based on the problem's specific sizes.

Wait, the kernel must be written in a way that works for any M, K, N. But the block dimensions can be chosen as constants, say 16x16. The grid will be ceil(M / BLOCK_SIZE) * ceil(N / BLOCK_SIZE).

Now, the shared memory variables:

__shared__ float shared_A[BLOCK_SIZE][BLOCK_SIZE]; // Not sure, need to think dimensions.

Wait, no. Let me recall the standard tiled approach:

Each thread block is responsible for a tile of the output matrix C. The tile is BLOCK_SIZE x BLOCK_SIZE. To compute this tile, the block must load multiple tiles from A and B along the K dimension.

Therefore, for each tile of K in steps of TILE_WIDTH (which is the block size), the block loads a tile of A of size BLOCK_SIZE x TILE_WIDTH and a tile of B of size TILE_WIDTH x BLOCK_SIZE into shared memory. Then the block computes the contributions to the C tile.

Wait, the TILE_WIDTH can be equal to the block size. Let me define:

#define BLOCK_SIZE 16
#define TILE_WIDTH BLOCK_SIZE

Then, each block will process tiles along K in increments of TILE_WIDTH (16). So the number of iterations is ceil(K / TILE_WIDTH).

Each iteration, the block loads a tile of A starting at row startingRow, column tile_k (the current tile step) and of size BLOCK_SIZE x TILE_WIDTH.

Similarly for B: starting at row tile_k and column startingCol, of size TILE_WIDTH x BLOCK_SIZE.

The shared memory for A and B must be of size BLOCK_SIZE x TILE_WIDTH and TILE_WIDTH x BLOCK_SIZE respectively. Since TILE_WIDTH is equal to BLOCK_SIZE, that's 16x16 for each, so total shared memory per block is 16*16*2 = 512 floats = 2KB. That's very manageable.

Wait, actually, the shared memory for A is of size BLOCK_SIZE x TILE_WIDTH, so 16x16, and B is TILE_WIDTH x BLOCK_SIZE, also 16x16. So each has 256 elements. So total shared memory per block is 256*2*4 bytes = 2KB, which is okay.

So the shared memory declarations would be:

__shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
__shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

Wait, the indices are row-major, so for A's tile, each row in shared_A corresponds to a row in the A matrix, and each column is the current tile's columns. For B's tile, each row in shared_B corresponds to a row in the B matrix (which is the same as the columns of A), so the columns of B correspond to the columns of the output.

The threads in the block will load into shared memory. Each thread is responsible for loading a part of A and B.

The loop over the tiles would be:

for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {

   // load tiles into shared memory

   // sync threads

   // compute the dot product for each element in the output tile

   // sync and repeat

}

Wait, but in CUDA, the loop over tile_k is done in a loop inside the kernel. Each iteration of the loop corresponds to a different tile along K.

In each iteration:

- Each thread loads a portion of A and B into shared memory.

Wait, for the A tile: The block is processing the output tile starting at (block_row, block_col). The A tile is from A's rows starting at block_row, and columns from tile_k to tile_k + TILE_WIDTH -1. But the columns may exceed K if tile_k + TILE_WIDTH exceeds K. So need to handle that.

Similarly, the B tile is from columns starting at block_col, and rows from tile_k to tile_k + TILE_WIDTH -1.

Each thread in the block will be responsible for loading a few elements of A and B into shared memory.

The A load:

The A tile is stored in shared_A. The thread (tx, ty) in the block can be mapped to a position in the A tile. For example, each thread (ty, tx) in the block can be responsible for shared_A[ty][tx], but this needs to be done in a way that covers all elements.

Alternatively, each thread is assigned a row in shared_A, and the columns are divided among the threads in the warp.

Wait, maybe each thread loads a single element from global memory to shared memory. For example, for the A tile:

Each thread (tx, ty) in the block will load the element at row = block_row + ty, column = tile_k + tx.

But this may not cover all elements if the tile is larger than the thread count. Wait, the block has 16x16=256 threads, and the A tile is 16x16. So each thread can handle one element. That works.

Similarly for B's tile: each thread (tx, ty) can load B's element at row = tile_k + ty, column = block_col + tx.

Wait, the B's tile is TILE_WIDTH x BLOCK_SIZE (16x16), so each thread can load one element.

Therefore:

In the A tile load:

int a_row = block_row + ty;
int a_col = tile_k + tx;

Similarly for B:

int b_row = tile_k + ty;
int b_col = block_col + tx;

But we have to make sure that a_col and b_row are within the valid range. So if tile_k + tx >= K, then those elements are 0, because B's columns are N, but B's rows are K. Wait, B's rows are K, so for B's rows beyond K, we can ignore them? Wait no, B's rows are exactly K. So when tile_k + tx exceeds K, the B element is out of bounds. Wait, the loop is over tile_k from 0 to K in steps of TILE_WIDTH. So the tile_k + tx can go up to tile_k + TILE_WIDTH -1. Since tile_k is incremented by TILE_WIDTH each time, the last tile might be less than TILE_WIDTH in size. So in that case, the tile's column in B would be up to K. So in the loop, we can cap the indices.

Alternatively, in the loop, tile_k is stepped by TILE_WIDTH, but the loop runs until tile_k < K. So the last iteration may have tile_k + TILE_WIDTH exceeding K. Therefore, in the loading code, we need to clamp the indices to ensure they don't exceed K.

Wait, in the loop:

for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {

   // load A and B tiles

}

Therefore, the tile's column (for A) is up to tile_k + TILE_WIDTH -1, but tile_k can be such that tile_k + TILE_WIDTH > K. So when a_col = tile_k + tx, if a_col >= K, then the value is 0. Because in matrix multiplication, beyond K is not part of the matrix. Wait no, matrix A has exactly K columns, so all elements beyond column K-1 are out of bounds. Therefore, when a_col >= K, those elements should be considered as 0. So in the code, we can check and set to 0.

But to avoid branching, perhaps it's better to have a_col be clamped to the maximum K-1, and multiply by a mask. Alternatively, just load zero if out of bounds.

Alternatively, in the code, when loading A and B, we can check if the indices are valid, and set to zero if not. But that would introduce a conditional branch, which may hurt performance. Alternatively, in the loop, the tile_k is such that tile_k + TILE_WIDTH is up to K. Wait, but the loop is tile_k += TILE_WIDTH until tile_k < K. So the last tile could have a width less than TILE_WIDTH. For example, if K=2949, and TILE_WIDTH=16, then the last tile would start at tile_k=2944 (since 2944+16=2960 which is beyond 2949). The actual columns would be 2944 to 2948 (since 2944 + 5 = 2949). Wait, no, K is 2949, so the last tile starts at tile_k = K - (K % TILE_WIDTH). Wait, perhaps better to have the loop run until tile_k < K, and in each iteration, the actual tile width is min(TILE_WIDTH, K - tile_k).

Therefore, in the loading code, for A's columns: the column index can be tile_k + tx. If tile_k + tx < K, then it's valid; else, 0. So:

float a_val = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

Similarly for B's rows:

float b_val = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

But that would require a conditional, which might be okay since it's only once per tile iteration.

Alternatively, since the loop runs tile_k up to K, then the maximum tile_k is K - 1. So tile_k + tx can be up to K-1 + tx. Wait, no, tx is from 0 to TILE_WIDTH-1. Wait, tile_k is up to K - 1, so tile_k + tx can be up to (K-1) + (TILE_WIDTH-1). So if K < tile_k + tx, that can still happen. So yes, we need the condition.

Alternatively, we can calculate the actual tile width for each iteration:

int tile_width = min(TILE_WIDTH, K - tile_k);

Then, the column for A is tile_k + tx, but only up to tile_k + tile_width -1. So if tx < tile_width, then valid. Else, 0. But this complicates the code.

Hmm, perhaps it's manageable.

Now, after loading the tiles into shared memory, we need to synchronize threads to ensure all data is loaded.

Then, each thread computes the dot product for their part. Wait, the computation is:

Each thread (tx, ty) in the block computes the sum over the current tile's columns (the TILE_WIDTH elements) of A[block_row + ty][tile_k + k] * B[tile_k + k][block_col + tx], for k from 0 to TILE_WIDTH-1.

Wait, actually, the element C[i][j] = sum_{k} A[i][k] * B[k][j]. In the tiled approach, each tile iteration processes a chunk of the sum. So for each tile_k, the contribution to C[i][j] is the sum over the current tile's k's (from tile_k to tile_k + tile_width -1) of A[i][k] * B[k][j].

Therefore, for each tile iteration, the block computes a partial sum, which is added to the output.

Therefore, each thread needs to compute their part of the sum for their (ty, tx) element in the output tile.

Wait, in the shared memory, each thread has access to the entire A and B tiles. So for the current tile's contribution, each thread (ty, tx) can compute the partial sum over the TILE_WIDTH elements:

float sum = 0.0f;
for (int k = 0; k < TILE_WIDTH; ++k) {
    sum += shared_A[ty][k] * shared_B[k][tx];
}

But this is per iteration of the tile loop. The total sum is accumulated over all tiles.

Wait, actually, in the tiled approach, the partial sum for each tile is added to the total. So each thread must have a private variable to accumulate the sum over all tiles.

Therefore, the kernel's code outline would be:

Each thread (tx, ty) in the block:

Initialize their sum to 0.

Loop over tile_k in steps of TILE_WIDTH:

   Load A and B tiles into shared memory (if their indices are valid).

   Synchronize.

   Compute the partial sum for this tile: for each k in 0..TILE_WIDTH-1:

       sum += shared_A[ty][k] * shared_B[k][tx]

   Synchronize (maybe not needed here, but after loading)

After all tiles, write the sum to the output matrix at (block_row + ty, block_col + tx).

Now, the problem is that the TILE_WIDTH may exceed the remaining columns in the last tile. So in the loop over k, we have to make sure that we don't access beyond K.

Wait, in the shared memory, for the A tile, the columns beyond K would be zero. Because when loading, if a_col >= K, the value is zero. So the computation can proceed without checking, as the excess elements are zero.

So the code can safely iterate over all k from 0 to TILE_WIDTH-1, even in the last tile, because the loaded A and B values beyond K are zero.

Therefore, the inner loop can proceed without branching.

Now, the code:

First, define constants:

#define BLOCK_SIZE 16
#define TILE_WIDTH BLOCK_SIZE

Then, the kernel function:

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    // Block indices
    int block_row = blockIdx.y * BLOCK_SIZE;
    int block_col = blockIdx.x * BLOCK_SIZE;

    // Thread indices within the block
    int ty = threadIdx.y;
    int tx = threadIdx.x;

    // Each thread computes one element of the output tile
    // The output tile's position in C is (block_row + ty, block_col + tx)

    // Initialize the sum for this thread's element
    float sum = 0.0f;

    // Iterate over all tiles along the K dimension
    for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {
        // Load the current tile of A into shared memory
        int a_row = block_row + ty;
        int a_col = tile_k + tx;
        float a_val = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

        // Load the current tile of B into shared memory
        int b_row = tile_k + ty;
        int b_col = block_col + tx;
        float b_val = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

        // Store in shared memory
        shared_A[ty][tx] = a_val;
        shared_B[ty][tx] = b_val;

        __syncthreads();

        // Compute the partial sum for this tile
        for (int k = 0; k < TILE_WIDTH; ++k) {
            sum += shared_A[ty][k] * shared_B[k][tx];
        }

        __syncthreads(); // Not sure if needed after compute
    }

    // Write the result to global memory
    int c_row = block_row + ty;
    int c_col = block_col + tx;
    if (c_row < M && c_col < N) {
        C[c_row * N + c_col] = sum;
    }
}

Wait, but the shared memory declarations have to be __shared__ arrays. Let me correct that:

Inside the kernel function, the shared memory variables would be declared as:

__shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
__shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

Wait, no. Wait, the A tile has dimensions BLOCK_SIZE rows (since the output tile is BLOCK_SIZE rows) by TILE_WIDTH columns (the current tile's width along K). So the shared_A array should be [BLOCK_SIZE][TILE_WIDTH].

Similarly, the B tile has dimensions TILE_WIDTH rows (current tile along K) by BLOCK_SIZE columns (output tile's columns). So shared_B is [TILE_WIDTH][BLOCK_SIZE].

Therefore, in the kernel function:

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    __shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
    __shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

    ... rest of code ...
}

Now, in the loading steps:

For shared_A:

Each thread (ty, tx) in the block loads a single element from A into shared_A[ty][tx]. The row in A is block_row + ty (since the block's output tile starts at block_row). The column is tile_k + tx. 

Wait, but tx is the thread's x coordinate (column in the block). So for the A tile's columns (which are along K), each thread in the block's x direction (tx) corresponds to the column in the A tile.

Wait, the A tile's columns go from tile_k to tile_k + TILE_WIDTH -1. So for each thread in the block's x direction (tx from 0 to TILE_WIDTH-1?), but the block's size is 16x16. Wait, this is getting a bit confusing.

Wait, the block has threads arranged in a 2D grid of BLOCK_SIZE x BLOCK_SIZE (since the block is dim3(BLOCK_SIZE, BLOCK_SIZE)). Wait, no, the block is a 2D grid of threads, so each thread has threadIdx.x and threadIdx.y. So for a block of size 16x16, threadIdx.x ranges from 0-15 and threadIdx.y from 0-15.

To load the A tile (BLOCK_SIZE rows by TILE_WIDTH columns), each thread in the block must cover the rows and columns of the A tile. Since the A tile is 16x16 (if TILE_WIDTH=16), each thread (ty, tx) can handle the element at row ty (since the block_row is the starting row) and column tx in the A tile. 

Wait, so for the A tile:

shared_A[ty][tx] = A's value at (block_row + ty, tile_k + tx).

But tx ranges from 0 to 15 (since threadIdx.x is up to 15), so when TILE_WIDTH is 16, tx can go up to 15, which covers the TILE_WIDTH columns. That's perfect.

Similarly for B's tile: the B tile is TILE_WIDTH rows (along K) and BLOCK_SIZE columns (the block's columns in the output). Each thread (ty, tx) in the block is assigned to load B's row tile_k + ty (ty is up to 15, so rows from tile_k to tile_k+15) and column block_col + tx. 

Wait, no. The B tile is TILE_WIDTH rows (along K) and BLOCK_SIZE columns (along output's columns). So each element in the B tile is at (row in K: tile_k + ty, column in output: block_col + tx). 

Wait, the B tile's columns are the output columns, so for each thread (ty, tx) in the block, the B element is at row tile_k + ty, column block_col + tx.

Wait, but the B's tile has TILE_WIDTH rows (along K) and BLOCK_SIZE columns (output columns). So each thread in the block can load one element from B's tile: (tile_k + ty, block_col + tx). 

Thus, shared_B[ty][tx] = B's value at (tile_k + ty, block_col + tx).

Wait, but the shared_B is declared as [TILE_WIDTH][BLOCK_SIZE], so the first index is the row in B's tile, and the second is the column. So storing B's value into shared_B[ty][tx] would mean row=ty and column=tx. Since ty is up to TILE_WIDTH-1 (since TILE_WIDTH=16, and ty is 0-15), that works.

Yes, that makes sense.

Therefore, the code for loading A and B would be:

For A:

a_row = block_row + ty (the row in the A matrix)
a_col = tile_k + tx (the column in the A matrix)
shared_A[ty][tx] = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

For B:

b_row = tile_k + ty (the row in B's matrix, since B's rows are K)
b_col = block_col + tx (the column in B's matrix, which corresponds to the output's column)
shared_B[ty][tx] = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

Yes.

After loading, sync threads to make sure all data is in shared memory.

Then, each thread computes the partial sum over the current tile's elements:

for (int k = 0; k < TILE_WIDTH; ++k) {
    sum += shared_A[ty][k] * shared_B[k][tx];
}

Here, shared_A[ty][k] accesses the ty-th row (the row in A's tile) and the k-th column (the current column in the tile). Similarly, shared_B[k][tx] is the k-th row (the current row in B's tile) and tx-th column (the column in B's tile, which corresponds to the output's column).

This multiplication accumulates the product for each element in the current tile's contribution to the output element (ty, tx) in the block's tile.

This loop runs for all k in 0..TILE_WIDTH-1, so each iteration handles a column in A and row in B of the current tile.

This is correct.

After processing all tiles, the sum is written to the output. The final indices are block_row + ty and block_col + tx. We need to check that these are within M and N, respectively.

Now, the grid and block dimensions:

The block dimensions are dim3(BLOCK_SIZE, BLOCK_SIZE) because the block is a 2D grid of threads to cover the output tile's rows and columns.

The grid dimensions are ceil(M / BLOCK_SIZE) in y and ceil(N / BLOCK_SIZE) in x. 

So in the kernel launch:

dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
dim3 blocksPerGrid( (N + BLOCK_SIZE -1)/BLOCK_SIZE, (M + BLOCK_SIZE -1)/BLOCK_SIZE );

Wait, since the blocks are arranged such that blockIdx.x corresponds to the column dimension and blockIdx.y to the row. So each block's column is blockIdx.x * BLOCK_SIZE, so the grid in x direction (columns) is ceil(N / BLOCK_SIZE).

Yes.

Now, putting this into code.

Wait, in Python, the kernel is compiled via load_inline. The source code must be a string.

Let me write the CUDA code as a string, with the defines and kernel.

Also, the code must handle the input tensors being on the GPU, and the output must be contiguous.

In the Python code:

The ModelNew class will replace the torch.matmul with the custom kernel.

First, define the CUDA kernel code as a string.

Constants: Let's choose a block size of 16. Or maybe 16 is better? Let's try 16 first.

Wait, but earlier thought was that 32 might be better. Let me choose 16 for now.

So:

elementwise_add_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 16
#define TILE_WIDTH BLOCK_SIZE

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    __shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
    __shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

    // Block indices
    int block_row = blockIdx.y * BLOCK_SIZE;
    int block_col = blockIdx.x * BLOCK_SIZE;

    // Thread indices within the block
    int ty = threadIdx.y;
    int tx = threadIdx.x;

    // Each thread computes one element of the output tile
    // The output tile's position in C is (block_row + ty, block_col + tx)

    // Initialize the sum for this thread's element
    float sum = 0.0f;

    for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {
        // Load the current tile of A into shared memory
        int a_row = block_row + ty;
        int a_col = tile_k + tx;
        float a_val = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

        // Load the current tile of B into shared memory
        int b_row = tile_k + ty;
        int b_col = block_col + tx;
        float b_val = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

        // Write to shared memory
        shared_A[ty][tx] = a_val;
        shared_B[ty][tx] = b_val;

        __syncthreads();

        // Compute the partial sum for this tile
        for (int k = 0; k < TILE_WIDTH; ++k) {
            sum += shared_A[ty][k] * shared_B[k][tx];
        }

        __syncthreads(); // Is this needed here?
    }

    // Write the result to global memory
    int c_row = block_row + ty;
    int c_col = block_col + tx;
    if (c_row < M && c_col < N) {
        C[c_row * N + c_col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    // Check dimensions
    int M = A.size(0);
    int K_A = A.size(1);
    int K_B = B.size(0);
    int N = B.size(1);
    assert(K_A == K_B);

    // Output tensor
    auto C = torch::empty({M, N}, A.options());

    // Define block and grid dimensions
    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocksPerGrid( (N + BLOCK_SIZE - 1) / BLOCK_SIZE, (M + BLOCK_SIZE - 1) / BLOCK_SIZE );

    // Launch kernel
    matmul_kernel<<<blocksPerGrid, threadsPerBlock>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, K_A, N
    );

    return C;
}
"""

Wait, but there's a mistake in the shared memory declarations. The shared_A is declared as [BLOCK_SIZE][TILE_WIDTH], which is 16x16. The shared_B is [TILE_WIDTH][BLOCK_SIZE], also 16x16. Correct.

Wait, in the code above, when we write to shared_A[ty][tx], the ty can go up to 15 (since threadIdx.y is up to 15), and tx up to 15. So for a 16x16 tile, this covers all elements. Yes.

Another thing: in the kernel launch, the block dimensions are (BLOCK_SIZE, BLOCK_SIZE), which in this case is 16x16. The threads are arranged in 2D.

Wait, but in the code, the threadIdx.x and y are used for both dimensions. So each block has 16x16=256 threads.

The __syncthreads() after loading into shared memory is necessary to ensure all writes are done before reading.

The second __syncthreads() after the inner loop (over k) is unnecessary, since the next iteration of the tile loop will do a new load. Probably can remove that.

Wait, in the code above, after the inner loop over k, there is a __syncthreads(). But that is redundant because the next tile iteration will start with a new load, so the syncthreads() can be removed. So I'll delete that line.

Another possible mistake: the loop over tile_k uses TILE_WIDTH as the step, which is 16. So the loop runs while tile_k < K. For K=2949, that's 2949 /16 â‰ˆ 184.3125, so 185 iterations. Each iteration loads a 16x16 tile from A and B.

The inner loop over k from 0 to 15 (since TILE_WIDTH is 16) will process all elements in the current tile's columns/rows.

Now, in the Python code, the function matmul_cuda is defined, which takes A and B tensors, checks the dimensions, creates the output tensor, computes the grid dimensions, and launches the kernel.

Now, to make sure that the input tensors are contiguous and on the same device (GPU). Since the problem states that the inputs are on the GPU (as the example uses .cuda()), so we can assume they are already on the GPU.

The Python code would look like this:

from torch.utils.cpp_extension import load_inline

matmul_cuda_source = """
// CUDA code from above
"""

matmul_cpp_source = """
// declarations if needed, but since we use load_inline with functions, maybe not needed.
"""

Wait, the example uses:

elementwise_add = load_inline(
    name="elementwise_add",
    cpp_sources=elementwise_add_cpp_source,
    cuda_sources=elementwise_add_source,
    functions=["elementwise_add_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

So for the matmul case, the functions list is ["matmul_cuda"], and the sources include the CUDA code.

Wait, in the CUDA code above, the kernel is declared in the source, and the wrapper function matmul_cuda is also in the CUDA source. So the functions to expose are ["matmul_cuda"].

Thus, the Python code:

matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 16
#define TILE_WIDTH BLOCK_SIZE

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    // ... kernel code as above ...
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    // ... implementation as above ...
}
"""

Then:

matmul = load_inline(
    name="matmul",
    cpp_sources="",  # maybe needed empty string?
    cuda_sources=matmul_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Wait, the example uses a separate cpp_source and cuda_source. The cpp_source is the header for the function declarations, but in this case, the CUDA source includes both the kernel and the wrapper. Maybe the cpp_sources can be omitted, but to be safe, perhaps include the function declarations in the cpp_sources.

Alternatively, the cpp_sources can be "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);" as a header.

So:

matmul_cpp_source = (
    "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"
)

Then:

matmul = load_inline(
    name="matmul",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Putting it all together, the Python code would be:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_cpp_source = (
    "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"
)

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 16
#define TILE_WIDTH BLOCK_SIZE

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    __shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
    __shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

    // Block indices
    int block_row = blockIdx.y * BLOCK_SIZE;
    int block_col = blockIdx.x * BLOCK_SIZE;

    // Thread indices within the block
    int ty = threadIdx.y;
    int tx = threadIdx.x;

    // Initialize the sum for this thread's element
    float sum = 0.0f;

    for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {
        // Load current tile of A into shared memory
        int a_row = block_row + ty;
        int a_col = tile_k + tx;
        float a_val = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

        // Load current tile of B into shared memory
        int b_row = tile_k + ty;
        int b_col = block_col + tx;
        float b_val = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

        // Write to shared memory
        shared_A[ty][tx] = a_val;
        shared_B[ty][tx] = b_val;

        __syncthreads();

        // Compute partial sum for this tile
        for (int k = 0; k < TILE_WIDTH; ++k) {
            sum += shared_A[ty][k] * shared_B[k][tx];
        }

        __syncthreads();  // This line may be unnecessary and can be removed
    }

    // Write result to global memory
    int c_row = block_row + ty;
    int c_col = block_col + tx;
    if (c_row < M && c_col < N) {
        C[c_row * N + c_col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K_A = A.size(1);
    int K_B = B.size(0);
    int N = B.size(1);
    assert(K_A == K_B && "Matrix dimensions must match");

    auto C = torch::empty({M, N}, A.options());

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocksPerGrid(
        (N + BLOCK_SIZE - 1) / BLOCK_SIZE,
        (M + BLOCK_SIZE - 1) / BLOCK_SIZE
    );

    matmul_kernel<<<blocksPerGrid, threadsPerBlock>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, K_A, N
    );

    return C;
}
"""

matmul = load_inline(
    name="matmul",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_cuda_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=["-arch=sm_70"],  # Assuming compute capability 7.0 or higher
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.matmul = matmul

    def forward(self, A, B):
        return self.matmul.matmul_cuda(A, B)
```

Wait, but there's an extra __syncthreads() after the inner loop which is unnecessary. Let me remove that line.

Also, need to ensure the kernel is correctly launched with the right grid and block dimensions. The block is 16x16 threads, so the threadsPerBlock is dim3(BLOCK_SIZE, BLOCK_SIZE), which is (16,16). The grid is calculated as (ceil(N/16), ceil(M/16)), which gives the number of blocks in x and y.

Now, testing if the code is correct:

When A is (M, K) and B is (K, N), the output C is (M, N). The kernel should compute C = A @ B.

The code should handle arbitrary dimensions, but for the given problem, it's optimized for the specific M, K, N. However, the code must still work for any dimensions.

Potential issues:

- The shared memory allocation: if the block size is too large, the shared memory might exceed the limit. For 16x16, it's okay.

- The condition (a_col < K) and (b_row < K): these are necessary to avoid out-of-bounds accesses. Since tile_k is looped up to K, the a_col and b_row can be up to K + TILE_WIDTH -1, but tile_k is incremented by TILE_WIDTH. However, the loop runs while tile_k < K, so tile_k can be up to K -1. Thus, a_col can be up to (K-1) + (TILE_WIDTH-1) which could exceed K. Hence, the condition is essential.

- The final write to C must check if the indices are within bounds to prevent writing to invalid memory (e.g., when the block is at the edge of the matrix).

Another optimization: the TILE_WIDTH could be larger to reduce the number of tiles. Let's see, using 32 instead of 16 might be better. Let me try modifying the code to use 32.

Changing the constants:

#define BLOCK_SIZE 32
#define TILE_WIDTH 32

This would reduce the number of tiles from 185 (for 16) to 93 (for 32), which is better.

The shared memory usage becomes 32x32 * 2 = 2048 elements, which is 8KB, still within limits.

Updating the code accordingly.

Also, the grid and block dimensions:

threadsPerBlock would be (32, 32), so each block has 1024 threads. But the maximum block size per dimension is 1024, so 32x32 is okay.

The grid dimensions would be ceil(N/32) and ceil(M/32).

Let me update the code to use BLOCK_SIZE 32:

```python
matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 32
#define TILE_WIDTH BLOCK_SIZE

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    __shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
    __shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

    // Block indices
    int block_row = blockIdx.y * BLOCK_SIZE;
    int block_col = blockIdx.x * BLOCK_SIZE;

    // Thread indices within the block
    int ty = threadIdx.y;
    int tx = threadIdx.x;

    // Initialize the sum for this thread's element
    float sum = 0.0f;

    for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {
        // Load current tile of A into shared memory
        int a_row = block_row + ty;
        int a_col = tile_k + tx;
        float a_val = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

        // Load current tile of B into shared memory
        int b_row = tile_k + ty;
        int b_col = block_col + tx;
        float b_val = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

        // Write to shared memory
        shared_A[ty][tx] = a_val;
        shared_B[ty][tx] = b_val;

        __syncthreads();

        // Compute partial sum for this tile
        for (int k = 0; k < TILE_WIDTH; ++k) {
            sum += shared_A[ty][k] * shared_B[k][tx];
        }
    }

    // Write result to global memory
    int c_row = block_row + ty;
    int c_col = block_col + tx;
    if (c_row < M && c_col < N) {
        C[c_row * N + c_col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K_A = A.size(1);
    int K_B = B.size(0);
    int N = B.size(1);
    assert(K_A == K_B && "Matrix dimensions must match");

    auto C = torch::empty({M, N}, A.options());

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocksPerGrid(
        (N + BLOCK_SIZE - 1) / BLOCK_SIZE,
        (M + BLOCK_SIZE - 1) / BLOCK_SIZE
    );

    matmul_kernel<<<blocksPerGrid, threadsPerBlock>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, K_A, N
    );

    return C;
}
"""
```

This might be better. Let me see for the given dimensions:

M=8205: ceil(8205/32) = 257 (since 32*256=8192, so 257).

N=5921: ceil(5921/32)=186 (since 32*185=5920, so 186).

The number of tiles along K is 2949 /32 = 92.156, so 93 iterations.

This reduces the number of loops, which is better.

Also, the shared memory is 32x32 *2 = 2048 floats (8KB), which is acceptable.

Another thing to consider: the thread indices in the A and B loads. For the A tile:

a_col = tile_k + tx. Since tx can be up to 31 (since threadIdx.x is 0-31), and tile_k steps by 32 each time, this ensures that a_col increments by 32 each iteration, covering all K columns.

Similarly for B's b_row = tile_k + ty (ty up to 31), so each iteration covers 32 rows in K.

The inner loop over k from 0 to 31: that's okay.

Potential error: in the B tile's indexing, the B matrix is stored in row-major order. So for a column-major matrix, but in C/CUDA it's row-major, so B's elements are stored as B[row][col], so the pointer access is correct.

Yes, in B's case, B[b_row * N + b_col] is correct, since row-major stores each row consecutively.

Now, this should work.

Another possible optimization: the TILE_WIDTH can be a variable, but since it's a compile-time constant, using a macro is okay.

Now, compiling this code with PyTorch's load_inline should work.

The final Python code would look like this:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_cpp_source = (
    "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"
)

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 32
#define TILE_WIDTH BLOCK_SIZE

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    __shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
    __shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

    // Block indices
    int block_row = blockIdx.y * BLOCK_SIZE;
    int block_col = blockIdx.x * BLOCK_SIZE;

    // Thread indices within the block
    int ty = threadIdx.y;
    int tx = threadIdx.x;

    // Initialize the sum for this thread's element
    float sum = 0.0f;

    for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {
        // Load current tile of A into shared memory
        int a_row = block_row + ty;
        int a_col = tile_k + tx;
        float a_val = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

        // Load current tile of B into shared memory
        int b_row = tile_k + ty;
        int b_col = block_col + tx;
        float b_val = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

        // Write to shared memory
        shared_A[ty][tx] = a_val;
        shared_B[ty][tx] = b_val;

        __syncthreads();

        // Compute partial sum for this tile
        for (int k = 0; k < TILE_WIDTH; ++k) {
            sum += shared_A[ty][k] * shared_B[k][tx];
        }
    }

    // Write result to global memory
    int c_row = block_row + ty;
    int c_col = block_col + tx;
    if (c_row < M && c_col < N) {
        C[c_row * N + c_col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K_A = A.size(1);
    int K_B = B.size(0);
    int N = B.size(1);
    assert(K_A == K_B && "Matrix dimensions must match");

    auto C = torch::empty({M, N}, A.options());

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocksPerGrid(
        (N + BLOCK_SIZE - 1) / BLOCK_SIZE,
        (M + BLOCK_SIZE - 1) / BLOCK_SIZE
    );

    matmul_kernel<<<blocksPerGrid, threadsPerBlock>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, K_A, N
    );

    return C;
}
"""

matmul = load_inline(
    name="matmul",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_cuda_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=["-arch=sm_70"],  # Adjust based on GPU architecture
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.matmul = matmul

    def forward(self, A, B):
        return self.matmul.matmul_cuda(A, B)
```

I think this should work. Testing would require running it on a GPU and comparing against PyTorch's matmul. Since the problem requires that it is faster than PyTorch's implementation, but without being able to test, I'll proceed with this code.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

matmul_cpp_source = (
    "torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);"
)

matmul_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

#define BLOCK_SIZE 32
#define TILE_WIDTH BLOCK_SIZE

__global__ void matmul_kernel(float *A, float *B, float *C, int M, int K, int N) {
    __shared__ float shared_A[BLOCK_SIZE][TILE_WIDTH];
    __shared__ float shared_B[TILE_WIDTH][BLOCK_SIZE];

    int block_row = blockIdx.y * BLOCK_SIZE;
    int block_col = blockIdx.x * BLOCK_SIZE;
    int ty = threadIdx.y;
    int tx = threadIdx.x;
    float sum = 0.0f;

    for (int tile_k = 0; tile_k < K; tile_k += TILE_WIDTH) {
        int a_row = block_row + ty;
        int a_col = tile_k + tx;
        float a_val = (a_col < K) ? A[a_row * K + a_col] : 0.0f;

        int b_row = tile_k + ty;
        int b_col = block_col + tx;
        float b_val = (b_row < K) ? B[b_row * N + b_col] : 0.0f;

        shared_A[ty][tx] = a_val;
        shared_B[ty][tx] = b_val;
        __syncthreads();

        for (int k = 0; k < TILE_WIDTH; ++k) {
            sum += shared_A[ty][k] * shared_B[k][tx];
        }

        __syncthreads();
    }

    int c_row = block_row + ty;
    int c_col = block_col + tx;
    if (c_row < M && c_col < N) {
        C[c_row * N + c_col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {
    int M = A.size(0);
    int K_A = A.size(1);
    int K_B = B.size(0);
    int N = B.size(1);
    assert(K_A == K_B && "Matrix dimensions must match");

    auto C = torch::empty({M, N}, A.options());

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocksPerGrid(
        (N + BLOCK_SIZE - 1) / BLOCK_SIZE,
        (M + BLOCK_SIZE - 1) / BLOCK_SIZE
    );

    matmul_kernel<<<blocksPerGrid, threadsPerBlock>>>(
        A.data_ptr<float>(),
        B.data_ptr<float>(),
        C.data_ptr<float>(),
        M, K_A, N
    );

    return C;
}
"""

matmul = load_inline(
    name="matmul",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_cuda_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=["-arch=sm_70"],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.matmul = matmul

    def forward(self, A, B):
        return self.matmul.matmul_cuda(A, B)
```