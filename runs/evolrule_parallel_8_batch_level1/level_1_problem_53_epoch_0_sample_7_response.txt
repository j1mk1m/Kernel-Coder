The optimized code must be a drop-in replacement for the original code. The inputs and outputs must be compatible with the original code. 

You can choose to replace any operators in the given architecture with custom CUDA operators. You can also make any changes to the code as long as it satisfies the above constraints. 

The ModelNew must have the same input and output as Model. The forward function must return the same output as the original Model.



Okay, I need to optimize the given PyTorch model that uses torch.min for reduction. The original model takes a tensor and returns the minimum along a specified dimension. The task is to replace this operation with a custom CUDA kernel to potentially speed things up.

First, I should understand how the current implementation works. The forward method calls torch.min with the given dim, and returns the values (not the indices). The kernel I write needs to perform the same operation but in a CUDA kernel for better performance, especially for large tensors like the ones with dimensions 4096 and 4095.

The input tensor has shape (batch_size, dim1, dim2). The reduction is over dim=self.dim. Depending on which dimension is chosen (dim in the model's __init__), the kernel needs to handle that. The example uses dim=1, but the code should work for any dimension.

I'll start by writing a CUDA kernel that computes the minimum along a specified dimension. Let's think about the approach. For a given dimension, each thread or block can handle a slice of the tensor. Since reduction operations are parallelizable but require aggregation, I need to structure the kernel to efficiently compute the min across the specified dimension.

Wait, the standard way to do a reduction in CUDA is to use a block per element along the reduced dimension, but maybe here, since the input is a 3D tensor, the approach will depend on the dimension being reduced over. Let's consider the case when dim=1. The input shape is (128, 4096, 4095). Reducing over dim=1 would result in a tensor of shape (128, 4095). Each element in the output is the min of the 4096 elements along that dimension for each position in the other dimensions.

Hmm, so for each position (b, d2) in the output, we need to find the min over the d1 dimension (since dim=1 is the second dimension). To compute this efficiently, perhaps each thread can handle one element in the output, and for each such thread, loop over the elements in the reduction dimension (d1) to compute the min.

Alternatively, maybe we can tile the work so that multiple threads collaborate. But for simplicity, maybe a straightforward approach is better for now.

Let me think of the dimensions. Suppose the input is a 3D tensor, and we need to reduce along axis dim. Let's say the input tensor is of shape (B, D1, D2). If dim is 0, then the output is (D1, D2), but the original example's get_init_inputs has [1], so perhaps the dim is set to 1. But the code must handle any dim.

Wait, in the given code, the dim is passed to the Model constructor. The get_init_inputs returns [1], which is probably the dimension to use. So the model's dim is 1. But the code must be general, so the kernel must accept the dimension as a parameter.

So the kernel must be flexible for any dimension. That complicates things a bit. Alternatively, perhaps the kernel can be designed to handle the dimension at compile time, but that's not feasible. So, the kernel must compute the minimum along a given dimension, which can be specified as an argument.

Hmm, CUDA kernels can't take dynamic parameters like the dimension to reduce over unless we use template parameters, but that's complicated. Alternatively, perhaps the kernel can be written in a way that the dimension is fixed, but that would require multiple kernels for each possible dimension, which is not scalable.

Wait, perhaps the best way is to handle the dimension as a parameter. Let me think of the input tensor as a multi-dimensional array and compute the index.

Alternatively, maybe the kernel can be written in a way that loops over the reduction dimension for each output element. Let me outline the steps:

Suppose the output tensor has dimensions (B, D1, D2) reduced along dim. Let's take the example of dim=1. The output will be (B, D2). For each element in the output, its position is (b, d2), and the value is the min over all elements (b, d1, d2) for d1 from 0 to D1-1.

So, the kernel would need to process each output element. Each thread can handle one output element. For each such element, the thread would loop over the reduction dimension (d1 in this case) and compute the min.

The key is to compute the indices correctly. Let me think of the input tensor as a 3D array. The total number of output elements is the product of all dimensions except the reduction dimension. For example, if the reduction is along dim=1 (size D1), then the output has B*D2 elements. So, each thread can process one of these elements.

The kernel can be structured as follows:

- Each thread is assigned an output element index (output_idx).
- Compute the input indices for this output element along the reduction dimension.
- For each position in the reduction dimension, read the value and keep track of the minimum.

This approach requires that each thread processes all elements along the reduction dimension. However, if the reduction dimension is large (like 4096), then this could be computationally intensive. For example, if each thread has to loop 4096 times, but with 4096 threads, it's manageable. Wait, no, the total number of threads would be the output elements. Let's see:

In the example, the output elements are B * D2 = 128 * 4095 = ~524k elements. Each thread would process one of these elements. For each element, the thread would loop over D1 (4096) elements. That's a lot of loops. This might be slow.

Hmm, that's a problem. So maybe the straightforward approach isn't efficient. Perhaps a better approach is to parallelize over the reduction dimension. Let's think differently.

Alternative approach: For each position in the output (excluding the reduction dimension), we can have a block of threads, each responsible for a slice of the reduction dimension. Then, each thread can compute a partial minimum and then combine the results. This is similar to a reduction pattern in CUDA.

Wait, the standard reduction approach for a dimension would involve dividing the work into chunks. For example, each block handles a chunk of the output elements, and within the block, threads compute partial minima and then combine them.

Alternatively, perhaps using a grid where each block corresponds to an output element, and each thread in the block handles a portion of the reduction dimension. For example, if the reduction dimension is of size N, and the block has T threads, then each thread can process N/T elements and compute a partial min, then combine the results.

This might be more efficient. Let me outline this.

Suppose the output element is at position (b, d2). The reduction is over the dim=1 (d1). The number of elements to reduce is D1 (e.g., 4096). Each thread in the block (say, 256 threads) can take a chunk of D1 elements. Each thread computes the min of their chunk, then the threads in the block perform a parallel reduction to get the final min.

This approach can be efficient, especially if the reduction dimension is large. Let me structure this:

Each output element (each has a unique index) is assigned to a block. Within the block, each thread is assigned a range of indices along the reduction dimension. Each thread computes the min of its chunk, then they do a reduction within the block to get the final min.

This requires the block size to be a power of two for efficient reduction steps. Let's see:

The steps would be:

For each output element (block):

1. Each thread in the block reads its assigned elements along the reduction dimension and compute the min of their chunk.

2. The threads then perform a parallel reduction within the block to combine all partial mins into the final min.

The key is to calculate the indices correctly based on the output element's position and the reduction dimension.

Now, to implement this, the kernel needs to:

- Determine the output index from the block index.

- Compute the indices along the reduction dimension for each thread's chunk.

- Calculate the input indices for the current chunk.

- Compute the partial min and then reduce within the block.

This requires some index calculations, which can be tricky.

Let me start by writing the kernel code structure.

First, the kernel function signature would be something like:

__global__ void min_reduction_kernel(const float* input, float* output, int dim, int dims[3], int output_size)

But passing dimensions and the reduction dimension as parameters is possible. Wait, but in CUDA, the kernel parameters can be passed as arguments.

Alternatively, we can have parameters that define the tensor's shape, the dimension to reduce over, and the output size.

Wait, perhaps it's better to precompute some parameters and pass them to the kernel. Let's see.

Suppose the input is a 3D tensor with shape (B, D1, D2). The reduction is over dim=1 (so D1 is the size of the reduction dimension). The output has shape (B, D2). The total number of output elements is B*D2.

The kernel can be launched with a grid of (output_size) blocks, each block handling one output element, and a block size of, say, 256 threads. Each block's threads will process chunks of the reduction dimension (D1 elements).

Wait, but if the block size is 256 and the reduction dimension is 4096, then each thread can handle 4096 / 256 = 16 elements. That seems manageable.

So, in the block, each thread processes 16 elements (for example) along the reduction dimension, computes their min, then participates in a block reduction.

The steps for a single block (handling one output element):

1. Determine the output element's index (blockIdx.x).

2. Compute the corresponding input indices (excluding the reduction dimension). For example, if the output element is at output_idx, then in the input, the non-reduction dimensions are fixed, and the reduction dimension is the loop variable.

Wait, let's see. Suppose the output index is output_idx. The output is a 2D tensor (B, D2). The output_idx can be mapped to (b, d2) coordinates.

The input has 3 dimensions, so the output's indices are over all dimensions except the reduction dimension. So for a 3D input and reduction over dim=1, the output indices are (B, D2). For each output element, the position in the input's non-reduction dimensions are fixed, and the reduction is over dim=1's elements.

So, to get the input indices for the current output element:

Suppose the output element is output_idx. Let's split it into the coordinates along each non-reduction dimension.

Alternatively, the input's dimensions are (B, D1, D2). The output's shape is (B, D2). The output's element at (b, d2) corresponds to the input's elements (b, 0:D1-1, d2).

Thus, for each output element (b, d2), the reduction is over the D1 elements in the second dimension.

So, for a given output index (output_idx), how do we compute b and d2?

Assuming the output is flattened into a 1D array, the output element's coordinates can be calculated as:

int b = output_idx / D2;

int d2 = output_idx % D2;

Then, the input indices for the reduction dimension would be along the D1 elements at (b, d1, d2), for d1 from 0 to D1-1.

So in the kernel, for the current output element (output_idx), the non-reduction coordinates are (b, d2).

Now, within each block (representing one output element), each thread processes a chunk of the reduction dimension's elements.

The block size (number of threads per block) can be chosen as, say, 256. Each thread handles a chunk of the reduction dimension. For example, if D1=4096 and block size=256, each thread handles 4096/256=16 elements.

Wait, but 256 threads per block may be too much if the grid is large. Wait, the maximum block size is 1024, but for reduction, 256 is common.

So per thread, the chunk size is (D1 + blockDim.x -1) / blockDim.x.

Alternatively, each thread can process multiple elements. Let's say each thread processes a chunk of elements along the reduction dimension.

The thread index within the block is threadIdx.x. Each thread's chunk starts at (threadIdx.x * chunk_size) and ends at (threadIdx.x+1)*chunk_size.

The chunk_size would be ceil(D1 / blockDim.x).

Wait, but D1 may not be a multiple of the blockDim. So, for each thread, the start and end indices need to be computed.

Alternatively, for a given thread, the start is threadIdx.x * (D1 / blockDim.x), and the end is (threadIdx.x+1) * (D1 / blockDim.x). But that would split evenly, but may not cover all elements. Hmm, perhaps a better approach is to compute:

start = threadIdx.x * (D1 / blockDim.x) + min(threadIdx.x, D1 % blockDim.x);

end = start + (D1 / blockDim.x) + (threadIdx.x < (D1 % blockDim.x) ? 1 : 0);

But that might complicate the code. Alternatively, just use a loop for each thread's portion.

Wait, perhaps the easiest way is to have each thread process a contiguous chunk. For example, for each thread in the block, process a range of indices in the reduction dimension.

Alternatively, each thread can process D1 / blockDim.x elements, and any remainder can be handled by some threads. But that could be error-prone.

Alternatively, use a for loop where each thread processes elements starting at their threadIdx.x and stepping by the block size. This is the standard way to handle this.

Wait, for example, each thread can loop over i = threadIdx.x; i < D1; i += blockDim.x. This way, each thread handles every blockDim.x-th element in the reduction dimension. The stride approach.

This is a common pattern. Let me try this approach.

In this case:

For each thread in the block, the thread index is tid = threadIdx.x.

The number of elements per thread would be roughly D1 / blockDim.x.

Thus, for each element in the reduction dimension at position d1 = tid + blockDim.x * i, where i is from 0 to ... until d1 exceeds D1-1.

Wait, but to process all elements, each thread can iterate over all elements where d1 = tid, tid + blockDim.x, tid + 2*blockDim.x, etc., until d1 < D1.

This way, each thread processes roughly D1 / blockDim.x elements.

Then, for each such d1, the input value is input[b * D1*D2 + d1 * D2 + d2].

Wait, the input is a 3D tensor, so the linear index would be calculated based on the strides. But to avoid that, perhaps it's easier to precompute the strides.

Alternatively, let's compute the input's linear index. Let me think in terms of 3D coordinates.

Suppose the input is stored in row-major order. For a 3D tensor with dimensions (B, D1, D2), the linear index for (b, d1, d2) is:

b * (D1 * D2) + d1 * D2 + d2.

So, for a given output element (b, d2), the elements along the reduction dimension are (b, 0:D1-1, d2).

Thus, for each d1 in 0 to D1-1:

input_val = input[ b * D1*D2 + d1 * D2 + d2 ]

The output's value is the min over all those input_vals.

In the kernel, for each thread in the block (handling the current output element), we need to compute the min over all d1 in 0..D1-1.

So, for each thread in the block (tid), the thread will process elements d1 = tid, tid + blockDim.x, tid + 2*blockDim.x, etc.

Each thread will compute the min of their assigned elements, store it in a shared memory array, then perform a block-wide reduction.

The steps would be:

1. Each thread in the block reads their assigned d1 elements and compute their partial min.

2. The partial mins are stored in shared memory.

3. Perform a block reduction to find the minimum among all partial mins.

4. The final min is written to the output.

This approach requires using shared memory for the intermediate steps.

Now, let's outline the kernel code.

First, the kernel parameters:

The kernel needs to know:

- The input tensor's data pointer.

- The output tensor's data pointer.

- The reduction dimension (dim).

- The dimensions of the input tensor (B, D1, D2).

- The output size (number of elements in the output).

Wait, but passing the dimensions as parameters can be done via an array or separate variables. For example, we can pass an array of the input dimensions (dims[3]), since it's 3D.

So the kernel signature could be:

__global__ void min_reduction_kernel(
    const float* input,
    float* output,
    int dim,
    int dims[3], // dimensions of the input tensor (B, D1, D2)
    int output_size
)

Wait, but how do we compute the output's coordinates from the block index?

The output_size is the total number of elements in the output, which is the product of the input dimensions except for the reduction dimension. So output_size = input.size() / dims[dim].

Alternatively, the kernel can compute output_size based on dims, but perhaps it's easier to pass it as a parameter.

Now, inside the kernel:

Each block corresponds to an output element. blockIdx.x is the output element's index (0 to output_size-1).

The thread's task:

First, compute the output's coordinates (b, d2) from the blockIdx.x.

Assuming the output is a 2D tensor (since we're reducing a 3D tensor along dim=1), but if the input was 3D and the reduction is along dim=0 or 2, then the output would have different dimensions. However, the code must handle any dimension.

Wait, perhaps the code should handle any dimension. Let me think about that.

The general case:

Suppose the input has N dimensions, and we reduce along dim. The output will have N-1 dimensions. Since the input here is 3D, the output is 2D regardless of the reduction dimension.

But to compute the coordinates, we need to know how the output indices map to the input's non-reduction dimensions.

This complicates things. To handle arbitrary dimensions, perhaps we need to precompute the strides or use a helper function. But in CUDA, this is a bit tricky. Alternatively, we can make some assumptions based on the problem's constraints.

Looking back at the problem's given code, the input is 3D (batch_size, dim1, dim2), and the get_init_inputs returns [1], which is the dim to reduce over. The example's dim is 1, so the output is (batch_size, dim2). The code must work for any dim, so we need to handle that.

To compute the coordinates for a general dimension, perhaps we can use the following approach:

Suppose the input dimensions are [B, D1, D2], and the reduction is along dim. Let's compute the output's shape:

output_shape = [B, D2] if dim ==1,

output_shape = [D1, D2] if dim ==0,

output_shape = [B, D1] if dim ==2.

The output's index 'output_idx' can be mapped to the output's coordinates:

If dim is 1:

output coordinates are (b, d2):

b = output_idx / D2

d2 = output_idx % D2

If dim is 0:

output coordinates are (d1, d2):

d1 = output_idx / D2

d2 = output_idx % D2

Wait, but then D1 would be the size along the first dimension (dim 0). So, the output would have shape (D1, D2). The output index is output_idx, so d1 = output_idx / D2, d2 = output_idx % D2.

Similarly for dim=2:

The output shape is (B, D1). The output index would map to (b, d1):

b = output_idx / D1,

d1 = output_idx % D1.

Thus, the coordinates depend on the reduction dimension. To compute this in the kernel, we need to know the reduction dimension and the input dimensions.

So, in the kernel, for a given output_idx and dim, we need to compute the coordinates in the input's non-reduction dimensions.

Alternatively, perhaps it's better to precompute the coordinates using a helper function.

Wait, let's think in terms of the input dimensions as an array: dims[3] = {B, D1, D2}

The reduction is along dim. The output's dimensions are the input dimensions with the dim-th dimension removed.

To compute the coordinates:

Let's say the output index is output_idx.

We need to compute the indices along each dimension except the reduction dimension.

This can be done by dividing by the appropriate strides.

Alternatively, here's a possible way:

The output coordinates are stored in an array of N-1 dimensions (since input is 3D and output is 2D).

But in CUDA code, we can compute them as follows:

For example, when dim is 1 (the middle dimension), then the output coordinates are (b, d2):

int b = output_idx / (dims[2]); // because after dim1 is removed, the second dimension is D2, so the stride is D2

int d2 = output_idx % (dims[2]);

Similarly, if dim is 0:

The output shape is (D1, D2), so:

int d1 = output_idx / (dims[2]);

int d2 = output_idx % (dims[2]);

If dim is 2:

The output shape is (B, D1), so:

int b = output_idx / (dims[1]);

int d1 = output_idx % (dims[1]);

Thus, in the kernel, we can compute the coordinates based on the dim value.

But this requires branching, which might be inefficient, but given that dim is a constant for a given model run, we could handle it with preprocessor macros, but that complicates things. Alternatively, since dim is passed as an argument, we can handle it with conditionals.

Alternatively, perhaps the kernel can compute the coordinates dynamically based on the input dimensions and the reduction dimension.

Putting this together:

Inside the kernel:

int output_idx = blockIdx.x;

// Compute coordinates in input's non-reduction dimensions.

int coord[2]; // since output is 2D.

int remaining_dims[2];

if (dim == 0) {

    remaining_dims[0] = dims[1]; // D1

    remaining_dims[1] = dims[2]; // D2

    coord[0] = output_idx / remaining_dims[1]; // d1 = output_idx / D2

    coord[1] = output_idx % remaining_dims[1]; // d2

} else if (dim == 1) {

    remaining_dims[0] = dims[0]; // B

    remaining_dims[1] = dims[2]; // D2

    coord[0] = output_idx / remaining_dims[1]; // b = output_idx / D2

    coord[1] = output_idx % remaining_dims[1]; // d2

} else if (dim == 2) {

    remaining_dims[0] = dims[0]; // B

    remaining_dims[1] = dims[1]; // D1

    coord[0] = output_idx / remaining_dims[1]; // b = output_idx / D1

    coord[1] = output_idx % remaining_dims[1]; // d1

}

Wait, but this approach requires knowing which dimensions are kept. For example, for dim=0, the first dimension is removed, so the remaining dimensions are [D1, D2]. For dim=1, the remaining are [B, D2], and for dim=2, [B, D1].

Thus, the coordinates are stored in coord[0] and coord[1], but how does this map to the actual indices in the input?

The input indices would be:

For dim=0 (reducing over B):

input coordinates are (coord[0], coord[1], ...) ?

Wait, let me think again.

Wait, the input's dimensions are [B, D1, D2].

If we reduce along dim=0 (the first dimension, which is B):

The output's shape is [D1, D2]. Each output element is the min over B elements along the first dimension.

Thus, for output element (d1, d2):

the input indices are (b from 0 to B-1, d1, d2).

Thus, the coordinates in the non-reduction dimensions are d1 and d2.

So the first coordinate in the output (d1) comes from the second dimension of the input, and the second coordinate (d2) is from the third dimension.

Similarly, when dim is 1 (middle dimension), the output's first coordinate is B (the first dimension), and the second coordinate is D2 (third dimension).

Hmm, perhaps a better way to compute the input indices is needed.

Alternatively, perhaps we can compute the indices for each case.

Alternatively, let's think of the input's indices as follows:

The coordinates in the input for the current output element are:

For each dimension except the reduction dimension, the coordinate is known from the output coordinates.

The reduction dimension is the one we are looping over (d1 in the example above).

Wait, perhaps the input coordinates can be represented as:

int input_coords[3] = {0, 0, 0};

for (int d = 0; d < 3; d++) {

    if (d == dim) {

        // this is the reduction dimension, which will vary from 0 to dims[dim]-1

    } else {

        // set to the coordinate from the output's dimensions.

        // need to map the output coordinates to the non-reduction dimensions.

    }

}

This requires more complex logic. Since the problem's input is 3D, and the code is for a specific case, perhaps hard-coding for the three possible cases (dim 0,1,2) would be manageable, but that's not scalable. Alternatively, the kernel can handle it dynamically.

Alternatively, we can compute the input indices as follows:

The output's coordinates are stored in variables, like b and d2 (for dim=1), then the input coordinates are (b, d1, d2), where d1 is the loop variable.

Thus, the input's linear index is:

b * D1 * D2 + d1 * D2 + d2.

Similarly, for dim=0, the input coordinates are (d1, d2, ...) ?

Wait, for dim=0 (reducing over B):

The output coordinates are (d1, d2). The input coordinates vary over B in the first dimension. So the input's linear index for the current output element and a given d0 (the reduction dimension):

input_linear = d0 * (D1 * D2) + d1 * D2 + d2.

Thus, in the kernel, for each thread's chunk of the reduction dimension (d0 for dim=0, d1 for dim=1, d2 for dim=2?), we need to compute the input's linear index.

Hmm, perhaps the code can be written with a switch statement based on the dim parameter. Since dim is a parameter passed to the kernel, but in CUDA kernels, the parameters are known at launch time, but the kernel can handle it with conditionals.

Alternatively, the dim value can be a constant known at kernel launch time, so the code can be optimized.

Alternatively, let's proceed with the following approach for the kernel:

Inside the kernel:

First, compute the output's coordinates.

Then, compute the input coordinates for the current output element's non-reduction dimensions.

Then, loop over the reduction dimension's indices and compute the min.

But the reduction dimension's indices are the ones we loop over.

Let me proceed step by step.

First, let's handle the coordinates for the non-reduction dimensions.

Suppose the kernel is launched with gridDim.x = output_size, and each block is responsible for one output element.

Within the block:

int output_idx = blockIdx.x;

// Compute the coordinates in the input's non-reduction dimensions.

// For this, we need to know the reduction dimension.

// Let's compute the coordinates based on dim.

int remaining_dims[2];

int coord0, coord1;

if (dim == 0) {

    // reduction over first dimension (B)

    // output dimensions are [D1, D2]

    remaining_dims[0] = dims[1]; // D1

    remaining_dims[1] = dims[2]; // D2

    coord0 = output_idx / remaining_dims[1]; // d1 = output_idx / D2

    coord1 = output_idx % remaining_dims[1]; // d2

} else if (dim == 1) {

    // reduction over second dimension (D1)

    // output dimensions are [B, D2]

    remaining_dims[0] = dims[0]; // B

    remaining_dims[1] = dims[2]; // D2

    coord0 = output_idx / remaining_dims[1]; // b = output_idx / D2

    coord1 = output_idx % remaining_dims[1]; // d2

} else if (dim == 2) {

    // reduction over third dimension (D2)

    // output dimensions are [B, D1]

    remaining_dims[0] = dims[0]; // B

    remaining_dims[1] = dims[1]; // D1

    coord0 = output_idx / remaining_dims[1]; // b = output_idx / D1

    coord1 = output_idx % remaining_dims[1]; // d1

}

// Now, the non-reduction coordinates are stored in coord0 and coord1.

// The reduction dimension's indices will be from 0 to (dims[dim]-1).

// The input's linear index for a given reduction index (rdim) is:

// Depending on the dim, the rdim is inserted into the appropriate position.

// Let's compute the base index for the non-reduction coordinates.

int base;

if (dim ==0) {

    // input coordinates are (rdim, coord0, coord1)

    base = coord0 * dims[2] + coord1; // D2 is the third dimension's size

    // then, for each rdim (0 to B-1), the input index is:

    // rdim * (D1 * D2) + base

    // Wait, the first dimension is B (dims[0]), so the first term is rdim * D1 * D2

    base = coord0 * dims[2] + coord1;

    base *= dims[0]; // Multiply by B to account for the first dimension's size?

    Wait no.

Wait, the first dimension is B (dims[0]). So, for a given (rdim, coord0, coord1), where rdim is the first dimension (since dim=0 is the reduction dimension):

The linear index is:

rdim * (dims[1] * dims[2]) + coord0 * dims[2] + coord1.

Wait, dims[1] is D1, so the first dimension's stride is D1 * D2.

Yes.

So for dim=0:

base = coord0 * dims[2] + coord1; // this is the offset within the second and third dimensions.

Thus, the input index for rdim is rdim * (dims[1] * dims[2]) + base.

Similarly for other dimensions.

Alternatively, to compute the base correctly, let's see:

For dim=0 (reduction over first dimension):

input index = rdim (the first dimension) * (D1 * D2) + (coord0 (d1) * D2 + coord1 (d2)).

Yes.

For dim=1 (reduction over second dimension (D1)):

input index = coord0 (B) * (D1 * D2) + rdim (d1) * D2 + coord1 (d2).

Yes.

For dim=2 (reduction over third dimension (D2)):

input index = coord0 (B) * D1 * D2 + coord1 (d1) * D2 + rdim (d2).

Wait, no: when reducing over D2 (third dimension), the third dimension is the one varying, so rdim is the third coordinate.

The input index is:

coord0 (B) * (D1 * D2) + coord1 (d1) * D2 + rdim (d2).

Yes.

Thus, the base can be computed as:

if (dim ==0):

    base = coord0 * dims[2] + coord1;

    base = base;

else if (dim ==1):

    base = coord0 * (dims[1] * dims[2]) + coord1;

    // Wait, no: the first term is coord0 (B) * (D1*D2) ?

    Wait, let me re-calculate:

For dim=1:

rdim is the second dimension (D1).

input index = coord0 (B) * (D1 * D2) + rdim (d1) * D2 + coord1 (d2).

Thus, the part without rdim is coord0*(D1*D2) + coord1.

Wait, no: the coord1 is d2, so the last term is coord1 (d2).

So the base is (coord0 * D1 * D2) + (rdim * D2) + coord1? No, that's not right.

Wait, the base should be the part that doesn't involve the reduction dimension's index.

Wait, to separate the reduction index (rdim) from the rest, we can write:

input index = (non-reduction part) + rdim * stride.

Thus, for dim=1:

non_reduction_part = coord0 * (dims[1] * dims[2]) + coord1.

Wait, no, the non-reduction coordinates are B (coord0) and D2 (coord1). Wait, when reducing over dim=1 (the second dimension), the input's first and third dimensions are fixed (B and d2). The second dimension is varying (rdim from 0 to D1-1).

So:

input index = (B * D1 * D2) ? No. Let's think again.

Wait, the input is stored as:

for b in 0..B-1,

for d1 in 0..D1-1,

for d2 in 0..D2-1:

the linear index is:

b * (D1 * D2) + d1 * D2 + d2.

Thus, for a given output element with:

dim=1: the reduction is over d1.

The output coordinates are (B, D2) → (b, d2).

Thus, the non-reduction coordinates are b and d2.

Thus, the base part (excluding the varying d1) is:

b * D1 * D2 + d2.

Wait, no. Wait, for a particular b and d2, varying d1:

The input index for d1 is:

b * (D1 * D2) + d1 * D2 + d2.

Thus, the term without d1 is b*(D1*D2) + d2.

Wait, but that can't be right, because d1 is multiplied by D2.

Wait, actually, the term with d1 is d1 * D2. So, the base part (without d1) would be b*(D1*D2) + d2, but that leaves out the D2 term from d1.

Wait, no. Let's see:

The total input index for (b, d1, d2) is:

b * (D1 * D2) → first dimension stride,

plus d1 * D2 → second dimension's contribution,

plus d2 → third dimension's contribution.

Thus, the term with d1 is d1 * D2. So, the base (without d1) would be b * D1*D2 + d2.

Thus, the input index can be written as:

base = b * D1*D2 + d2,

input_index = base + d1 * D2.

Wait, no:

input_index = b*(D1*D2) + d1 * D2 + d2 = (b*(D1*D2) + d2) + d1 * D2 = base + d1*D2.

Thus, the base is b*(D1*D2) + d2.

Wait, that can't be, because adding d1 * D2 gives:

base + d1 * D2 = (b*D1*D2 + d2) + d1 * D2 = b*D1*D2 + d1*D2 +d2 → yes, that's correct.

Thus, for dim=1 (reduction over d1):

The base is (coord0 (b) * D1 * D2) + coord1 (d2).

The varying part is d1 * D2, but since d1 is the reduction index (rdim), the stride is D2.

Wait, but how do I get D2? Since dims[2] is D2.

Thus, the input index is base + rdim * D2.

Wait, no, the stride for the second dimension (d1) is D2. So each increment in d1 adds D2 to the index.

Thus, for each rdim (d1), the input index is base + rdim * D2.

Therefore, the base can be precomputed as:

for dim=0:

base = (coord0 (d1) * D2) + coord1 (d2).

Then, the input index is rdim (the first dimension) * (D1*D2) + base.

For dim=1:

base = (coord0 (b) * D1*D2) + coord1 (d2).

input index = base + rdim (d1) * D2.

For dim=2:

base = (coord0 (b) * D1*D2) + coord1 (d1).

input index = base + rdim (d2).

Wait, yes. For dim=2 (reducing over third dimension), the input index is:

b*(D1*D2) + d1*D2 + rdim (d2).

Which is (b*(D1*D2) + d1*D2) + rdim → base is b*(D1*D2)+d1*D2, and rdim is d2.

Thus, the stride for rdim (dim=2) is 1, since each step in d2 increments by 1.

So, putting this all together:

We can compute the base and the stride for the reduction dimension.

Let me define variables:

int rdim_stride; // the step between elements for varying rdim.

int base;

// Compute base and rdim_stride based on the dim.

if (dim ==0) {

    // reduction over first dimension (B)

    // input index = rdim * (D1*D2) + (coord0 * D2 + coord1)

    base = coord0 * dims[2] + coord1;

    rdim_stride = dims[1] * dims[2]; // D1 * D2.

} else if (dim ==1) {

    // reduction over second dimension (D1)

    base = coord0 * (dims[1] * dims[2]) + coord1;

    rdim_stride = dims[2]; // D2.

} else if (dim ==2) {

    // reduction over third dimension (D2)

    base = coord0 * (dims[1] * dims[2]) + coord1 * dims[2];

    rdim_stride = 1;

}

Then, the input index for a given rdim is:

input_idx = base + rdim * rdim_stride;

But rdim ranges from 0 to (dims[dim] - 1).

Thus, the kernel can compute this.

Now, the threads in the block need to compute the min over all rdim from 0 to (dims[dim] -1).

To do this efficiently, each thread in the block can process a portion of the rdim indices.

The idea is:

Each thread in the block processes a chunk of the rdim indices using a stride of blockDim.x.

So, for a thread with tid = threadIdx.x:

int start_rdim = tid;

int end_rdim = dims[dim];

int stride = blockDim.x;

int min_val = INFINITY;

for (int rdim = start_rdim; rdim < end_rdim; rdim += stride) {

    int input_idx = base + rdim * rdim_stride;

    float val = input[input_idx];

    if (val < min_val) {

        min_val = val;

    }

}

Then, after all threads in the block have their partial min, they need to reduce to find the overall min.

This requires using shared memory to store each thread's partial min and then perform a reduction within the block.

The steps would be:

1. Each thread computes its partial min.

2. They store their partial min in shared memory.

3. Synchronize.

4. Perform a block-wide reduction using shared memory.

5. The final min is written to the output.

Let's detail the kernel code.

First, declare shared memory for the partial mins.

__shared__ float shared_min[256]; // assuming block size <= 256.

But to be flexible, we can use blockDim.x size.

float* sdata = shared_min;

But need to make sure the array is big enough. So, using a maximum block size, say 256.

Thus:

const int tid = threadIdx.x;

// compute base and rdim_stride as above.

// compute partial min.

float partial_min = INFINITY;

for (int rdim = tid; rdim < dims[dim]; rdim += blockDim.x) {

    int input_idx = base + rdim * rdim_stride;

    float val = input[input_idx];

    if (val < partial_min) {

        partial_min = val;

    }

}

// write to shared memory.

sdata[tid] = partial_min;

__syncthreads();

// now perform block reduction.

// code for block reduction.

int i = blockDim.x / 2;

while (i != 0) {

    if (tid < i) {

        if (sdata[tid] > sdata[tid + i]) {

            sdata[tid] = sdata[tid + i];

        }

    }

    __syncthreads();

    i /= 2;

}

// the final min is in sdata[0].

if (tid == 0) {

    output[output_idx] = sdata[0];

}

This is a standard block reduction approach. The reduction uses a binary reduction where each step halves the number of elements to consider.

Wait, but this assumes that the shared memory array is filled with the partial mins. Each thread writes its partial min to sdata[tid], then the reduction proceeds by comparing and taking the minimum.

Wait, in the code above, the first step is each thread writes its partial min to sdata[tid].

Then, in the reduction steps, for i = blockDim.x / 2 down to 1:

Each thread with tid < i compares its value with tid + i and takes the minimum.

Thus, after each iteration, the number of active threads needed is halved.

Finally, the thread 0 writes the result to the output.

This should work.

Now, putting all together into the kernel:

First, the kernel code.

The kernel function would look like this:

#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

template <typename scalar_t>
__global__ void min_reduction_kernel(
    const scalar_t* input,
    scalar_t* output,
    int dim,
    int dims[3], // B, D1, D2
    int output_size
) {

    // Each block handles one output element.
    int output_idx = blockIdx.x;

    // Compute the coordinates in non-reduction dimensions.
    int remaining_dims[2];
    int coord0, coord1;

    if (dim == 0) {
        // reducing over first dimension (B)
        remaining_dims[0] = dims[1]; // D1
        remaining_dims[1] = dims[2]; // D2
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 1) {
        // reducing over second dimension (D1)
        remaining_dims[0] = dims[0]; // B
        remaining_dims[1] = dims[2]; // D2
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 2) {
        // reducing over third dimension (D2)
        remaining_dims[0] = dims[0]; // B
        remaining_dims[1] = dims[1]; // D1
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    }

    // Compute base and stride for the reduction dimension.
    int base = 0;
    int rdim_stride = 0;
    if (dim == 0) {
        base = coord0 * dims[2] + coord1;
        rdim_stride = dims[1] * dims[2]; // D1*D2
    } else if (dim == 1) {
        base = coord0 * (dims[1] * dims[2]) + coord1;
        rdim_stride = dims[2]; // D2
    } else if (dim == 2) {
        base = coord0 * (dims[1] * dims[2]) + coord1 * dims[2];
        rdim_stride = 1;
    }

    // Now compute the partial min.
    __shared__ float shared_min[256];
    const int tid = threadIdx.x;

    float partial_min = std::numeric_limits<float>::infinity();

    for (int rdim = tid; rdim < dims[dim]; rdim += blockDim.x) {
        int input_idx = base + rdim * rdim_stride;
        float val = input[input_idx];
        if (val < partial_min) {
            partial_min = val;
        }
    }

    shared_min[tid] = partial_min;
    __syncthreads();

    // Block reduction.
    int i = blockDim.x >> 1;
    while (i > 0) {
        if (tid < i) {
            if (shared_min[tid] > shared_min[tid + i]) {
                shared_min[tid] = shared_min[tid + i];
            }
        }
        __syncthreads();
        i >>= 1;
    }

    if (tid == 0) {
        output[output_idx] = shared_min[0];
    }
}

Wait, but I used scalar_t, but the input is a float tensor. Let's assume it's float for now.

Wait, but in the code, the user's original model uses torch.min, which for a float tensor will return a float. So assuming input is float.

Thus, the kernel can be written for float.

Now, the kernel is defined, and then we need to write a wrapper function to launch it.

The wrapper function in C++ would take the input tensor, the dimension, and the output tensor, and launch the kernel.

But in PyTorch's cpp extension, we need to handle the tensors and their data pointers.

So, the wrapper function:

torch::Tensor min_reduction_cuda(torch::Tensor input, int dim) {

    // Get the input dimensions.
    int dims[3];
    dims[0] = input.size(0);
    dims[1] = input.size(1);
    dims[2] = input.size(2);

    // Compute output size.
    int output_size = 1;
    for (int i = 0; i < 3; i++) {
        if (i != dim) {
            output_size *= input.size(i);
        }
    }

    // Create output tensor.
    auto output = torch::empty({output_size}, input.options());

    // Launch the kernel.
    const int block_size = 256;
    const dim3 grid(output_size);
    const dim3 block(block_size);

    // Get pointers.
    auto input_data = input.data_ptr<float>();
    auto output_data = output.data_ptr<float>();

    min_reduction_kernel<float><<<grid, block>>>(input_data, output_data, dim, dims, output_size);

    // Check for errors.
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("CUDA error: %s\n", cudaGetErrorString(err));
    }

    return output;
}

Wait, but the kernel's dim3 grid is set to output_size, which can be very large. For example, if the output_size is 128*4095 = ~524,288 elements, the grid will have 524k blocks, which is acceptable as CUDA allows grids up to 2^31-1 blocks in each dimension.

However, launching this many blocks may have some performance considerations, but it's manageable.

Now, the kernel function uses the 'min_reduction_kernel' with the template for float.

Now, compiling this code inline in Python using load_inline.

The code in Python would look like:

First, define the CUDA source code for the kernel.

min_reduction_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

template <typename scalar_t>
__global__ void min_reduction_kernel(
    const scalar_t* input,
    scalar_t* output,
    int dim,
    int dims[3], // B, D1, D2
    int output_size
) {

    int output_idx = blockIdx.x;

    int remaining_dims[2];
    int coord0, coord1;

    if (dim == 0) {
        remaining_dims[0] = dims[1]; // D1
        remaining_dims[1] = dims[2]; // D2
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 1) {
        remaining_dims[0] = dims[0]; // B
        remaining_dims[1] = dims[2]; // D2
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 2) {
        remaining_dims[0] = dims[0]; // B
        remaining_dims[1] = dims[1]; // D1
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    }

    int base = 0;
    int rdim_stride = 0;
    if (dim == 0) {
        base = coord0 * dims[2] + coord1;
        rdim_stride = dims[1] * dims[2]; // D1*D2
    } else if (dim == 1) {
        base = coord0 * (dims[1] * dims[2]) + coord1;
        rdim_stride = dims[2]; // D2
    } else if (dim == 2) {
        base = coord0 * (dims[1] * dims[2]) + coord1 * dims[2];
        rdim_stride = 1;
    }

    __shared__ scalar_t shared_min[256];
    const int tid = threadIdx.x;

    scalar_t partial_min = std::numeric_limits<scalar_t>::infinity();

    for (int rdim = tid; rdim < dims[dim]; rdim += blockDim.x) {
        int input_idx = base + rdim * rdim_stride;
        scalar_t val = input[input_idx];
        if (val < partial_min) {
            partial_min = val;
        }
    }

    shared_min[tid] = partial_min;
    __syncthreads();

    int i = blockDim.x >> 1;
    while (i > 0) {
        if (tid < i) {
            if (shared_min[tid] > shared_min[tid + i]) {
                shared_min[tid] = shared_min[tid + i];
            }
        }
        __syncthreads();
        i >>= 1;
    }

    if (tid == 0) {
        output[output_idx] = shared_min[0];
    }
}

torch::Tensor min_reduction_cuda(torch::Tensor input, int dim) {
    int dims[3];
    dims[0] = input.size(0);
    dims[1] = input.size(1);
    dims[2] = input.size(2);

    int output_size = 1;
    for (int i = 0; i < 3; i++) {
        if (i != dim) {
            output_size *= input.size(i);
        }
    }

    auto output = torch::empty({output_size}, input.options());

    const int block_size = 256;
    dim3 grid(output_size);
    dim3 block(block_size);

    auto input_data = input.data_ptr<scalar_t>();
    auto output_data = output.data_ptr<scalar_t>();

    min_reduction_kernel<scalar_t><<<grid, block>>>(input_data, output_data, dim, dims, output_size);

    return output;
}
"""

Wait, but in the wrapper function, the template parameter is needed. However, since we're using float, we can specify the type.

Alternatively, the wrapper function can be written for float:

torch::Tensor min_reduction_cuda(torch::Tensor input, int dim) {

    ... (as before)

    min_reduction_kernel<float><<<grid, block>>>(input_data, output_data, dim, dims, output_size);

    return output;

}

So the kernel is launched with the scalar_t as float.

Now, putting this into the Python code.

The CPP source for the header would be:

min_reduction_cpp_source = """
torch::Tensor min_reduction_cuda(torch::Tensor input, int dim);
"""

Wait, but the function signature needs to be declared.

Thus, the full code:

First, in the Python script, after importing torch and the necessary modules, we define the CUDA kernel source code and the CPP header.

Then, we compile the CUDA code using load_inline.

Then, the ModelNew class would use this function instead of torch.min.

Wait, the original Model's forward function is:

def forward(self, x: torch.Tensor) -> torch.Tensor:
    return torch.min(x, dim=self.dim)[0]

Thus, the new model's forward function should call the custom CUDA function.

So, the ModelNew class would have a forward function that calls min_reduction_cuda with the input and dim.

But since the dim is part of the model's state (self.dim), it needs to be passed into the CUDA function.

However, in the current setup, the CUDA function requires the dim as an integer argument. So the model must pass the dim parameter when calling the CUDA function.

Thus, the code for ModelNew would be:

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.min_reduction = min_reduction  # The loaded module.

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.min_reduction.min_reduction_cuda(x, self.dim)

Wait, but how to handle the module.

Wait, the custom function is compiled into a module, so in the Python code:

After compiling with load_inline, the returned module has the function min_reduction_cuda.

Thus, the code:

min_reduction = load_inline(
    name="min_reduction",
    cuda_sources=min_reduction_source,
    cpp_sources=min_reduction_cpp_source,
    functions=["min_reduction_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Then, in the ModelNew class, the forward function calls this.

Putting it all together.

Now, potential issues:

1. The CUDA kernel's template needs to be for float. The input tensor must be a float tensor, which it is in the given example (get_inputs uses torch.rand which is float).

2. The output tensor is created with the correct shape. The output_size is computed correctly.

3. The block size is 256, which may not be optimal. For large reduction dimensions, a larger block size might be better. But for now, let's proceed with 256.

4. The shared memory array is of size 256, which must be at least as large as the block size. Since the block size is 256, this is okay.

5. The reduction loop in the kernel uses blockDim.x, which is 256. Thus, if the reduction dimension size (dims[dim]) is less than the block size, some threads will not process anything, but the loop will still work.

Now, testing:

The original code's get_inputs function returns a tensor of shape (128, 4096, 4095).

Suppose dim is 1 (the default in get_init_inputs). The output should be of shape (128, 4095), which matches the computed output_size.

The kernel should correctly compute the min along the second dimension.

Potential errors to check:

- Index calculation for coordinates.

- The base and rdim_stride calculations.

- The block reduction steps.

- Shared memory usage.

But assuming the kernel code is correct, this should work.

Now, writing the complete Python code:

The final code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

min_reduction_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

template <typename scalar_t>
__global__ void min_reduction_kernel(
    const scalar_t* input,
    scalar_t* output,
    int dim,
    int dims[3],
    int output_size
) {
    int output_idx = blockIdx.x;

    int remaining_dims[2];
    int coord0, coord1;

    if (dim == 0) {
        remaining_dims[0] = dims[1];
        remaining_dims[1] = dims[2];
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 1) {
        remaining_dims[0] = dims[0];
        remaining_dims[1] = dims[2];
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 2) {
        remaining_dims[0] = dims[0];
        remaining_dims[1] = dims[1];
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    }

    int base = 0;
    int rdim_stride = 0;
    if (dim == 0) {
        base = coord0 * dims[2] + coord1;
        rdim_stride = dims[1] * dims[2];
    } else if (dim == 1) {
        base = coord0 * (dims[1] * dims[2]) + coord1;
        rdim_stride = dims[2];
    } else if (dim == 2) {
        base = coord0 * (dims[1] * dims[2]) + coord1 * dims[2];
        rdim_stride = 1;
    }

    __shared__ scalar_t shared_min[256];
    const int tid = threadIdx.x;

    scalar_t partial_min = std::numeric_limits<scalar_t>::infinity();

    for (int rdim = tid; rdim < dims[dim]; rdim += blockDim.x) {
        int input_idx = base + rdim * rdim_stride;
        scalar_t val = input[input_idx];
        if (val < partial_min) {
            partial_min = val;
        }
    }

    shared_min[tid] = partial_min;
    __syncthreads();

    int i = blockDim.x >> 1;
    while (i > 0) {
        if (tid < i) {
            if (shared_min[tid] > shared_min[tid + i]) {
                shared_min[tid] = shared_min[tid + i];
            }
        }
        __syncthreads();
        i >>= 1;
    }

    if (tid == 0) {
        output[output_idx] = shared_min[0];
    }
}

torch::Tensor min_reduction_cuda(torch::Tensor input, int dim) {
    int dims[3];
    dims[0] = input.size(0);
    dims[1] = input.size(1);
    dims[2] = input.size(2);

    int output_size = 1;
    for (int i = 0; i < 3; i++) {
        if (i != dim) {
            output_size *= input.size(i);
        }
    }

    auto output = torch::empty({output_size}, input.options());

    const int block_size = 256;
    dim3 grid(output_size);
    dim3 block(block_size);

    auto input_data = input.data_ptr<float>();
    auto output_data = output.data_ptr<float>();

    min_reduction_kernel<float><<<grid, block>>>(input_data, output_data, dim, dims, output_size);

    return output;
}
"""

min_reduction_cpp_source = """
torch::Tensor min_reduction_cuda(torch::Tensor input, int dim);
"""

min_reduction = load_inline(
    name="min_reduction",
    cuda_sources=min_reduction_source,
    cpp_sources=min_reduction_cpp_source,
    functions=["min_reduction_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return min_reduction.min_reduction_cuda(x, self.dim)
```

Wait, but in the wrapper function, the template was instantiated for float, so the kernel call is min_reduction_kernel<float>... Thus, the input tensor must be float. Which it is in the example.

The ModelNew's forward function passes the input and dim to the CUDA function.

This should work.

I think this is the correct approach. The kernel handles the reduction by parallelizing over the output elements, each computed in a block, with threads within the block handling chunks of the reduction dimension. The shared memory-based reduction within the block ensures that the minimum is computed efficiently.

Now, let me double-check some parts.

The shared_min array is declared as size 256, which matches the block size of 256. So if the block size is 256, all threads can store their partial min in the shared array.

The reduction loop in the kernel starts with i = blockDim.x >> 1 (which is 128 for block size 256), then halves each iteration until i reaches 0.

Yes.

The initial partial_min is set to infinity, so any valid value (since input is random, which has negative values as well) will be less than infinity.

Edge cases: if the reduction dimension has size 0, but according to the problem's get_inputs, the dimensions are non-zero.

Another possible issue: the output tensor is created with the correct shape. The output_size is computed correctly.

Testing the code with the given get_inputs would require checking that the outputs are the same as the original model.

Overall, this should work.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

min_reduction_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

template <typename scalar_t>
__global__ void min_reduction_kernel(
    const scalar_t* input,
    scalar_t* output,
    int dim,
    int dims[3],
    int output_size
) {
    int output_idx = blockIdx.x;

    int remaining_dims[2];
    int coord0, coord1;

    if (dim == 0) {
        remaining_dims[0] = dims[1];
        remaining_dims[1] = dims[2];
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 1) {
        remaining_dims[0] = dims[0];
        remaining_dims[1] = dims[2];
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    } else if (dim == 2) {
        remaining_dims[0] = dims[0];
        remaining_dims[1] = dims[1];
        coord0 = output_idx / remaining_dims[1];
        coord1 = output_idx % remaining_dims[1];
    }

    int base = 0;
    int rdim_stride = 0;
    if (dim == 0) {
        base = coord0 * dims[2] + coord1;
        rdim_stride = dims[1] * dims[2];
    } else if (dim == 1) {
        base = coord0 * (dims[1] * dims[2]) + coord1;
        rdim_stride = dims[2];
    } else if (dim == 2) {
        base = coord0 * (dims[1] * dims[2]) + coord1 * dims[2];
        rdim_stride = 1;
    }

    __shared__ scalar_t shared_min[256];
    const int tid = threadIdx.x;

    scalar_t partial_min = std::numeric_limits<scalar_t>::infinity();

    for (int rdim = tid; rdim < dims[dim]; rdim += blockDim.x) {
        int input_idx = base + rdim * rdim_stride;
        scalar_t val = input[input_idx];
        if (val < partial_min) {
            partial_min = val;
        }
    }

    shared_min[tid] = partial_min;
    __syncthreads();

    int i = blockDim.x >> 1;
    while (i > 0) {
        if (tid < i) {
            if (shared_min[tid] > shared_min[tid + i]) {
                shared_min[tid] = shared_min[tid + i];
            }
        }
        __syncthreads();
        i >>= 1;
    }

    if (tid == 0) {
        output[output_idx] = shared_min[0];
    }
}

torch::Tensor min_reduction_cuda(torch::Tensor input, int dim) {
    int dims[3];
    dims[0] = input.size(0);
    dims[1] = input.size(1);
    dims[2] = input.size(2);

    int output_size = 1;
    for (int i = 0; i < 3; i++) {
        if (i != dim) {
            output_size *= input.size(i);
        }
    }

    auto output = torch::empty({output_size}, input.options());

    const int block_size = 256;
    dim3 grid(output_size);
    dim3 block(block_size);

    auto input_data = input.data_ptr<float>();
    auto output_data = output.data_ptr<float>();

    min_reduction_kernel<float><<<grid, block>>>(input_data, output_data, dim, dims, output_size);

    return output;
}
"""

min_reduction_cpp_source = """
torch::Tensor min_reduction_cuda(torch::Tensor input, int dim);
"""

min_reduction = load_inline(
    name="min_reduction",
    cuda_sources=min_reduction_source,
    cpp_sources=min_reduction_cpp_source,
    functions=["min_reduction_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return min_reduction.min_reduction_cuda(x, self.dim)
```