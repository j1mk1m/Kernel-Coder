The solution requires that you implement a fused CUDA kernel for Batch Normalization (forward pass only) to replace the existing PyTorch nn.BatchNorm2d layer. The fused kernel must handle the computation of mean, variance, normalization, and scaling/shift steps in a single CUDA kernel. Please make sure that the kernel is correct, efficient, and handles all required steps in a single kernel. You are allowed to use PyTorch's built-in functions for some parts, but the core computation (mean, variance, normalization, scaling) must be fused into a single kernel.

To implement the fused batch normalization kernel, follow these steps:

1. Compute the mean and variance across the batch and spatial dimensions. 
   - Mean is the average over all elements except the channel dimension.
   - Variance is computed using the mean, again over the same dimensions.

2. Normalize the input using the computed mean and variance. 
   - The formula is: (x - mean) / sqrt(variance + epsilon), where epsilon is a small value (default 1e-5).

3. Scale and shift the normalized output using gamma and beta parameters (affine transformation).
   - The final output is: gamma * normalized + beta.

4. The fused kernel must process all these steps in a single kernel launch. 

Make sure your kernel correctly handles the dimensions and data access patterns, including the channel dimension which is independent (each channel has its own mean/variance). Also, the kernel should be efficient and minimize memory accesses.

To compute the mean and variance efficiently, you can use a parallel reduction approach within each channel. Since each channel is independent, you can process each channel in separate CUDA thread blocks. For each channel, compute the sum and sum of squares across all elements in that channel, then compute mean and variance from those sums. 

The steps for the kernel design are as follows:

1. Structure the kernel to handle each channel independently. Each block processes a single channel.

2. Within each block, threads compute partial sums and partial sums of squares for the channel's elements.

3. Use parallel reduction within each block to compute the total sum and sum of squares for the channel.

4. After reduction, compute the mean and variance per channel.

5. Then, perform the normalization, scaling, and shifting in the same kernel.

You will need to handle synchronization within the block for the reduction steps and ensure that all threads in the block have completed the reduction before proceeding to the normalization step.

Also, you must precompute the gamma, beta, mean, and variance outside the normalization step, but within the same kernel.

Your fused kernel must be written in CUDA C++ and called from Python via the PyTorch C++ extension.

The final ModelNew class should replace the nn.BatchNorm2d layer with a call to this custom CUDA kernel.

Your code must include the necessary PyTorch bindings so that the kernel can be called from Python. Use the same structure as the example given (i.e., using load_inline for compiling the CUDA code).

Make sure that the kernel correctly handles the input dimensions (NCHW format, since BatchNorm2d in PyTorch expects this). The input tensor is 4-dimensional: (batch_size, num_features, height, width).

Also, note that the BatchNorm layer has learnable parameters gamma (weight) and beta (bias), which need to be stored in the model. So, your ModelNew should have parameters equivalent to the original BatchNorm layer (i.e., weight and bias).

When you write the kernel, make sure that it uses these parameters correctly. The parameters are of size equal to the number of features (channels), and they are learned during training. However, since we are only implementing the forward pass, you don't need to handle gradients or parameter updates in the kernel.

You must also handle the epsilon term to prevent division by zero. The default epsilon for PyTorch's BatchNorm is 1e-5.

Finally, ensure that the kernel is memory efficient, reusing memory where possible and avoiding unnecessary copies.

I will now provide the correct code for the fused batch normalization kernel. 

The code must contain:

- A CUDA kernel for the fused batch norm forward pass.

- A PyTorch extension that wraps this kernel and can be called from Python.

- The ModelNew class that replaces the nn.BatchNorm2d with this kernel, including the necessary parameters (weight and bias) of the same size as the original model's BatchNorm layer.

The kernel must correctly compute mean and variance per-channel, perform normalization, scaling, and shifting all in one kernel. The reduction steps must be implemented with CUDA threads and blocks appropriately.

Okay, I think I need to write a fused CUDA kernel for the forward pass of BatchNorm2d. Let me start by recalling the steps involved. The kernel needs to compute mean and variance for each channel, then normalize the input using those, and apply the scale and shift.

First, the input is NCHW format. Each channel has its own mean and variance. So, for each channel, I need to compute the sum and sum of squares over all elements in that channel (across batch, height, width). 

To do this efficiently, I can structure the kernel so that each block handles a single channel. Each block will process all elements of that channel. The threads in the block can compute partial sums and partial sums of squares. Then, within each block, perform a parallel reduction to get the total sum and sum of squares. 

Once the sums are obtained, compute mean and variance. Then, proceed to normalize each element, multiply by gamma, and add beta. All of this should be done in a single kernel launch to minimize overhead.

Wait, but how do I handle the reduction? Let me think. For each channel, each thread can process a chunk of the data. Let's say the input is of size (N, C, H, W). For a given channel c, there are N*H*W elements. Each thread in the block can process a portion of these elements. The total number of elements per channel is N*H*W. Let's denote this as M = N*H*W. 

Each thread can read an element, compute its value and square, accumulate to shared memory. Then, perform a reduction in shared memory. 

The steps for the kernel could be:

1. Each thread block is assigned to a channel. 

2. Each thread in the block processes a few elements of that channel. 

3. Compute partial sums and partial sums of squares in shared memory.

4. After reduction, compute mean and variance (using epsilon).

5. Then, compute the normalized value for each element, apply gamma and beta, and write to output. 

Wait, but after the reduction, how do the threads know the mean and variance? Since the reduction is done in shared memory, once the partial sums are reduced, the mean and variance can be stored in shared memory, and all threads in the block can access it. 

Then, the same threads can go through the elements again to compute the normalized values. But that might require reading the data again. Alternatively, maybe the threads can process the data in two passes: first compute the sums, then compute the normalized values using the already computed mean and variance. 

Alternatively, can we structure it so that in a single kernel, the threads first do the reduction to get mean and variance, then process each element. 

Yes. Here's an outline:

- For each channel (each block handles one channel):

   - Compute sum and sum_sq across all elements in the channel (using shared memory reduction)

   - Compute mean = sum / M

   - Compute variance = (sum_sq - (sum^2)/M) / M + epsilon (Wait, variance is computed as E[x^2] - (E[x])^2, so (sum_sq/M) - (sum/M)^2, then add epsilon?)

   - Then, each thread processes their assigned elements again to compute (x - mean)/sqrt(variance) * gamma + beta

So, each thread in the block would need to process the elements twice: once for the reduction, once for the normalization. 

But how to handle that in a single kernel? 

Alternatively, perhaps in the first phase, each thread loads their element, computes the value and square, and accumulates to shared memory. Then after reduction, the mean and variance are stored in shared memory. Then, the threads can process each element again, using the mean and variance from shared memory to compute the normalized value. 

This would require that each thread has access to the elements again. Since the elements are in global memory, the threads would need to read them again. 

Alternatively, can we store the elements in shared memory? But for large tensors, that might not be feasible. 

Hmm, for high performance, ideally, we want to process each element once. But perhaps the double pass is manageable. 

Alternatively, maybe we can compute the mean and variance first, then in the same kernel, each thread can process their elements again to compute the normalized value. 

Let me outline the steps for the kernel code:

Each thread block corresponds to a channel. Each block has a number of threads (e.g., 256). Each thread in the block is responsible for processing a range of elements in the channel. 

First, we need to calculate the number of elements per channel: M = N * H * W. 

Each thread can process a chunk of elements. Let's say each thread in the block handles M / (blockDim.x) elements. 

But wait, for large M, this might be too much for a single thread. Alternatively, use a grid-stride loop where each thread processes multiple elements in a loop.

So, first phase: 

1. Each block (per channel) starts by initializing shared memory arrays for sum and sum_sq. 

2. Each thread in the block loops over their assigned elements in the channel, accumulating to shared memory. 

3. After all threads have done their accumulation, perform a reduction in shared memory to compute the total sum and sum_sq for the channel. 

4. Compute mean and variance. 

5. Then, each thread loops over their elements again, using the computed mean and variance to compute the normalized value, multiply by gamma, add beta, and write to the output. 

This way, the kernel does two passes over the data: first for the reduction, second for the normalization. 

But this may involve some overhead, but given that the reduction is required and must be done per channel, perhaps this is the way to go. 

Now, the CUDA kernel code structure:

First, in the kernel function, each thread block handles one channel. 

The block index is the channel index. 

So, for the kernel, the grid dimensions would be (C, 1, 1), and the block dimensions (blockDim.x, 1, 1). 

The number of blocks is equal to the number of channels (features). 

Each thread in the block handles a portion of the elements in that channel. 

The code would be something like:

__global__ void fused_batch_norm_forward(
    const float* input,
    float* output,
    const float* gamma,
    const float* beta,
    int N,
    int C,
    int H,
    int W,
    float epsilon
) {
    int c = blockIdx.x; // current channel
    extern __shared__ float shared[];

    // shared memory for sum and sum_sq
    float* s_sum = shared;
    float* s_sum_sq = s_sum + blockDim.x; // enough space?

    // Each thread processes a set of elements in this channel
    int tid = threadIdx.x;
    int num_elements = N * H * W;
    int stride = blockDim.x; // threads are in block of size blockDim.x

    // Phase 1: Compute sum and sum_sq
    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;

    for (int i = tid; i < num_elements; i += stride) {
        // Compute the index in input
        // input is NCHW: so for channel c, the elements are arranged as N * H * W elements
        // So the linear index for the channel is c * num_elements + i
        float x = input[c * num_elements + i];
        local_sum += x;
        local_sum_sq += x * x;
    }

    // Write to shared memory
    atomicAdd(s_sum, local_sum);
    atomicAdd(s_sum_sq, local_sum_sq);

    __syncthreads();

    // After all threads in the block have contributed, the block needs to perform reduction
    // But how to do this?

    // Maybe we need to do a reduction in shared memory. 

    // Wait, but the atomicAdd is not necessary. Alternatively, each thread can write to their own position in shared memory, then do a reduction.

    Hmm, perhaps my initial approach is flawed. Let me think again.

Alternative approach for sum and sum_sq:

Instead of using atomic operations, perhaps each thread's local_sum and local_sum_sq can be stored in shared memory, then a reduction step is done. 

Wait, here's a better way:

Each thread first accumulates their local sums into local variables, then writes to shared memory. Then, perform a reduction in shared memory. 

Wait, perhaps first, each thread accumulates into their own local variables, then in a first step, each thread writes their local_sum and local_sum_sq into shared memory. 

Wait, but how to handle the accumulation? Let me structure it step by step.

First, each thread loads their portion of the data and computes their local_sum and local_sum_sq.

Then, all threads write their partial sums into shared memory. Since multiple threads are writing, but to separate locations, this is okay. 

Wait, but if each thread writes to a unique position in shared memory, but the shared memory has to be big enough to hold all the partial sums. 

Alternatively, since the threads are in a block, perhaps they can use a block-wide reduction. 

Alternatively, the first phase is:

Each thread computes a local_sum and local_sum_sq over their assigned elements.

Then, they write their local values to their thread's index in shared memory.

Then, perform a reduction across the block to compute the total sum and sum_sq. 

Wait, but how to do that?

Maybe a better approach is to first have each thread compute their local sums, then store in shared memory. Then, do a parallel reduction in shared memory.

Let me try to outline this.

Phase 1: Accumulate local sums

Each thread in the block:

- Reads their assigned elements and accumulates to local_sum and local_sum_sq.

Phase 2: Write local sums to shared memory.

Phase 3: Reduction step to compute the total sum and sum_sq.

Phase 4: Compute mean and variance.

Phase 5: Normalize and apply gamma/beta.

Wait, this is getting complicated. Let's try to code this step by step.

First, in the kernel:

Each thread in the block processes some elements. 

First, compute the total elements in the channel: num_elements = N * H * W.

Each thread is responsible for num_elements / blockDim.x elements (rounded up).

Wait, but using a loop:

for (int i = tid; i < num_elements; i += blockDim.x) {

   process element i in the channel.

}

Then, after all threads have done that, they write their local_sum and local_sum_sq to shared memory.

Wait, but shared memory needs to be allocated for each block. Let me think:

The shared memory for the block can be two arrays of size blockDim.x each? Or maybe just two arrays of size blockDim.x each?

Alternatively, each thread can write their partial sums to their thread index in shared memory.

Wait, the shared memory should be allocated as:

The kernel's shared memory is divided into two parts: one for sum, one for sum_sq. Each part is of size blockDim.x.

Wait, so:

float* s_sum = (float*) shared;
float* s_sum_sq = s_sum + blockDim.x;

Each thread writes their local_sum to s_sum[tid], and local_sum_sq to s_sum_sq[tid].

Then, after writing, we perform a reduction across the block to compute the total sum and sum_sq.

The reduction can be done via a block-wide reduction.

For example, for the sum:

for (int s=blockDim.x/2; s>0; s>>=1) {

   if (tid < s) {

       s_sum[tid] += s_sum[tid + s];

   }

   __syncthreads();

}

Similarly for sum_sq.

Wait, but this requires that after each step, the thread indices are less than the current step. 

Alternatively, using a standard block reduction.

This could be done for both s_sum and s_sum_sq.

After the reduction, the total sum is s_sum[0], and total sum_sq is s_sum_sq[0].

Then, the mean is sum / num_elements.

The variance is (sum_sq / num_elements - (mean)^2) + epsilon.

Wait, variance formula is:

var = (sum_sq / N) - (mean)^2, then add epsilon.

Yes. 

Once mean and variance are computed, then each thread can process their elements again to compute the normalized value.

Wait, but to compute the normalized value, each thread needs to read the mean and variance, which are the same for all elements in the channel.

So, after computing mean and variance, all threads in the block can read them from shared memory (or from registers), and then process their elements again.

Wait, but how to compute the variance?

Once the mean and sum_sq are known:

variance = (sum_sq / num_elements) - (mean * mean) + epsilon;

Wait, actually, the variance is (sum_sq - sum^2 / num_elements) / num_elements + epsilon?

Wait, let me re-calculate:

The variance is computed as the average of the squared deviations from the mean. 

variance = E[(x - mean)^2] = E[x^2] - (E[x])^2

Therefore:

variance = (sum_sq / num_elements) - (mean)^2

Then, we add epsilon to avoid division by zero.

Thus, the variance term is variance + epsilon, but actually, the formula is (variance + epsilon). Wait, no, the formula is variance + epsilon, but in the code, we compute variance as the above, then add epsilon.

Wait, the correct formula is:

var = (sum_sq / num_elements - (mean)^2) + epsilon

Wait no: variance is the term under the sqrt in the normalization. The formula is:

normalized = (x - mean) / sqrt(variance + epsilon)

Wait, the PyTorch documentation says that the variance is computed with the unbiased estimator (divided by N instead of N-1), but in the case of BatchNorm, it's divided by N (since the training mode uses the mini-batch statistics). 

Thus, variance is sum_sq / N - mean^2.

Then, variance plus epsilon is under the sqrt.

So, variance_plus_epsilon = (sum_sq / N - mean*mean) + epsilon;

Wait, but actually, variance_plus_epsilon = ( (sum_sq / N) - (mean)^2 ) + epsilon.

Yes.

Once that is computed, then the normalization step is (x - mean) / sqrt(variance_plus_epsilon).

Then, multiply by gamma and add beta.

Now, the second phase:

Each thread in the block processes their assigned elements again, this time using the computed mean and variance_plus_epsilon.

So, the steps after reduction are:

Compute mean and variance_plus_epsilon.

Then, each thread loops over their assigned elements again, computing the normalized value and applying gamma and beta.

But this requires reading the elements again from global memory, which could be a problem for performance. Alternatively, can we compute the normalized value in the first pass?

Hmm, but we can't, since the mean and variance depend on all elements. So, two passes are necessary.

Now, implementing this in code.

First, the shared memory allocation:

The shared memory per block is needed for the partial sums. Let's see:

The s_sum and s_sum_sq each need blockDim.x floats. So, total shared memory per block is 2 * blockDim.x * sizeof(float).

But we can also compute the reduction in shared memory.

Let me try writing the kernel code step by step.

First, the kernel:

template <typename scalar_t>
__global__ void fused_batch_norm_forward_kernel(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    const scalar_t* __restrict__ gamma,
    const scalar_t* __restrict__ beta,
    int N, int C, int H, int W,
    float epsilon) {

    int c = blockIdx.x;
    extern __shared__ float shared[];
    float* s_sum = (float*) shared;
    float* s_sum_sq = s_sum + blockDim.x;

    const int tid = threadIdx.x;
    const int num_elements = N * H * W;
    const int num_channels = C;

    // Phase 1: Compute local sums
    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;

    for (int i = tid; i < num_elements; i += blockDim.x) {
        const int idx = c * num_elements + i;
        scalar_t x = input[idx];
        local_sum += (float)x;
        local_sum_sq += (float)x * (float)x;
    }

    // Write local sums to shared memory
    s_sum[tid] = local_sum;
    s_sum_sq[tid] = local_sum_sq;
    __syncthreads();

    // Phase 2: Reduce the sums within the block
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_sum[tid] += s_sum[tid + s];
            s_sum_sq[tid] += s_sum_sq[tid + s];
        }
        __syncthreads();
    }

    // After reduction, the total sums are in s_sum[0] and s_sum_sq[0]
    float total_sum = s_sum[0];
    float total_sum_sq = s_sum_sq[0];
    __syncthreads();

    // Compute mean and variance_plus_epsilon
    float mean = total_sum / num_elements;
    float var = (total_sum_sq / num_elements) - mean * mean;
    float variance_plus_epsilon = var + epsilon;
    float inv_std = 1.0f / sqrtf(variance_plus_epsilon);

    // Phase 3: Normalize and apply gamma/beta
    for (int i = tid; i < num_elements; i += blockDim.x) {
        const int idx = c * num_elements + i;
        scalar_t x = input[idx];
        scalar_t normalized = (x - mean) * inv_std;
        output[idx] = normalized * gamma[c] + beta[c];
    }
}

Wait, but in the reduction phase, after the first loop, each thread is writing their local_sum and local_sum_sq into shared memory. Then, the reduction loop uses the standard block reduction method. 

Wait, in the code above, after the initial loop, each thread writes their local_sum and local_sum_sq to their thread index in shared memory. Then, they do a reduction loop. 

The reduction loop reduces the values in s_sum and s_sum_sq. The for loop goes from blockDim.x / 2 down to 1, each time halving the step. 

At each step, threads with tid < s add the value from tid + s to their current value. 

This way, after the loop completes, the total sum is in s_sum[0], and total_sum_sq is in s_sum_sq[0].

Then, we compute mean, variance_plus_epsilon, and inv_std.

Then, in phase 3, each thread loops over their elements again and computes the normalized value using the precomputed mean and inv_std, and applies gamma and beta. 

But gamma and beta are per channel. The gamma is gamma[c], and beta[c].

This seems correct. 

Now, the shared memory needs to be allocated as (blockDim.x * 2) floats. 

So, in the kernel launch, when we call fused_batch_norm_forward_kernel, the shared memory size should be:

size_t shared_size = 2 * blockDim.x * sizeof(float);

But in the CUDA kernel call, when launching, we have to specify the shared memory size. 

Now, in the PyTorch wrapper function, when we call the kernel, we need to set the shared memory size. 

Now, moving on to the wrapper function.

The wrapper function in PyTorch needs to:

- Accept input tensor, gamma, beta, epsilon, and the dimensions N, C, H, W.

- Launch the kernel with appropriate grid and block dimensions.

- Handle the shared memory size.

First, the kernel is templated, but in CUDA, we can specialize it for float.

Wait, in the code above, the kernel is written with template <typename scalar_t>, but when we call it, we need to instantiate it for float.

Alternatively, we can write the kernel for float.

Let me adjust the kernel to use float instead of template:

__global__ void fused_batch_norm_forward_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    int N, int C, int H, int W,
    float epsilon) {

    // ... same as above ...
}

This simplifies things. 

Now, in the wrapper function in PyTorch:

The function signature would be:

torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int64_t N,
    int64_t C,
    int64_t H,
    int64_t W,
    float epsilon
) {
    // ... code here ...
}

Wait, but the parameters N, C, H, W can be obtained from the input tensor's shape. So perhaps we can compute them inside the function.

Alternatively, the user can pass them. But in the code, since the input is a tensor of shape (N, C, H, W), we can get the sizes via input.size(0), input.size(1), etc.

So the wrapper function can compute N, C, H, W from the input tensor's shape.

Thus, the function can be written as:

torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    float epsilon
) {
    const int64_t N = input.size(0);
    const int64_t C = input.size(1);
    const int64_t H = input.size(2);
    const int64_t W = input.size(3);

    // ... rest of code ...
}

Now, the grid and block dimensions:

The grid size is C (number of channels). Each channel is handled by a block.

The block size can be chosen, say 256 threads per block. 

But need to ensure that the block size divides evenly into the number of threads required. 

The shared memory per block must be 2 * block_size * sizeof(float). 

Thus, in the kernel launch:

const int block_size = 256;
dim3 grid(C);
dim3 block(block_size);
size_t shared_mem_size = 2 * block_size * sizeof(float);

fused_batch_norm_forward_kernel<<<grid, block, shared_mem_size, stream>>>(...);

Wait, but PyTorch uses the current CUDA stream. 

In the wrapper function, we can get the current stream:

at::cuda::CUDAStream stream = at::cuda::getCurrentCUDAStream();

Then, launch the kernel with that stream. 

Putting this all together.

Now, the ModelNew class must include the parameters gamma and beta. 

In the original Model class, the BatchNorm layer has parameters gamma (weight) and beta (bias). Therefore, the ModelNew must have these parameters as well.

So, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        # also, the fused kernel function
        # Need to load the CUDA kernel here.

Wait, but the fused kernel needs to be compiled and loaded. So in the Python code, similar to the example, we need to define the CUDA source code as a string, then use load_inline to compile it.

Thus, the code structure would be:

First, define the CUDA kernel source code as a string.

Then, in the Python code, load it into a module, and make it callable.

Then, in the ModelNew class, the forward method calls this kernel function, passing the necessary parameters.

Putting all together.

Now, handling the parameters:

The kernel expects gamma and beta as tensors. So in the forward function:

def forward(self, x):
    # get gamma and beta from the model parameters
    gamma = self.gamma.contiguous()
    beta = self.beta.contiguous()
    # call the kernel function
    output = fused_batch_norm_forward(x, gamma, beta, epsilon)
    return output

Wait, but the epsilon must be a parameter. The original BatchNorm uses a default epsilon of 1e-5. The user might want to set it, but in this problem, since we're replicating the original, we can hardcode it to 1e-5 unless specified.

Alternatively, include epsilon as a parameter in the model. However, the original problem states that the kernel must handle the default epsilon of 1e-5. 

Thus, in the kernel, set epsilon to 1e-5.

Wait, in the problem statement, it says: "the default epsilon for PyTorch's BatchNorm is 1e-5." So in the kernel, we can hardcode it. 

Thus, in the kernel, the epsilon is a parameter passed to the kernel function. But in the PyTorch wrapper function, we can set it to 1e-5.

Alternatively, make it a parameter of the function, but since the problem says to use the default, we can hardcode it.

Wait, the problem says "the kernel must correctly handle the epsilon term to prevent division by zero. The default epsilon for PyTorch's BatchNorm is 1e-5."

Therefore, in the kernel, we can pass epsilon as an argument, but in the wrapper function, we can set it to 1e-5.

Thus, in the wrapper function:

epsilon = 1e-5

But in the kernel, it's a parameter. 

Alternatively, let me make the kernel take epsilon as an argument, and the wrapper function passes it. 

Thus, the wrapper function's signature includes epsilon, which is set to 1e-5 by default.

Therefore, in Python code:

def fused_batch_norm_forward(
    input,
    gamma,
    beta,
    epsilon=1e-5
):
    # ... 

Now, putting all together.

Now, let me write the complete code step by step.

First, the CUDA kernel code:

The CUDA kernel must be written as a string in Python.

Here's the CUDA code for the kernel:

cuda_source = """
__global__ void fused_batch_norm_forward_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    int N, int C, int H, int W,
    float epsilon
) {
    int c = blockIdx.x;
    extern __shared__ float shared[];
    float* s_sum = (float*) shared;
    float* s_sum_sq = s_sum + blockDim.x;

    int tid = threadIdx.x;
    int num_elements = N * H * W;

    // Phase 1: Compute local sums
    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;

    for (int i = tid; i < num_elements; i += blockDim.x) {
        int idx = c * num_elements + i;
        float x = input[idx];
        local_sum += x;
        local_sum_sq += x * x;
    }

    s_sum[tid] = local_sum;
    s_sum_sq[tid] = local_sum_sq;
    __syncthreads();

    // Phase 2: Reduce sums
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_sum[tid] += s_sum[tid + s];
            s_sum_sq[tid] += s_sum_sq[tid + s];
        }
        __syncthreads();
    }

    float total_sum = s_sum[0];
    float total_sum_sq = s_sum_sq[0];
    __syncthreads();

    // Compute mean, variance_plus_epsilon, inv_std
    float mean = total_sum / num_elements;
    float var = (total_sum_sq / num_elements) - (mean * mean);
    float variance_plus_epsilon = var + epsilon;
    float inv_std = 1.0f / sqrtf(variance_plus_epsilon);

    // Phase 3: Normalize and apply gamma/beta
    for (int i = tid; i < num_elements; i += blockDim.x) {
        int idx = c * num_elements + i;
        float x = input[idx];
        float normalized = (x - mean) * inv_std;
        output[idx] = normalized * gamma[c] + beta[c];
    }
}

torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    float epsilon
) {
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    dim3 grid(C);
    dim3 block(block_size);
    size_t shared_mem_size = 2 * block_size * sizeof(float);

    auto stream = at::cuda::getCurrentCUDAStream();
    fused_batch_norm_forward_kernel<<<grid, block, shared_mem_size, stream>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        N, C, H, W,
        epsilon
    );

    return output;
}
"""

Then, the corresponding header (cpp source):

cpp_source = """
torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    float epsilon);
"""

Wait, but the function definition in the header must match the implementation.

Now, in Python, we can load this inline CUDA code.

Then, the ModelNew class would use this function.

Putting it all together in Python code:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

cuda_source = """
__global__ void fused_batch_norm_forward_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    int N, int C, int H, int W,
    float epsilon
) {
    // ... (kernel code as above)
}

torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    float epsilon
) {
    // ... (wrapper function as above)
}
"""

cpp_source = """
torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    float epsilon);
"""

# Compile the CUDA code
fused_batch_norm = load_inline(
    name="fused_batch_norm",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["fused_batch_norm_forward"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
    
    def forward(self, x):
        # Ensure parameters are contiguous and on the same device as x
        gamma = self.gamma.contiguous()
        beta = self.beta.contiguous()
        # Call the CUDA kernel
        return fused_batch_norm.fused_batch_norm_forward(x, gamma, beta, 1e-5)

Wait, but in the kernel code, the parameters gamma and beta are accessed as gamma[c] and beta[c], which requires that gamma and beta are 1D tensors of size num_features. The parameters in the ModelNew are stored as nn.Parameters, which are 1D tensors, so that's okay.

Testing the dimensions:

Suppose input is (N, C, H, W). The kernel's gamma and beta are tensors of size (C,). 

Yes, so this should work.

Now, checking if the kernel is correctly handling the input dimensions.

In the kernel:

The input is accessed as input[c * num_elements + i].

The num_elements is N * H * W.

So for each channel c, the elements are stored in contiguous blocks of N*H*W elements. Since the input is in NCHW format, this should be correct. 

Because in a tensor of shape (N, C, H, W), the strides are such that moving along the channel dimension jumps by N*H*W elements. 

Yes, so the way the input is indexed is correct.

Now, the output tensor is initialized as empty_like(input), so it has the same shape.

Now, the block size is set to 256, which is a common choice, but needs to be a power of two? Not necessarily, but for block reductions it's better. 256 is a good choice.

Now, in the kernel, when blockDim.x is 256, the shared memory size is 2*256 * 4 bytes = 2048 bytes per block, which is acceptable.

Now, in the reduction loop, the steps halve the s variable, so for 256 threads, it will take log2(256)=8 steps.

This should be manageable.

Potential issues:

- If the number of elements per channel (N*H*W) is less than the block size, then in the first loop (i += blockDim.x), each thread will process at most one element. But since the loop is over all elements, it should still work.

- The shared memory is allocated as 2 * blockDim.x floats. Since each thread writes their local_sum and local_sum_sq to their thread's index, that's correct.

Another point: in the first phase, when multiple threads are writing to shared memory, but each thread writes to their own thread's position, so no overlapping.

Now, checking for possible errors:

In the kernel code:

Wait, in the first phase, after the loop over elements, the code writes to s_sum[tid] and s_sum_sq[tid]. 

But before that, after the loop, they need to syncthreads? Wait no: the code after the first loop is:

s_sum[tid] = local_sum;

s_sum_sq[tid] = local_sum_sq;

Then, __syncthreads();

Wait, the code as written does:

After the first loop, each thread writes to their own position in shared memory. Then, a __syncthreads() is called.

Yes, so that's okay. 

Then, the reduction loop is done.

Then, after reduction, compute total_sum and total_sum_sq, then __syncthreads() again?

Wait in the code above, after the reduction loop, there's a __syncthreads() before computing mean.

Wait in the code:

After the reduction loop:

float total_sum = s_sum[0];
float total_sum_sq = s_sum_sq[0];
__syncthreads();

Wait, that's incorrect. After the reduction loop, the total_sum is in s_sum[0], so after the loop completes, all threads have to wait until the reduction is done. The __syncthreads() after that is redundant. 

Wait the code as written:

After the reduction loop:

float total_sum = s_sum[0];
float total_sum_sq = s_sum_sq[0];
__syncthreads();

Wait that last __syncthreads() is unnecessary. Because after the reduction loop, all threads have already synchronized. 

Actually, the __syncthreads() after the reduction loop is redundant. So that line can be removed.

Wait, in the reduction loop, the last step of the loop will have s=1, then the threads with tid < 1 (tid=0) will perform the addition. Then, after the loop, the total_sum is in s_sum[0].

Then, after the loop, all threads are synchronized (since the for loop ends with a __syncthreads().

Wait the loop is:

for (int s = blockDim.x / 2; s > 0; s >>= 1) {

    if (tid < s) {
        s_sum[tid] += s_sum[tid + s];
        s_sum_sq[tid] += s_sum_sq[tid + s];
    }
    __syncthreads();
}

So after each iteration of the loop, a __syncthreads() is called. 

Thus, after the loop completes, all threads are synchronized. 

Therefore, the line __syncthreads() after the for loop is redundant and can be removed.

So the code should be:

After the for loop:

float total_sum = s_sum[0];
float total_sum_sq = s_sum_sq[0];

Then, compute mean and variance.

Then, __syncthreads(); is not needed there.

Wait, but the variables total_sum and total_sum_sq are computed by thread 0, but actually, all threads can read s_sum[0], since after the reduction, all threads have the same value in s_sum[0].

Thus, all threads can read it without issue.

Therefore, the __syncthreads() after the for loop is not needed and can be removed.

Similarly, after that, when computing inv_std, all threads can proceed.

Now, in phase 3, the threads loop over their elements again, using the computed mean and inv_std. 

This should be okay.

Another point: the gamma and beta are passed as tensors. The kernel expects their data pointers. 

In the wrapper function, gamma and beta are passed as contiguous tensors, so their data pointers are correct.

Now, in the ModelNew class, the parameters gamma and beta are initialized as ones and zeros, which matches the default initialization of PyTorch's BatchNorm2d (gamma is initialized to 1, beta to 0).

Thus, the parameters are correctly initialized.

Now, testing the dimensions:

Suppose input is (64, 64, 512, 512). Then, C=64, so the grid is 64 blocks. Each block (channel) processes 64*512*512 elements.

With a block size of 256, each thread in the block processes (64*512*512)/256 elements per iteration in phase 1.

Wait, 64*512*512 = 16,777,216 elements per channel.

Divided by 256 threads, each thread processes 65,536 elements in the first loop.

Which is manageable, but may require multiple threads per block to process a large number of elements. 

Alternatively, perhaps a larger block size would be better? But 256 is a common choice. 

Alternatively, if the block size is too small, the number of threads per channel may be insufficient. 

But with 256 threads, for 16 million elements, each thread has to process about 65k elements in the loop. Which could be okay, but may have a lot of iterations. 

Alternatively, maybe a larger block size like 512 or 1024 would be better. 

However, the maximum number of threads per block is 1024 on many GPUs, so 256 is safe.

Alternatively, the code can be adjusted to use a dynamic block size, but for simplicity, we can keep it at 256.

Another potential issue is the shared memory size. For 256 threads, it's 2 * 256 * 4 bytes = 2048 bytes per block, which is acceptable.

Now, in the kernel, the gamma and beta are accessed as gamma[c] and beta[c]. Since gamma and beta are 1D tensors of length C, this is correct.

Thus, the code should work.

Therefore, the final Python code with the CUDA kernel is as follows.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

cuda_source = """
__global__ void fused_batch_norm_forward_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    int N, int C, int H, int W,
    float epsilon
) {
    int c = blockIdx.x;
    extern __shared__ float shared[];
    float* s_sum = (float*) shared;
    float* s_sum_sq = s_sum + blockDim.x;

    int tid = threadIdx.x;
    int num_elements = N * H * W;

    // Phase 1: Compute local sums
    float local_sum = 0.0f;
    float local_sum_sq = 0.0f;

    for (int i = tid; i < num_elements; i += blockDim.x) {
        int idx = c * num_elements + i;
        float x = input[idx];
        local_sum += x;
        local_sum_sq += x * x;
    }

    s_sum[tid] = local_sum;
    s_sum_sq[tid] = local_sum_sq;
    __syncthreads();

    // Phase 2: Reduce sums
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_sum[tid] += s_sum[tid + s];
            s_sum_sq[tid] += s_sum_sq[tid + s];
        }
        __syncthreads();
    }

    float total_sum = s_sum[0];
    float total_sum_sq = s_sum_sq[0];

    // Compute mean, variance_plus_epsilon, inv_std
    float mean = total_sum / num_elements;
    float var = (total_sum_sq / num_elements) - (mean * mean);
    float variance_plus_epsilon = var + epsilon;
    float inv_std = 1.0f / sqrtf(variance_plus_epsilon);

    // Phase 3: Normalize and apply gamma/beta
    for (int i = tid; i < num_elements; i += blockDim.x) {
        int idx = c * num_elements + i;
        float x = input[idx];
        float normalized = (x - mean) * inv_std;
        output[idx] = normalized * gamma[c] + beta[c];
    }
}

torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    float epsilon
) {
    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    dim3 grid(C);
    dim3 block(block_size);
    size_t shared_mem_size = 2 * block_size * sizeof(float);

    auto stream = at::cuda::getCurrentCUDAStream();
    fused_batch_norm_forward_kernel<<<grid, block, shared_mem_size, stream>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        N, C, H, W,
        epsilon
    );

    return output;
}
"""

cpp_source = """
torch::Tensor fused_batch_norm_forward(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    float epsilon);
"""

# Compile the CUDA code
fused_batch_norm = load_inline(
    name="fused_batch_norm",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["fused_batch_norm_forward"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
    
    def forward(self, x):
        # Ensure parameters are contiguous and on the same device as x
        gamma = self.gamma.contiguous().to(x.device)
        beta = self.beta.contiguous().to(x.device)
        # Call the CUDA kernel
        return fused_batch_norm.fused_batch_norm_forward(x, gamma, beta, 1e-5)
```

This code defines a fused CUDA kernel for batch normalization, including mean and variance computation, normalization, and scaling/shift steps in a single kernel. The `ModelNew` class replaces the PyTorch `nn.BatchNorm2d` layer with this custom kernel while maintaining learnable parameters (gamma and beta). The kernel uses parallel reduction to efficiently compute channel-wise statistics and processes each element in two passes to ensure correctness and efficiency.