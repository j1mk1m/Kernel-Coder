Your architecture is for large N (like N=4096) and you can assume that the input matrices are upper triangular. This means that elements below the main diagonal (indices where row > column) are zero. You need to exploit this sparsity to optimize the computation. 

The key is to perform the matrix multiplication efficiently by taking advantage of the sparsity of upper triangular matrices. 

The final output of the model must be upper triangular, so you can optimize the computation by only calculating the upper triangular part of the resulting matrix. 

Make sure the code you write is correct and actually faster than the original implementation. 

I will execute your code and check if the output is correct and faster. 

Here are some hints to help you:

- The matrix multiplication of two upper triangular matrices results in an upper triangular matrix. Therefore, only the upper triangular part needs to be calculated.

- The standard matrix multiplication (like torch.matmul) does not take advantage of the sparsity, so it is inefficient.

- You can exploit the sparsity by only iterating over the elements where row <= column in the resulting matrix.

- For element C[i][j] = sum_{k=0}^{N-1} A[i][k] * B[k][j]

- Since A is upper triangular, A[i][k] is zero when k < i.

- Similarly, B[k][j] is zero when j < k.

- Therefore, the product A[i][k] * B[k][j] is non-zero only when i <= k <= j.

- Therefore, for each C[i][j], the summation can be restricted to k from i to j.

- This reduces the number of operations from O(N^3) to O(N^3/3) in the worst case.

- Additionally, you can parallelize the computation across the elements of the upper triangle.

- To further optimize, you can use shared memory, coalesced memory access, or other CUDA optimization techniques.

- You can also consider using __syncthreads() and other CUDA primitives to manage thread synchronization.

- It's important to structure your kernel to maximize memory access efficiency and minimize divergence.




Alright, so I need to optimize the matrix multiplication for two upper triangular matrices using CUDA. The original code uses torch.matmul which doesn't take advantage of sparsity, so I need a custom kernel that exploits the upper triangular structure.

First, let me think about the matrix multiplication formula: C[i][j] = sum_{k=0}^{N-1} A[i][k] * B[k][j]. Since A and B are upper triangular, A[i][k] is zero when k < i and B[k][j] is zero when j < k. So the product is non-zero only when i <= k <= j. That means for each C[i][j], the sum can be from k = max(i, 0) to min(j, N-1). Wait, actually, since A is upper triangular, A[i,k] is zero when k < i, so the product A[i,k]*B[k,j] can only be non-zero if k >= i. Similarly, B[k,j] is zero when k > j, so the product is non-zero only when k <= j. So combining those, k must be between i and j (inclusive). Therefore, the sum can be from k = i to k = j. That reduces the number of terms in each sum from N to (j - i + 1), which is a big saving for upper triangles.

Therefore, for each C[i][j], where i <= j (since the result is also upper triangular), the computation can be done with a sum from k = i to k = j. So instead of iterating over all k from 0 to N-1, we can limit it to k from i to j. This reduces the number of operations.

Now, how do I structure the CUDA kernel? Since the result is upper triangular, I can loop over all (i, j) where i <= j. Each thread can compute one element C[i][j]. But for large N like 4096, the number of elements in the upper triangle is N*(N+1)/2, which for N=4096 is about 8 million elements. Each of these elements requires (j - i + 1) multiplications and additions. Wait, but even that might be computationally intensive. Hmm, but maybe the kernel can be optimized further.

Alternatively, maybe it's better to parallelize the computation over the k indices. Hmm, but that might complicate things. Alternatively, for each C[i][j], the loop over k can be done in a thread. Let's think of each thread handling one element C[i][j], and within the thread, loop over k from i to j.

But threads can be grouped by blocks. Let me consider how to map threads to the elements. Since each element C[i][j] is independent (they don't interfere with each other, except in writing to memory, which is fine as long as each thread writes to its own location), we can assign each thread to handle one element. The grid size would be the number of elements in the upper triangle, which is N*(N+1)/2. That's a lot of threads, but maybe manageable with a large grid.

Wait, but for N=4096, N^2 is about 16 million, so upper triangle is ~8 million. Each thread would need to compute a varying number of terms (since for each (i,j), the number of k terms is j-i+1). That might lead to unbalanced workloads between threads. For example, the diagonal (i=j) requires only 1 term, but the top-right corner (i=0, j=4095) requires 4096 terms. That could lead to some threads taking much longer than others, which is bad for performance.

Hmm, maybe this approach isn't the best. Maybe a better way is to tile the computation in a way that's more amenable to shared memory or coalesced memory access.

Alternatively, perhaps we can structure the computation in terms of the outer loops. Let's think of the standard matrix multiplication, but with the sparsity constraints.

The standard approach for matrix multiplication on GPUs often uses tiled matrices, where blocks of threads handle tiles of the matrices, and use shared memory to cache parts of the matrices for reuse. However, in this case, the sparsity might allow us to optimize further.

Alternatively, considering the constraints for the upper triangular matrices, maybe we can split the computation into blocks where each block handles a tile of the C matrix, and within the block, each thread computes a subset of the terms. But I'm not sure.

Alternatively, perhaps the best way is to proceed with the initial idea, where each thread computes one element C[i][j], and loops over the necessary k terms. The problem is the varying number of iterations per thread. But for the worst case (i=0, j=4095), the loop would run 4096 times, which is a lot. But since the number of threads is 8 million, and each such thread would take a long time, this might be inefficient.

Hmm, maybe another approach is to consider that the k loop can be parallelized. For each k, compute the contributions to all possible (i,j) where i <= k <= j. But that might complicate the synchronization.

Alternatively, perhaps reorganize the computation in terms of the k index. Let me think: for each k, we can process all i <= k and j >=k, but that's getting complicated.

Alternatively, let's consider that the computation of C[i][j] requires only the rows i of A and column j of B. Since A is upper triangular, row i of A has non-zero elements from column i to N-1. Similarly, column j of B has non-zero elements from row 0 to j. Therefore, their product's contribution to C[i][j] comes from the intersection of the non-zero elements. Wait, but in the original formula, the sum over k is from 0 to N-1, but in reality, only k between i and j are non-zero. So for each C[i][j], the sum is over k from i to j, inclusive.

Therefore, for each element (i,j), the inner loop can start at k = i and end at k = j. So the number of terms in the inner loop is (j - i +1). Since this is variable, the kernel might have threads with varying workloads.

To mitigate this imbalance, perhaps we can tile the computation such that each thread block handles a block of elements (i,j) in a way that the work is more balanced. Alternatively, perhaps use a different approach where we compute the matrix multiplication in a way that takes advantage of the sparsity in a different way.

Wait, another idea: since A and B are upper triangular, their product's (i,j) element can be written as the sum_{k=i}^j A[i,k] * B[k,j]. So if we can iterate over i and j, and for each (i,j), loop over k from i to j, then we can compute each element's value correctly.

But how to structure the CUDA kernel for this. Let's think of each thread handling a single (i,j) pair where i <= j. The number of threads needed is N*(N+1)/2, which is about 8 million for N=4096. Let's see: 4096*4097/2 = 8,388,608 threads. That's a lot, but manageable with a grid of that size.

The kernel would have each thread compute the sum for its assigned (i,j) pair. The steps would be:

1. Assign each thread to an (i,j) pair where i <= j. To do this, we can use a 1D thread index and map it to (i,j). For example, for a thread index t, compute i and j such that i <= j. The mapping would require calculating the triangular index. Alternatively, we can use a 2D grid where each block is a square of threads, but that might complicate.

Alternatively, in the kernel, for each thread, we can compute i and j from the thread indices. Let me think of a way to index all (i,j) pairs with i <= j using a 1D index.

Let me consider the triangular numbers. The number of elements up to row n is (n(n+1))/2. To find the i and j for a given index t:

The idea is to find the row i such that the cumulative sum up to row i-1 is less than t, and cumulative up to row i is greater or equal. Then, j is t minus the cumulative sum up to row i-1, plus i.

Wait, here's an algorithm to compute i and j from t:

Let me think:

The total number of elements up to row m is m*(m+1)/2. So for a given t, find the largest i such that i*(i+1)/2 < t. Then, the remaining elements are t - i*(i+1)/2, so j = i + (t - (i*(i+1)/2) + 1) ?

Alternatively, perhaps it's easier to compute for each thread's global index t, compute i and j where i <= j:

Let me think of the mapping as follows:

The first element (0,0) is t=0.

Then, (0,1), (0,2), ..., (0, N-1): t=1 to t=N-1.

Then row 1: (1,1), (1,2), ..., (1, N-1): next N-1 elements.

So the first row has N elements (including (0,0)), the second row has N-1 elements (starting at (1,1)), etc. So for a given t, starting from 0:

Find the row i where the cumulative count up to row i-1 is less than or equal to t, and the cumulative up to row i is greater.

Wait here's a method:

The row index i can be found by solving i*(i+1)/2 <= t < (i+1)*(i+2)/2.

Wait actually, the cumulative up to row i (starting from row 0) is (i+1)(i+2)/2 -1? Wait maybe better to think:

Total elements up to row m (starting from 0) is sum_{k=0}^m (m - k +1) ? Not sure. Let me think again.

The total number of elements in rows 0 through m is (m+1)*(m+2)/2. Because row 0 has m+1 elements (from (0,0) to (0,m)), row 1 has m elements, etc., so total sum from k=0 to m of (m -k +1) = (m+1)(m+2)/2. Wait actually, the sum from k=1 to n of k is n(n+1)/2. So the total number of elements up to row m (assuming each row m has (m+1) elements? Wait no, actually, in row i, there are N - i elements (since j starts from i to N-1). So for N=4, row 0 has 4 elements (0,0;0,1;0,2;0,3), row1 has 3 elements (1,1;1,2;1,3), row2 has 2, row3 has 1. So the total up to row m is sum_{k=0}^m (N -k). So for N=4, up to row 3: 4+3+2+1=10, which is 4*5/2. So general formula for N rows up to m is (m+1)(2N - m)/2 ?

Wait maybe better to think of it as (N)(N+1)/2 - ( (N - m -1)(N - m)/2 ) ?

Alternatively, the total number of elements up to row i is (i+1)*(2N - i)/2 ?

Wait maybe a better approach is to compute i such that i*(2*N - i +1)/2 <= t, but this might be computationally expensive. Alternatively, in code, for a given t, find i such that the cumulative up to row i-1 is less than or equal to t, and cumulative up to row i is greater. But in CUDA, we might need to compute this for each thread, which could be slow.

Alternatively, maybe use a different indexing strategy where the kernel uses a 2D grid of threads, but only activates threads where i <= j. However, that would mean many threads are inactive, which is bad for performance.

Hmm, perhaps the best way is to use a 1D grid where each thread is assigned an index t, and computes the i and j as above. Let's see how to code that.

Alternatively, maybe we can precompute a lookup table for i and j given t, but for N=4096, the table size would be about 8 million entries, which might be memory intensive. So better to compute it on the fly.

The steps for the kernel would be:

For each thread:

1. Compute the global thread index t = blockIdx.x * blockDim.x + threadIdx.x

2. Find i and j such that i <= j corresponds to the t-th element in the upper triangle.

3. If t exceeds the total number of elements, skip.

4. Compute the sum_{k = i}^j (A[i][k] * B[k][j])

5. Store the result in C[i][j]

So the main challenge is step 2. Let me think of how to compute i and j from t.

An efficient way to compute i and j:

The cumulative number of elements up to row m is S(m) = m*(2N - m +1)/2 ?

Wait let me think again. The number of elements in row i is (N - i). So the total up to row i is sum_{k=0}^i (N -k) = (i+1)*N - sum_{k=0}^i k = (i+1)N - i(i+1)/2 = (i+1)(2N -i)/2.

So S(i) = (i+1)(2N -i)/2.

We need to find the largest i such that S(i) <= t.

Wait no, because S(i) is the cumulative count up to row i. So the total elements up to row i is S(i). So to find i where S(i-1) <= t < S(i).

Wait let me clarify:

Let me denote S(m) as the total number of elements in rows 0 through m.

So to find the row i such that S(i-1) <= t < S(i), where i starts from 0? Wait maybe.

Wait let me see:

Suppose t is the index starting from 0.

Suppose N is 4. Then the elements are:

row0: 0 (t=0), 1 (t=1), 2 (t=2), 3 (t=3)

row1: 4 (t=4),5 (t=5),6 (t=6)

row2:7,8 (t=7 and 8)

row3:9 (t=9)

Total elements 10.

For S(0) = 4 (since row0 has 4 elements)

Wait S(0) is the cumulative up to row0: sum_{k=0}^0 (N -k) = 4 elements. So S(m) is the cumulative up to row m.

Wait the S(m) formula is correct as (m+1)(2N - m)/2.

Wait for N=4, m=0:

(0+1)(2*4 -0)/2 = 1*8/2 =4, which matches.

m=1: 2*(8-1)/2 =2*7/2=7. Which is 4 (row0) +3 (row1)=7. Correct.

Yes, so the formula holds.

So given t, find the i where S(i-1) <= t < S(i).

Once i is found, then the position within the row is pos_in_row = t - S(i-1).

Then, since row i has (N -i) elements (from j=i to N-1), the j is i + pos_in_row.

Wait, let's see for t=5 (which is row1's second element, j=2):

i is 1 (since S(0)=4 <=5 < S(1)=7). So pos_in_row =5 -4 =1. So j =i + pos_in_row =1+1=2, which is correct (element (1,2)).

Similarly, t=6 (the third element of row1, j=3):

pos_in_row =6-4=2 → j=1+2=3 → correct.

Another example: t=7 (row2, first element (2,2)):

S(1)=7 (row0+row1). So S(1)=7. Wait t=7 is at S(1)=7? So S(1) is the cumulative up to row1, which is 7. Then t=7 would be in row2. Since S(1)=7, so i must be 2? Because S(2) is ?

Wait for i=2, S(2)= (2+1)(2*4 -2)/2 =3*6/2=9?

Wait N=4, m=2:

S(2)= (2+1)*(2*4 -2)/2 → 3*(8-2)/2 → 3*6/2=9. So S(2)=9. Then for t=7:

We need S(i-1) <=7 < S(i). Let's see:

Looking for i where S(i-1) <=7 < S(i).

Check for i=2: S(1)=7, so 7 <=7 <9 → yes. So i=2, and pos_in_row =7 - S(1)=0. Thus j =2 +0 =2 → (2,2) correct.

Yes, this formula works.

So the steps are:

Given t:

1. Find i such that S(i-1) <= t < S(i). S is as above.

2. pos_in_row = t - S(i-1)

3. j = i + pos_in_row

But how to compute i efficiently in CUDA code?

This requires solving for i in the equation S(i-1) <=t < S(i). Since S is a quadratic function of i, perhaps we can solve it numerically.

Alternatively, use binary search. Since N is up to 4096, binary search over i from 0 to N-1 is feasible.

Alternatively, approximate the value of i using the quadratic formula.

Let me think of S(i) = (i+1)(2N -i)/2 = (2N i +2N -i² -i)/2 = ( -i² + (2N -1)i + 2N )/2.

We need to find the i where S(i-1) <= t < S(i).

Let me denote t as the given value. To find i such that S(i-1) <=t < S(i).

Let me rearrange S(i) in terms of i:

S(i) = (i+1)(2N -i)/2.

We can set this equal to t and solve for i.

But since it's a quadratic equation, perhaps we can approximate.

Alternatively, let me consider that S(i) ≈ N²/2 for large N, but maybe that's not helpful.

Alternatively, approximate the quadratic equation:

Let me rearrange S(i) ≈ (2N i - i²)/2.

We can set this equal to t and solve for i:

i² -2N i + 2t ≈0.

Solutions i = [2N ± sqrt(4N² -8t)] /2 = N ± sqrt(N² - 2t).

We can approximate i as N - sqrt(N² -2t). Since we want the lower root? Not sure.

Wait perhaps the approximate value of i is roughly sqrt(2t). Since S(i) ~ i^2 for large i? Not sure. Alternatively, let's try plugging in numbers.

Alternatively, in code:

We can perform a binary search over possible i values (from 0 to N). Let's code that.

Here's a possible approach:

Initialize low =0, high = N. While low < high:

mid = (low + high +1)/2

if S(mid-1) > t → set high = mid-1

else → set low = mid

Once the loop exits, low is the desired i.

This binary search would take log2(N) steps, which is acceptable for CUDA.

So in code, within the kernel, for a given t, perform this binary search to find i. Then compute pos_in_row and j.

Okay, so that's the plan for mapping t to i and j.

Now, once we have i and j, the next step is to compute the sum from k = i to k =j of A[i][k] * B[k][j].

Each thread needs to:

- Read the row i of A from column k (starting at i) to column j.

- Read the column j of B from row k =i to row j.

- Multiply the elements and accumulate the sum.

But how to access the data?

Assuming A and B are stored in row-major order (as they are in PyTorch tensors).

So for A, the element A[i][k] is located at position i*N +k.

Similarly, B[k][j] is at position k*N +j.

Wait but B is stored as a matrix, so B[k][j] is stored in row-major. So yes.

So for each k from i to j, the thread computes A[i][k] * B[k][j], then accumulates.

The number of terms in the sum is (j -i +1).

So for each thread, the loop over k is from i to j, inclusive.

This loop can be done with a for loop in the kernel. However, for large j-i (like j -i = 4096), this could be slow. But for N=4096, the maximum j-i is 4096 (when i=0, j=4095). So that's a loop of 4096 iterations per thread, which might be expensive.

Hmm, this could be a problem. For such a thread, it's doing 4096 iterations, which is a lot, and this might be a bottleneck.

Alternative idea: perhaps the kernel can be restructured to compute the sum in parallel across multiple threads for each (i,j). But that complicates things because each (i,j) is a single element, so we can't really split the work unless we have multiple threads per element. But that might not be efficient either.

Hmm, maybe we need to find a way to vectorize the inner loop. But in CUDA, vectorization would require using intrinsics like __ldg for coalesced memory access, but I'm not sure.

Alternatively, perhaps reorganize the computation so that the inner loop is over k, and for each k, process all (i,j) pairs where i <=k <=j. But that might be complicated.

Alternatively, note that the product A[i][k] * B[k][j] is part of the sum for C[i][j], but also for other elements. Not sure if that helps.

Another Idea: Since the matrices are upper triangular, the product C = A * B can be computed more efficiently using the structure of the upper triangular matrices. For example, the standard matrix multiplication's outer loops can be adjusted to only compute the upper triangular part.

Wait, let me think of the standard matrix multiplication algorithm. The standard three loops:

for i in 0..N-1

    for j in 0..N-1

        for k in 0..N-1

            C[i][j] += A[i][k] * B[k][j]

But since C is upper triangular, we can limit j to be >=i, and then within that, for each (i,j), k can be from i to j.

But even so, the standard approach would still require O(N^3/3) operations, but perhaps there's a way to structure it in a CUDA-friendly way.

Alternatively, maybe using a tiled approach where each thread block processes a block of the matrix, and the threads within the block compute the necessary k terms.

Wait, perhaps a better approach is to compute the product in a way that each thread processes a small block of the matrix. For example, using the standard matrix multiplication's tiled approach, but with the sparsity considered.

In standard tiled matrix multiplication, the matrix is divided into tiles of size B (e.g., 16x16), and each block of threads processes a tile. The threads load tiles of A and B into shared memory and compute the products.

But with the sparsity, we can optimize further. For upper triangular matrices, the tiles in the lower part of the matrix can be skipped. Also, within the tiles, the computations can be limited to the upper triangular part.

Alternatively, let's think of the tiled approach:

Suppose we use a tile size of Bx B. Each block processes a tile of C at (blockRow, blockCol). The block's threads compute the tile's elements by iterating over k in tiles.

But since C is upper triangular, we can skip tiles where blockRow > blockCol.

Within a tile that's in the upper part, the threads can compute their elements by only considering k up to the diagonal.

Hmm, this might be more complex but could offer better performance because shared memory can be used to cache rows and columns, leading to better memory access.

Alternatively, since the matrices are upper triangular, their tiles would also be upper triangular, so maybe the standard tiled approach can be adapted with reduced computations.

Alternatively, let's consider that for the product of two upper triangular matrices, the resulting matrix is upper triangular, so only the upper triangle needs to be computed. So perhaps a tiled approach where each tile is in the upper triangle and processed accordingly.

But this requires more complex indexing and might complicate the code.

Hmm, given the time constraints, perhaps the first approach (each thread computes one (i,j) pair and loops over k from i to j) is manageable, even with the inner loop, provided that the number of threads is large enough to hide latency.

Let's proceed with that approach, and try to code it.

Now, the CUDA kernel:

First, the kernel function:

__global__ void upper_tri_mult_kernel(float *A, float *B, float *C, int N) {

    int t = blockIdx.x * blockDim.x + threadIdx.x;

    if (t >= N*(N+1)/2) return; // beyond upper triangle

    // Compute i and j from t.

    // Find i via binary search.

    int i =0;

    int low=0, high=N;

    while (low < high) {

        int mid = (low + high +1)/2;

        int s = (mid)*(2*N - (mid-1)) /2; // S(mid-1) = (mid-1+1)(2N - (mid-1)))/2 → mid*(2N - mid+1)/2 ?

        Wait, S(m) is (m+1)(2N - m)/2. So S(mid-1) = (mid)*(2N - (mid-1)))/2 ?

Wait the formula S(m) is (m+1)(2N -m)/2. So S(i-1) is when m =i-1:

S(i-1) = (i)*(2N - (i-1)) /2.

Wait let's recalculate:

Yes, S(m) is (m+1)*(2N - m)/2.

So for mid-1, m=mid-1:

S(mid-1) = (mid)*(2N - (mid-1)) / 2.

Wait let's plug in:

Let me compute S(i-1) = (i)*(2N - (i-1)) /2.

Wait perhaps better to code it as:

int mid = (low + high +1)/2;

int m = mid-1;

int s = (m+1)*(2*N - m)/2;

Wait, m = mid-1 → m+1 = mid, and 2N -m = 2N - (mid-1) = 2N -mid +1.

Thus s = mid * (2N - mid +1)/2.

Wait but perhaps this can be done with integer math.

But in code, in CUDA, variables are integers, so:

In the loop:

int mid = (low + high +1) /2;

int m = mid-1;

int s = (m+1)*(2*N - m) /2; // S(m) is S(mid-1)

Wait no, S(m) is (m+1)(2N -m)/2.

Wait S(m) is for row m. So S(mid-1) is the cumulative up to row (mid-1).

Wait yes, so m is mid-1.

Thus, S(m) = (m+1)*(2N - m)/2.

So to compute S(m) for m=mid-1, that's the cumulative up to row (mid-1).

Wait but the code needs to compute S(mid-1) and compare to t.

Wait the condition is if S(mid-1) > t → then we need to adjust the high.

Wait in the binary search, we need to find the smallest i where S(i-1) <=t and S(i) >t.

Wait let me code the binary search step.

    int low =0;

    int high = N; // since i can be up to N-1 (since j must be >=i, and N is the size)

    while (low < high) {

        int mid = (low + high +1) /2;

        int m = mid -1;

        int s = (m +1)*(2*N - m)/2;

        if (s > t) {

            high = mid -1;

        } else {

            low = mid;

        }

    }

    int i = low;

    // Now compute pos_in_row = t - S(i-1)

    int m = i -1;

    int s_prev = (m +1)*(2*N - m)/2; // S(i-1) is S(m)= m+1 is (i-1 +1) ?

Wait, no: S(m) is for m =i-1. So S(i-1) is (i-1 +1)*(2N - (i-1))/2 → i*(2N -i +1)/2.

Wait let me compute s_prev = S(i-1).

Wait when i is found, then i is the correct row, so:

s_prev = (i-1 +1)*(2*N - (i-1)))/2 → i*(2N -i +1)/2.

Wait but perhaps better to compute it as:

int s_prev = (i)*(2*N - i +1)/2;

Wait, for example, if i=0, then m = -1? Hmm, perhaps we need to handle the case when i=0 separately?

Wait no, because when i is 0, then m = -1. But since in the binary search, the initial low=0 and high=N.

Wait let's see when i=0.

If t=0, then the loop would have found low=0.

So s_prev = (0)*(2*N -0 +1)/2 → 0. So pos_in_row =0 -0 =0.

Thus j=0+0=0. Correct.

Another example, i=1:

s_prev = (i)*(2*N -i +1)/2 → 1*(2N -1 +1)/2 = 2N/2 =N → but for N=4 and i=1:

S(0) is 4, which is correct.

Wait for N=4 and i=1, the previous row is row 0, which has S(0)=4. So yes, s_prev =4.

Yes, so the formula is correct.

Thus, s_prev = i*(2*N -i +1)/2.

Wait, let me confirm with N=4 and i=1:

i*(2*4 -1 +1)/2 → 1*(8)/2 =4 → correct.

So pos_in_row = t - s_prev.

Then j =i + pos_in_row.

Thus:

    int s_prev = i ==0 ? 0 : (i)*(2*N - i +1)/2;

    // Wait, when i=0, the previous is S(-1), but we can just compute as above.

    int s_prev = i*(2*N -i +1)/2;

    int pos_in_row = t - s_prev;

    int j =i + pos_in_row;

    // Now compute C[i][j]

    float sum =0.0f;

    for (int k =i; k <=j; ++k) {

        float a_val = A[i * N + k]; // row-major, A[i][k] is stored at A[i*N +k]

        float b_val = B[k * N + j]; // B[k][j] is B[k*N +j]

        sum += a_val * b_val;

    }

    // write to C

    C[i * N + j] = sum;

}

Wait but in C, since it's upper triangular, we can write to C[i][j], which is the correct location.

But the C array is stored as a regular matrix. Since all threads are writing to different locations (each thread computes a unique (i,j)), there's no race condition.

Now, the kernel function is outlined.

Now, the next step is to set up the grid and blocks.

The total number of threads needed is N*(N+1)/2. For N=4096, that's 4096*4097/2 = ~8,388,608 threads.

The maximum number of threads per block is typically 1024 (or higher, depending on GPU). So the number of blocks would be ceil(threads / threads_per_block).

Thus, the kernel launch would be:

dim3 threadsPerBlock(256); // or 1024

dim3 numBlocks( (num_elements + threadsPerBlock -1)/threadsPerBlock );

upper_tri_mult_kernel<<<numBlocks, threadsPerBlock>>>(A, B, C, N);

But in PyTorch's CUDA code, the tensors are passed as pointers, so the kernel needs to be written to take pointers to A, B, and C.

Now, considering the data types: all tensors are float, so the kernel uses float pointers.

Now, potential optimizations:

- The inner loop over k can be unrolled if possible, but for varying k length, that's not feasible.

- The access patterns for A and B can be optimized. For example, A[i][k] is contiguous in row i, so for a given i, the row is stored contiguously. So when looping over k from i to j, the elements A[i][k] are contiguous, so accessing them in order is good for coalesced memory access. Similarly, for B[k][j], since B is row-major, the column j is non-contiguous in memory. Thus, accessing B[k][j] for varying k would be non-coalesced. That might be a problem.

Hmm, accessing B's column j is problematic because the elements are scattered across different rows. For B, to get B[k][j], the memory location is B + k*N +j. Since k varies from i to j, and j is fixed for a given thread, the accesses to B would be to different rows, so the addresses are spread out, leading to non-coalesced reads. This could be a significant performance bottleneck.

Hmm, that's a problem. To mitigate this, perhaps transpose B to store it in column-major order. But since we're using PyTorch tensors, which are row-major, that's not possible unless we transpose it, which adds overhead.

Alternatively, perhaps swap the roles of A and B in the computation. Wait, let's think:

The product C[i][j] = sum_{k=i}^j A[i][k] * B[k][j].

If we instead loop over k, but that might not help.

Alternatively, precompute the transpose of B so that accessing columns is row-wise. But storing the transpose would require memory, and may not be feasible.

Alternatively, in the inner loop over k, for a given thread, B[k][j] can be fetched sequentially by varying k. But since j is fixed, each B[k][j] is in a different row. So for B's data layout, this is a column access, which is not coalesced.

Thus, this might be a major performance issue.

Hmm, maybe another approach is needed.

Alternative Idea: Let's consider the matrix multiplication as:

C = A * B, but since A and B are upper triangular, we can exploit the sparsity in both matrices.

The standard matrix multiplication can be expressed as:

for i in 0..N-1

    for k in i..N-1 // since A[i][k] is zero for k <i

        for j in k..N-1 // since B[k][j] is zero for j <k

            C[i][j] += A[i][k] * B[k][j]

This way, each term is non-zero. The loops are ordered so that i <=k <=j.

This approach might be more efficient because the loops can be structured to have better memory access patterns.

Let me think:

The outer loop is over i from 0 to N-1.

For each i, the middle loop is over k from i to N-1.

For each k, the inner loop is over j from k to N-1.

Then, the contribution A[i][k] * B[k][j] is added to C[i][j].

This way, the order of computation is such that for each (i,k), we loop over j from k to N-1. Thus, for each (i,k), we can compute all j >=k.

This approach might allow for better memory access, especially for B[k][j], since for a fixed k and varying j, the B[k][j] elements are contiguous in row k (since B is stored in row-major). Thus, accessing B[k][j] for j from k to N-1 would be a contiguous block, leading to coalesced reads.

This could be a better approach.

Now, how to parallelize this. The total number of operations is still O(N^3/3), but the memory access pattern is better.

Perhaps we can structure the computation in a way that is amenable to CUDA.

Let me think of the three loops:

for i in 0..N-1

    for k in i..N-1

        for j in k..N-1

            C[i][j] += A[i][k] * B[k][j]

But how to map this to CUDA threads.

Possible approach: Each thread handles a particular (i,k), and then loops over j from k to N-1. The thread then accumulates the product term into the appropriate C[i][j] elements.

This way, the inner loop over j is contiguous in B's row k, and contiguous in C's row i.

The number of (i,k) pairs is N*(N+1)/2. For N=4096, this is the same as the previous approach (~8 million).

Each thread (i,k) would process the j from k to N-1, and for each j, add A[i][k] * B[k][j] to C[i][j].

This way, the inner loop over j has (N -k) iterations per thread. The number of iterations varies from 1 (when k=N-1) up to N-i (when k=i and j goes up to N-1).

This approach may have a better memory access pattern because for B[k][j], j runs from k to N-1, which is a contiguous block in row k. So the B accesses are coalesced.

The A accesses are A[i][k], which for a given thread (i,k) is a single value, so no problem.

For C[i][j], the j runs from k to N-1, so the C elements are in row i, positions j from k to N-1. These are contiguous in memory, so writing to them should be coalesced.

Thus, this approach may have better performance.

Now, the problem is to parallelize the (i,k) pairs.

Each thread can handle an (i,k) pair where i <=k.

Thus, the number of threads is again N*(N+1)/2, which is manageable.

The kernel would look like:

__global__ void upper_tri_mult_kernel(float *A, float *B, float *C, int N) {

    int t = blockIdx.x * blockDim.x + threadIdx.x;

    if (t >= N*(N+1)/2) return;

    // find i and k from t.

    // similar to previous approach, but now the i and k are such that i <=k.

    // The mapping is now over the upper triangle of (i,k).

    // The number of elements is N*(N+1)/2.

    // compute i and k from t.

    // Let's use the same method as before, but for i and k.

    // The cumulative up to row m (for i) is S(m) = sum_{i'=0 to m} (N -i').

    // Wait no, the (i,k) pairs are ordered such that for each i, k ranges from i to N-1.

    // The total number of elements up to row i (for i as the first index) is S(i) = (i+1)*(2N -i)/2 ?

Wait no, actually, the count of (i,k) pairs where i <=k is the same as the upper triangle count, which is N*(N+1)/2. The mapping is similar to the previous problem but for the (i,k) indices instead of (i,j).

So to map t to i and k where i <=k:

The same formula applies:

Let me denote m = i.

The cumulative count up to row m is sum_{i'=0}^m (N -i') → same as before.

Thus, the same binary search approach can be used to find i.

Once i is found, then the position within the row is pos_in_row = t - S(i-1).

Then k = i + pos_in_row.

Wait:

For example, with N=4 and t=0:

i=0, pos_in_row=0 → k=0.

t=1: i=0, pos_in_row=1 →k=1.

t=3 (for N=4):

i=0, pos_in_row=3 →k=3.

t=4: i=1, since S(0)=4, so pos_in_row=0 →k=1+0=1.

Thus, this works.

Thus, the code for finding i and k is the same as before, just with (i,k) instead of (i,j).

Once i and k are determined, the thread can process all j from k to N-1:

float a_val = A[i * N +k];

float b_val = B[k * N +j];

Wait, but for each j in k to N-1, the thread needs to loop over j and compute the product and add to C[i][j].

Wait, but how to loop over j in the kernel.

The thread would:

for (int j=k; j<N; j++) {

    C[i*N +j] += a_val * B[k*N +j];

}

Wait, but in this case, the initial C is all zeros. So we can replace the += with direct assignment if there are no previous contributions, but actually, each (i,k) pair's contribution is added to C[i][j], so we have to accumulate.

Wait, but in this approach, each (i,k) thread is responsible for adding their product term to all C[i][j] for j >=k. However, multiple threads can contribute to the same C[i][j].

Wait, no. Wait for each (i,k), the contribution is a_val * B[k][j], which is added to C[i][j]. But for a given (i,j), this can be contributed by all k from i to j.

Thus, each (i,j) is the sum over k from i to j of A[i][k]*B[k][j].

Therefore, the approach of processing each (i,k) and adding to all C[i][j] for j >=k will require all the k terms to be accumulated for each (i,j). However, this requires atomic operations or some way to accumulate.

Wait this is a problem. Because if two threads (i,k1) and (i,k2) both write to C[i][j] (for j >=k1 and j >=k2), then we have a race condition.

Thus, this approach is not feasible unless we have a way to accumulate the contributions.

This is a major issue. So the previous approach (each thread computes C[i][j] directly by summing over k from i to j) avoids this problem because each thread computes the entire sum and writes it once.

But in the alternative approach, the accumulation must be done in a thread-safe way. Since each (i,j) is written to by (j -i +1) threads (each k from i to j), this requires atomic adds, but atomic operations are slow, especially on large numbers.

Thus, this approach might not be feasible.

Hmm, so going back to the original idea where each thread computes C[i][j] directly might be the only way to ensure correctness without race conditions.

However, the memory access pattern for B's columns being scattered is a problem.

Alternative Idea: Since B is stored in row-major, the column j is spread out, but for a given j, the elements B[k][j] for varying k are in different rows. However, for the kernel where each thread computes (i,j), and loops over k from i to j:

The loop over k would access A[i][k] (which is in row i, so contiguous) and B[k][j], which is in column j, but for varying k. The accesses to B are scattered across different rows, leading to poor memory coalescing.

This might be a significant performance issue.

Possible solution: transpose B to column-major storage. But since we can't change the storage order in PyTorch easily, perhaps we can transpose it in code.

Wait, but transposing a 4096x4096 matrix would take memory. The total size is about 128MB (4096^2 *4 bytes), so transposing is manageable.

Wait, perhaps we can transpose B once in the kernel, but that's not efficient. Alternatively, during the kernel launch, we can have a pointer to B's transposed matrix, but that requires precomputing it.

Alternatively, in the kernel, for each thread, when accessing B[k][j], it can be computed as B[j][k] if B is transposed. But this requires pre-transposing B.

Thus, the plan could be:

- Precompute B_transposed = B.T (in PyTorch), so that B_transposed is stored in row-major as columns of the original B.

Then, in the kernel, B[k][j] would become B_transposed[j][k], which is stored contiguously in row j of B_transposed.

Thus, when looping over k from i to j, the B accesses would be to B_transposed[j][k], which are contiguous in row j of B_transposed, leading to coalesced reads.

This way, the memory access pattern for B would be better.

This requires transposing B once before the kernel launch, which is acceptable.

Thus, the steps would be:

1. In the PyTorch code, before launching the kernel, transpose B to B_T = B.t().contiguous().

2. Pass B_T to the kernel.

3. In the kernel, to access B's original B[k][j], we use B_T[j][k].

Thus, the inner loop would be:

for (int k =i; k<=j; k++) {

    float a_val = A[i*N +k];

    float b_val = B_T[j*N +k]; // since B_T is B transposed.

    sum += a_val * b_val;

}

This way, for a given j, the elements B_T[j][k] are contiguous in memory, so accessing them in order k from i to j would be contiguous.

Thus, this would improve the memory access pattern for B.

This is a crucial optimization to consider.

So the kernel would need to take B_T as an argument.

Now, the code would have to transpose B before launching the kernel.

In PyTorch, this is done with B_t = B.t().contiguous().cuda().

So the steps are:

In the forward function:

def forward(self, A, B):

    N = A.size(0)

    C = torch.zeros(N, N).cuda()

    B_t = B.t().contiguous()

    # launch kernel with A, B_t, C, N

    return C

Wait, but in the kernel, we need to pass pointers to A, B_t, and C.

Thus, this is manageable.

Thus, with this approach, the memory accesses for both A and B_T are contiguous in their respective rows.

This should significantly improve the performance.

Now, implementing this:

The kernel would have:

__global__ void upper_tri_mult_kernel(float *A, float *B_T, float *C, int N) {

    int t = blockIdx.x * blockDim.x + threadIdx.x;

    if (t >= N*(N+1)/2) return;

    // find i and j from t (since this is the (i,j) element)

    // as in the first approach.

    // compute i and j via binary search.

    // then compute the sum over k from i to j:

    // sum += A[i][k] * B_T[j][k]

    // (since B_T is B transposed, so B_T[j][k] = B[k][j])

    float sum =0.0f;

    for (int k =i; k <=j; k++) {

        float a_val = A[i * N +k];

        float b_val = B_T[j * N +k];

        sum += a_val * b_val;

    }

    C[i*N +j] = sum;

}

This way, the accesses to A[i][k] are contiguous in row i (since k increases).

The accesses to B_T[j][k] are contiguous in row j of B_T (since for varying k, moving along the row j of B_T, which corresponds to the column j of B).

Thus, both memory accesses are coalesced, which should help performance.

Now, the code needs to transpose B before passing it to the kernel.

Now, the kernel code is manageable.

Now, implementing the binary search for i and j.

Let me reiterate the code for finding i and j from t.

The binary search for i:

As before, the code is:

int low =0;

int high = N;

while (low < high) {

    int mid = (low + high +1)/2;

    int m = mid-1;

    int s = (m+1)*(2*N -m)/2;

    if (s > t) high = mid-1;

    else low = mid;

}

i = low;

Then, s_prev = i*(2*N -i +1)/2;

pos_in_row = t - s_prev;

j = i + pos_in_row;

Thus, this is correct.

Now, compiling this into code.

Now, the CUDA kernel code would look like:

__global__ void upper_tri_mult_kernel(float *A, float *B_T, float *C, int N) {

    int t = blockIdx.x * blockDim.x + threadIdx.x;

    if (t >= (N * (N +1))/2) return;

    // Compute i and j from t

    int low = 0;

    int high = N;

    int i = 0;

    while (low < high) {

        int mid = (low + high + 1) / 2;

        int m = mid -1;

        int s = (m +1) * (2*N - m) /2; // S(mid-1)

        if (s > t) {

            high = mid -1;

        } else {

            low = mid;

        }

    }

    i = low;

    int s_prev = i * (2*N -i +1) /2;

    int pos_in_row = t - s_prev;

    int j = i + pos_in_row;

    // Now compute the sum over k from i to j

    float sum = 0.0f;

    for (int k = i; k <=j; k++) {

        float a_val = A[i * N + k];

        float b_val = B_T[j * N + k]; // B_T is B transposed

        sum += a_val * b_val;

    }

    C[i * N + j] = sum;

}

This should work.

Now, the code in PyTorch would involve compiling this kernel.

Now, in the Python code:

We need to define the CUDA source code, compile it with load_inline, and then use it in the ModelNew.

So the steps:

Import torch, nn, and load_inline from torch.utils.cpp_extension.

Define the CUDA source code as a string.

Also, the kernel requires a function to launch it.

The kernel function is upper_tri_mult_kernel, which takes A, B_T (transposed B), C, and N.

The function to call from Python would need to handle the tensor arguments.

Thus, the Python code would have:

kernel_source = """
// The kernel code above
"""

Then, we need a host function to call the kernel:

extern "C" {

    void upper_tri_mult_cuda(torch::Tensor A, torch::Tensor B, torch::Tensor C, int N) {

        const int threadsPerBlock = 256;

        const int num_elements = N*(N+1)/2;

        const int num_blocks = (num_elements + threadsPerBlock -1)/threadsPerBlock;

        // Transpose B to B_T

        torch::Tensor B_T = B.t().contiguous();

        // Launch kernel

        upper_tri_mult_kernel<<<num_blocks, threadsPerBlock>>>(
            A.data_ptr<float>(),
            B_T.data_ptr<float>(),
            C.data_ptr<float>(),
            N
        );

        // Check for errors

        cudaDeviceSynchronize();

    }

}

Wait, but in the kernel, we need B_T to be passed as a tensor. The function upper_tri_mult_cuda takes B as input, transposes it, and passes it to the kernel.

Thus, in the host function, we transpose B and pass it to the kernel.

This should work.

Thus, the complete Python code would be:

First, define the CUDA kernel source:

kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void upper_tri_mult_kernel(float *A, float *B_T, float *C, int N) {
    int t = blockIdx.x * blockDim.x + threadIdx.x;
    if (t >= N * (N + 1) / 2) return;

    // Find i and j from t
    int low = 0;
    int high = N;
    int i = 0;
    while (low < high) {
        int mid = (low + high + 1) / 2;
        int m = mid - 1;
        int s = (m + 1) * (2 * N - m) / 2;
        if (s > t) {
            high = mid - 1;
        } else {
            low = mid;
        }
    }
    i = low;

    int s_prev = i * (2 * N - i + 1) / 2;
    int pos_in_row = t - s_prev;
    int j = i + pos_in_row;

    float sum = 0.0f;
    for (int k = i; k <= j; ++k) {
        float a_val = A[i * N + k];
        float b_val = B_T[j * N + k];
        sum += a_val * b_val;
    }

    C[i * N + j] = sum;
}

extern "C" {
    void upper_tri_mult_cuda(torch::Tensor A, torch::Tensor B, torch::Tensor C, int N) {
        const int threadsPerBlock = 256;
        const int num_elements = N * (N + 1) / 2;
        const int num_blocks = (num_elements + threadsPerBlock - 1) / threadsPerBlock;

        // Transpose B to B_T
        torch::Tensor B_T = B.t().contiguous();

        upper_tri_mult_kernel<<<num_blocks, threadsPerBlock>>>(
            A.data_ptr<float>(),
            B_T.data_ptr<float>(),
            C.data_ptr<float>(),
            N
        );

        // Synchronize to check for errors
        cudaError_t err = cudaGetLastError();
        if (err != cudaSuccess) {
            printf("CUDA error: %s\\n", cudaGetErrorString(err));
        }
    }
}
"""

Then, compiling this with load_inline:

from torch.utils.cpp_extension import load_inline

module = load_inline(
    name='upper_tri_mult',
    cpp_sources='',
    cuda_sources=kernel_source,
    functions=['upper_tri_mult_cuda'],
    verbose=True
)

Then, in the ModelNew:

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, A, B):
        N = A.size(0)
        C = torch.empty(N, N, device=A.device, dtype=A.dtype)
        module.upper_tri_mult_cuda(A, B, C, N)
        return torch.triu(C)  # Ensure upper triangular (though kernel ensures this already)

Wait, but the kernel ensures that only upper triangular elements are computed. The rest are zeros since C is initialized with torch.empty. However, to be safe, maybe initialize with zeros.

Wait in the code above, C is created with torch.empty, which may have garbage values. But since the kernel writes only to the upper triangle, and the result is supposed to be upper triangular, the lower triangle remains uninitialized. Thus, to avoid that, we should initialize C with zeros.

So in the forward function:

C = torch.zeros(N, N, device=A.device, dtype=A.dtype)

But in the kernel, we overwrite all the upper triangle elements. The lower triangle remains zero.

Thus, the final C is correct.

However, in the original code, the output was torch.triu(torch.matmul(A,B)), which effectively sets the lower triangle to zero. Since our kernel only computes the upper triangle and leaves the lower as zeros (initialized), it is correct.

Thus, the forward function can return C directly.

But in the original code, the output is wrapped in triu(), but since our kernel only computes the upper triangle and the lower is zero, it's redundant. However, to ensure that even if the kernel has some bug and writes to the lower triangle, it's safe to apply triu(). Alternatively, since the kernel only writes to i<=j, it should be okay.

Thus, the forward function can return C.

Now, putting it all together.

The complete Python code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void upper_tri_mult_kernel(float *A, float *B_T, float *C, int N) {
    int t = blockIdx.x * blockDim.x + threadIdx.x;
    if (t >= N * (N + 1) / 2) return;

    // Compute i and j from t
    int low = 0;
    int high = N;
    int i = 0;
    while (low < high) {
        int mid = (low + high + 1) / 2;
        int m = mid - 1;
        int s = (m + 1) * (2 * N - m) / 2;
        if (s > t) {
            high = mid - 1;
        } else {
            low = mid;
        }
    }
    i = low;

    int s_prev = i * (2 * N - i + 1) / 2;
    int pos_in_row = t - s_prev;
    int j = i + pos_in_row;

    float sum = 0.0f;
    for (int k = i; k <= j; ++k) {
        float a_val = A[i * N + k];
        float b_val = B_T[j * N + k];
        sum += a_val * b_val;
    }

    C[i * N + j] = sum;
}

extern "C" {
    void upper_tri_mult_cuda(torch::Tensor A, torch::Tensor B, torch::Tensor C, int N) {
        const int threadsPerBlock = 256;
        const int num_elements = N * (N + 1) / 2;
        const int num_blocks = (num_elements + threadsPerBlock - 1) / threadsPerBlock;

        // Transpose B to B_T
        torch::Tensor B_T = B.t().contiguous();

        upper_tri_mult_kernel<<<num_blocks, threadsPerBlock>>>(
            A.data_ptr<float>(),
            B_T.data_ptr<float>(),
            C.data_ptr<float>(),
            N
        );

        cudaError_t err = cudaGetLastError();
        if (err != cudaSuccess) {
            fprintf(stderr, "CUDA error: %s\\n", cudaGetErrorString(err));
        }
    }
}
"""

module = load_inline(
    name='upper_tri_mult',
    cpp_sources='',
    cuda_sources=kernel_source,
    functions=['upper_tri_mult_cuda'],
    verbose=True,
    extra_cuda_cflags=['-std=c++14']
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, A, B):
        N = A.size(0)
        C = torch.zeros(N, N, device=A.device, dtype=A.dtype)
        module.upper_tri_mult_cuda(A, B, C, N)
        return C  # The kernel ensures upper triangular, so no need for triu()
```

Wait, but in the original code, the output was wrapped in triu(), but the kernel only computes upper elements. So the output is already upper triangular. Thus, returning C is sufficient.

However, the kernel's output should be exactly the upper triangular part, and the lower is zero. So it's safe.

Testing this code should give the correct results and be faster than the original implementation.

Potential optimizations:

- The kernel's binary search could be optimized, but for N=4096, it's manageable.

- The inner loop could be vectorized using CUDA intrinsics, but that might be complex.

- Using shared memory for caching parts of A and B_T could help, but for large matrices, the working set might not fit.

- Tuning the block size (threadsPerBlock) to find the optimal value for the GPU.

- Precomputing the B_T tensor once instead of every forward pass.

Wait, in the forward function, every time it's called, B is transposed. For a fixed B, like in a model's layer, this could be done once during initialization. However, the problem states that the inputs A and B are generated each time (as per get_inputs), so B can vary each time. Hence, transposing must be done each time.

Thus, the current code should be correct.

Now, checking for possible errors:

In the kernel source, the extern "C" function upper_tri_mult_cuda uses B_T = B.t().contiguous(), which is correct.

The kernel function uses B_T's data pointer correctly.

The loop over k from i to j is correct.

Thus, this should work.

Another possible issue is the division in the binary search. Since integers can cause overflow for large N. For N=4096, the maximum terms in the formula are manageable with 32-bit integers.

For example, N*(N+1)/2 for N=4096 is (4096*4097)/2 = ~8 million, which is within 32-bit integer's range (max ~2^31).

Thus, it should be okay.

Another optimization: precomputing N as a constant in the kernel? But N is passed as an argument.

Overall, this code should work and be faster than the original torch.matmul approach.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void upper_tri_mult_kernel(float *A, float *B_T, float *C, int N) {
    int t = blockIdx.x * blockDim.x + threadIdx.x;
    if (t >= N * (N + 1) / 2) return;

    // Compute i and j from t
    int low = 0;
    int high = N;
    int i = 0;
    while (low < high) {
        int mid = (low + high + 1) / 2;
        int m = mid - 1;
        int s = (m + 1) * (2 * N - m) / 2;
        if (s > t) {
            high = mid - 1;
        } else {
            low = mid;
        }
    }
    i = low;

    int s_prev = i * (2 * N - i + 1) / 2;
    int pos_in_row = t - s_prev;
    int j = i + pos_in_row;

    float sum = 0.0f;
    for (int k = i; k <= j; ++k) {
        float a_val = A[i * N + k];
        float b_val = B_T[j * N + k];
        sum += a_val * b_val;
    }

    C[i * N + j] = sum;
}

extern "C" {
    void upper_tri_mult_cuda(torch::Tensor A, torch::Tensor B, torch::Tensor C, int N) {
        const int threadsPerBlock = 256;
        const int num_elements = N * (N + 1) / 2;
        const int num_blocks = (num_elements + threadsPerBlock - 1) / threadsPerBlock;

        // Transpose B to B_T
        torch::Tensor B_T = B.t().contiguous();

        upper_tri_mult_kernel<<<num_blocks, threadsPerBlock>>>(
            A.data_ptr<float>(),
            B_T.data_ptr<float>(),
            C.data_ptr<float>(),
            N
        );

        cudaError_t err = cudaGetLastError();
        if (err != cudaSuccess) {
            fprintf(stderr, "CUDA error: %s\\n", cudaGetErrorString(err));
        }
    }
}
"""

module = load_inline(
    name='upper_tri_mult',
    cpp_sources='',
    cuda_sources=kernel_source,
    functions=['upper_tri_mult_cuda'],
    verbose=True,
    extra_cuda_cflags=['-std=c++14']
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, A, B):
        N = A.size(0)
        C = torch.zeros(N, N, device=A.device, dtype=A.dtype)
        module.upper_tri_mult_cuda(A, B, C, N)
        return C
```