The given architecture uses 3D Average Pooling. I want you to write a custom CUDA operator for the 3D Average Pooling. The goal is to optimize this operation for performance. You can choose to either:

- Write a custom CUDA kernel that replaces the entire AveragePool3d operator, or
- Find a way to fuse multiple operators (if any) into a single kernel (though in this case, there is only a single operator). 

In any case, your implementation should achieve a speedup over the PyTorch native implementation on a GPU. 

Additionally, in the custom CUDA code, please compute the average pooling with the given parameters (kernel_size, stride, padding) but do not use PyTorch's built-in functions (e.g., F.avg_pool3d or nn.AvgPool3d). You must implement the computation from scratch in CUDA. 

You must ensure that the code you write is correct (produces the same output as the original) and is faster than the original implementation. 

When writing the CUDA kernel, you can use any PyTorch helper functions or CUDA primitives as needed. 

Make sure the ModelNew class uses your custom CUDA operator instead of the PyTorch version. 

Please provide the full code for the optimized ModelNew class along with the necessary CUDA kernel definitions and compilation steps using load_inline as in the example. 

**Final Answer**
The optimized code replaces the PyTorch AvgPool3d with a custom CUDA kernel, which reduces memory overhead and kernel launch overhead by fusing the computation into a single kernel. The kernel efficiently computes the average by iterating over the input tensor with the specified kernel size, stride, and padding, and uses parallel threads to process each output element. This approach avoids intermediate storage and leverages CUDA's parallel processing capabilities, leading to performance improvements.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for 3D Average Pooling
avg_pool_3d_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void avg_pool3d_forward_kernel(
    const scalar_t* __restrict__ input,
    scalar_t* __restrict__ output,
    const int batch_size,
    const int channels,
    const int in_depth,
    const int in_height,
    const int in_width,
    const int kernel_size,
    const int stride,
    const int padding) {

    const int out_depth = (in_depth + 2 * padding - kernel_size) / stride + 1;
    const int out_height = (in_height + 2 * padding - kernel_size) / stride + 1;
    const int out_width = (in_width + 2 * padding - kernel_size) / stride + 1;

    // Compute output element indices
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * out_depth * out_height * out_width) return;

    // Calculate the output's indices
    int w_out = idx % out_width;
    int h_out = (idx / out_width) % out_height;
    int d_out = (idx / (out_width * out_height)) % out_depth;
    int c = (idx / (out_width * out_height * out_depth)) % channels;
    int n = idx / (out_width * out_height * out_depth * channels);

    // Compute input indices with padding
    int in_d_start = d_out * stride - padding;
    int in_h_start = h_out * stride - padding;
    int in_w_start = w_out * stride - padding;

    scalar_t sum = 0.0;
    int valid_count = 0;

    for (int kd = 0; kd < kernel_size; ++kd) {
        int in_d = in_d_start + kd;
        if (in_d < 0 || in_d >= in_depth) continue;

        for (int kh = 0; kh < kernel_size; ++kh) {
            int in_h = in_h_start + kh;
            if (in_h < 0 || in_h >= in_height) continue;

            for (int kw = 0; kw < kernel_size; ++kw) {
                int in_w = in_w_start + kw;
                if (in_w < 0 || in_w >= in_width) continue;

                // Compute input index
                int in_idx = n * channels * in_depth * in_height * in_width +
                            c * in_depth * in_height * in_width +
                            in_d * in_height * in_width +
                            in_h * in_width +
                            in_w;

                sum += input[in_idx];
                valid_count++;
            }
        }
    }

    // Compute average
    output[idx] = sum / valid_count;
}

torch::Tensor avg_pool3d_forward_cuda(
    torch::Tensor input,
    int kernel_size,
    int stride,
    int padding) {

    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int in_depth = input.size(2);
    const int in_height = input.size(3);
    const int in_width = input.size(4);

    const int out_depth = (in_depth + 2 * padding - kernel_size) / stride + 1;
    const int out_height = (in_height + 2 * padding - kernel_size) / stride + 1;
    const int out_width = (in_width + 2 * padding - kernel_size) / stride + 1;

    auto output = torch::zeros({batch_size, channels, out_depth, out_height, out_width}, input.options());

    const int num_threads = 512;
    const int num_elements = batch_size * channels * out_depth * out_height * out_width;
    const int num_blocks = (num_elements + num_threads - 1) / num_threads;

    AT_DISPATCH_FLOATING_TYPES(input.type(), "avg_pool3d_forward_cuda", ([&] {
        avg_pool3d_forward_kernel<scalar_t><<<num_blocks, num_threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            channels,
            in_depth,
            in_height,
            in_width,
            kernel_size,
            stride,
            padding);
    }));

    return output;
}
"""

avg_pool_3d_cpp_source = """
torch::Tensor avg_pool3d_forward_cuda(
    torch::Tensor input,
    int kernel_size,
    int stride,
    int padding);
"""

# Compile the CUDA kernel
avg_pool3d = load_inline(
    name="avg_pool3d",
    cpp_sources=avg_pool_3d_cpp_source,
    cuda_sources=avg_pool_3d_source,
    functions=["avg_pool3d_forward_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0):
        super(ModelNew, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
        self.avg_pool_cuda = avg_pool3d

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.avg_pool_cuda.avg_pool3d_forward_cuda(
            x, self.kernel_size, self.stride, self.padding
        )
```

This implementation replaces the PyTorch `AvgPool3d` layer with a custom CUDA kernel. The kernel is designed to compute the 3D average pooling efficiently by:

1. **Kernel Launch Optimization**: Using a single kernel launch to compute all output elements.
2. **Memory Efficiency**: Directly accessing input elements with padding and stride logic, avoiding intermediate storage.
3. **Thread Parallelism**: Each thread computes one output element, leveraging CUDA's parallel processing.
4. **Valid Region Handling**: Checks for valid input indices to account for padding and boundary conditions.
5. **Type Dispatch**: Using `AT_DISPATCH_FLOATING_TYPES` to support all floating-point types.

The kernel's loop structure iterates over each kernel element, accumulating the sum and counting valid contributions to compute the average. The `num_blocks` and `num_threads` are chosen to maximize GPU utilization. This approach reduces overhead compared to PyTorch's implementation by fusing the computation into a single kernel and minimizing memory transactions.