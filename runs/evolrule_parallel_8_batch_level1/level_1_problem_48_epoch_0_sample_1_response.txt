**Additional Requirements:** 

- Your code must compile and run with torch 2.1.0 and CUDA 12.1
- Your code must use the same inputs and produce the same outputs as the original architecture.
- Do not use any third-party libraries, only PyTorch and the Python standard library.
- Your code must not have any dependencies beyond what's in the original starter code.
- Make sure to handle all tensor sizes and dtypes correctly.
- Your code must correctly use CUDA and produce a speedup over the original implementation.
- Make sure to return the same output tensor shape as the original implementation.
- Your code must work for any input shape and dimension, not just the given dimensions.
- Please do not copy the original code, you are required to replace the mean operator with your custom implementation.
- The custom CUDA kernel must be inlined using the load_inline function from torch.utils.cpp_extension.

Okay, I need to optimize the given Model class which uses torch.mean along a specific dimension. The goal is to replace the mean operation with a custom CUDA kernel to get a speedup. Let me think through how to approach this.

First, the original Model's forward method calls torch.mean(x, dim=self.dim). The task is to implement this mean calculation in a custom CUDA kernel. 

The steps for the CUDA kernel would involve:

1. Understanding how mean works. It sums all elements along the specified dimension and then divides by the number of elements in that dimension.

2. So the kernel needs to compute the sum along the given dimension first, then divide by the size of that dimension.

3. To compute the sum efficiently, perhaps we can use parallel reduction. Each thread can handle a portion of the elements, sum them, and then combine the results.

4. The challenge is handling different tensor shapes and dimensions. The kernel must be flexible to work for any input tensor shape and any dimension (0, 1, 2, etc.), not just the example dimensions provided.

Hmm, how to structure the kernel. Let's think about the dimensions. Suppose the input is a multi-dimensional tensor. Let's denote the size of the dimension to reduce as S. The output tensor will have one less dimension in that axis.

The kernel can process each element along the reduction dimension. For example, if the reduction is along dimension dim, each thread can process a slice across that dimension and accumulate the sum. Alternatively, maybe using a grid of threads where each threadblock handles a block of elements.

Wait, maybe it's better to compute the sum first. Let's think of the sum along the dimension first, then divide by the size. 

The steps would be:

- For each position not in the reduced dimension, compute the sum of all elements along that dimension.

- Then, divide each sum by the size of the dimension.

But how to parallelize this?

The input tensor can be of arbitrary rank. So, the kernel needs to handle any number of dimensions. Let's see:

Suppose the input has shape (d0, d1, d2, ..., dn), and we are reducing along dim. The output will have shape (d0, ..., dn) without the dim's size. 

The total number of elements in the output is the product of all dimensions except the reduced dimension. Let's call this output_size. 

Each output element is the mean of the elements along the reduction dimension in the input.

So, for each output element, the sum over the dim's elements must be calculated. 

Perhaps a straightforward approach is to have each thread handle one output element, and each such thread loops over the elements along the dim to compute their sum, then divides.

But this might be inefficient if the reduction dimension is large, as each thread would have to loop through all elements in that dimension. However, if the dimension is large, this could be slow. Alternatively, a parallel reduction approach would be better.

Alternatively, we can tile the work so that multiple threads contribute to the same sum.

Hmm, perhaps using a tiled approach. Let me think of the input as a 2D case for simplicity (even though it's arbitrary dimensions). Let's say we have a tensor of shape (N, D) and we want to reduce along dimension 1. Each output element is N elements, and each element is the mean of the D elements in that row.

In this case, each row can be processed by a block of threads. Each thread in the block can handle a portion of the D elements, accumulate partial sums, then combine using shared memory.

But for arbitrary dimensions, this might be complicated. Alternatively, let's think in terms of indices.

Each output element can be addressed by an index in the output space, which corresponds to a position in the input space excluding the reduction dimension. For each such position, we need to iterate over the reduction dimension's elements and sum them.

The problem is that if the reduction dimension is large (like 4096 as in the example), looping over all elements in a single thread would take time. But with enough threads, maybe it's manageable.

Alternatively, perhaps we can compute the sum in parallel by distributing the elements along the reduction dimension across threads.

Wait, perhaps the following approach:

The output tensor has size output_size = total_input_elements / input.size(dim). So each output element is the mean of a group of input elements along the reduction dimension.

Each thread can be assigned to compute one output element. To compute that, the thread would need to loop over all elements along the reduction dimension for that output's position.

The loop for each thread would be over the reduction dimension's size. For example, if the reduction dimension has size S, each thread's loop runs S times. However, if S is large (like 4096), then this loop might be slow, but with enough threads, it could be manageable.

The total number of threads needed would be equal to the number of output elements. Since each thread does S operations (summing S elements), the total work is O(N*S), where N is the number of output elements and S is the size of the reduction dimension.

Alternatively, perhaps using shared memory and parallel reduction within a block to compute the sum for each output element.

Let me think of the steps in code:

The kernel function would take the input tensor, the dimension to reduce, the output tensor, and the size of the reduction dimension.

The kernel would have to be written in such a way that it can handle any dimension.

First, let's consider how to map threads to output elements.

The kernel can be launched with grid size equal to the number of output elements, and block size 1 (since each thread handles one output). Alternatively, use a block size that can handle multiple outputs, but maybe simpler to have one thread per output element.

Wait, but for large output sizes, the number of threads could be very large. CUDA has limits on grid size. So maybe a better approach is to use a grid of blocks where each block handles a chunk of output elements.

Alternatively, let's see:

The input is a tensor of arbitrary dimensions. Let's denote the size of the reduction dimension as S = input.size(dim). The output has shape input.shape without the dim-th dimension.

The total number of output elements is N = input.numel() / S.

Each thread can process one output element. The thread index (thread_id) would range from 0 to N-1.

For each thread, given thread_id, compute the corresponding indices in the input tensor, then loop over the reduction dimension to accumulate the sum.

First, need to compute the indices:

The input has dimensions: let's say the shape is [d0, d1, d2, ..., dn], and the reduction is along dim. So the output shape is [d0, ..., dn] without the dim's dimension.

The thread's task is to compute the sum over the elements in the reduction dimension for the position specified by the output indices.

The steps for the kernel would be:

For each thread in the grid:

1. Compute the output element's index in the output tensor (output_index).

2. Convert output_index to the corresponding indices in the input tensor, excluding the reduction dimension.

3. Compute the starting index along the reduction dimension (start = ... ?)

Wait, perhaps it's easier to compute the input indices by considering the strides of the tensor.

Alternatively, think of the input as a flattened array, and the output as a flattened array. Each output element corresponds to a group of S elements in the input. The group is determined by the reduction dimension.

Alternatively, perhaps the following approach:

The input tensor can be viewed as a contiguous array. The reduction dimension's stride determines how the elements are grouped. 

But handling arbitrary dimensions with strides might be complex. Maybe it's better to compute the multi-dimensional indices.

Alternatively, let's represent the input as a multi-dimensional array. To compute the index along the reduction dimension, we can compute the offset based on the output index.

Let me think of the input as a 3-dimensional tensor for example, with shape (d0, d1, d2), and reducing along dimension 1. The output will have shape (d0, d2).

Each output element's position can be represented by (i0, i2), and corresponds to the elements (i0, *, i2) in the input, where * ranges over 0 to d1-1.

The output index can be computed as:

output_index = i0 * d2 + i2.

To get the input indices (i0, i1, i2) for the elements contributing to this output element, i1 ranges from 0 to d1-1, and the other indices are fixed.

So, for a given output_index, we can compute i0 and i2, then iterate over i1 from 0 to d1-1, and sum the elements at (i0, i1, i2).

The problem is to compute i0 and i2 from output_index. To do that, we need to know the strides or the dimensions.

Hmm, maybe using the strides of the tensor is the way to go. The input tensor's strides can be used to compute the address of the elements.

Alternatively, perhaps we can precompute the strides in the kernel. Wait, but the kernel would need to know the tensor's shape and the reduction dimension's size.

Alternatively, the kernel can take as parameters the input tensor, the dimension to reduce, and the size of the reduction dimension.

Wait, but in CUDA kernel parameters, we can pass integers. So perhaps the kernel can have parameters like dim_size (the size of the reduction dimension), and the total number of output elements.

Alternatively, let's structure the kernel as follows:

Each thread is responsible for one output element. The thread's index is the output element's position in the output tensor.

The kernel will have to compute for each output element the sum over the reduction dimension.

So, steps for the kernel:

1. For each thread (output element index):

   a. Compute the position in the input tensor (excluding the reduction dimension).

   b. Iterate over all elements along the reduction dimension (size S) and accumulate their sum.

   c. Divide by S to get the mean.

But how to compute the input indices?

To compute the indices, perhaps we can represent the input as a multi-dimensional array and calculate the offsets. However, this requires knowing the shape of the tensor and the reduction dimension's position.

Alternatively, perhaps using the input's strides to compute the starting address for each output element, then stepping through the reduction dimension.

Wait, the input tensor's strides can be precomputed. For example, in PyTorch, the strides tell how many elements to step in memory for each dimension.

Suppose the input is a tensor with shape (d0, d1, d2), strides (s0, s1, s2). For a given output element's indices (i0, i2), the corresponding starting position in the input is:

offset = i0 * s0 + 0 * s1 + i2 * s2.

Then, the elements along the reduction dimension (d1) can be accessed by stepping through the s1 stride. So for i1 from 0 to d1-1:

element_address = offset + i1 * s1.

Thus, the value is input_data[offset + i1 * s1].

Therefore, the thread can compute the sum by iterating over i1 from 0 to S-1 (where S = d1) and adding those elements.

This approach requires knowing the strides and the reduction dimension's size. But how do we pass this information into the kernel?

In the CUDA kernel, the input tensor is passed as a pointer, and we can get its strides and shape via the tensor's properties. Wait, but in the kernel code, we can't directly access the tensor's metadata like strides. The kernel can only access the data pointers and the size.

Hmm, perhaps the kernel needs to be given the dimensions and strides as parameters. Alternatively, the kernel can compute the necessary strides based on the input's shape and the reduction dimension.

Alternatively, perhaps it's easier to handle the input tensor as a flattened array and compute the indices manually. But this might not be efficient.

Alternatively, the kernel can be written to handle the case when the reduction dimension is the last dimension. But the problem requires handling any dimension.

Hmm, this is getting a bit complicated. Let me think of the kernel's parameters:

The kernel needs to:

- The input tensor's data pointer.

- The output tensor's data pointer.

- The dimension to reduce (dim).

- The size of the reduction dimension (dim_size).

- The total number of output elements (output_size).

- The strides of the input tensor (so that we can compute the offsets).

Wait, but passing all strides might be a bit tricky. Alternatively, the kernel can compute the strides based on the input's shape. But how?

Alternatively, perhaps we can precompute the strides in Python and pass them as parameters to the kernel. For example, when calling the kernel, we can pass the strides as an array.

Alternatively, perhaps the problem is manageable by treating the input as a multi-dimensional array and using the reduction dimension's index and size to compute the correct indices.

Alternatively, maybe it's easier to have the kernel handle the case when the reduction dimension is the last one, and then permute the tensor's dimensions in Python before calling the kernel. But that might add overhead.

Hmm, perhaps the best way is to proceed with the following plan:

The kernel will take the input tensor, the output tensor, the dimension to reduce (dim), the size of that dimension (dim_size), and the total number of output elements (output_size). The kernel will also need the strides of the input tensor, or the necessary information to compute the element offsets.

Wait, perhaps the strides can be passed as an array of integers. For example, in the Python code, we can get the strides of the input tensor and pass them as a list to the kernel.

Alternatively, in the kernel code, for each output index, we can compute the input indices by considering the reduction dimension. Let me think step by step.

Let me first think of the input as a multi-dimensional array with shape (d0, d1, ..., dn). The output has shape (d0, d1, ..., dn) without the dim-th dimension.

Each output element is identified by an index in the output's flattened array. Let's say the output has output_size elements. Each output element corresponds to a position in the input excluding the dim-th dimension.

The thread's index is the output index, which can be mapped to the input indices except for the reduction dimension. 

Let's consider the input indices as a list of integers, where each dimension has a certain size. Let the reduction dimension be dim, with size S = input.size(dim).

The input indices can be represented as (i0, i1, ..., idim-1, idim, idim+1, ..., in). The output indices omit idim, so the output's index corresponds to (i0, ..., idim-1, idim+1, ..., in). 

To compute the sum for the output element, we need to iterate over all possible idim from 0 to S-1, and sum the elements at those indices.

To compute the input indices given the output index, we need to compute the other indices. The problem is how to map the output index to the input indices except for the reduction dimension.

This requires knowing the strides or the sizes of each dimension except the reduction dimension.

Alternatively, the kernel can compute the strides based on the input's shape. Let's suppose that in the kernel, we have access to the input's shape. However, in CUDA code, the kernel can't directly query the tensor's shape. So perhaps the Python code will need to pass the necessary parameters to the kernel.

Alternatively, perhaps the kernel can be written in a way that the reduction dimension is the last dimension. Then, in Python, before calling the kernel, we can permute the input tensor's dimensions so that the reduction dimension is the last, compute the mean along the last dimension, and then permute back. However, permuting the tensor might have some overhead, but perhaps it's manageable.

Wait, permuting could be a good idea. Let's think:

Suppose the input has dimensions [d0, d1, d2, ..., dn], and the reduction is along dimension dim. If we can permute the tensor so that the dim becomes the last dimension, then the reduction becomes the last dimension, and the kernel can process it more easily.

For example, if dim is 1 (in the middle), we can permute the axes so that the shape becomes (d0, d2, d3, ..., dn, d1). Then, the last dimension is the one we need to reduce.

The permutation would require creating a new tensor, but in PyTorch, transpose and permute are views, so they might be efficient if possible.

Wait, but permuting would require contiguous storage. Maybe the input needs to be transposed, but in PyTorch, transpose can be a view (non-contiguous). However, when passing to a CUDA kernel, the data might need to be contiguous, so we might have to permute and then make it contiguous, which could involve a copy. That might add overhead. Hmm, maybe this approach is not ideal.

Alternatively, perhaps the kernel can handle any dimension by using the strides.

Let me think of the input as a 3D tensor (for example) with shape (a, b, c). Suppose we are reducing along the middle dimension (dim=1). The output shape will be (a, c).

For a given output index, say the flattened index k, how do I get the indices in the original tensor?

Let's denote the output's strides. The output's first dimension is a, and the second is c. The total elements are a*c. The index k can be mapped to (i,j) where i = k // c, j = k % c.

In the original tensor, the corresponding indices would be (i, *, j), where * ranges from 0 to b-1. The sum over * is the sum over the middle dimension.

The starting position in the input tensor for (i,j) would be:

offset = i * (a's stride) + 0*(b's stride) + j * (c's stride). Wait, but the strides are in terms of elements. Let's suppose the input's strides are (s0, s1, s2). For example, in a row-major order, the strides would be s0 = b*c, s1 = c, s2 = 1.

So the starting offset for (i,j) in the output would correspond to the input indices (i, 0, j). The offset would be i*s0 + 0*s1 + j*s2 = i*s0 + j*s2.

Then, to iterate over the middle dimension (dim=1), each step increments the index by s1 (which is c). So the elements are at offsets: offset + k*s1 for k in 0 to b-1.

Therefore, the sum can be computed as sum += input_data[offset + k*s1], for k from 0 to b-1.

Therefore, if the kernel can have access to the strides and the size of the reduction dimension, it can compute the sum for each output element by stepping through the input.

Thus, in the kernel, for each output element at index k:

1. Compute the input's indices except for the reduction dimension. The starting offset can be calculated based on the strides excluding the reduction dimension.

2. Iterate over the reduction dimension's size, stepping by the reduction dimension's stride each time, accumulating the sum.

So, in code, the kernel might look like this:

__global__ void mean_kernel(const float* input_data, float* output_data, int output_size, int dim, int dim_size, const int* strides, int input_ndim) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= output_size) return;
    
    // Compute the input indices excluding the dim dimension
    // Need to figure out the offsets based on the strides
    // This part is tricky, as we need to compute the base offset for the current output index
    
    // Let's assume strides is passed as an array of the input's strides
    // input_ndim is the number of dimensions of the input tensor
    
    // The problem is how to compute the base offset given the output index
    
    // To compute the base offset, the output index corresponds to a position in the input tensor excluding the dim dimension.
    // The output index can be mapped to the indices of the input tensor (without dim) as follows:
    // For example, if input shape is (d0, d1, d2, d3), and dim=2 (so reduction over d2),
    // then the output has shape (d0,d1,d3). The output index k is mapped to indices (i0,i1,i3)
    // The base offset for this in input is i0*d0_stride + i1*d1_stride + 0*d2_stride + i3*d3_stride
    // Then, stepping through the d2 dimension (stride d2_stride)
    
    // But how to compute i0,i1,i3 from the output index k?
    
    // This requires knowing the shape of the output's dimensions, which is input.shape without dim.
    
    // Alternatively, perhaps the kernel can compute the offset by treating the input as a flattened array,
    // but that would require knowing the strides of the input excluding the reduction dimension.
    
    // This is getting complicated. Maybe the kernel can be written assuming that the reduction dimension is the last one,
    // and then in Python, the input is permuted so that the reduction dim becomes last, then permuted back.
    
    // Let's try that approach for simplicity.
    
    // So, the plan:
    // In Python, before launching the kernel, we permute the input tensor so that the reduction dimension is the last.
    // Then, the kernel can assume the reduction is the last dimension, so the offset calculation is easier.
    
    // For example, if the original dim is 1 (in a 3D tensor), we transpose to (0,2,1), making the reduction dim last.
    
    // The kernel can then compute the base offset as (output_idx // (output_shape[-1])) * ... ?

Wait, perhaps that approach is manageable. Let's see:

Suppose we transpose the input so that the reduction dimension is the last one. Then the input is of shape (..., S), where S is the size of the reduction dimension.

The output shape is (..., ), i.e., without the last dimension.

Each output element corresponds to a position in the first dimensions (excluding the last), and the sum is over the last dimension's elements.

The kernel can then be written to process this case. The kernel can be launched with grid size equal to the product of all dimensions except the last (output_size = input.numel() / S).

Each thread can process one output element. The base index for the input elements is the output index multiplied by the stride needed to reach the first element of the last dimension.

Wait, in this case, the input's last dimension is contiguous. For example, in a tensor of shape (a, b, S), the last dimension's stride is 1 (assuming row-major). Then, for each output element at position (i,j), the base offset is i*a_stride + j*b_stride. The elements to sum are located at base_offset + k*S? Wait, no. For a tensor of shape (a, b, S), each output element (i,j) corresponds to the sum over the S elements in the last dimension. The base_offset is (i, j, 0), so the offset is i*(b*S) + j*S + 0, but that's not right. Wait, perhaps I need to think of the strides.

Wait, in row-major order, for a 3D tensor (d0, d1, d2), the strides are:

strides[0] = d1 * d2

strides[1] = d2

strides[2] = 1

Therefore, for an element at indices (i,j,k), the offset is i*strides[0] + j*strides[1] + k*strides[2]

If we permute the axes to make the last dimension the reduction dimension (originally dim), then the new shape would have the reduction dimension at the end.

Suppose the original shape is (d0, d1, d2), and dim is 1 (so we reduce along d1). The permuted shape is (d0, d2, d1). So the last dimension is d1, which is S = d1.

Thus, the new strides would be:

new_strides[0] = d2 * S (since new shape is (d0, d2, S))

new_strides[1] = S

new_strides[2] = 1

Thus, for an output element at indices (i,j) in the permuted tensor, the base offset is i*new_strides[0] + j*new_strides[1]

The elements to sum are at offsets base_offset + k*new_strides[2] for k from 0 to S-1.

Wait, the last dimension's stride is 1, so each element in the last dimension is adjacent in memory. Therefore, the elements to sum are at base_offset + 0, base_offset + 1, ..., base_offset + S-1.

Therefore, the kernel can be written to process each output element by:

sum = 0.0

for k in 0 to S-1:

    sum += input_data[base_offset + k]

then divide by S.

Thus, the kernel can be written for this case, assuming the reduction dimension is the last.

This approach requires permuting the input tensor in Python before calling the kernel, then permuting back the output.

The permutation steps in Python would be:

original_dim = dim

permuted_indices = list(range(input.dim()))

permuted_indices[-1], permuted_indices[dim] = permuted_indices[dim], permuted_indices[-1]

permuted_input = input.permute(permuted_indices).contiguous()

Wait, but permuting and then contiguous() may involve copying, but it's necessary to have the reduction dimension at the end.

The output of the kernel will be of shape (d0, d2), which when permuted back (swapping the last and original dim positions) would give the correct shape.

Wait, actually, the permuted input's shape after permutation (assuming we swapped dim and the last axis) is such that the output after kernel is the first dimensions except the last, so the output shape is (d0, d2) in the permuted case. When we swap back, the output's dimensions would need to be adjusted.

Hmm, perhaps the permutation approach is manageable.

So, in Python, the code would:

1. Permute the input to move the reduction dimension to the last axis.

2. Compute the mean along the last axis using a kernel that can handle that case.

3. The output tensor will have the shape of permuted_input.shape[:-1].

4. Permute the output tensor back to the original shape (excluding the reduced dimension).

Thus, the kernel can be written for the last dimension case, and the Python code handles the permutation.

This approach simplifies the kernel code, so let's proceed with that.

Now, writing the CUDA kernel for the last dimension case:

The kernel needs to process each output element (each in the flattened output array), and for each, sum the elements along the last dimension (size S).

Each output element's position corresponds to an index in the first dimensions (excluding the last). The base address is offset = output_idx * S. Then, the elements to sum are at offset, offset+1, ..., offset + S-1.

Wait, no. Because the input is permuted so that the last dimension is the reduction dimension (size S). So the first dimensions are all the other dimensions, and the last is S. Thus, each output element corresponds to a block of S elements in the input.

Wait, the input's shape is (..., S), and the output is (...,).

The total number of output elements is total = (input.numel()) / S.

Each output element's index is from 0 to total-1. The base address for the input elements is:

base = output_idx * S.

Wait, only if the input is contiguous. Since we call contiguous() after permuting, it will be contiguous.

Therefore, for each output element at index k:

sum = 0.0

for i in 0 to S-1:

    sum += input_data[k*S + i]

sum /= S

output_data[k] = sum

That's simple! The kernel can do exactly that.

Therefore, the kernel can be written as follows:

The kernel function will:

For each thread, process one output element. The thread index is the output index.

Compute the sum by iterating over the S elements, then divide by S.

This is straightforward and easy to code.

Now, in CUDA code:

The kernel would be:

__global__ void mean_last_dim_kernel(const float* input, float* output, int output_size, int dim_size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= output_size) return;

    const float* input_ptr = input + idx * dim_size;
    float sum = 0.0f;
    for (int i = 0; i < dim_size; ++i) {
        sum += input_ptr[i];
    }
    output[idx] = sum / dim_size;
}

This is very simple. 

Now, the Python code needs to handle the permutation.

In the forward function of the new Model:

def forward(self, x: torch.Tensor) -> torch.Tensor:

    dim = self.dim

    # Permute the tensor so that the reduction dim is last
    original_shape = x.shape
    ndim = x.dim()
    permutation = list(range(ndim))
    permutation[-1], permutation[dim] = permutation[dim], permutation[-1]
    permuted_x = x.permute(permutation).contiguous()

    # Compute the output shape
    output_shape = permuted_x.shape[:-1]

    # Get the dim_size (size of the last dimension after permutation)
    dim_size = permuted_x.size(-1)

    # Launch the kernel
    output = torch.empty(output_shape, device=x.device, dtype=x.dtype)

    # Calculate grid and block dimensions
    threads_per_block = 256
    blocks_per_grid = (output.numel() + threads_per_block - 1) // threads_per_block

    # Call the kernel
    self.mean_last_dim_kernel(permuted_x.data_ptr(), output.data_ptr(), output.numel(), dim_size, blocks_per_grid, threads_per_block)

    # Permute back to original dimensions except the reduced dimension
    # The permutation is the inverse of the previous permutation
    # To invert the permutation: swap the last and dim again, then permute
    inv_permutation = list(range(ndim -1))
    inv_permutation.insert(dim, ndim-1)  # ?

    Wait, perhaps the inverse permutation can be constructed as follows:

    Since the original permutation was swapping dim and last (perm[-1] and perm[dim] are swapped), the inverse is the same.

    Because permuting again would return to the original.

    So to get back to the original dimensions except the reduced one, we need to permute the output's dimensions back, except that the reduced dimension is removed.

    Wait, the output after kernel is permuted_x.shape[:-1], which is the original shape except the dim dimension was moved to last and then removed.

    So, the permutation to revert to the original dimensions (without the dim) is:

    For example, original dimensions were [d0, d1, ..., dim, ..., dn], after permutation to [d0,...,dn, dim], and after reduction, the output shape is [d0,...,dn] (without the dim's size).

    To revert the permutation, the output's dimensions are permuted to put the last dimension (originally dim) back into its original position, but since that dimension was reduced, we have to exclude it.

    Hmm, perhaps the permutation to apply to the output is the inverse of the permutation applied to the input, excluding the last dimension (since it's removed).

    Let me think of an example:

    Suppose original shape is (a, b, c), and dim=1 (middle). The permutation becomes (a, c, b). After reduction along last (b), output shape is (a,c). To get back to the original dimensions except dim=1, which was of size b, the desired output shape is (a, c), which is already the case. So no permutation is needed.

    Wait, but if the original dim was 0, then:

    Original shape (a, b, c), dim=0.

    permutation would swap 0 and last (2). So permuted shape is (c, b, a). After reduction, output shape is (c, b). To get back to original shape without dim 0 (a), the desired shape is (b, c) ?

    Wait, no. The original desired output shape when reducing along dimension 0 (size a) would be (b, c). But the permuted input was (c, b, a), reduction along last (a) gives shape (c, b). To get to (b, c), we need to swap the first and second dimensions.

    Hmm, so perhaps the permutation needed for the output is the inverse permutation, but without considering the last dimension (since it's removed).

    Let me see:

    Let's denote:

    The original permutation was:

    perm = list(range(ndim))

    perm[-1], perm[dim] = perm[dim], perm[-1]

    The permutation applied to the input.

    The output after kernel has shape: permuted_x.shape[:-1] → which is (original_shape[:dim], original_shape[dim+1:], ...), with the last element being original_shape[dim], but without the last dimension (since we removed the last dimension).

    To revert back, we need to swap the permuted's last dimension (original dim) back into the original position, but since it's now part of the first dimensions.

    For example, in the case where original dim is 1 in (a,b,c):

    permutation becomes [0,2,1]. The permuted input's shape is (a,c,b). After reduction, the output is (a,c). 

    The desired output shape is (a, c) (since original shape after removing dim 1 (b) is (a,c)), so no permutation is needed.

    If the original dim was 0 (a):

    Original shape (a,b,c).

    perm becomes [2,1,0]. So permuted input's shape is (c,b,a). After reduction along last (a), output is (c,b). The desired output shape is (b,c). So to get that, need to swap the first and second dimensions of the output (c and b → becomes b and c). 

    Thus, the inverse permutation for the output is to swap the position of the original dim and the previous last dimension (now first dimension in the output's case?).

    This is getting a bit complex. Maybe the way to compute the inverse permutation is:

    The output permutation needs to be such that the dimensions are in the original order except the reduced dimension is removed.

    Let's think of the permutation as:

    The original permutation swaps the dim and the last dimension. So the output's dimensions are the same as the original, except the dim and last are swapped, and the last is removed.

    To get back to the original order (without the dim), we need to shift the dimensions after the dim back.

    Alternatively, perhaps the permutation to apply to the output is:

    The inverse permutation would be the same as the permutation applied to the input (since swapping twice brings it back). But since the output is missing the last dimension (which was the original dim), the permutation for the output must account for that.

    Alternatively, perhaps the output's permutation can be done by creating a list of the output's dimensions' indices in the desired order.

    The output's shape is permuted_shape[:-1], which is (original_shape[0], ..., original_shape[dim-1], original_shape[dim+1], ..., original_shape[-1], original_shape[dim])?

    Not sure. Alternatively, maybe it's easier to permute the output's dimensions back by swapping the position of the original dim and the previous last dimension (now the last of the output's shape).

    Wait, let's think with the example where dim was 0 (original shape (a,b,c)):

    After permutation, the input becomes (c, b, a). After reduction, the output shape is (c,b).

    The desired output shape is (b,c). The output's dimensions are (c, b). To get to (b,c), we need to swap the two dimensions.

    The original dim was 0 (a). The last dimension of the permuted input was a (original dim 0). After reduction, the output's last dimension is b (original dim 1), and the first is c (original dim 2). The desired output is (b, c) → so swap the first and second dimensions of the output.

    The permutation for the output would be [1,0].

    The permutation can be calculated as follows:

    The output's current dimensions correspond to the original permutation's first dimensions except the last (the reduced one).

    The desired permutation for the output's dimensions is to place the original dimensions in order, excluding the reduced dimension.

    Let me think of the permutation list for the output:

    Let original permutation is perm = [p0, p1, ..., p_{n-1}], where the last element is swapped with dim.

    The output's dimensions are the first n-1 elements of permuted input's shape (since the last dimension is reduced).

    The desired output's dimensions should be the original dimensions except the reduced dim.

    So, to get the desired permutation for the output:

    The output's dimensions are ordered as per the original permutation except the last (reduced) dimension. We need to map these to the original dimensions except the reduced one.

    The permutation list can be constructed as follows:

    Let the permutation for the input was swapping dim and last (n-1):

    perm = list(range(n))
    perm[-1], perm[dim] = perm[dim], perm[-1]

    The output's dimensions are perm[0], perm[1], ..., perm[n-2]

    The desired output's dimensions should be the original dimensions except perm[dim] (which was the original dim, now moved to last).

    Wait, perhaps the correct way is to compute the inverse permutation that would map the output's current dimensions back to the original order except the dim.

    Alternatively, the inverse permutation for the output can be constructed by taking the inverse of the input permutation's first n-1 elements.

    This might be complicated. Perhaps the easiest way is to compute the desired permutation for the output as follows:

    The desired output dimensions should be the original dimensions, excluding the reduced dimension (dim).

    The current output dimensions after reduction are the permuted input's dimensions except the last.

    To get the desired dimensions, we can compute the permutation list that maps the current dimensions to the desired order.

    Let me see with the example where dim=0 (original shape (a,b,c)):

    Original dimensions: [0,1,2]

    permutation for input: [2,1,0]

    permuted input's shape: (c, b, a)

    after reduction, output shape: (c,b)

    desired output dimensions: [1,2] (since original dimensions excluding 0 are 1 and 2 → which correspond to b and c)

    The current output's dimensions are [2,1] (since the permuted input's first two dimensions are 2 (original 2) and 1 (original 1). 

    To get to [1,2], the permutation would be [1,0].

    So the permutation list for the output is [1,0].

    To generalize:

    The current output's dimensions are the first (n-1) elements of the input's permutation list.

    The desired output's dimensions are the original dimensions excluding the reduced dim (dim).

    Let's list all the original dimensions except dim:

    desired_order = list(range(n))

    desired_order.remove(dim)

    Now, the current output's dimensions are perm[:n-1], where perm is the input's permutation list.

    The permutation needed to transform current into desired is the inverse of the mapping from desired to current.

    To compute the permutation list for the output:

    Let current_dims = perm[:-1]

    desired_dims = list(range(n))

    desired_dims.pop(dim)

    The permutation list for the output is a list such that for each i in desired_dims, the current index is found in current_dims.

    For each desired dim d in desired_dims:

        find the position in current_dims where current_dims[j] == d → then the permutation list at position j will be the index in desired.

    This is getting complicated, but perhaps manageable in Python.

Alternatively, perhaps it's easier to construct the permutation list for the output as follows:

The desired order of the output's dimensions is the original order except the dim is omitted.

The current order of the output's dimensions is the permuted input's first n-1 dimensions (perm[:n-1]).

To get from current to desired, we need to compute a permutation that maps the current indices to the desired indices.

Let me see with the example where dim=0:

desired_dims = [1,2]

current_dims = [2,1]

To get from current to desired [1,2], the permutation list would be [1,0].

The permutation is the indices where current_dims has the desired elements.

desired[0] = 1 → current_dims has 1 at index 1.

desired[1] = 2 → current_dims has 2 at index 0.

So permutation is [1, 0].

Another example: if dim=1 (original shape (a,b,c)), permutation for input is [0,2,1].

permuted input's first two dimensions are 0 and 2 → current_dims [0,2].

desired_dims = [0,2] (since original dimensions excluding 1 are 0 and 2).

Wait, the desired order is the original dimensions except dim=1, so [0,2].

The current_dims are [0,2], so no permutation needed.

Thus the permutation list is [0,1].

So the permutation list for the output is the list of indices in current_dims that match the desired order.

Thus, the permutation list for the output can be computed as:

current_dims = perm[:-1]

desired_dims = list(range(n))

desired_dims.pop(dim)

output_permutation = []

for d in desired_dims:

    output_permutation.append(current_dims.index(d))

Wait, but in the first example, current_dims is [2,1]. desired_dims are [1,2]. So:

for d in [1,2]:

current_dims.index(1) → 1

current_dims.index(2) → 0

So output_permutation is [1,0], which is correct.

Yes! So this approach works.

Thus, in code:

original_permutation = list(range(ndim))

original_permutation[-1], original_permutation[dim] = original_permutation[dim], original_permutation[-1]

current_dims = original_permutation[:-1]

desired_dims = list(range(ndim))

desired_dims.pop(dim)

output_permutation = [current_dims.index(d) for d in desired_dims]

Then, to permute the output tensor:

output = output.permute(tuple(output_permutation)).contiguous()

Wait, but the output_permutation is the indices that map current_dims to desired order.

Yes.

Therefore, in the Python code for the forward function:

def forward(self, x: torch.Tensor) -> torch.Tensor:
    dim = self.dim
    ndim = x.dim()
    
    # Permute input to move reduction dim to last
    permutation = list(range(ndim))
    permutation[-1], permutation[dim] = permutation[dim], permutation[-1]
    permuted_x = x.permute(permutation).contiguous()
    
    dim_size = permuted_x.size(-1)
    output_shape = permuted_x.shape[:-1]
    
    output = torch.empty(output_shape, device=x.device, dtype=x.dtype)
    
    # Launch kernel
    threads = 256
    blocks = (output.numel() + threads - 1) // threads
    
    self.mean_last_dim_kernel(permuted_x.data_ptr(), output.data_ptr(), output.numel(), dim_size, blocks_per_grid=blocks, threads_per_block=threads)
    
    # Compute permutation to revert to original order except dim
    current_dims = permutation[:-1]  # current_dims are the first n-1 elements of permutation
    desired_dims = list(range(ndim))
    desired_dims.pop(dim)
    
    output_permutation = []
    for d in desired_dims:
        output_permutation.append(current_dims.index(d))
    
    # Apply permutation
    output = output.permute(output_permutation).contiguous()
    
    return output

Wait, but in the kernel call, how is the kernel being called?

Wait, in the previous example, the kernel was a function returned by load_inline. In the code for the ModelNew class, the elementwise_add was assigned to the function.

Wait, in the example provided, the kernel function is compiled via load_inline and stored in elementwise_add, then called as elementwise_add.elementwise_add_cuda(a, b).

In this case, the kernel function (mean_last_dim_kernel) is part of the CUDA source code. The Python wrapper function (mean_last_dim_cuda) would be called.

Wait, let's structure the CUDA code properly.

The CUDA code will need to define a kernel and a wrapper function.

The CUDA source code would be:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_last_dim_kernel(const float* input, float* output, int output_size, int dim_size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= output_size) return;

    const float* input_ptr = input + idx * dim_size;
    float sum = 0.0f;
    for (int i = 0; i < dim_size; ++i) {
        sum += input_ptr[i];
    }
    output[idx] = sum / dim_size;
}

torch::Tensor mean_last_dim_cuda(torch::Tensor input, int dim) {
    // Permutation is handled in Python, so here input is already permuted with dim at last
    // The dim here is the original dim, but in Python code, we have already done the permutation.
    // Wait, no: the CUDA function is expecting the input to have the reduction dim at last.
    // The Python code should handle the permutation before calling this function.

    // Wait, in this case, the CUDA function is only handling the last dimension case, so the input must have the reduction dim as last.
    // Thus, in the Python code, after permuting, we can call this kernel.

    // So the parameters here are: input is already permuted to have the reduction dim last.
    // dim parameter is not used here, but perhaps needed for the permutation in Python.

    // Wait, in the CUDA function, perhaps we can ignore the dim parameter since the input is already in the right format.

    // Get the last dimension size:
    int dim_size = input.size(-1);
    auto output_size = input.sizes().slice(0, input.dim()-1);

    auto output = torch::empty(output_size, input.options());

    const int threads_per_block = 256;
    const int blocks_per_grid = (output_size[0] + threads_per_block - 1) / threads_per_block;

    mean_last_dim_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        output.numel(),
        dim_size
    );

    return output;
}

Wait, but in Python, the function will be called with the permuted input, and the kernel will process it.

Wait, but in the Python code, after permuting, the input is passed to this CUDA function. However, in the Python code, the forward function is handling the permutation and the kernel call. So maybe the CUDA function is redundant. 

Alternatively, perhaps the kernel is called directly via the load_inline function. Let me re-express the code as per the initial example.

The code structure would be:

1. Define the CUDA kernel and the wrapper function in the CUDA source.

2. Use load_inline to compile it, and create a Python function that calls it.

Thus, the Python code for the ModelNew would:

- In the __init__, load the CUDA kernel.

So, the full code would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the CUDA code for the kernel
mean_last_dim_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_last_dim_kernel(const float* input, float* output, int output_size, int dim_size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= output_size) return;

    const float* input_ptr = input + idx * dim_size;
    float sum = 0.0f;
    for (int i = 0; i < dim_size; ++i) {
        sum += input_ptr[i];
    }
    output[idx] = sum / dim_size;
}

torch::Tensor mean_last_dim_cuda(torch::Tensor input, int dim_size) {
    auto output_size = input.sizes().slice(0, input.dim()-1);
    auto output = torch::empty(output_size, input.options());

    const int threads_per_block = 256;
    const int blocks_per_grid = (output_size[0] + threads_per_block - 1) / threads_per_block;

    mean_last_dim_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        output.numel(),
        dim_size
    );

    return output;
}
"""

mean_last_dim_cpp_source = "torch::Tensor mean_last_dim_cuda(torch::Tensor input, int dim_size);"

# Compile the CUDA code
mean_last_dim = load_inline(
    name="mean_last_dim",
    cpp_sources=mean_last_dim_cpp_source,
    cuda_sources=mean_last_dim_source,
    functions=["mean_last_dim_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.mean_last_dim = mean_last_dim  # store the module for calling

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        dim = self.dim
        ndim = x.dim()
        
        # Permute input to move reduction dim to last
        permutation = list(range(ndim))
        permutation[-1], permutation[dim] = permutation[dim], permutation[-1]
        permuted_x = x.permute(permutation).contiguous()
        
        dim_size = permuted_x.size(-1)
        output = self.mean_last_dim.mean_last_dim_cuda(permuted_x, dim_size)
        
        # Compute permutation to revert to original order except the reduced dimension
        current_dims = permutation[:-1]  # the first n-1 elements of permutation
        desired_dims = list(range(ndim))
        desired_dims.pop(dim)
        
        output_permutation = []
        for d in desired_dims:
            output_permutation.append(current_dims.index(d))
        
        # Apply the permutation to the output
        output = output.permute(output_permutation).contiguous()
        
        return output

Wait, but in the CUDA function mean_last_dim_cuda, the input is expected to have the reduction dimension as last. The Python code ensures that by permuting.

Wait, in the CUDA function, the 'dim_size' is passed as a parameter, but in the Python code, we can compute it from permuted_x.size(-1).

Yes, the CUDA function takes 'dim_size' as an argument, so in the Python code, we pass permuted_x.size(-1).

Now, testing this code:

Suppose the input is a 3D tensor (dim=1):

Original dim is 1. The permutation moves it to last. The kernel computes the mean along the last dimension.

After kernel, the output is permuted back to original order except the dim.

The permutation for output is computed as follows:

current_dims = permutation[:-1], which after permutation [0,2,1] (assuming original permutation was [0,2,1] for dim=1 in a 3D tensor), so current_dims is [0,2].

desired_dims are [0,2], so output_permutation is [0,1]. No change, so output.permute([0,1]) is same.

Thus, the output is correct.

Another test case: dim=0 in 3D (a,b,c).

Permutation is [2,1,0], so permuted_x is (c,b,a). 

dim_size is a (original dim 0's size).

After kernel, output is (c,b).

desired_dims = [1,2] (original dimensions 1 and 2).

current_dims = [2,1] (permutation[:-1] is [2,1]).

desired_dims are [1,2].

output_permutation:

for d in [1,2], find their indices in current_dims:

1 is at index 1 in current_dims (since current_dims[1] is 1)

2 is at index 0 (current_dims[0] is 2).

Thus output_permutation is [1,0].

So output.permute([1,0]) swaps the dimensions from (c,b) to (b,c), which is correct.

This seems to work.

Now, check the CUDA code.

In the kernel, the input is passed as a contiguous tensor, so the pointer arithmetic works.

The kernel uses a for-loop to sum the elements. For large dim_size (like 4096), this could be slow, but perhaps the overhead is acceptable compared to PyTorch's implementation.

Wait, but in PyTorch's implementation, the mean operation is likely optimized with parallel reductions, so maybe this approach is slower for large dimensions. Hmm, but the problem requires to replace the mean operator with a custom kernel for speedup. So perhaps there's a better way.

Wait, the for-loop in the kernel is O(S) per thread, where S is the reduction dimension's size. For S=4096 and a large output (like 128*4095), this could be a lot of operations. For example, if output_size is 128*4095 = ~524k, each thread does 4096 iterations, so total 2.15e9 operations, which might be slow.

Hmm, perhaps this approach is not efficient enough. Need a better parallel approach.

Alternative approach: use parallel reduction.

Let me think of a better way to compute the sum.

Each block can handle one output element. Each thread in the block processes a chunk of the reduction dimension.

For example, suppose the reduction dimension size is S. Each thread in a block of threads (e.g., 256 threads) can process a portion of the S elements.

The block can accumulate the partial sums in shared memory and then compute the total.

This would reduce the number of operations per thread from S to S / threads_per_block.

Let's think of the block processing each output element.

The kernel would have one block per output element.

Each block has threads = min(S, 256) ?

Wait, but the block size must be fixed at compile time.

Alternatively, set the block size to 256, and for each block, each thread processes a portion of the S elements.

For example, the number of threads per block is 256. Each thread i in the block can process elements i, i + 256, i + 512, etc.

The total number of elements per thread is ceil(S / 256).

Each thread sums their portion and writes to shared memory.

Then, a reduction within the block can be done to get the total sum.

This approach can reduce the number of operations per thread.

Let me structure the kernel with parallel reduction:

__global__ void mean_last_dim_parallel(const float* input, float* output, int output_size, int dim_size) {
    extern __shared__ float shared[];

    int output_idx = blockIdx.x;
    int tid = threadIdx.x;
    float sum = 0.0f;

    for (int i = tid; i < dim_size; i += blockDim.x) {
        sum += input[output_idx * dim_size + i];
    }

    // Write to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Perform block reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[output_idx] = shared[0] / dim_size;
    }
}

This kernel uses shared memory and block-wise reduction.

The block size can be up to 256. The shared memory size is blockDim.x * sizeof(float).

This would be better for large S.

The launch configuration would be:

threads_per_block = 256 (or a smaller number if S is smaller)

blocks_per_grid = output_size

Thus, each block processes one output element.

The kernel uses a shared memory array of size blockDim.x.

This approach reduces the per-thread computation from O(S) to O(S/B) where B is the block size.

For S=4096 and B=256, each thread does 16 iterations, and then the reduction.

This should be much faster.

Thus, I should revise the kernel to use this parallel approach.

Now, updating the CUDA code:

The kernel becomes:

__global__ void mean_last_dim_parallel(const float* input, float* output, int output_size, int dim_size) {
    extern __shared__ float shared[];
    int tid = threadIdx.x;
    int block_id = blockIdx.x;
    float sum = 0.0f;

    // Each thread processes a chunk of the dim_size elements
    for (int i = tid; i < dim_size; i += blockDim.x) {
        sum += input[block_id * dim_size + i];
    }

    // Write to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Block reduction
    for (int s = blockDim.x >> 1; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[block_id] = shared[0] / dim_size;
    }
}

The wrapper function in CUDA:

torch::Tensor mean_last_dim_cuda(torch::Tensor input, int dim_size) {
    auto output_size = input.sizes().slice(0, input.dim()-1);
    auto output = torch::empty(output_size, input.options());

    const int threads_per_block = 256;
    const int blocks_per_grid = output_size[0];  // since output_size is a tensor with the product of dimensions except last

    // Compute shared memory size needed: blockDim.x * sizeof(float)
    int smem_size = threads_per_block * sizeof(float);

    mean_last_dim_parallel<<<blocks_per_grid, threads_per_block, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        output.numel(),
        dim_size
    );

    return output;
}

This should be much faster for large dimensions.

Therefore, I should replace the previous kernel with this parallel version.

Thus, the updated code would have this kernel.

Now, the CUDA source becomes:

mean_last_dim_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_last_dim_parallel(const float* input, float* output, int output_size, int dim_size) {
    extern __shared__ float shared[];
    int tid = threadIdx.x;
    int block_id = blockIdx.x;
    float sum = 0.0f;

    // Each thread processes a chunk of the dim_size elements
    for (int i = tid; i < dim_size; i += blockDim.x) {
        sum += input[block_id * dim_size + i];
    }

    // Write to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Block reduction
    for (int s = blockDim.x >> 1; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[block_id] = shared[0] / dim_size;
    }
}

torch::Tensor mean_last_dim_cuda(torch::Tensor input, int dim_size) {
    auto output_size = input.sizes().slice(0, input.dim()-1);
    auto output = torch::empty(output_size, input.options());

    const int threads_per_block = 256;
    const int blocks_per_grid = output.numel();

    // Shared memory size is threads_per_block * sizeof(float)
    int smem_size = threads_per_block * sizeof(float);

    mean_last_dim_parallel<<<blocks_per_grid, threads_per_block, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        output.numel(),
        dim_size
    );

    return output;
}
"""

The rest of the Python code remains the same.

This should provide a significant speedup for large reduction dimensions.

Testing this code should give correct results and faster performance.

Now, putting it all together.

The final code for ModelNew would have:

- The CUDA code using parallel reduction.

- The permutation steps in Python.

Thus, the complete code is as follows.
```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

mean_last_dim_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_last_dim_parallel(const float* input, float* output, int output_size, int dim_size) {
    extern __shared__ float shared[];
    int tid = threadIdx.x;
    int block_id = blockIdx.x;
    float sum = 0.0f;

    for (int i = tid; i < dim_size; i += blockDim.x) {
        sum += input[block_id * dim_size + i];
    }

    shared[tid] = sum;
    __syncthreads();

    for (int s = blockDim.x >> 1; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[block_id] = shared[0] / dim_size;
    }
}

torch::Tensor mean_last_dim_cuda(torch::Tensor input, int dim_size) {
    auto output_size = input.sizes().slice(0, input.dim()-1);
    auto output = torch::empty(output_size, input.options());

    const int threads_per_block = 256;
    const int blocks_per_grid = output.numel();
    int smem_size = threads_per_block * sizeof(float);

    mean_last_dim_parallel<<<blocks_per_grid, threads_per_block, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        output.numel(),
        dim_size
    );

    return output;
}
"""

mean_last_dim_cpp_source = "torch::Tensor mean_last_dim_cuda(torch::Tensor input, int dim_size);"

mean_last_dim = load_inline(
    name="mean_last_dim",
    cpp_sources=mean_last_dim_cpp_source,
    cuda_sources=mean_last_dim_source,
    functions=["mean_last_dim_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.mean_last_dim = mean_last_dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        dim = self.dim
        ndim = x.dim()

        # Permute input to move reduction dim to last
        permutation = list(range(ndim))
        permutation[-1], permutation[dim] = permutation[dim], permutation[-1]
        permuted_x = x.permute(permutation).contiguous()
        dim_size = permuted_x.size(-1)

        # Launch kernel
        output = self.mean_last_dim.mean_last_dim_cuda(permuted_x, dim_size)

        # Compute permutation to revert to original order except the reduced dimension
        current_dims = permutation[:-1]
        desired_dims = list(range(ndim))
        desired_dims.pop(dim)
        output_permutation = []
        for d in desired_dims:
            output_permutation.append(current_dims.index(d))

        # Apply permutation
        output = output.permute(output_permutation).contiguous()

        return output
```
```

Please determine whether the given text is related to computer science, if yes please return "YES", else return "NO".