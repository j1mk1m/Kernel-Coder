The given model is a simple sum reduction over a dimension. The forward function calls torch.sum. Your job is to replace this with a custom CUDA kernel. 

The input tensor has shape (batch_size, dim1, dim2). The reduction is over dim1 (since the example parameters set reduce_dim=1). The output is (batch_size, 1, dim2). 

The input tensor is of size 128x4096x4095. You need to design a CUDA kernel that can efficiently compute the sum over the 4096 dimension. 

Your goal is to make this operation as fast as possible. You can choose any algorithm, kernel configuration, etc. You can also use CUDA built-in functions like atomicAdd or __syncthreads(), but you must implement the reduction yourself (do not call any existing PyTorch/CUDA functions for reduction). 

Make sure your code compiles and runs correctly. You can use the same inline method as in the example. 

Please also make sure your code is correct. For example, if using parallelism, make sure all threads contribute to the sum and there are no race conditions.



```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for sum reduction along a specified dimension
sum_reduction_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cstdint>

template<typename scalar_t>
__global__ void reduce_sum_kernel(const scalar_t* __restrict__ input,
                                 scalar_t* __restrict__ output,
                                 int64_t dim_size,
                                 int64_t outer_dim,
                                 int64_t inner_dim) {
    int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= outer_dim * inner_dim) return;

    int64_t outer = idx / inner_dim;
    int64_t inner = idx % inner_dim;

    scalar_t sum = 0;
    for (int64_t d = 0; d < dim_size; ++d) {
        int64_t input_idx = outer * dim_size * inner_dim + d * inner_dim + inner;
        sum += input[input_idx];
    }
    output[idx] = sum;
}

torch::Tensor sum_reduce_cuda(torch::Tensor input, int64_t dim) {
    auto input_dims = input.sizes().vec();
    int64_t outer_dim = 1, inner_dim = 1;
    for (int64_t i = 0; i < dim; ++i) {
        outer_dim *= input_dims[i];
    }
    for (int64_t i = dim + 1; i < input_dims.size(); ++i) {
        inner_dim *= input_dims[i];
    }
    int64_t dim_size = input_dims[dim];

    auto output_shape = input_dims;
    output_shape[dim] = 1;
    auto output = torch::empty(output_shape, input.options());

    int64_t total_elements = outer_dim * inner_dim;
    const int block_size = 256;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    AT_DISPATCH_ALL_TYPES(input.scalar_type(), "sum_reduce_cuda", ([&] {
        reduce_sum_kernel<scalar_t><<<num_blocks, block_size>>>(
            input.data<scalar_t>(),
            output.data<scalar_t>(),
            dim_size,
            outer_dim,
            inner_dim);
    }));

    cudaDeviceSynchronize();
    return output;
}
"""

sum_reduction_cpp_source = """
torch::Tensor sum_reduce_cuda(torch::Tensor input, int64_t dim);
"""

# Compile the inline CUDA code for sum reduction
sum_reduce = load_inline(
    name="sum_reduce",
    cpp_sources=sum_reduction_cpp_source,
    cuda_sources=sum_reduction_source,
    functions=["sum_reduce_cuda"],
    verbose=True,
    extra_cflags=["-D_GLIBCXX_USE_CXX11_ABI=0"],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        self.reduce_op = sum_reduce

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.reduce_op.sum_reduce_cuda(x, self.dim)

# The original get_inputs and get_init_inputs are unchanged, so they are omitted here
# However, the user should ensure that inputs are moved to CUDA if necessary in their setup
# For example, in get_inputs(), tensors should be created with .cuda()
```
</think>

To optimize the given model's sum reduction operation using a custom CUDA kernel, I've designed a kernel that efficiently computes the sum along the specified dimension. Here's the breakdown of the approach:

1. **Kernel Design**: The `reduce_sum_kernel` is a CUDA kernel that processes each element in the output tensor by iterating over the reduction dimension. This avoids using PyTorch's built-in reduction functions and instead manually accumulates the sum.

2. **Memory Layout**: The kernel treats the input tensor as a 3D structure (outer, dim, inner) to simplify indexing. Each thread is responsible for one output element, iterating over the reduction dimension to compute the sum.

3. **Parallelization**: Threads are distributed across the outer and inner dimensions, with each thread handling one inner dimension element. This ensures good memory access patterns and parallelism.

4. **Dispatching**: The kernel uses `AT_DISPATCH_ALL_TYPES` to handle different data types, ensuring flexibility. The kernel launch parameters are computed based on the input size to maximize occupancy.

5. **Efficiency**: By manually managing the reduction loop, we avoid overhead from PyTorch's abstractions, leading to faster execution, especially on large tensors like the 128x4096x4095 input.

This implementation ensures correctness through proper indexing and thread management, while maximizing performance through optimized CUDA kernel configuration.