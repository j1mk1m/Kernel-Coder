The user will run the following test to verify your code:

Your new code must pass this test. You can copy-paste the code here:

```python
import torch
import sys
sys.path.append('solution')

from model import ModelNew

def get_init_inputs():
    return [8, 32, 32, (3,7), (1,1), (1,3)]

model = ModelNew(*get_init_inputs())
x = torch.rand(8, 32, 512, 1024)
out = model(x)
print(out.shape)
```

The expected output is:

torch.Size([8, 32, 512, 1024])

Your code must not change the model's behavior. Your code must not have syntax errors. Do not use any 3rd-party libraries. The code must run on Python 3.8, PyTorch 1.13.1, and CUDA 11.7. 

You may assume that all the inputs to the forward method are on the same device (probably CUDA). 

To get you started, here's a template: 

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel(s) here
# (include both the source and the header, use load_inline)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):
        super().__init__()
        # Initialize your custom operators here
        pass  # remove this

    def forward(self, x):
        # Implement the forward pass using your custom operators
        pass  # remove this
```



Okay, I need to optimize the given PyTorch model which uses a ConvTranspose2d layer. The user wants me to replace some operators with custom CUDA kernels for speedups. Let me think about how to approach this.

First, the original model uses nn.ConvTranspose2d. The goal is to replace this with a custom CUDA kernel. Since PyTorch's ConvTranspose2d is already optimized, maybe there's a way to make it even faster by fusing operations or optimizing the kernel itself. Alternatively, perhaps there are specific parameters in this case (like the kernel size being asymmetric) that can be exploited.

The test case specifies kernel_size (3,7), stride (1,1), and padding (1,3). The input is 8x32x512x1024. The output shape is the same as the input's spatial dimensions because padding is set such that it compensates for the kernel's extension. For stride 1, the output height and width should be calculated as: 

Output Height = (input_height - 1)*stride[0] - 2*padding[0] + kernel_size[0] + output_padding. Wait, but in PyTorch's ConvTranspose2d, the formula is:

out_shape = (input.shape[2] - 1) * stride[0] - 2 * padding[0] + kernel_size[0] + output_padding[0]

But the user's code doesn't mention output_padding, so it's default 0. Let me check the given padding (1,3). The input is 512x1024. Let me compute:

Height_out = (512 -1)*1 - 2*1 + 3 = 512 -1 -2 +3 = 512. So that's correct. Similarly for width: (1024-1)*1 - 2*3 +7 = 1024-1-6+7 = 1024. So the output shape matches.

The problem is to replace the ConvTranspose2d with a custom CUDA kernel. Since PyTorch's implementation might be already efficient, but maybe the kernel can be optimized for specific cases. However, writing a full ConvTranspose2d from scratch is quite involved. Alternatively, perhaps the user wants to use the same parameters but implement the convolution in a custom kernel.

Alternatively, maybe the user expects to use the existing PyTorch operator but replace it with a custom kernel. Since ConvTranspose is a big operation, perhaps I should write a custom CUDA kernel for the transpose convolution. However, implementing that from scratch would require a lot of code. 

Wait, but the user provided an example where they replaced a simple add with a kernel. Maybe the idea here is to implement the transpose convolution in a custom kernel, but that's a huge task. Alternatively, maybe the user expects to use the existing ConvTranspose2d but with some optimizations, but that might not be possible without modifying the operator.

Alternatively, perhaps the key is to fuse operations. But in the given model, there's only the ConvTranspose2d. So maybe the only option is to reimplement the transpose convolution as a custom CUDA kernel.

Hmm, but writing a custom ConvTranspose2d kernel is quite complex. Let me think if there's a smarter way. Maybe using a pre-existing implementation as a reference. Alternatively, perhaps the problem expects us to use the existing operator but in a different way? Like using a different padding or stride? No, the parameters are fixed.

Alternatively, perhaps the user wants to implement the transpose convolution using a forward convolution with some transpose and padding. Wait, transpose convolutions can be implemented via regular convolutions with transposed kernels and appropriate padding, but that might not be straightforward.

Alternatively, maybe the idea is to use the existing PyTorch's implementation but just wrap it in a custom kernel? Not sure. Alternatively, perhaps the problem expects us to use a custom kernel for the backward pass, but the user's instruction says to replace operators, so likely forward.

Alternatively, maybe the user is okay with just replacing the ConvTranspose2d operator with a custom CUDA implementation. Since that's the only operator in the forward pass, that would be the target.

But how to implement a custom ConvTranspose2d kernel?

First, I need to recall how transpose convolution works. The transpose convolution is equivalent to a forward convolution with the kernel transposed (swapped in spatial dimensions, and the input and output channels are swapped) applied to the input upsampled by the stride. Wait, no, actually the transpose convolution is a bit different. Let me think.

The transpose convolution (also called deconvolution) can be seen as the gradient with respect to the input of a regular convolution. The computation involves expanding the input, then convolving with the kernel's transpose.

The algorithm for transpose convolution involves:

1. Compute the output size based on input dimensions, kernel size, stride, and padding.

2. For each output pixel, the kernel is applied to the corresponding input pixels, but in a way that depends on the stride.

Alternatively, the transpose convolution can be implemented as:

output[i, j] = sum_{k, l} input[ (i + padding - k) / stride , (j + padding - l)/stride ] * kernel[k, l]

But this is a simplified version. The exact implementation requires handling the spatial dimensions correctly.

Given that the kernel is 3x7, and stride (1,1), padding (1,3). The input is H=512, W=1024. The output dimensions are H, W as per the test.

Implementing this in CUDA requires a kernel that efficiently computes the convolution over the input. But writing such a kernel is quite involved, especially considering the dimensions and the need for efficient memory access.

Alternatively, maybe the user expects to use a pre-existing implementation, but the code must be written inline. 

Alternatively, perhaps the problem is expecting to use the existing ConvTranspose2d but with some optimizations. Wait, the user's example replaced a simple addition with a CUDA kernel. So in this case, the same approach is needed for the ConvTranspose2d.

Therefore, the plan is:

- Create a custom CUDA kernel that performs the same computation as the ConvTranspose2d layer.

But implementing the entire transpose convolution in CUDA is going to be complex. Let me think of the steps needed:

The transpose convolution's forward pass can be implemented as follows:

Given an input tensor of shape (N, C_in, H_in, W_in), the output tensor is (N, C_out, H_out, W_out).

The output spatial dimensions are computed as:

H_out = (H_in - 1) * stride[0] - 2 * padding[0] + kernel_size[0] + output_padding[0]

Similarly for W_out. In the test case, with output_padding=0, these formulas give the correct dimensions.

The kernel for the transpose convolution would need to loop over each output pixel, and for each, compute the weighted sum of input pixels multiplied by the kernel's weights, considering the stride and padding.

However, writing this in CUDA requires handling the spatial indices, the batch and channel dimensions, and efficiently accessing the memory.

Given that this is time-consuming, maybe the user expects a simplified version. Alternatively, perhaps the problem expects to use a custom kernel that wraps the existing PyTorch's implementation, but that wouldn't make sense.

Alternatively, maybe the key is to use the existing operator but optimize it via kernel fusion. But in this case, there's only one operator, so that's not applicable.

Wait, maybe the user is mistaken and wants to replace the ConvTranspose2d with a custom implementation. The problem states that the user can choose any operators to replace. So, perhaps the only way is to implement the transpose convolution in CUDA.

Let me start by outlining the steps for the CUDA kernel:

The custom kernel will need to take:

- Input tensor (x)
- Kernel weights (from the model's parameters)
- Bias (if any)
- Stride, padding, kernel size, etc.

The forward pass requires for each output position (n, c_out, h_out, w_out) to compute the sum over the kernel's spatial positions and input channels.

The kernel can be structured as a grid of threads where each thread computes one output element, or blocks of threads for better parallelism.

Alternatively, using a tiled approach to handle the spatial dimensions and channels.

But this is quite involved. Let me think of a way to structure the kernel.

First, the output tensor has dimensions (N, C_out, H_out, W_out). For each output element (n, c_out, h_out, w_out), we need to compute:

sum_{c_in=0 to C_in-1} sum_{kh=0 to kernel_h-1} sum_{kw=0 to kernel_w-1} x[n, c_in, h_in, w_in] * kernel[c_out, c_in, kh, kw]

where h_in = (h_out + 2*padding_h - kh - 1)/stride_h ?

Wait, the exact formula for the input indices in transpose convolution is a bit tricky. The standard approach is that the input indices are computed as:

h_in = (h_out + padding_h - kh - output_padding_h) / stride_h

Similarly for w_in, but this depends on how the padding and stride are applied.

Alternatively, the formula for the input index is:

h_in = (h_out + 2*padding_h - kh - 1) / stride_h 

Wait, perhaps I need to refer to the PyTorch documentation for the exact indices.

Alternatively, the transpose convolution can be considered as the gradient of a forward convolution. Therefore, the output of the transpose convolution is equivalent to the input to the forward convolution's gradient. The kernel for the transpose is the transposed kernel of the forward convolution. So the kernel is swapped in input and output channels, and spatial dimensions are flipped?

Wait, the kernel in transpose convolution is the same as the kernel of the forward convolution, but transposed in a specific way. For a standard convolution, the kernel is (C_out, C_in, kh, kw), and in the transpose convolution, it's (C_in, C_out, kh, kw) but I might be mixing up the dimensions.

Actually, the weight of ConvTranspose2d in PyTorch is of shape (in_channels, out_channels, kernel_h, kernel_w). Wait, no: according to PyTorch's documentation, the weight for ConvTranspose2d is (in_channels, out_channels, kernel_size[0], kernel_size[1]). Wait, no, actually the ConvTranspose2d has the same weight shape as the standard Conv2d but the order is different. Let me check.

Wait, in PyTorch's Conv2d, the weight is (out_channels, in_channels, kh, kw). For ConvTranspose2d, the weight is (in_channels, out_channels, kh, kw). Because the transpose convolution is effectively the backward pass of a forward convolution's input, so the kernel is transposed accordingly.

Thus, for the transpose convolution, the kernel is stored as (in_channels, out_channels, kh, kw). But when applying the convolution, the kernel is effectively transposed in terms of the input and output channels.

Therefore, in the custom kernel, the weight tensor should be accessed correctly.

Now, the main challenge is to compute the output's value for each position by iterating over the kernel's dimensions and the input's channels.

The CUDA kernel would need to loop over the kernel's spatial dimensions and input channels for each output element.

The problem is that this can be computationally intensive and requires efficient memory access patterns.

Given the time constraints, perhaps it's better to structure the kernel as follows:

Each thread is responsible for computing a single output element (h_out, w_out) in a certain batch and output channel. The grid dimensions can be set such that each block handles a certain region.

Alternatively, using a grid where each thread handles a single output element (n, c_out, h_out, w_out). The batch and channel can be handled via blockIdx and threadIdx.

Alternatively, since the input and output tensors can be large, the kernel must be designed for efficiency.

First, let's outline the steps in code.

The custom CUDA function will take the input tensor, the weight, the bias (optional), stride, padding, and output_padding (which is 0 in the test case).

The kernel function will have to loop over the kernel's height and width, and the input channels. For each output position (h_out, w_out), compute the corresponding input position (h_in, w_in) based on the stride and kernel position.

But let's formalize the indices.

Suppose the output has height H_out and width W_out. The output position is h_out, w_out.

The input's height is H_in, width W_in.

The formula for h_in (input height coordinate) corresponding to h_out is:

h_in = (h_out + 2*padding_h - kh - 1) / stride_h ?

Wait, according to PyTorch's documentation, the output shape is calculated as:

H_out = (H_in - 1) * stride[0] - 2 * padding[0] + kernel_size[0] + output_padding[0]

But the actual computation is such that the input's indices are derived from the output's indices.

Alternatively, the formula for the input index when the kernel is applied at position (kh, kw) in the kernel for the output position (h_out, w_out) is:

h_in = (h_out - kh + 2 * padding_h) / stride_h - output_padding_h ?

Hmm, this is getting complicated. Perhaps it's better to reference the forward pass of the convolution.

Alternatively, here's a standard way to compute the indices in transpose convolution:

For each output position (h_out, w_out), the input indices that contribute to this position are given by:

h_in = (h_out + 2 * padding_h - kh - 1) / stride_h 

Wait, perhaps it's better to see an example.

Suppose stride=1, padding=1, kernel_h=3.

Then, for h_out=0, the h_in would be (0 - kh + 2*1) / 1 ?

Wait, perhaps the correct formula is:

h_in = (h_out - kh - padding_h) / stride_h + 1 ?

Wait, I'm getting confused here. Let me think numerically.

Suppose the input is of size H_in=512, and the output is H_out=512 (as per the test case with stride 1, padding (1,3), kernel 3,7).

With stride=1, padding=1:

H_out = (512 -1)*1 -2*1 +3 = 512-1-2+3 = 512. So the output is the same as input height.

For a given output h_out=0, the kernel's height positions (kh=0,1,2):

The corresponding h_in would be:

h_in = (h_out - kh + 2*padding_h - output_padding_h) / stride_h ?

Wait, according to PyTorch's documentation for ConvTranspose2d, the padding is applied to the input. The exact formula for the indices is a bit tricky, but perhaps it's easier to consider that the transpose convolution can be implemented as the regular convolution but with the kernel transposed and the input upsampled by the stride.

Alternatively, perhaps the kernel can be written as follows:

The output at position (h_out, w_out) is the sum over c_in, kh, kw of:

input[n][c_in][ (h_out - kh + 2*padding_h -1)/stride_h ][ (w_out - kw + 2*padding_w -1)/stride_w ] * weight[c_in][c_out][kh][kw]

Wait, but this requires that (h_out - kh + 2*padding_h -1) must be divisible by stride_h. Since in the problem's case, the stride is (1,1), the division is straightforward.

Wait, perhaps the correct formula for the input index when using kernel position kh is:

h_in = (h_out + padding_h - kh - 1) / stride_h 

Similarly for w_in.

Wait, let's see with an example:

Suppose kernel size is 3, padding is 1, stride 1.

For h_out=0, kh=0:

h_in = (0 + 1 - 0 -1)/1 = 0 → which is okay.

kh=1: (0 +1 -1 -1)/1 → -1 → which is invalid, so this would be outside the padding?

Hmm, perhaps the formula is different.

Alternatively, the input index h_in is computed as:

h_in = (h_out - kh + padding_h) // stride_h 

Wait, maybe it's better to refer to the implementation details of PyTorch. But given time constraints, perhaps the user expects a simplified version where the kernel is written with the correct indices.

Alternatively, maybe the padding in the ConvTranspose2d is applied to the output, not the input, which complicates things.

Alternatively, the key is to loop over the kernel's spatial dimensions and compute the input index accordingly. If the kernel is 3x7, then for each output position, we loop over 3 rows and 7 columns of the kernel.

Assuming that the kernel is applied in such a way that each output position aggregates the input values multiplied by the kernel.

Given the complexity, perhaps the code should be structured as follows:

The custom CUDA function will take the input tensor, the weight, bias, stride, padding, etc. Then, for each output element, compute the sum over the kernel's elements.

The CUDA kernel will have to loop over the kernel's spatial dimensions and input channels.

Let me try writing the CUDA code step by step.

First, the custom kernel function declaration in C++:

The function will have to handle the input tensor, weights, stride, padding, etc. Let's assume the following signature:

torch::Tensor conv_transpose2d_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int stride_h, int stride_w, int padding_h, int padding_w, int output_padding_h, int output_padding_w);

In the test case, output_padding is 0.

The kernel function itself will be written in CUDA.

Now, the CUDA kernel code:

__global__ void conv_transpose2d_kernel(
    const float* input_data,
    const float* weight_data,
    float* output_data,
    int batch_size,
    int in_channels,
    int out_channels,
    int input_height,
    int input_width,
    int kernel_h,
    int kernel_w,
    int stride_h,
    int stride_w,
    int padding_h,
    int padding_w,
    int output_padding_h,
    int output_padding_w,
    int output_height,
    int output_width,
    bool has_bias,
    const float* bias_data) {

    // Each thread computes one output element (n, c_out, h_out, w_out)
    int n = blockIdx.x;
    int c_out = blockIdx.y;
    int h_out = threadIdx.y * blockDim.x + threadIdx.x;
    // ... or some other indexing scheme. Maybe this is not the best way.

Alternatively, using a 3D grid:

But the problem is that the output has 4 dimensions (batch, channels_out, H, W), so the kernel needs to map threads to each element.

Alternatively, each thread can handle a single output element:

Compute the indices as:

int index = blockIdx.x * blockDim.x + threadIdx.x;
int n = index / (out_channels * output_height * output_width);
int rem = index % (out_channels * output_height * output_width);
int c_out = rem / (output_height * output_width);
rem %= (output_height * output_width);
int h_out = rem / output_width;
int w_out = rem % output_width;

But this could be inefficient for large tensors.

Alternatively, using a 3D grid where each block corresponds to a batch and output channel, and threads handle spatial positions.

Alternatively, the code can be structured as follows:

Each thread computes a single output element (h_out, w_out) for a particular batch and output channel. To handle this, the grid can be:

dim3 grid(batch_size, out_channels);
dim3 block(output_height, output_width);

Then, in the kernel:

int n = blockIdx.x;
int c_out = blockIdx.y;
int h_out = threadIdx.y;
int w_out = threadIdx.x;

But this might lead to very large blocks if H and W are large (like 512x1024), so this approach is not feasible.

Hmm, perhaps using a 1D grid and 1D threads:

Each thread computes one element. The total number of elements is batch_size * out_channels * output_height * output_width.

So, the block size would be, say, 256 threads per block, and the number of blocks would be ceil(total_elements / 256).

But for large tensors, this might require a huge number of blocks. For example, 8*32*512*1024 elements is 8*32=256, 256*512*1024=131072*256=33,554,432 elements. That's 33 million elements. With 256 threads per block, that's ~131k blocks, which is manageable.

So, the thread index is:

int index = blockIdx.x * blockDim.x + threadIdx.x;

Then compute the indices:

int n = index / (out_channels * output_height * output_width);
int rem = index % (out_channels * output_height * output_width);
int c_out = rem / (output_height * output_width);
rem %= (output_height * output_width);
int h_out = rem / output_width;
int w_out = rem % output_width;

Once the indices are determined, the kernel can compute the output value.

The output value at (n, c_out, h_out, w_out) is the sum over all input channels, kernel rows and columns:

float sum = 0.0f;
for (int c_in = 0; c_in < in_channels; ++c_in) {
    for (int kh = 0; kh < kernel_h; ++kh) {
        for (int kw = 0; kw < kernel_w; ++kw) {
            // compute input_h_in and input_w_in
            int input_h = (h_out + padding_h - kh - 1 + output_padding_h) / stride_h;
            int input_w = (w_out + padding_w - kw - 1 + output_padding_w) / stride_w;
            if (input_h < 0 || input_h >= input_height || input_w <0 || input_w >= input_width) {
                // out of bounds, skip this kernel element
                continue;
            }
            // get the kernel weight
            int weight_idx = c_in * out_channels * kernel_h * kernel_w +
                            c_out * kernel_h * kernel_w +
                            kh * kernel_w + kw;
            float weight_val = weight_data[weight_idx];
            // get input value
            int input_offset = n * in_channels * input_height * input_width +
                              c_in * input_height * input_width +
                              input_h * input_width + input_w;
            float input_val = input_data[input_offset];
            sum += input_val * weight_val;
        }
    }
}

if (has_bias) {
    sum += bias_data[c_out];
}

// store the result
int output_offset = n * out_channels * output_height * output_width +
                   c_out * output_height * output_width +
                   h_out * output_width + w_out;
output_data[output_offset] = sum;

Wait, but I'm not sure about the indices. Let me check the weight indices. The weight is stored as (in_channels, out_channels, kernel_h, kernel_w), so for each input channel c_in, output channel c_out, and kernel position kh, kw, the weight index should be correct.

Wait, the weight tensor's shape is (in_channels, out_channels, kernel_h, kernel_w). So the first dimension is in_channels, then out_channels, then kernel_h, then kernel_w.

So the index is computed as c_in * (out_channels * kernel_h * kernel_w) + c_out * (kernel_h * kernel_w) + kh * kernel_w + kw. Yes, that seems correct.

The input's offset is correct as well.

The problem is the computation of input_h and input_w. Let's see for the test case's parameters.

In the test case:

input_height =512, input_width=1024

stride_h=1, stride_w=1

padding_h=1, padding_w=3

output_padding_h=0, output_padding_w=0

kernel_h=3, kernel_w=7

For an output h_out=0:

input_h = (0 +1 - kh -1 + 0)/1 → (0 +1 -kh -1) → (0 -kh) → For kh=0 → 0, kh=1 → -1 (invalid), kh=2 →-2 (invalid). So only kh=0 contributes.

Wait that can't be right. Because kernel_h=3, so kh can be 0,1,2.

Wait, let's compute for kh=0: input_h = (0 +1 -0 -1) =0 → valid.

kh=1: input_h = (0 +1 -1 -1) = -1 → invalid.

kh=2: (0+1-2-1)= -2 → invalid.

So only kh=0 contributes for h_out=0.

Similarly, for h_out=1:

input_h = (1 +1 -kh -1)/1 = (1 -kh) 

kh=0: 1 → valid

kh=1: 0 → valid

kh=2: -1 → invalid.

Wait, but the stride is 1, and padding is 1. The output height is 512, same as input.

Wait, perhaps the formula for input_h is incorrect. Let's think differently.

The standard formula for the input index in the transpose convolution is:

input_h = (h_out + 2 * padding_h - kh - 1) / stride_h + output_padding_h

Wait, maybe that's the correct formula. Let me see.

If we have output_padding set to 0, then:

input_h = (h_out + 2*padding_h - kh -1) / stride_h 

In the test case:

For h_out=0:

input_h = (0 + 2*1 - kh -1)/1 → (1 - kh) → For kh=0:1, kh=1:0, kh=2:-1.

Wait, that would give for kh=0:1 (input_h=1), but then for h_out=0, the input_h would be 1, 0, etc. Hmm.

Alternatively, maybe the formula should be:

input_h = (h_out - kh + padding_h) / stride_h 

Wait, not sure. This is getting too time-consuming. Perhaps I should look for an alternative approach.

Alternatively, since the kernel is being written for the specific parameters given in the test case (stride=1, padding (1,3)), perhaps we can hardcode those values to simplify the code. But the user's problem expects the code to work for general cases, as the __init__ function takes parameters.

Hmm, perhaps the code is better written with the general formula, but I'll proceed with the kernel as above, even if the indices might be off. Since it's a template, perhaps it can be adjusted later.

Putting it all together, the code would be:

First, in the Python code, the model's __init__ must store the parameters, and the forward function will call the custom CUDA kernel.

But the kernel requires the weight and bias tensors. So the model needs to have the parameters, like the original ConvTranspose2d.

Wait, in the original model, the model has a ConvTranspose2d layer, which stores the weights and bias. So in the new model, we need to store those parameters.

Therefore, the ModelNew class should have parameters for the weights and bias, just like the original model. So:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.bias = bias

        # Initialize weight and bias like PyTorch's ConvTranspose2d
        self.weight = nn.Parameter(torch.empty(in_channels, out_channels, *kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)

        # Initialize parameters (same as PyTorch's initialization)
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

        # Load the CUDA kernel
        self.conv_transpose_cuda = load_inline(...)

    def forward(self, x):
        # Call the custom CUDA kernel
        return self.conv_transpose_cuda(x, self.weight, self.bias, ...)

But the problem is that the custom CUDA kernel must be written to take these parameters.

Now, in the CUDA code:

The kernel function needs to take the input tensor, weight tensor, bias (if any), and the parameters like stride, padding, etc.

So the Python code must pass all these parameters to the CUDA function.

The CUDA source code would look something like:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void conv_transpose2d_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int in_channels,
    int out_channels,
    int input_height,
    int input_width,
    int kernel_h,
    int kernel_w,
    int stride_h,
    int stride_w,
    int padding_h,
    int padding_w,
    int output_padding_h,
    int output_padding_w,
    int output_height,
    int output_width,
    bool has_bias) {

    // ... the code as above, using the indices and loops
}

torch::Tensor conv_transpose2d_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
                                   int stride_h, int stride_w, int padding_h, int padding_w,
                                   int output_padding_h, int output_padding_w) {
    // Get the parameters
    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int out_channels = weight.size(1); // since weight is (in_channels, out_channels, ...)
    int input_height = input.size(2);
    int input_width = input.size(3);
    int kernel_h = weight.size(2);
    int kernel_w = weight.size(3);
    int output_height = (input_height - 1) * stride_h - 2 * padding_h + kernel_h + output_padding_h;
    int output_width = (input_width - 1) * stride_w - 2 * padding_w + kernel_w + output_padding_w;

    // Create output tensor
    auto output = torch::empty({batch_size, out_channels, output_height, output_width}, input.options());

    // Launch kernel
    int total_elements = batch_size * out_channels * output_height * output_width;
    int block_size = 256;
    int grid_size = (total_elements + block_size -1) / block_size;

    conv_transpose2d_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        input_height, input_width,
        kernel_h, kernel_w,
        stride_h, stride_w,
        padding_h, padding_w,
        output_padding_h, output_padding_w,
        output_height, output_width,
        bias.defined()
    );

    return output;
}

Then, the Python code would need to compile this CUDA code using load_inline.

However, there are several issues here:

1. The indices in the kernel may be incorrect. If the input indices are computed incorrectly, the output will be wrong.

2. The code may have many mistakes in the kernel's loops or memory accesses.

3. The CUDA kernel needs to handle the case where the input indices are out of bounds. The current approach skips those, but that might not be correct.

4. The weight's indices may be wrong. For example, the weight is stored as (in_channels, out_channels, kernel_h, kernel_w), so the indices are correct as written.

5. The kernel's grid and block dimensions might be inefficient.

6. The code may not handle the bias correctly.

Given the time constraints, perhaps this is the best possible approach. However, to ensure correctness, I need to make sure the indices are correctly calculated.

Alternatively, perhaps the formula for input_h should be:

input_h = (h_out - kh + 2 * padding_h) / stride_h 

Wait, let me try another approach.

Suppose in the forward pass of a regular convolution with input x, kernel w, stride s, padding p:

output_h = (input_h + 2*p - kh)/s +1 

For transpose convolution, the output of the transpose convolution is such that it's the input to the forward convolution's gradient. Thus, the transpose convolution's output is the input to the forward's gradient, which is the same as the output of the forward convolution.

Wait, perhaps it's better to think in terms of the transpose convolution being the gradient of the input of a forward convolution. Therefore, the transpose's output is the input to the forward convolution's gradient.

The formula for the indices is a bit tricky. Perhaps the correct formula for input_h in the transpose convolution is:

input_h = (h_out + 2*padding_h - kh -1) // stride_h 

Wait, let's see with the test case's parameters:

For output h_out=0, kh=0:

input_h = (0 +2*1 -0 -1)/1 → (1)/1 =1 → so the input position is 1?

But the output height is equal to the input height, so perhaps this is correct.

Then for h_out=1:

kh=0 → (1 +2*1 -0-1)/1 → 2 → input_h=2.

So for each h_out, the kernel's kh=0 would access input_h = (h_out +1 -0)/1 → h_out +1? That seems to be moving the kernel forward.

Hmm, perhaps this is the correct way.

Alternatively, the input_h should be (h_out + padding_h - kh) / stride_h 

Wait, if padding_h is added, then:

input_h = (h_out + padding_h - kh -1) / stride_h 

But I'm not sure.

This is really time-consuming and I might not get it right without more time. Since the user's test case requires the output shape to be the same as the input spatial dimensions, perhaps the indices can be set so that the input_h is h_out - kh + padding_h - something.

Alternatively, perhaps the formula is:

input_h = (h_out - kh + 2 * padding_h) // stride_h 

Wait, in the test case, with stride=1, padding=1:

input_h = (h_out - kh +2)/1 → (h_out -kh +2)

For kh=0:

input_h = h_out +2 → which would go beyond the input's height (512). So that's incorrect.

Hmm.

Alternatively, perhaps the formula is:

input_h = (h_out - kh + padding_h) // stride_h 

With padding_h=1:

input_h = (h_out -kh +1)/1 

For h_out=0, kh=0: (0+1)=1 → input_h=1

kh=1: (0-1+1)=0 → input_h=0

kh=2: (0-2+1) = -1 → invalid.

Thus, for h_out=0:

Only kh=0 and kh=1 contribute, but kh=2's input_h is -1, which is invalid.

Thus, the kernel for h_out=0 would have contributions from kh=0 (input_h=1) and kh=1 (input_h=0).

Wait, but then the output at h_out=0 would be the sum over those valid kernel positions.

But the output height is equal to the input's height (512), so for h_out=511:

input_h for kh=0: (511 -0 +1)/1 →512 → which is beyond the input height (512). So input_h would be 512, but the input has indices up to 511.

Thus, that would be out of bounds. So perhaps the formula is:

input_h = (h_out - kh + padding_h) / stride_h 

and we check if it's within 0 to input_height-1.

In that case, for h_out=511, kh=0:

input_h =511 +1 -0 →512 → which is invalid.

Hmm, this is a problem. Maybe the formula is different.

Alternatively, perhaps the correct formula is:

input_h = (h_out + 2 * padding_h - kh -1) / stride_h 

In this case, with h_out=511, padding_h=1, kh=0:

input_h = (511 + 2 -0 -1)/1 →512 → which is also invalid.

Hmm, I'm stuck here. Perhaps the best way is to proceed with the initial code and hope that the user's test case passes.

Alternatively, maybe the formula is:

input_h = (h_out + padding_h - kh) / stride_h 

For h_out=0, kh=0:

input_h = (0+1-0)/1 →1.

kh=1: (0+1-1)/1 →0.

kh=2: (0+1-2)/1 →-1 → invalid.

That would be better.

So the formula would be input_h = (h_out + padding_h - kh) / stride_h 

Then for h_out=511, kh=0:

input_h =511+1 -0 →512 → divided by 1 →512 → which is out of bounds (max is 511). So that's invalid.

Hmm, this suggests that the formula is incorrect.

Alternatively, perhaps the formula is:

input_h = (h_out - kh + padding_h) / stride_h 

For h_out=511, kh=0 → (511 -0 +1)/1 →512 → out of bounds.

Hmm, perhaps the correct formula is:

input_h = (h_out - kh + padding_h) // stride_h 

But this still causes issues at the edges.

This is getting too time-consuming. Given the time constraints, perhaps the best approach is to proceed with the initial kernel code and see if it can be adjusted. 

Now, putting it all together in the Python code:

The CUDA source code will be a string in Python, and then loaded via load_inline.

The ModelNew class must handle the parameters and call the kernel.

So the complete code would be:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import math

conv_transpose_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void conv_transpose2d_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int in_channels,
    int out_channels,
    int input_height,
    int input_width,
    int kernel_h,
    int kernel_w,
    int stride_h,
    int stride_w,
    int padding_h,
    int padding_w,
    int output_padding_h,
    int output_padding_w,
    int output_height,
    int output_width,
    bool has_bias) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= batch_size * out_channels * output_height * output_width) return;

    // Compute indices
    int n = index / (out_channels * output_height * output_width);
    int rem = index % (out_channels * output_height * output_width);
    int c_out = rem / (output_height * output_width);
    rem %= (output_height * output_width);
    int h_out = rem / output_width;
    int w_out = rem % output_width;

    float sum = 0.0f;

    for (int c_in = 0; c_in < in_channels; ++c_in) {
        for (int kh = 0; kh < kernel_h; ++kh) {
            for (int kw = 0; kw < kernel_w; ++kw) {
                // Compute input indices
                int input_h = (h_out + padding_h - kh - 1 + output_padding_h) / stride_h;
                int input_w = (w_out + padding_w - kw - 1 + output_padding_w) / stride_w;
                
                if (input_h < 0 || input_h >= input_height || input_w < 0 || input_w >= input_width) {
                    continue;  // Skip out of bounds
                }
                
                // Get weight index
                int weight_offset = c_in * out_channels * kernel_h * kernel_w +
                                    c_out * kernel_h * kernel_w +
                                    kh * kernel_w + kw;
                float w_val = weight[weight_offset];
                
                // Get input value
                int input_offset = n * in_channels * input_height * input_width +
                                   c_in * input_height * input_width +
                                   input_h * input_width + input_w;
                float in_val = input[input_offset];
                
                sum += in_val * w_val;
            }
        }
    }

    if (has_bias) {
        sum += bias[c_out];
    }

    // Write output
    int output_offset = n * out_channels * output_height * output_width +
                        c_out * output_height * output_width +
                        h_out * output_width + w_out;
    output[output_offset] = sum;
}

torch::Tensor conv_transpose2d_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
                                   int stride_h, int stride_w, int padding_h, int padding_w,
                                   int output_padding_h, int output_padding_w) {
    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int input_height = input.size(2);
    int input_width = input.size(3);
    int kernel_h = weight.size(2);
    int kernel_w = weight.size(3);
    int out_channels = weight.size(1);  // since weight is [in_channels, out_channels, kernel_h, kernel_w]

    // Calculate output dimensions
    int output_height = (input_height - 1) * stride_h - 2 * padding_h + kernel_h + output_padding_h;
    int output_width = (input_width - 1) * stride_w - 2 * padding_w + kernel_w + output_padding_w;

    auto output = torch::empty({batch_size, out_channels, output_height, output_width}, input.options());

    int total_elements = batch_size * out_channels * output_height * output_width;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;

    conv_transpose2d_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        input_height, input_width,
        kernel_h, kernel_w,
        stride_h, stride_w,
        padding_h, padding_w,
        output_padding_h, output_padding_w,
        output_height, output_width,
        bias.defined()
    );

    cudaDeviceSynchronize();  // To catch errors

    return output;
}
"""

conv_transpose_header = """
torch::Tensor conv_transpose2d_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
                                   int stride_h, int stride_w, int padding_h, int padding_w,
                                   int output_padding_h, int output_padding_w);
"""

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.bias = bias
        self.output_padding = (0, 0)  # Assuming no output padding unless specified

        # Initialize weight and bias like PyTorch's ConvTranspose2d
        self.weight = nn.Parameter(torch.empty(in_channels, out_channels, kernel_size[0], kernel_size[1]))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)

        # Initialize parameters
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

        # Load the CUDA kernel
        self.conv_transpose_cuda = load_inline(
            name="conv_transpose_cuda",
            cpp_sources=conv_transpose_header,
            cuda_sources=conv_transpose_source,
            functions=["conv_transpose2d_cuda"],
            verbose=False,
            extra_cflags=["-O3"],
            extra_cuda_cflags=["-O3"]
        )

    def forward(self, x):
        # Extract parameters
        stride_h, stride_w = self.stride
        padding_h, padding_w = self.padding
        output_padding_h, output_padding_w = self.output_padding

        # Call the custom CUDA function
        if self.bias is not None:
            bias = self.bias
        else:
            bias = torch.empty(0)  # Pass empty tensor if no bias

        return self.conv_transpose_cuda.conv_transpose2d_cuda(
            x,
            self.weight,
            bias,
            stride_h,
            stride_w,
            padding_h,
            padding_w,
            output_padding_h,
            output_padding_w
        )
```

But I'm not confident about the indices. For instance, in the test case, when the input has height 512, and kernel_h=3, stride=1, padding_h=1:

output_height = (512-1)*1 -2*1 +3 +0 =512-1-2+3=512.

For h_out=511:

input_h = (511 + 1 - kh -1 +0)/1 → (511 - kh)/1.

For kh=0 →511 → valid.

kh=1 →510 → valid.

kh=2 →509 → valid.

Thus, for h_out=511, all kernel positions (kh=0,1,2) are valid.

For h_out=0:

input_h = (0 +1 -kh-1) → (0 -kh).

For kh=0 →0 → valid.

kh=1 →-1 → invalid.

kh=2 →-2 → invalid.

Thus, only kh=0 contributes. So that's okay.

The output height is correct. Similarly for width.

But in the code's formula:

input_h = (h_out + padding_h - kh -1 + output_padding_h) / stride_h

With padding_h=1, output_padding_h=0:

input_h = h_out +1 -kh -1 → h_out - kh.

So for h_out=511 and kh=0:511-0=511 → valid.

Yes.

For h_out=0 and kh=0 →0 → valid.

kh=1 →-1 → invalid.

Thus, the formula seems correct.

The same applies for the width.

So the formula for input_h is correct.

Therefore, the kernel should work.

Now, in the ModelNew's __init__, the parameters are initialized with the same initialization as PyTorch's ConvTranspose2d. The forward function calls the custom CUDA kernel with the correct parameters.

The user's test case initializes the model with get_init_inputs() which returns [8, 32, 32, (3,7), (1,1), (1,3)]. These are in_channels, out_channels, kernel_size, stride, padding.

Wait, the parameters in the __init__ are in_channels, out_channels, kernel_size, stride, padding, bias.

In the test case, get_init_inputs() returns [8, 32, 32, (3,7), (1,1), (1,3)]. So:

in_channels=8, out_channels=32? Wait, no. Wait the test case's code:

The get_init_inputs() is supposed to return the parameters for the Model's __init__.

Original Model's __init__ parameters are:

def __init__(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=(0,0), bias=False):

In the user-provided test code:

def get_init_inputs():
    return [8, 32, 32, (3,7), (1,1), (1,3)]

Wait, that would be in_channels=8, out_channels=32, kernel_size=(3,7), stride=(1,1), padding=(1,3). But the third parameter is 32, which would be the out_channels.

But the test code's get_init_inputs() is for the Model's __init__ parameters. The original Model is initialized with in_channels=32 (from the test code's batch_size=8, in_channels=32, etc. Wait in the test code's first line:

batch_size =8
in_channels =32
out_channels =32
kernel_size = (3,7)
height=512
width=1024
stride=(1,1)
padding=(1,3)

The get_init_inputs() is supposed to return the parameters for the Model's __init__.

Thus, get_init_inputs() should return [in_channels, out_channels, kernel_size, stride, padding, bias]. 

But in the provided test code for the user's problem:

The user says:

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, bias]

Thus, in the test case:

def get_init_inputs():
    return [8, 32, 32, (3,7), (1,1), (1,3)]

Wait, but that would mean in_channels=8, which contradicts the test code's in_channels=32.

Ah, the user-provided code for testing has:

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]

Wait, looking back:

The user's test code for their original model:

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]

Wait, in the problem's given code:

The user's get_init_inputs() function is:

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]

But the Model's __init__ requires in_channels, out_channels, kernel_size, stride, padding, bias. The default for bias is False.

The user's test code:

def get_init_inputs():
    return [8, 32, 32, (3,7), (1,1), (1,3)]

Wait no, the problem's initial code has:

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]

But the user-provided example for the test code is:

import sys
sys.path.append('solution')
from model import ModelNew
def get_init_inputs():
    return [8, 32, 32, (3,7), (1,1), (1,3)]

model = ModelNew(*get_init_inputs())

Thus, the parameters passed to ModelNew are in_channels=8, out_channels=32, kernel_size=(3,7), stride=(1,1), padding=(1,3), and bias is the last element (32 is out_channels, so the sixth element is (1,3) is padding, but perhaps the user made a mistake.

Wait, the user's test code's get_init_inputs() returns [8, 32, 32, (3,7), (1,1), (1,3)], which would be in_channels=8, out_channels=32, kernel_size=(3,7), stride=(1,1), padding=(1,3), and the sixth argument is bias. But 32 is the third element, which is out_channels. The last element is (1,3) which is padding. Thus, the bias is omitted or is the sixth element is the bias? That can't be right.

Ah, the user's test code for their problem says:

The user's test code includes:

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]

In the problem's original code.

But in the user's example code provided for the test, the get_init_inputs() returns [8, 32, 32, (3,7), (1,1), (1,3)] which has six elements, implying that the last is bias. But the Model's __init__ requires 6 parameters (in_channels, out_channels, kernel_size, stride, padding, bias), so the sixth element is bias.

But in the problem's original code's get_init_inputs(), it returns five elements. The user might have made an error here.

Assuming that the correct get_init_inputs() for the test case is returning the six parameters (including bias), then the ModelNew's __init__ should correctly initialize the parameters.

In the provided code for ModelNew, the __init__ takes in_channels, out_channels, kernel_size, stride, padding, bias as parameters. Thus, the code should work.

Thus, the code should be correct.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import math

conv_transpose_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void conv_transpose2d_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int in_channels,
    int out_channels,
    int input_height,
    int input_width,
    int kernel_h,
    int kernel_w,
    int stride_h,
    int stride_w,
    int padding_h,
    int padding_w,
    int output_padding_h,
    int output_padding_w,
    int output_height,
    int output_width,
    bool has_bias) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= batch_size * out_channels * output_height * output_width) return;

    // Compute indices
    int n = index / (out_channels * output_height * output_width);
    int rem = index % (out_channels * output_height * output_width);
    int c_out = rem / (output_height * output_width);
    rem %= (output_height * output_width);
    int h_out = rem / output_width;
    int w_out = rem % output_width;

    float sum = 0.0f;

    for (int c_in = 0; c_in < in_channels; ++c_in) {
        for (int kh = 0; kh < kernel_h; ++kh) {
            for (int kw = 0; kw < kernel_w; ++kw) {
                // Compute input indices
                int input_h = (h_out + padding_h - kh - 1 + output_padding_h) / stride_h;
                int input_w = (w_out + padding_w - kw - 1 + output_padding_w) / stride_w;

                if (input_h < 0 || input_h >= input_height || input_w < 0 || input_w >= input_width) {
                    continue;  // Skip out of bounds
                }

                // Get weight index
                int weight_offset = c_in * out_channels * kernel_h * kernel_w +
                                    c_out * kernel_h * kernel_w +
                                    kh * kernel_w + kw;
                float w_val = weight[weight_offset];

                // Get input value
                int input_offset = n * in_channels * input_height * input_width +
                                   c_in * input_height * input_width +
                                   input_h * input_width + input_w;
                float in_val = input[input_offset];

                sum += in_val * w_val;
            }
        }
    }

    if (has_bias) {
        sum += bias[c_out];
    }

    // Write output
    int output_offset = n * out_channels * output_height * output_width +
                        c_out * output_height * output_width +
                        h_out * output_width + w_out;
    output[output_offset] = sum;
}

torch::Tensor conv_transpose2d_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
                                   int stride_h, int stride_w, int padding_h, int padding_w,
                                   int output_padding_h, int output_padding_w) {
    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int input_height = input.size(2);
    int input_width = input.size(3);
    int kernel_h = weight.size(2);
    int kernel_w = weight.size(3);
    int out_channels = weight.size(1);  // weight shape: [in_channels, out_channels, kh, kw]

    // Calculate output dimensions
    int output_height = (input_height - 1) * stride_h - 2 * padding_h + kernel_h + output_padding_h;
    int output_width = (input_width - 1) * stride_w - 2 * padding_w + kernel_w + output_padding_w;

    auto output = torch::empty({batch_size, out_channels, output_height, output_width}, input.options());

    int total_elements = batch_size * out_channels * output_height * output_width;
    int block_size = 256;
    int grid_size = (total_elements + block_size - 1) / block_size;

    conv_transpose2d_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.defined() ? bias.data_ptr<float>() : nullptr,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        input_height, input_width,
        kernel_h, kernel_w,
        stride_h, stride_w,
        padding_h, padding_w,
        output_padding_h, output_padding_w,
        output_height, output_width,
        bias.defined()
    );

    cudaDeviceSynchronize();  // Ensure completion

    return output;
}
"""

conv_transpose_header = """
torch::Tensor conv_transpose2d_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
                                   int stride_h, int stride_w, int padding_h, int padding_w,
                                   int output_padding_h, int output_padding_w);
"""

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, 
                 stride: tuple = (1, 1), padding: tuple = (0, 0), bias: bool = False):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.bias = bias
        self.output_padding = (0, 0)  # Default no output padding

        # Initialize parameters
        self.weight = nn.Parameter(torch.empty(in_channels, out_channels, *kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)

        # Parameter initialization (same as PyTorch's default)
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

        # Load the custom CUDA kernel
        self.conv_transpose_cuda = load_inline(
            name="conv_transpose_cuda",
            cpp_sources=conv_transpose_header,
            cuda_sources=conv_transpose_source,
            functions=["conv_transpose2d_cuda"],
            verbose=False,
            extra_cflags=["-O3"],
            extra_cuda_cflags=["-O3"]
        )

    def forward(self, x):
        # Extract parameters
        stride_h, stride_w = self.stride
        padding_h, padding_w = self.padding
        output_padding_h, output_padding_w = self.output_padding

        # Prepare bias tensor (empty if not present)
        bias_tensor = self.bias if self.bias is not None else torch.empty(0)

        # Execute the custom CUDA kernel
        return self.conv_transpose_cuda.conv_transpose2d_cuda(
            x,
            self.weight,
            bias_tensor,
            stride_h,
            stride_w,
            padding_h,
            padding_w,
            output_padding_h,
            output_padding_w
        )
```