The ModelNew should have the same API as the original Model class. That is, the user should be able to construct and use ModelNew exactly as before. The code for get_inputs() and get_init_inputs() will be used to generate inputs for the original and the new model, so make sure that the new model has the same interface as the original one, so that when we call model(x) with x from get_inputs(), it runs correctly. 

You may need to replace the Conv2d layer with your own kernel. If you choose to keep it, then you can leave it as it is. 

You can add any helper functions, imports, etc. as needed. 

The fused CUDA kernel must have the same functionality as the original model. 

You may assume that the input tensors are in CUDA and contiguous. 

You may assume that the input tensor dimensions are as in the given architecture. 

The code should be self-contained. 

The fused CUDA kernel must be compatible with PyTorch and use the same data types as the original implementation.


<think>
Okay, I need to optimize the given PyTorch model which uses a pointwise 2D convolution. The original model uses nn.Conv2d with kernel_size 1, so it's essentially a 1x1 convolution. Since this is a pointwise operation, maybe writing a custom CUDA kernel can speed things up.

First, I should think about how the convolution is implemented. A 1x1 convolution in this case is equivalent to a matrix multiplication. For each spatial location (height and width), the input channels are transformed to output channels. Since the kernel size is 1, there's no spatial filtering, so it's just a linear combination of channels.

The standard PyTorch Conv2d might have some overhead for handling larger kernels, so a custom kernel could be more efficient. Let's see: the input is (batch, in_channels, H, W). The output is (batch, out_channels, H, W). 

The computation for each output element (n, o, h, w) is the sum over i of (input[n, i, h, w] * weight[i, o, 0, 0]) plus bias if present. Since the kernel is 1x1, the spatial indices are 0,0. 

This can be vectorized. For each element in H and W, the batch and channels can be handled in parallel. Maybe we can reshape the input tensor to (batch * H * W, in_channels), multiply by the weight matrix (in_channels x out_channels), and then reshape back. But that might be similar to what PyTorch already does. However, perhaps a custom CUDA kernel can be faster by avoiding the overhead of the general convolution code.

Alternatively, the kernel can process each element efficiently using CUDA threads. Let me think about the kernel structure.

The input tensor is 4D: batch, in_channels, H, W. The output is similar but with out_channels. Since H and W are 1024 each, the total elements are 16 * 64 * 1024 * 1024, which is a lot. So parallelism is crucial.

Each thread could handle a specific output element. For example, each thread can process one (n, o, h, w). But that might be too much. Alternatively, we can process each spatial position (h,w) and handle all channels and batch in a block.

Wait, maybe the best way is to vectorize across the channels. Since the computation for each output channel is independent across the in_channels, we can compute each output channel's value by a dot product between the input channels and the corresponding kernel weights.

The kernel can be structured so that each thread block handles a spatial position (h, w) and a batch element. Each thread in the block processes an output channel. The input channels for that spatial position are loaded once, and then multiplied by the corresponding weights for each output channel.

Alternatively, for each output element (n, o, h, w), the value is sum_{i} input[n, i, h, w] * weight[i][o][0][0]. Since the kernel is 1x1, the spatial indices are fixed. 

Let me think of the steps:

1. The input is of size (N, C_in, H, W). The weight is (C_out, C_in, 1, 1). The bias is (C_out).

So, the weight can be viewed as a matrix of size (C_in, C_out), transposed. The multiplication would be input @ weight.view(C_in, C_out) + bias.

But in PyTorch, the Conv2d's weight is (out_channels, in_channels, kernel_size...), so the weight for 1x1 is (C_out, C_in, 1, 1). So to perform the matrix multiplication, we can do:

output = input.view(N, C_in, -1) @ weight.view(C_out, C_in).t().contiguous() + bias.view(1, C_out, 1, 1)

But maybe the custom kernel can do this more efficiently, especially if we can avoid the reshape and transpose steps by directly accessing the memory.

Alternatively, writing a CUDA kernel where each thread handles an output element (n, o, h, w), and computes the sum over in_channels. The problem is that for each output element, you have to loop over in_channels, which could be 64 in this case. So per output element, that's 64 multiplies and adds. But with 1024x1024 spatial dimensions and 16 batch, that's a lot of elements. The total number of operations is N * H * W * C_out * C_in. For the given numbers: 16 * 1024^2 * 128 * 64 = that's a huge number, but maybe the kernel can be optimized.

Wait, but doing this with a for loop over in_channels in each thread might be slow. Instead, we can unroll the loop or use shared memory to cache the weights or inputs. Hmm, perhaps using shared memory to cache the in_channels for a block could help, but it depends on the block size.

Alternatively, the kernel can be designed such that each thread processes one output channel for a given input position. Let's think in terms of threads and blocks.

Suppose the grid is divided into blocks, each responsible for a certain spatial position (h, w) and batch element. Each block has a number of threads equal to the number of output channels. Then, each thread can compute the sum for its output channel.

The steps would be:

For each block (n, h, w):

- Load the input channels (all C_in values for this n, h, w)

- For each output channel o (handled by each thread in the block):

  - Compute sum over i of input[i] * weight[o][i][0][0]

  - Add bias[o]

- Store the result in the output tensor.

This way, the input channels are loaded once per block, and then each thread processes an output channel. The input channels can be stored in shared memory to avoid reloading. 

The block dimensions would need to have at least as many threads as there are output channels. Wait, but if C_out is 128, that would need a block size of 128, which is possible. Alternatively, use a grid of blocks where each block handles a spatial position and batch, and threads handle the output channels.

The key idea is to minimize memory accesses and maximize parallelism.

So the kernel would look something like this:

Each thread is responsible for an output channel o.

The block is for (n, h, w). The block index would be calculated based on n, h, w.

First, each block loads the input channels (C_in elements) into shared memory. Then each thread (o) loops over the input channels and computes the sum.

Wait, but how to structure the block indices.

Alternatively, the block size is set to the number of output channels. Each block corresponds to a (n, h, w) position, and each thread in the block computes one output channel.

The steps:

1. For each block (index computed from n, h, w):

   a. Compute the global indices n, h, w from the block index.

   b. Load the input channels (C_in elements) for this position into shared memory.

2. Each thread (thread_id) corresponds to an output channel o = thread_id.

   a. Compute the sum over i of input[i] * weight[o][i][0][0].

   b. Add the bias if applicable.

   c. Write the result to output[n][o][h][w].

But how do we handle the weight storage? The weights are stored as a tensor of shape (C_out, C_in, 1, 1). So for the kernel, we can treat the weight as a matrix of size (C_out, C_in). 

The kernel needs to access the weights as weight[o][i], so in the kernel, for each o and i, we can get weight[o][i].

But how do we pass the weight and bias to the kernel? Since in PyTorch's custom kernels, the tensors are passed as arguments.

The plan is to write a CUDA kernel that takes the input tensor, the weight tensor, the bias tensor (optional), and produces the output. 

The kernel will process each (n, h, w) position, and for each, compute all output channels.

Now, the kernel code.

First, the kernel function:

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

    // Each block processes a (n, h, w) position.
    int n = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;

    // Each thread corresponds to an output channel o.
    int o = threadIdx.x;
    if (o >= out_channels) return;

    // Load input data for this position (all in_channels)
    // Need to read input[n][i][h][w] for all i.

    // Since the input is (batch, in_channels, H, W), the index is:
    // input_offset = n * in_channels * H * W + i * H * W + h * W + w
    // But for each i, we can loop through them.

    float sum = 0;
    for (int i = 0; i < in_channels; ++i) {
        int input_idx = n * in_channels * height * width 
            + i * height * width 
            + h * width + w;
        float val = input[input_idx];
        // weight is (out_channels, in_channels, 1, 1)
        // So weight[o][i][0][0] is stored as linear index:
        int weight_idx = o * in_channels + i;
        sum += val * weight[weight_idx];
    }

    // Add bias if present
    if (bias) {
        sum += bias[o];
    }

    // Write to output
    int output_idx = n * out_channels * height * width 
        + o * height * width 
        + h * width + w;
    output[output_idx] = sum;
}

Wait, but the weight storage might be in a different order. Let me check the weight's storage.

The weight tensor for Conv2d is of shape (out_channels, in_channels, kernel_size[0], kernel_size[1]). Since kernel_size is (1,1), the last two dimensions are 1 each. So the actual storage is a 2D matrix of shape (out_channels, in_channels). Thus, the weight[o][i] can be accessed as weight[o * in_channels + i].

So the above code should work.

The block dimensions would need to have as many threads as there are output channels. So the block size is (out_channels, 1, 1). But since the maximum threads per block is 1024, and out_channels is 128 here, that's okay. 

The grid dimensions are batch_size * height * width. So the grid is (batch_size, height, width). 

Wait, but in CUDA, the grid can have up to 3 dimensions (blockIdx.x, y, z). So the block dimensions can be 3D, but the maximum is 65535 for each dimension. For the given problem, batch_size is 16, height and width are 1024 each. So the grid size would be:

gridDim.x = batch_size (16)

gridDim.y = height (1024)

gridDim.z = width (1024)

But the maximum grid dimensions for each dimension (for compute capability >=3.5) are 2^31-1, so that's okay.

However, the product of grid dimensions (16 * 1024 * 1024) is 16,777,216 blocks. Wait, but that's way too big. Because the maximum number of blocks in a grid is 2^31-1 per dimension, but the total number here is 16*1024*1024 = 16 million blocks, which is manageable. But each block is very small (threads per block is 128). But in CUDA, the total number of blocks must be within the limits. Since each dimension can be up to 2^31-1, that's okay.

Alternatively, maybe we can structure the grid differently, but given the problem constraints, this approach should work.

Now, in the kernel launch, we need to set the block size to (out_channels, 1, 1) and the grid size as (batch_size, height, width).

Wait, the block is 1D, so the block dimensions would be threadsPerBlock = out_channels. The block dimensions are 1D. The grid is 3D? No, the grid can be 3D but the block is 1D. So:

block = (out_channels, 1, 1)

grid = (batch_size, height, width)

Wait, no. The block is a 1D array of threads, so block.x is the number of threads (out_channels). The grid is 3D, so each block is addressed by blockIdx.x (batch), blockIdx.y (height), blockIdx.z (width).

Yes, that works.

Now, the kernel function needs to be launched with:

dim3 block(out_channels); // or block.x = out_channels

dim3 grid(batch_size, height, width);

elementwise_add_kernel<<<grid, block>>>(...);

But need to ensure that out_channels <= maximum threads per block (1024). Since in the example, out_channels is 128, which is okay.

Now, the kernel code is written. Next, the Python wrapper function that calls it.

The wrapper function in Python will take input, weight, bias, and returns the output tensor.

So, in the ModelNew class, instead of using the PyTorch Conv2d, we'll have to store the weights and bias as parameters, then pass them to the CUDA kernel.

Wait, but the original Model uses nn.Conv2d, which has parameters. So in ModelNew, we need to have parameters for the weight and bias, just like the original model.

Thus, in the ModelNew class, the __init__ should create parameters for weight and bias (if bias is enabled). The forward function will call the CUDA kernel, passing the input, weight, and bias tensors.

So the steps for the ModelNew:

1. In __init__, initialize the parameters with the same dimensions as the original Conv2d layer. So:

self.weight = nn.Parameter(torch.empty(out_channels, in_channels, 1, 1))

self.bias = nn.Parameter(torch.empty(out_channels)) if bias else None

Then, initialize them (maybe using the same initialization as Conv2d, but the user can do that as usual).

2. The forward function will call the CUDA kernel function, which is compiled inline.

So putting this together:

First, the CUDA kernel code.

The kernel code needs to be written as a string.

Also, in the kernel function, the parameters are passed as pointers. The wrapper function in C++ will handle getting the pointers and sizes.

Wait, in the Python wrapper, the function will be something like:

def pointwise_conv_cuda(input, weight, bias=None):

    ... compute the output ...

Thus, the CUDA kernel must take these tensors as inputs.

Now, the CUDA code:

First, the kernel function as above.

Then, the wrapper function in C++:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {

    // Check that the inputs are on the same device and contiguous.
    CHECK(input.device() == weight.device());
    if (bias.defined()) {
        CHECK(bias.device() == input.device());
    }
    CHECK(input.is_contiguous());
    CHECK(weight.is_contiguous());
    if (bias.defined()) CHECK(bias.is_contiguous());

    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int out_channels = weight.size(0);
    int height = input.size(2);
    int width = input.size(3);

    // Output tensor has same dimensions except channel.
    auto output = torch::empty({batch_size, out_channels, height, width}, 
                              input.options());

    // Launch kernel
    dim3 block(out_channels);
    dim3 grid(batch_size, height, width);

    // Need to get pointers to the data
    const float* input_data = input.data_ptr<float>();
    const float* weight_data = weight.data_ptr<float>();
    const float* bias_data = bias.defined() ? bias.data_ptr<float>() : nullptr;
    float* output_data = output.data_ptr<float>();

    // Launch the kernel
    pointwise_conv_kernel<<<grid, block>>>(
        input_data, weight_data, bias_data,
        output_data,
        batch_size, in_channels, out_channels,
        height, width
    );

    // Check for errors
    cudaDeviceSynchronize();
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}

Wait, but the kernel is expecting the parameters to be passed in the right order. Also, the dimensions must be correct.

Now, in the code, the kernel is defined as:

__global__ void pointwise_conv_kernel(...) { ... }

This must be included in the CUDA sources.

Putting this all together in the Python code:

The CUDA sources are:

elementwise_add_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

    int n = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int o = threadIdx.x;
    if (o >= out_channels) return;

    float sum = 0;
    for (int i = 0; i < in_channels; ++i) {
        int input_idx = n * in_channels * height * width + 
            i * height * width + 
            h * width + w;
        sum += input[input_idx] * weight[o * in_channels + i];
    }

    if (bias) {
        sum += bias[o];
    }

    int output_idx = n * out_channels * height * width + 
        o * height * width + 
        h * width + w;
    output[output_idx] = sum;
}

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {
    // ... as above ...
}
"""

Wait, but in the code above, the kernel function is defined, and the wrapper function. The code must be correct.

Now, in the Python code:

We need to compile this inline CUDA code. So using load_inline.

The cpp sources must have the function declarations.

The header for the C++ function is:

extern "C" torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias);

Wait, in the load_inline, the functions list must include the name of the function.

So, putting it all together.

The Python code for ModelNew would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for pointwise convolution
pointwise_conv_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

    int n = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int o = threadIdx.x;
    if (o >= out_channels) return;

    float sum = 0;
    for (int i = 0; i < in_channels; ++i) {
        int input_idx = n * in_channels * height * width +
            i * height * width +
            h * width + w;
        sum += input[input_idx] * weight[o * in_channels + i];
    }

    if (bias) {
        sum += bias[o];
    }

    int output_idx = n * out_channels * height * width +
        o * height * width +
        h * width + w;
    output[output_idx] = sum;
}

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {
    // Check that the inputs are on the same device and contiguous.
    TORCH_CHECK(input.device() == weight.device(), "Input and weight must be on the same device");
    if (bias.defined()) {
        TORCH_CHECK(bias.device() == input.device(), "Bias must be on the same device as input");
    }
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(weight.is_contiguous(), "Weight tensor must be contiguous");
    if (bias.defined()) {
        TORCH_CHECK(bias.is_contiguous(), "Bias tensor must be contiguous");
    }

    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int out_channels = weight.size(0);
    int height = input.size(2);
    int width = input.size(3);

    // Output tensor has same dimensions except channel.
    auto output = torch::empty({batch_size, out_channels, height, width}, 
                              input.options());

    // Launch kernel
    dim3 block(out_channels);
    dim3 grid(batch_size, height, width);

    // Get pointers
    const float* input_data = input.data_ptr<float>();
    const float* weight_data = weight.data_ptr<float>();
    const float* bias_data = bias.defined() ? bias.data_ptr<float>() : nullptr;
    float* output_data = output.data_ptr<float>();

    pointwise_conv_kernel<<<grid, block>>>(
        input_data, weight_data, bias_data,
        output_data,
        batch_size, in_channels, out_channels,
        height, width
    );

    // Synchronize and check errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}
"""

# The header for the C++ function
pointwise_conv_cpp_source = """
#include <torch/extension.h>

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias);
"""

# Compile the CUDA code
pointwise_conv = load_inline(
    name="pointwise_conv",
    cpp_sources=pointwise_conv_cpp_source,
    cuda_sources=pointwise_conv_source,
    functions=["pointwise_conv_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_flags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, 1, 1))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        # Initialize weights and bias
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Ensure that the tensors are on the correct device and contiguous
        x = x.contiguous()
        weight = self.weight.contiguous()
        bias = self.bias.contiguous() if self.bias is not None else torch.Tensor()
        return pointwise_conv.pointwise_conv_cuda(x, weight, bias)

Wait, but in the forward function, the bias is passed as a Tensor. If there is no bias, the function expects a Tensor. Wait, in the C++ code, the bias is optional (with .defined()). So in Python, if bias is not present, we can pass an empty tensor, but in the C++ code, the bias.defined() will check.

Alternatively, in the forward function, if bias is None, we can pass a torch.Tensor() (empty), but the C++ code will check if it's defined.

Wait, in the code above, when the bias is None (self.bias is None), then:

bias = self.bias.contiguous() if self.bias is not None else torch.Tensor()

But torch.Tensor() creates a tensor with no elements. However, in the C++ code, the bias tensor is passed as 'bias' which is a torch::Tensor. If the Python passes an empty tensor (with .defined() being false?), wait, no: in PyTorch, even an empty tensor is still 'defined', but has zero elements. So perhaps it's better to pass an undefined tensor.

Wait, in the code, when the bias is not present, in Python, the code can pass an empty torch.Tensor() (which is defined but has no elements). But in the C++ code, the bias.defined() will return true, but then accessing bias[o] would be invalid.

Hmm, that's a problem. To handle this correctly, when the bias is not present, we should pass an undefined tensor. But in Python, how to do that?

Alternatively, in the C++ function, the bias can be an optional parameter. Wait, but in the current code, the function signature is:

def pointwise_conv_cuda(input, weight, bias=None):

Wait no, the C++ code's function is defined to take a torch::Tensor bias. So in Python, when the bias is not present, we need to pass a tensor that is not defined, but how?

Alternatively, in the C++ code, we can have a function that takes an optional parameter. Wait, in the C++ function, the third parameter can be optional. Let me adjust the code:

In the C++ function definition:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor());

Then, in the Python side, when bias is not present, we can pass an empty tensor (torch.Tensor()) or omit it. But the Python function's wrapper should handle that.

Alternatively, adjust the C++ code to accept an optional bias. Let me update the code.

First, in the C++ function signature:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor());

Then, in the kernel call, check if bias.defined().

Then, in the Python code, when the bias is not present, pass an empty tensor (which is undefined in C++? No, in C++ torch::Tensor is empty if it's default-constructed). So in Python:

if self.bias is not None:
    bias_tensor = self.bias.contiguous()
else:
    bias_tensor = torch.Tensor()  # which is an empty tensor, but in C++ it will be torch::Tensor()

Wait, but in the C++ code, if bias is passed as an empty tensor (with zero elements), then bias.defined() will return true, but the tensor has no data. So that's bad.

Hmm, need to make sure that when the bias is not present, the bias parameter is not defined. To do that, in the Python code, when bias is None, pass an undefined torch::Tensor. But in Python, how to pass an undefined tensor? It's not possible. So perhaps the Python function should accept a None and handle that.

Wait, in the C++ code's function, the third parameter is a torch::Tensor which can be default-constructed (empty). But to indicate that the bias is not present, the user must pass an empty tensor (like torch.Tensor()), which in C++ would be a tensor with .defined() == false?

Wait, no. In PyTorch, a default-constructed torch::Tensor in C++ has .defined() == false. However, in Python, when you create a torch.Tensor(), it creates a tensor with some default values (maybe 0-dimensional). So in Python, passing an empty tensor (like torch.tensor([])) would still have .defined() == true. 

This is a problem. To properly pass an undefined bias, perhaps in the Python code, we should check if bias is None and pass None, but then in the C++ code, we need to handle the case where the third argument is None.

Alternatively, in the Python wrapper function, the third argument is optional, and if not given, bias is omitted. 

Wait, maybe better to adjust the function signature so that bias is an optional parameter. 

In the C++ code, the function is defined as:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor());

But in the Python wrapper, when the user does not pass a bias, it will be set to the default. 

Wait, perhaps the best way is to make the bias optional in the Python function. Let me adjust the code.

In the C++ code:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor());

Then, in the Python code:

def pointwise_conv_cuda(input, weight, bias=None):

    if bias is None:
        return _C.pointwise_conv_cuda(input, weight)
    else:
        return _C.pointwise_conv_cuda(input, weight, bias)

Wait, but the load_inline function would need the function to have the correct parameters. Hmm, perhaps the C++ function can have an optional third parameter, and in the Python wrapper, the code can handle it. 

Alternatively, the kernel code can be modified to accept a nullptr for bias. Wait, but in the C++ code, we have to pass the bias as a torch::Tensor. 

Alternatively, in the C++ function, if bias is not given, the user can pass an empty tensor (like torch::Tensor()), and then in the code, check if bias.defined().

So in the Python code:

if self.bias is not None:
    bias_tensor = self.bias.contiguous()
else:
    bias_tensor = torch.Tensor()  # creates an empty tensor with no elements

But in C++, bias.defined() will be true, but accessing its data would be invalid.

Wait, no. If in Python you create an empty tensor with no elements (like torch.Tensor()), then in C++, the tensor is considered "defined" but has zero elements. Thus, trying to access bias[o] would be out of bounds. 

This is a problem. So to avoid this, perhaps the Python code should pass None, and in the C++ code, the bias is optional. But how?

Alternatively, in the C++ function, the bias is a torch::Tensor, but in the Python wrapper, the code can check whether the bias is provided and pass a tensor with zero elements or not. 

Wait, perhaps the best way is to have a separate overload for when bias is present or not, but that's more complex. Alternatively, in the C++ code, if the bias tensor has size 0, then treat it as not present. 

Alternatively, in the Python code, when there is no bias, pass a tensor with zero elements, and in the C++ code, check if the bias tensor's size is zero. 

Wait, but the bias tensor's size should be (out_channels,). So in the case of no bias, the user can pass a tensor with size (0, ), and in the C++ code:

if (bias.defined() && bias.size(0) == out_channels) ... else use no bias.

Hmm, that could work. 

Alternatively, in the Python code, when no bias, pass a tensor of size (0,), and in the C++ code, check if bias.size(0) is 0.

Alternatively, let's adjust the code in the forward function:

In the ModelNew's forward:

bias = self.bias if self.bias is not None else torch.empty(0, device=x.device)

Then in C++, check if bias.size(0) == 0. But in the kernel, if the bias tensor has size 0, then we can skip adding it.

Wait, but in the code, the kernel is written as:

if (bias) { ... }

Wait, no: the 'bias' is a pointer. Wait, no, in the kernel, the 'bias' is a pointer to the data of the tensor. 

Wait, no, in the kernel code, the 'bias' is a pointer to the bias data. So in the C++ code:

if (bias.defined() && bias.size(0) >= out_channels) ?

Alternatively, perhaps in the Python code, when there is no bias, we pass a tensor with zero elements, and in C++, check if the bias is empty.

Alternatively, the kernel code can handle it by checking if bias is null. But in the current setup, the 'bias' is passed as a pointer. Wait in the kernel code:

The third parameter to the kernel is a const float* bias_data. 

Wait, in the kernel function's parameters, the 'bias' is a const float*, and in the kernel code, the line is:

if (bias) { sum += bias[o]; }

Wait, no! Looking back at the kernel code I wrote earlier:

In the kernel code, the 'bias' parameter is a pointer. Wait, in the kernel function's parameters:

Wait, looking back, the kernel function is defined as:

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

So the 'bias' is a pointer to the bias data. 

Therefore, in the C++ wrapper function, when there is no bias, the code passes bias_data as a null pointer:

const float* bias_data = bias.defined() ? bias.data_ptr<float>() : nullptr;

Thus, in the kernel code, the check is:

if (bias) { sum += bias[o]; }

Therefore, in the Python code, when there is no bias, the bias tensor passed should be undefined. 

Thus, in Python, when there is no bias, we need to pass a tensor that is not defined. But how?

Wait in the C++ function, the third parameter is a torch::Tensor. So if in Python, the bias is passed as an empty tensor (torch.Tensor()), then in C++ it's a tensor that is defined but has zero elements. Thus, bias.defined() is true, but the data is invalid.

Therefore, to pass an undefined tensor, the Python code must pass an object that is not a tensor. But that's not possible.

Hmm, this is a problem. 

Alternative approach:

In the Python code, when there is no bias, we can pass a tensor with zero elements (size 0), and in the kernel, check if the bias tensor has elements. 

Wait, in the C++ code, when bias.defined() is true, then:

if (bias.size(0) == 0) then treat as no bias. 

Alternatively, in the kernel code, check if the bias pointer is not null and then access the bias[o].

Wait, in the C++ wrapper function:

bias_data is set to bias.defined() ? bias.data_ptr<float>() : nullptr.

Thus, in the kernel code, the check is:

if (bias != nullptr) { ... }

So, in the C++ wrapper, when there is no bias (self.bias is None in Python), then bias_data is nullptr, so the kernel code will not add the bias.

So in the Python code, when there is no bias, the code should pass a torch.Tensor() (empty), but in the C++ code, the bias is passed as torch.Tensor(), which is still defined (since it's a valid tensor, but empty). 

Wait, no. If in Python, when the bias is None, we can pass a None instead of a tensor. But the C++ function requires a torch::Tensor. 

Hmm, this is tricky. Let me think again.

The C++ function has signature:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias);

Thus, the third parameter must be a torch::Tensor. 

Therefore, in Python, when there is no bias, we have to pass a tensor that is not defined. But how?

Wait, in Python, to pass an undefined tensor, you can't. The only way is to pass an empty tensor (with zero elements), but that's defined. 

Thus, perhaps in the C++ code, when the bias tensor has zero elements, we treat it as not present.

Thus, in the C++ wrapper:

if (bias.defined() && bias.size(0) > 0) then set bias_data to its data, else nullptr.

Wait, in the wrapper function code:

const float* bias_data;
if (bias.defined() && bias.size(0) == out_channels) {
    bias_data = bias.data_ptr<float>();
} else {
    bias_data = nullptr;
}

Wait, but out_channels is known from the weight.size(0). 

Wait, in the wrapper function:

int out_channels = weight.size(0);

if (bias.defined() && bias.size(0) == out_channels) {
    bias_data = bias.data_ptr<float>();
} else {
    bias_data = nullptr;
}

But then, if the user passes a tensor with incorrect size, it will be treated as no bias, which could be an error. 

Alternatively, the code should check for the bias size. 

Thus, in the C++ code:

// Check the bias dimensions if present
if (bias.defined()) {
    int bias_size = bias.size(0);
    TORCH_CHECK(bias_size == out_channels, 
                "Bias tensor must have size equal to out_channels (", out_channels, ")");
}

Thus, forcing the bias tensor to have the correct size. 

Then, when the user passes a bias tensor with size 0 (invalid), the check will throw an error. 

Therefore, in the Python code, when the bias is not present, we must pass an undefined tensor. But in Python, how?

Wait, in Python, when the bias is None, the code in the forward function can pass None. However, the C++ function expects a torch::Tensor. 

Thus, the only way is to pass a tensor with size (out_channels) but not initialized. Wait, perhaps the user can create a tensor with zero elements but that's not possible. 

Hmm, maybe in the Python code, when there is no bias, the code should pass an empty tensor but in the C++ code, we can check whether the bias tensor has size 0 and treat it as none. 

Alternatively, in the Python code, when there is no bias, we can pass a tensor of zeros but that would add bias zero, which is incorrect.

Alternatively, the Python code can pass a dummy tensor with size (out_channels) but filled with zeros. But that's not the same as no bias. 

Wait, but if the kernel code checks if bias is null, then passing a tensor with zero elements (size 0) will cause the check to fail (since bias_data is null only if the bias tensor is not defined). 

This is getting complicated. Let me think of the correct approach.

In the kernel function:

The bias is passed as a pointer. If there is no bias, pass a null pointer. 

In the C++ wrapper function:

bias_data is null only if the bias is not present (i.e., when self.bias is None). 

Thus, in the Python code, when there is no bias, we can pass an undefined tensor. But in Python, how to create an undefined tensor?

Alternatively, in the C++ code, the third parameter can be optional. Let's modify the C++ function to accept an optional tensor.

Wait, in C++, the function can be:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt);

Then, in the Python code, you can pass the bias as an optional parameter. 

To do this, in the C++ code:

#include <c10/macros/Macros.h>
#include <c10/core/TensorOptions.h>

Then:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt) {

    // Check the bias
    const float* bias_data = nullptr;
    if (bias.has_value()) {
        auto bias_tensor = bias.value();
        TORCH_CHECK(bias_tensor.size(0) == weight.size(0), "Bias must have size equal to out_channels");
        bias_data = bias_tensor.data_ptr<float>();
    }

    // ... rest of the code ...
}

Then, in the Python code, the wrapper function can be written to accept an optional bias parameter. 

But how to expose this in the Python wrapper via load_inline? 

The functions list must include the function with the optional parameter. 

Wait, the function signature in the C++ code is now:

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt);

But in Python, when using load_inline, the functions must be declared with the correct signature. 

Alternatively, the Python function can have a third parameter that is optional.

Thus, the Python code can define:

def pointwise_conv_cuda(input, weight, bias=None):

    if bias is None:
        return _C.pointwise_conv_cuda(input, weight)
    else:
        return _C.pointwise_conv_cuda(input, weight, bias)

But for this to work, the C++ function must have an overload with and without the third parameter. However, the C++ function can have a single overload with an optional parameter.

Thus, the C++ function can be written as above with the optional parameter. 

This requires adjusting the kernel code and wrapper. Let me rework the C++ code accordingly.

Thus, the revised C++ code:

#include <torch/extension.h>
#include <cuda_runtime.h>
#include <c10/core/TensorOptions.h>

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

    // same as before ...
}

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt) {

    // Check that the inputs are on the same device and contiguous.
    TORCH_CHECK(input.device() == weight.device(), "Input and weight must be on the same device");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.device() == input.device(), "Bias must be on the same device as input");
    }
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(weight.is_contiguous(), "Weight tensor must be contiguous");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.is_contiguous(), "Bias tensor must be contiguous");
        TORCH_CHECK(b.size(0) == weight.size(0), "Bias size must match out_channels");
    }

    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int out_channels = weight.size(0);
    int height = input.size(2);
    int width = input.size(3);

    auto output = torch::empty({batch_size, out_channels, height, width}, 
                              input.options());

    dim3 block(out_channels);
    dim3 grid(batch_size, height, width);

    const float* input_data = input.data_ptr<float>();
    const float* weight_data = weight.data_ptr<float>();
    const float* bias_data = bias.has_value() ? bias.value().data_ptr<float>() : nullptr;

    pointwise_conv_kernel<<<grid, block>>>(
        input_data, weight_data, bias_data,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        height, width
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}

The kernel code remains the same.

Now, the Python code can have the wrapper function:

def pointwise_conv_cuda(input, weight, bias=None):
    # implemented in C++ via load_inline
    pass

Then, in the load_inline call:

functions=["pointwise_conv_cuda"]

This should work.

Now, in the Python ModelNew class's forward function:

bias = self.bias if self.bias is not None else None

So, the code would be:

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, 1, 1))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        # Initialize the parameters as needed
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.contiguous()
        weight = self.weight.contiguous()
        bias = self.bias if self.bias is not None else None
        return pointwise_conv.pointwise_conv_cuda(x, weight, bias)

This should handle the optional bias correctly.

Now, compiling this code. The CUDA kernel source and the C++ function now have the optional parameter.

Now, checking for any other potential issues:

- The input, weight, and output must be contiguous. The forward function ensures that x and weight are contiguous. The output is created as contiguous.

- The grid and block dimensions are correctly set.

- The kernel correctly calculates the indices.

Potential issues:

The kernel uses blockIdx.z for width. The maximum dimensions for blockIdx are 65535 for each dimension. For width=1024, that's okay, but if any dimension exceeds that, it would be a problem. But in the given problem, width and height are 1024, which is acceptable.

Also, the block size is out_channels (128 in the example), which is under the maximum 1024 threads per block.

Another consideration is the loop over in_channels (64 iterations per thread). For 64 iterations, it's manageable. Unrolling the loop might help, but might not be worth it.

Also, the kernel uses a 3D grid, which could have some limitations. For example, the maximum grid size for each dimension is 2^31-1, but 16*1024*1024 is way under that.

Now, the final code:

Putting it all together:

The CUDA source code needs to include the necessary headers and use the c10::optional.

Thus, the complete code for the kernel and wrapper is as follows.

The kernel and wrapper code:

pointwise_conv_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <c10/core/TensorOptions.h>

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

    int n = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int o = threadIdx.x;
    if (o >= out_channels) return;

    float sum = 0;
    for (int i = 0; i < in_channels; ++i) {
        int input_idx = n * in_channels * height * width + 
            i * height * width + 
            h * width + w;
        sum += input[input_idx] * weight[o * in_channels + i];
    }

    if (bias) {
        sum += bias[o];
    }

    int output_idx = n * out_channels * height * width + 
        o * height * width + 
        h * width + w;
    output[output_idx] = sum;
}

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt) {
    // Check device and contiguity
    TORCH_CHECK(input.device() == weight.device(), "Input and weight must be on the same device");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.device() == input.device(), "Bias must be on the same device as input");
    }
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(weight.is_contiguous(), "Weight tensor must be contiguous");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.is_contiguous(), "Bias tensor must be contiguous");
        TORCH_CHECK(b.size(0) == weight.size(0), "Bias size must match out_channels");
    }

    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int out_channels = weight.size(0);
    int height = input.size(2);
    int width = input.size(3);

    auto output = torch::empty({batch_size, out_channels, height, width}, 
                              input.options());

    dim3 block(out_channels);
    dim3 grid(batch_size, height, width);

    const float* input_data = input.data_ptr<float>();
    const float* weight_data = weight.data_ptr<float>();
    const float* bias_data = bias.has_value() ? bias.value().data_ptr<float>() : nullptr;

    pointwise_conv_kernel<<<grid, block>>>(
        input_data, weight_data, bias_data,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        height, width
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}
"""

The header (cpp_sources):

pointwise_conv_cpp_source = """
#include <torch/extension.h>

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt);
"""

Now, the Python code:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

pointwise_conv = load_inline(
    name="pointwise_conv",
    cpp_sources=pointwise_conv_cpp_source,
    cuda_sources=pointwise_conv_source,
    functions=["pointwise_conv_cuda"],
    verbose=True,
    extra_cuda_flags=["-O3"],
    extra_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, 1, 1))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        # Initialize parameters
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.contiguous()
        weight = self.weight.contiguous()
        bias = self.bias if self.bias is not None else None
        return pointwise_conv.pointwise_conv_cuda(x, weight, bias)

Wait, but the import of math is needed for the initializers.

Wait, in the __init__ function, math is used in the initialization, so we need to import math:

import math

So adding that to the top.

Also, the initializers for the parameters need to be properly handled. The code above assumes that the user will call the initialization, which is correct.

Thus, the complete code would be:

```python
import torch
import torch.nn as nn
import math
from torch.utils.cpp_extension import load_inline

pointwise_conv_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <c10/core/TensorOptions.h>

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

    int n = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int o = threadIdx.x;
    if (o >= out_channels) return;

    float sum = 0;
    for (int i = 0; i < in_channels; ++i) {
        int input_idx = n * in_channels * height * width + 
            i * height * width + 
            h * width + w;
        sum += input[input_idx] * weight[o * in_channels + i];
    }

    if (bias) {
        sum += bias[o];
    }

    int output_idx = n * out_channels * height * width + 
        o * height * width + 
        h * width + w;
    output[output_idx] = sum;
}

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt) {
    // Check device and contiguity
    TORCH_CHECK(input.device() == weight.device(), "Input and weight must be on the same device");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.device() == input.device(), "Bias must be on the same device as input");
    }
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(weight.is_contiguous(), "Weight tensor must be contiguous");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.is_contiguous(), "Bias tensor must be contiguous");
        TORCH_CHECK(b.size(0) == weight.size(0), "Bias size must match out_channels");
    }

    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int out_channels = weight.size(0);
    int height = input.size(2);
    int width = input.size(3);

    auto output = torch::empty({batch_size, out_channels, height, width}, 
                              input.options());

    dim3 block(out_channels);
    dim3 grid(batch_size, height, width);

    const float* input_data = input.data_ptr<float>();
    const float* weight_data = weight.data_ptr<float>();
    const float* bias_data = bias.has_value() ? bias.value().data_ptr<float>() : nullptr;

    pointwise_conv_kernel<<<grid, block>>>(
        input_data, weight_data, bias_data,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        height, width
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}
"""

pointwise_conv_cpp_source = """
#include <torch/extension.h>

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt);
"""

pointwise_conv = load_inline(
    name="pointwise_conv",
    cpp_sources=pointwise_conv_cpp_source,
    cuda_sources=pointwise_conv_source,
    functions=["pointwise_conv_cuda"],
    verbose=True,
    extra_cuda_flags=["-O3"],
    extra_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, 1, 1))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        # Initialize parameters
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.contiguous()
        weight = self.weight.contiguous()
        bias = self.bias if self.bias is not None else None
        return pointwise_conv.pointwise_conv_cuda(x, weight, bias)
```

This should be the complete code. Let me check for any possible errors:

- The kernel function uses blockIdx.z for width. Since the input is (batch, channels, height, width), the width is the last dimension, so the indexing is correct.

- The output tensor is created with the same options as the input, ensuring the correct device and dtype.

- The parameters are stored as nn.Parameters, so they are registered correctly for training.

- The initialization uses the same method as the original Conv2d layer, ensuring compatibility.

- The forward function ensures that inputs are contiguous, which is necessary for the kernel.

Potential issues:

- The grid dimensions may be too large for some CUDA architectures. For example, gridDim.x (batch_size) is 16, which is fine. But grid dimensions are 3D, which should be okay.

- The block size (out_channels) must not exceed the maximum threads per block (1024), which is the case here (128).

- The loop over in_channels may be slow for large in_channels, but for 64 it's manageable.

Overall, this should work.
</think>

```python
import torch
import torch.nn as nn
import math
from torch.utils.cpp_extension import load_inline

pointwise_conv_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <c10/core/TensorOptions.h>

__global__ void pointwise_conv_kernel(
    const float* input, const float* weight, const float* bias,
    float* output,
    int batch_size, int in_channels, int out_channels,
    int height, int width) {

    int n = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int o = threadIdx.x;
    if (o >= out_channels) return;

    float sum = 0;
    for (int i = 0; i < in_channels; ++i) {
        int input_idx = n * in_channels * height * width + 
            i * height * width + 
            h * width + w;
        sum += input[input_idx] * weight[o * in_channels + i];
    }

    if (bias) {
        sum += bias[o];
    }

    int output_idx = n * out_channels * height * width + 
        o * height * width + 
        h * width + w;
    output[output_idx] = sum;
}

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt) {
    // Check device and contiguity
    TORCH_CHECK(input.device() == weight.device(), "Input and weight must be on the same device");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.device() == input.device(), "Bias must be on the same device as input");
    }
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(weight.is_contiguous(), "Weight tensor must be contiguous");
    if (bias.has_value()) {
        auto b = bias.value();
        TORCH_CHECK(b.is_contiguous(), "Bias tensor must be contiguous");
        TORCH_CHECK(b.size(0) == weight.size(0), "Bias size must match out_channels");
    }

    int batch_size = input.size(0);
    int in_channels = input.size(1);
    int out_channels = weight.size(0);
    int height = input.size(2);
    int width = input.size(3);

    auto output = torch::empty({batch_size, out_channels, height, width}, 
                              input.options());

    dim3 block(out_channels);
    dim3 grid(batch_size, height, width);

    const float* input_data = input.data_ptr<float>();
    const float* weight_data = weight.data_ptr<float>();
    const float* bias_data = bias.has_value() ? bias.value().data_ptr<float>() : nullptr;

    pointwise_conv_kernel<<<grid, block>>>(
        input_data, weight_data, bias_data,
        output.data_ptr<float>(),
        batch_size, in_channels, out_channels,
        height, width
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}
"""

pointwise_conv_cpp_source = """
#include <torch/extension.h>

torch::Tensor pointwise_conv_cuda(torch::Tensor input, torch::Tensor weight, c10::optional<torch::Tensor> bias = c10::nullopt);
"""

pointwise_conv = load_inline(
    name="pointwise_conv",
    cpp_sources=pointwise_conv_cpp_source,
    cuda_sources=pointwise_conv_source,
    functions=["pointwise_conv_cuda"],
    verbose=True,
    extra_cuda_flags=["-O3"],
    extra_cflags=["-O3"],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, bias: bool = False):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, 1, 1))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
        # Initialize parameters using the same method as PyTorch's Conv2d
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.contiguous()
        weight = self.weight.contiguous()
        bias = self.bias if self.bias is not None else None
        return pointwise_conv.pointwise_conv_cuda(x, weight, bias)
```