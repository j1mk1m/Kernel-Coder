To ensure correctness, the ModelNew must have the same input/output as the original Model.

The optimization strategy is: 

The L2 normalization can be fused into a custom CUDA kernel. This avoids the overhead of multiple tensor operations (like norm computation, division) and can be vectorized efficiently. 

The problem with the original approach is that computing the norm and then dividing may involve multiple memory accesses and kernel launches, which can be slow for large batch sizes and dimensions. 

The plan is to create a custom CUDA kernel that computes the norm and performs the division in a single kernel, reducing memory traffic and kernel launch overhead.

The optimization is to replace the two operations (norm and division) with a single fused kernel.

The fused kernel will first compute the norm along the specified dimension (dim=1 in this case), then perform element-wise division of the input by the norm tensor. Since the norm computation is along a specific axis, it can be done per sample efficiently in parallel.

The challenge is to efficiently compute the L2 norm along the given dimension for each element in the batch and then divide each element by its corresponding norm. 

The key steps to implement this kernel are:

1. For each sample (along dim=1), compute the squared sum of elements in that dimension.
2. Take the square root to get the norm.
3. Broadcast the norm (since keepdim=True) and divide each element by the norm.

But in a single kernel, so we can do the computation in shared memory or in a way that minimizes memory accesses.

First, let's think about how to compute the norm.

For a given sample in the batch (each row if dim=1), the L2 norm is sqrt(sum(x_i^2)).

To compute this in parallel for all elements in a row:

- Each thread can be responsible for a block of elements in the row.
- Sum the squares of their assigned elements, then combine the partial sums to get the total sum for the row.
- Then compute the square root.

Then, the division can be done in the same kernel by using the computed norm.

We can structure the kernel as follows:

- Each block corresponds to a sample (row).
- Threads within the block process elements of the row.
- Compute the sum of squares within the block using shared memory for reduction.
- Compute the norm (sqrt of sum).
- Then, each thread divides its element by the norm.

This approach ensures that the norm is computed and used in the same kernel, avoiding multiple passes over the data.

Implementation steps:

- Launch one block per row (since dim=1).
- Threads per block can be chosen based on the dimension size (dim).
- Use shared memory to accumulate the sum of squares within a block.
- After reduction, compute the norm and store it in shared memory.
- Then, each thread can read the norm from shared memory and perform the division.

This requires careful handling of shared memory to avoid race conditions, especially when accumulating the sum.

Also, since the input and output are the same size, we can write the output directly to the output tensor.

Now, to handle the division part, after computing the norm, each thread can compute their element divided by the norm.

Potential optimizations:

- Use warp-level primitives for reduction to speed up the sum computation.
- Ensure coalesced memory access patterns when reading the input and writing the output.
- Handle the case where the norm is zero (though L2 norm is always non-negative, so division by zero is undefined; perhaps set to zero or handle as per user's requirement).

But for the problem's sake, assuming that the input has non-zero norms.

Now, let's proceed to code this.

First, the CUDA kernel:

We need to define a kernel that takes in the input tensor x, and outputs the normalized tensor.

The kernel will process each row (since dim=1).

Each block handles a single row. The number of blocks is equal to batch_size.

Each block has a number of threads, say 256 threads per block. Depending on the dimension (dim), threads can be assigned to process chunks of the elements in the row.

First, let's think about the dimensions:

The input is of shape (batch_size, dim) = (32768, 65535). So each row has 65535 elements.

To handle a row with 65535 elements, each block needs to process this.

Each thread in the block can process a certain number of elements.

For example, if we have 256 threads per block, then each thread can process (65535 / 256) ~ 256 elements per thread.

But in practice, the number of elements per thread can vary.

The steps:

For a given block (corresponding to row i):

1. Each thread loads a chunk of elements from the input row.
2. Compute the sum of squares of their chunk.
3. Use a reduction in shared memory to compute the total sum for the row.
4. Compute the norm (sqrt(sum)) and store it in shared memory.
5. Each thread then computes their chunk's elements divided by the norm.

The reduction can be done via a parallel reduction in shared memory.

Implementation steps in code:

First, the kernel:

__global__ void l2_norm_kernel(float *x, float *out, int batch_size, int dim) {

    // Each block handles a row (sample)
    int row = blockIdx.x;
    if (row >= batch_size) return;

    // Each thread in the block processes a portion of the elements in the row
    // Shared memory for the block to accumulate the sum of squares
    __shared__ float sdata[256]; // assuming threads per block <= 256?

    // Each thread loads their portion
    float sum = 0.0f;
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        float val = x[row * dim + i];
        sum += val * val;
    }

    // Write to shared memory
    sdata[threadIdx.x] = sum;
    __syncthreads();

    // Perform reduction in shared memory
    for (int s=blockDim.x/2; s>0; s>>=1) {
        if (threadIdx.x < s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }

    // The root thread (0) has the total sum
    if (threadIdx.x == 0) {
        float norm = sqrt(sdata[0]);
        // Store the norm in shared memory
        sdata[0] = norm;
    }
    __syncthreads();

    // Now, compute the output elements
    float norm = sdata[0];
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        float val = x[row * dim + i];
        out[row * dim + i] = val / norm;
    }
}

Wait, but in the above code, after the reduction, the norm is stored in sdata[0], so all threads can read it from there.

But this requires that all threads wait for the root thread to compute the norm and write it back.

The problem is that the code above may have a race condition where the root thread writes to sdata[0], and other threads read it. But since after the __syncthreads(), all threads are synchronized, so the root thread would have already written the norm to sdata[0].

However, in the reduction step, after the first part (computing the sum), the reduction is done in shared memory. The first part (the for loop) reduces the sdata array.

Wait, the reduction code is as follows:

Initially, each thread's partial sum is stored in sdata[threadIdx.x].

Then, the for loop reduces sdata from blockDim.x elements down to 1 element (in sdata[0]).

Then, the root thread (0) computes the norm (sqrt(sdata[0])), then stores it back into sdata[0].

Then, after __syncthreads(), all threads can read sdata[0] as the norm.

This should work.

The kernel requires that blockDim.x is a power of two for the reduction steps, but that's a common assumption.

Now, the kernel would need to be launched with:

- gridDim = batch_size (each block per row)
- blockDim = min(dim, some_max_threads). For example, 256 threads per block. Since dim is 65535, which is larger than 256, each thread will process dim / 256 elements.

But the kernel is designed to handle any dim.

Now, the CUDA code for this kernel can be written, and then the Python wrapper function.

But first, in the Python code, we need to define the kernel and the wrapper.

Let's structure the code as per the example provided.

First, in the Python code:

We need to write the CUDA kernel code as a string, then compile it using load_inline.

The kernel function will be called l2_norm_cuda, which takes the input tensor x and returns the normalized tensor.

The kernel must handle the input and output pointers.

Now, the code for the kernel.

But first, let's think about the memory layout.

The input tensor x is of shape (batch_size, dim). The CUDA kernel is processing each row (along dim=1). The data is stored in a 1D array (since in CUDA, it's stored in row-major order). So the index for element (row, col) is row * dim + col.

In the kernel code, each block corresponds to a row. Each thread in the block processes a set of columns.

Now, let's write the CUDA code.

The CUDA kernel code:

#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void l2_norm_kernel(const float* x, float* out, int batch_size, int dim) {
    int row = blockIdx.x;
    if (row >= batch_size) return;

    __shared__ float sdata[256]; // Assuming max threads per block is 256

    float sum = 0.0f;
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        float val = x[row * dim + i];
        sum += val * val;
    }

    sdata[threadIdx.x] = sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float norm = sqrtf(sdata[0]);
        sdata[0] = norm;
    }
    __syncthreads();

    float norm = sdata[0];
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        out[row * dim + i] = x[row * dim + i] / norm;
    }
}

torch::Tensor l2_norm_cuda(torch::Tensor x) {
    int batch_size = x.size(0);
    int dim = x.size(1);

    auto out = torch::empty_like(x);

    const int block_size = 256;
    const dim3 grid(batch_size);
    const dim3 block(block_size);

    // Launch the kernel
    l2_norm_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), batch_size, dim);

    return out;
}

Wait, but the shared memory size depends on the block size. Since we have block_size=256, the sdata array is of size 256, which is okay. However, if the block size is larger than 256, this will cause an error. To make it more flexible, we can set the sdata array size as blockDim.x. But in CUDA, the shared memory size must be known at compile time. Hmm, that's a problem.

Wait, in the code above, if the block size is 256, then the shared memory array is okay. But if the block size is different, say 1024, then the shared memory would need to be 1024 elements, but in the code it's fixed to 256. Hence, the kernel is hard-coded to a block size of 256.

Alternatively, to make it more flexible, perhaps use a dynamic approach, but that complicates things. Alternatively, set the block size to 256 as a fixed value.

In our case, since the problem's dim is 65535, which is quite large, using a block size of 256 threads allows each thread to process ~256 elements (since 65535 / 256 ~ 256). That seems manageable.

Alternatively, perhaps use a larger block size, but let's proceed with 256.

Now, in the Python code, we need to ensure that the block size is set to 256 when launching the kernel.

Wait, in the code above, the block is set to block_size=256. The kernel is launched with block_size=256.

Thus, the code should work.

Now, compiling this kernel.

But in the kernel code, the shared memory is declared as float sdata[256]; which is okay if the block size is 256.

Wait, if the block size is 256, then the number of threads per block is 256, so the sdata array of size 256 can hold each thread's sum. That's correct.

If we use a larger block size, this would be a problem, but here we choose 256 as the block size, so it's okay.

Now, in the Python code, the CUDA kernel is defined as above. The wrapper function l2_norm_cuda takes a tensor x and returns the normalized tensor.

Now, putting this into the Python code:

The code structure would be similar to the example given.

First, define the CUDA source code as a string.

Then, define the C++ header (the function declaration).

Then, compile using load_inline.

Then, create the ModelNew class which uses this function.

Now, let's write the full code.

First, the CUDA source code:

l2_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void l2_norm_kernel(const float* x, float* out, int batch_size, int dim) {
    int row = blockIdx.x;
    if (row >= batch_size) return;

    __shared__ float sdata[256]; // Assuming blockDim.x <= 256

    float sum = 0.0f;
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        float val = x[row * dim + i];
        sum += val * val;
    }

    sdata[threadIdx.x] = sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float norm = sqrtf(sdata[0]);
        sdata[0] = norm;
    }
    __syncthreads();

    float norm = sdata[0];
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        out[row * dim + i] = x[row * dim + i] / norm;
    }
}

torch::Tensor l2_norm_cuda(torch::Tensor x) {
    int batch_size = x.size(0);
    int dim = x.size(1);

    auto out = torch::empty_like(x);

    const int block_size = 256;
    const dim3 grid(batch_size);
    const dim3 block(block_size);

    // Launch the kernel
    l2_norm_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), batch_size, dim);

    return out;
}
"""

Then, the header declaration:

l2_norm_cpp_source = (
    "torch::Tensor l2_norm_cuda(torch::Tensor x);"
)

Then, compile the code:

l2_norm = load_inline(
    name="l2_norm",
    cpp_sources=l2_norm_cpp_source,
    cuda_sources=l2_norm_source,
    functions=["l2_norm_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Then, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.l2_norm = l2_norm

    def forward(self, x):
        return self.l2_norm.l2_norm_cuda(x)

Wait, but the function is named l2_norm_cuda, so:

return self.l2_norm.l2_norm_cuda(x)

But in the load_inline, the functions parameter is ["l2_norm_cuda"], so that's correct.

However, in the example provided, the module was stored as self.elementwise_add, and then called with self.elementwise_add.elementwise_add_cuda(a, b). So similarly here, the module (l2_norm) has the function l2_norm_cuda.

Therefore, the forward function should be:

return self.l2_norm.l2_norm_cuda(x)

Now, the full code.

But also, the input and output must be on the GPU. The original code's get_inputs() returns tensors on CPU, but in the problem description, the original code has get_inputs() as:

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

Wait, in the given code, the original Model's get_inputs() function generates tensors on the CPU. But in the example optimization, the example used .cuda().

Wait, looking back at the original problem's given architecture:

The original Model's get_inputs is:

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

But in the example optimization, the original code had inputs generated on the CPU, but the optimized code had inputs on the GPU.

Wait, the user's problem says that the original code may have inputs on CPU, but when testing the optimized code, inputs should be on the GPU.

Wait, the problem says "You may replace multiple operators with custom implementations, consider operator fusion opportunities...", but in the example, the inputs were moved to CUDA.

Therefore, in the optimized code, the inputs must be on the GPU. Hence, in the new code's get_inputs() function, the inputs should be on the GPU. But in the problem's given code, the original get_inputs() does not use .cuda(). However, in the user's problem, the user might have expected that the inputs are on the GPU. Alternatively, perhaps the user expects that the code will handle it.

Wait, in the problem's given architecture, the get_inputs() function is:

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

But in the example provided by the user, the example had:

def get_inputs():
    a = torch.randn(1, 128).cuda()
    b = torch.randn(1, 128).cuda()
    return [a, b]

Hence, the user's problem may expect that the inputs are on the GPU. However, in the original code's get_inputs() function, the tensors are on the CPU. So in the optimized code, perhaps we should generate the inputs on the GPU.

But the problem says "ModelNew must have the same input/output as the original Model". Since the original Model's inputs are CPU tensors, but when using the optimized code, the inputs would have to be on the GPU. This is a problem.

Wait, but the user's example had the original code with inputs on CPU, but the optimized code moved them to GPU. So perhaps the user expects that the inputs are on the GPU, so the ModelNew's forward function can take GPU tensors.

Therefore, in the problem's original code, the get_inputs() function is probably intended to be on CPU, but in the optimized code, the inputs must be on GPU. However, to ensure that the ModelNew's inputs and outputs match the original, we must have the inputs and outputs as tensors, but the original code may have been run on CPU.

Wait, the problem says "ModelNew must have the same input/output as the original Model". Therefore, if the original model's inputs are on CPU, then the new model must accept CPU tensors. But the optimized code would be on GPU.

This is a contradiction.

Wait, perhaps the user assumes that the model is run on GPU, so the inputs are on the GPU. Let me check the problem's original code.

Looking at the given architecture:

The problem says:

When writing kernels, consider the following tips:

You are given the following architecture:

The code includes:

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []

There is no .cuda() call here, so the inputs are on CPU. However, the optimization example had inputs on GPU. Since the user wants the optimized code to replace operators with custom CUDA kernels, the inputs must be on the GPU for the kernels to run.

Therefore, the user probably expects that the inputs are on the GPU. Therefore, in the ModelNew, the forward function will take GPU tensors, and the inputs generated by get_inputs() should be on the GPU.

Therefore, the correct approach is to modify the get_inputs() function to return tensors on the GPU. But the problem says "do not output testing code", so perhaps the user expects that the code is correct in terms of inputs being on GPU, but the get_inputs() is part of the architecture.

Wait, the problem says:

The user's original code includes get_inputs() which returns tensors on CPU. The user's problem says "Output the new code in codeblocks in markdown format (i.e. ```python or ```cpp). Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Do not output testing code."

Therefore, the ModelNew must have the same input/output as the original Model, which includes inputs on CPU. But the custom CUDA kernel requires inputs on GPU.

This is a problem. Therefore, the solution must have the inputs on GPU. But the original Model's inputs are on CPU, so to make ModelNew compatible, the forward function must handle tensors on CPU, but the kernel requires GPU.

Therefore, the code must move the inputs to GPU in the forward function.

Alternatively, the user might have intended that the inputs are on GPU, so the code can assume that. Alternatively, perhaps the user expects that the get_inputs() function is not part of the model code, and the model's forward function can accept GPU tensors.

The problem states that the original architecture includes the get_inputs() function, but the user is to write the ModelNew class, which must have the same inputs and outputs. The inputs must be the same (CPU or GPU?), but the code must run the kernel on the GPU, so the inputs must be on GPU.

Therefore, the code should include a step to move the input tensor to the GPU in the forward function, but that would change the input requirements.

Alternatively, perhaps the user's original code is intended to be run on GPU, and the get_inputs() function should be on GPU. But in the given code, it's not.

Therefore, perhaps the problem expects that the inputs are on the GPU, so in the ModelNew's forward function, the input x is on the GPU, and the code can proceed.

Therefore, the code will assume that the input x is on the GPU. The get_inputs() function in the original code should have x.cuda(), but since the user says not to output testing code, perhaps we can proceed.

Therefore, the code for ModelNew is as above.

Now, checking for possible issues.

First, the shared memory allocation: the kernel uses sdata[256], but if the block size is 256, then it's okay. The block size is fixed to 256.

Second, the reduction in shared memory: the code uses blockDim.x as the initial size. The reduction steps are correct.

Third, handling of very large dimensions (like 65535). Each thread processes dim / 256 elements, which is about 256 elements per thread. That should be manageable.

Fourth, the kernel's grid is set to batch_size, which is 32768. That's a large number of blocks. CUDA allows up to 2^31 blocks, so 32768 is okay.

Potential issues:

- The shared memory size: 256 floats per block. Each block needs 256 * 4 bytes = 1KB of shared memory, which is acceptable.

- The division by the norm: if the norm is zero, but since the input is random (as per get_inputs()), it's unlikely. However, in practice, the norm can be zero if all elements are zero. In such a case, the division would be undefined. To handle this, perhaps add a check:

if (norm > 0) ... else set to zero?

But the problem's original code would also have division by zero, so the kernel can proceed as is.

Now, compiling the code.

The load_inline function is used, which compiles the CUDA code.

The functions parameter is ["l2_norm_cuda"], so the function must be declared.

The code should work.

Therefore, the complete code is:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

l2_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void l2_norm_kernel(const float* x, float* out, int batch_size, int dim) {
    int row = blockIdx.x;
    if (row >= batch_size) return;

    __shared__ float sdata[256]; // Assuming blockDim.x is 256

    float sum = 0.0f;
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        float val = x[row * dim + i];
        sum += val * val;
    }

    sdata[threadIdx.x] = sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float norm = sqrtf(sdata[0]);
        sdata[0] = norm;
    }
    __syncthreads();

    float norm = sdata[0];
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        out[row * dim + i] = x[row * dim + i] / norm;
    }
}

torch::Tensor l2_norm_cuda(torch::Tensor x) {
    int batch_size = x.size(0);
    int dim = x.size(1);

    auto out = torch::empty_like(x);

    const int block_size = 256;
    const dim3 grid(batch_size);
    const dim3 block(block_size);

    l2_norm_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), batch_size, dim);

    return out;
}
"""

l2_norm_cpp_source = (
    "torch::Tensor l2_norm_cuda(torch::Tensor x);"
)

# Compile the inline CUDA code
l2_norm = load_inline(
    name="l2_norm",
    cpp_sources=l2_norm_cpp_source,
    cuda_sources=l2_norm_source,
    functions=["l2_norm_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.l2_norm = l2_norm

    def forward(self, x):
        return self.l2_norm.l2_norm_cuda(x)

batch_size = 32768
dim = 65535

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()
    return [x]

def get_init_inputs():
    return []
```

Wait, but the original code's get_inputs() does not have .cuda(). To make the ModelNew compatible with the original's inputs (which are CPU tensors), the forward function would need to move the input to GPU. However, the problem states that the inputs and outputs must be the same as the original. Hence, the original's forward function takes CPU tensors, but the custom kernel requires GPU tensors.

This is a problem.

Therefore, the correct approach is to have the input x be on the GPU in the forward function.

Hence, the get_inputs() function should be modified to return GPU tensors. However, the problem says to not output testing code, so perhaps the user expects that the code assumes inputs are on the GPU. Therefore, the get_inputs() function in the problem's original code should have .cuda().

But in the problem's given architecture, the get_inputs() is:

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

Hence, the generated code should have the ModelNew's forward function accept CPU tensors and move them to GPU internally? But that would change the input/output contract.

Alternatively, perhaps the original model's forward function is on CPU, but the optimized code is supposed to run on GPU, so the inputs must be on GPU.

The problem states that "the ModelNew must have the same input/output as the original Model". Therefore, if the original model takes CPU tensors, the new model must also take CPU tensors, but the kernel must process them on GPU. Thus, the kernel must handle the data transfer implicitly, but that complicates things.

Alternatively, perhaps the user expects that the code is run on GPU, so the inputs are on GPU. Thus, the get_inputs() in the problem's code should have .cuda(), but since the problem provided the code without it, perhaps the user expects that the code assumes inputs are on GPU.

Therefore, the code is written with the assumption that the input x is on the GPU. Hence, in the provided code, the get_inputs() function in the problem's original code must be modified to include .cuda().

However, in the problem's instruction, the user says:

"Output the new code in codeblocks in markdown format (i.e. ```python or ```cpp). Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Do not output testing code."

Therefore, the code must include the get_inputs() function as part of the architecture, but the user's original code's get_inputs() is given without .cuda(). To adhere to the requirement that ModelNew must have the same input/output as original Model, the inputs must remain on CPU. But then the custom CUDA kernel would require moving the tensor to GPU, which would change the input.

This is a contradiction. Therefore, the only way is to assume that the inputs are on GPU, and thus the get_inputs() must be modified. Since the user says not to output testing code, but the code must include get_inputs(), then the code must include the .cuda() call in get_inputs().

Hence, in the generated code, the get_inputs() function should have .cuda().

Therefore, the final code will have the get_inputs() function with .cuda().

Hence, the final code is as above, with get_inputs() modified to include .cuda().

So the final code is as written above, with the get_inputs() returning a GPU tensor.

Therefore, the answer is the code above.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

l2_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void l2_norm_kernel(const float* x, float* out, int batch_size, int dim) {
    int row = blockIdx.x;
    if (row >= batch_size) return;

    __shared__ float sdata[256];

    float sum = 0.0f;
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        float val = x[row * dim + i];
        sum += val * val;
    }

    sdata[threadIdx.x] = sum;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float norm = sqrtf(sdata[0]);
        sdata[0] = norm;
    }
    __syncthreads();

    float norm = sdata[0];
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        out[row * dim + i] = x[row * dim + i] / norm;
    }
}

torch::Tensor l2_norm_cuda(torch::Tensor x) {
    int batch_size = x.size(0);
    int dim = x.size(1);

    auto out = torch::empty_like(x);

    const int block_size = 256;
    const dim3 grid(batch_size);
    const dim3 block(block_size);

    l2_norm_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), batch_size, dim);

    return out;
}
"""

l2_norm_cpp_source = (
    "torch::Tensor l2_norm_cuda(torch::Tensor x);"
)

l2_norm = load_inline(
    name="l2_norm",
    cpp_sources=l2_norm_cpp_source,
    cuda_sources=l2_norm_source,
    functions=["l2_norm_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.l2_norm = l2_norm

    def forward(self, x):
        return self.l2_norm.l2_norm_cuda(x)

batch_size = 32768
dim = 65535

def get_inputs():
    x = torch.rand(batch_size, dim).cuda()
    return [x]

def get_init_inputs():
    return []
```