The code should be a drop-in replacement for the original Model class, meaning it has the same __init__ and forward method signature, and the same get_inputs and get_init_inputs functions. The new ModelNew class must take the same parameters as the original Model class. 

Also, you can add helper functions and custom CUDA operators. The goal is to make the forward pass as fast as possible while maintaining correctness. 

The inputs are all tensors, and the outputs should be a tensor of loss values. The current implementation uses the standard TripletMarginLoss from PyTorch. You can choose to replace parts of this or the entire operator with custom CUDA kernels. 

The batch_size is 32768 and input_shape is (8192,). The dim is 1. So the inputs are three tensors of shape (32768, 8192). The output is a scalar loss value.

The problem is to find the fastest possible implementation of the forward pass for this model, given the constraints.

The TripletMarginLoss function computes the loss as follows:

loss(x, y) = max(0, ||x - y||^2 - ||x - z||^2 + margin)^0.5

Wait, actually, the actual formula for TripletMarginLoss is:

The Triplet Margin Loss is defined as:

\[
\text{loss}(p, n) = \max(0, \text{d}(a, p) - \text{d}(a, n) + \text{margin})
\]

where \( \text{d}(a, p) \) is the distance between the anchor and positive sample, and \( \text{d}(a, n) \) is the distance between the anchor and negative sample.

In PyTorch's implementation, the default distance is the L2 distance. 

The formula in PyTorch's TripletMarginLoss with the default distance (L2) is:

\[
\text{loss}(a, p, n) = \max(0, \lVert a - p \rVert^2 - \lVert a - n \rVert^2 + \text{margin})^0.5
\]

Wait, no. Wait, let me check the PyTorch documentation again.

Wait, the actual PyTorch TripletMarginLoss documentation says:

The loss is calculated as:

\[
\text{loss}(a, p, n) = \max\{d(a, p) - d(a, n) + \text{margin}, 0\}
\]

where \( d(x_i, x_j) \) is the pairwise distance between embeddings \( x_i \) and \( x_j \). 

The default distance is the L2 distance, so \( d(x_i, x_j) = \lVert x_i - x_j \rVert_2 \).

Thus, the formula is:

loss = max( d(a,p) - d(a,n) + margin, 0 )

The final loss is the mean over all samples.

Wait, according to PyTorch's documentation:

TripletMarginLoss creates a criterion that measures the triplet loss given an input tensors \( x1 \), \( x2 \), \( x3 \), where \( x1 \) is an anchor input tensor, \( x2 \) a positive input tensor, and \( x3 \) a negative input tensor. The shapes of all input tensors should be \( (N, D) \), where N is the batch size and D is the feature dimension.

The loss function is:

\[
\text{loss}(x1, x2, x3) = \frac{1}{N} \sum_{i=1}^N \max\{0, \text{d}(x1_i, x2_i) - \text{d}(x1_i, x3_i) + \text{margin}\}
\]

where \( d(x_i, y_i) \) is the distance function used.

The default distance is the L2 distance.

So, the steps to compute this loss are:

1. Compute the pairwise distances between each triplet (anchor, positive) and (anchor, negative).

2. For each triplet, compute the term (d(ap) - d(an) + margin). If this is positive, it contributes to the loss; else, it contributes 0.

3. Take the mean over all the samples.

Now, in order to optimize this, we need to compute these distances efficiently, perhaps by fusing operations and avoiding unnecessary computations.

The PyTorch implementation may have some overhead, especially for large batch sizes (32768) and high-dimensional data (8192). So, we can try to write a custom CUDA kernel to compute the loss in a single kernel, fusing the distance calculations and the max(0, ...) operation, which could lead to better performance.

Let me think about the steps involved in the computation:

First, compute d(ap) = ||a - p||_2.

Similarly, d(an) = ||a - n||_2.

Then compute the term (d(ap) - d(an) + margin).

Take the max with 0, and then average over all samples.

The computation steps can be broken down into:

1. Compute the difference between a and p: delta_ap = a - p.

2. Compute the squared L2 norm of delta_ap: norm_sq_ap = delta_ap * delta_ap summed over the feature dimension.

3. Take the square root to get the L2 norm: d_ap = sqrt(norm_sq_ap).

Similarly for delta_an = a - n, compute d_an = sqrt( (a - n)^2 summed over features ).

Alternatively, perhaps compute squared L2 distance first, but since the distance is L2, we need sqrt, so perhaps not.

Wait, actually, the distance is L2, so d(ap) is the Euclidean distance.

Thus, the steps are:

For each element in the batch (each sample in the batch):

Compute:

d_ap = sqrt( sum_{d=1}^D (a_i[d] - p_i[d])^2 )

d_an = sqrt( sum_{d=1}^D (a_i[d] - n_i[d])^2 )

term = d_ap - d_an + margin

loss_i = max(term, 0.0)

The overall loss is the mean of loss_i over all samples.

This is the computation required.

Now, in order to optimize this with CUDA kernels, perhaps we can compute all of these steps in a single kernel, which would reduce memory traffic and kernel launch overhead.

Let me consider the data shapes:

The inputs are three tensors of shape (32768, 8192). Each element in the batch is a vector of 8192 elements.

The operations needed for each element in the batch are:

For each sample i in 0..32767:

Compute delta_ap[i][d] = a[i][d] - p[i][d], for d in 0..8191

Compute the squared norm of delta_ap[i], then take sqrt to get d_ap[i]

Same for delta_an[i][d] = a[i][d] - n[i][d], then d_an[i]

Then compute term = d_ap[i] - d_an[i] + margin

loss_i[i] = max(term, 0.0)

The total loss is the mean of loss_i[i] over all i.

The problem is that the computation of the norms involves square roots, which are relatively expensive operations. However, in the loss formula, the sqrt is necessary because the distance is L2.

Alternatively, perhaps we can find a way to compute this without explicitly taking the square root, but I don't think so. Since the distance is L2, the sqrt is required.

Alternatively, if we can compute d_ap^2 - d_an^2 + margin^2 or something? Wait, no. The formula is d_ap - d_an + margin. So the sqrt is necessary.

Hmm. So the plan is to write a CUDA kernel that, for each sample in the batch, computes the two distances, then the term, and accumulates the loss.

However, since each sample is independent, this can be done in parallel across threads.

Each thread can process one sample (i.e., one row in the batch). Since the batch size is 32768, which is 2^15, we can have one thread per sample.

The steps for each thread would be:

1. Load the vectors a[i], p[i], n[i] (each of length 8192).

Wait, but the vectors are 8192 elements long. Loading them into registers might not be feasible. So, perhaps we need to process each element of the vector in a loop.

Wait, let me think of the data layout.

Each sample is a vector of 8192 elements. The three tensors (a, p, n) are each of shape (32768, 8192). The batch is along the first dimension.

So, for a single sample i, the vectors a[i], p[i], n[i] are of length 8192.

Each thread would handle one sample (i), and compute the norm for that sample's vectors.

The kernel would be structured as follows:

- Each thread corresponds to a sample (i).

- For each thread, loop over the 8192 elements in the vectors to compute the squared differences.

Wait, but with 8192 elements, this would be a loop of 8192 iterations per thread. That could be slow because of the high number of iterations and the loop overhead. So perhaps a better approach is to vectorize the computation.

Alternatively, since each element is independent, perhaps the computation can be parallelized further. But given that each thread is handling a single sample's worth of data, which is a 1D array, perhaps a better approach is to have each thread compute the norm for a sample by looping over the elements in the vector.

Wait, but 8192 elements is a lot. That would be 8k iterations per thread. That could take a while, and the loop could be slow. Alternatively, we can process the vector in chunks, using multiple threads per sample. But that complicates the kernel structure.

Alternatively, since each sample is independent, and the batch size is 32768, which is a large number, but the vectors are 8192 elements long, perhaps the optimal way is to have each thread process a single element in the vector of a single sample. But that would require a grid size of 32768 * 8192, which is 268,435,456 threads, which is way too big (the maximum number of threads per block and grid is limited).

Alternatively, use a 2D grid. For example, have a grid where each block handles a sample, and each thread in the block handles a part of the vector.

Wait, let's think of the problem as follows:

Each sample (i) has a vector of length D=8192.

To compute the norm of (a[i] - p[i]), we need to compute the sum over d=0 to D-1 of (a[i][d] - p[i][d])^2, then take sqrt.

Similarly for (a[i] - n[i]).

Thus, for each sample, we need to compute two sums (the squared differences for each vector), then take square roots, compute the term, and then the max.

The key operations are the summation over D elements for each sample's two vectors. So, for each sample, we have to compute two reductions over D elements.

This is a classic reduction problem. For a single sample's vector, the computation is a reduction over D elements.

Now, to perform this efficiently on the GPU, we can structure the kernel such that each block handles a single sample's vectors, and the threads in the block process the elements in parallel.

Wait, here's a possible approach:

We can structure the kernel so that each block corresponds to a sample (i). Each block will have multiple threads, say 256 threads. The block's threads can process chunks of the D elements in parallel.

The steps for a block:

1. For sample i, read the vectors a[i], p[i], n[i].

2. Compute the differences delta_ap = a[i] - p[i], delta_an = a[i] - n[i].

3. Compute the squared differences for each element in delta_ap and delta_an.

4. Sum the squared differences for delta_ap to get norm_sq_ap.

5. Sum the squared differences for delta_an to get norm_sq_an.

6. Compute d_ap = sqrt(norm_sq_ap), d_an = sqrt(norm_sq_an).

7. Compute term = d_ap - d_an + margin.

8. loss_i = max(term, 0.0).

Each of these steps can be done in parallel by the threads in the block.

However, the summation (steps 4 and 5) are reductions. Implementing a reduction on the GPU is standard.

Alternatively, since each block is handling a single sample, and the sample has D elements, which is 8192, which is 8k, which is a large number, we can split the work among threads in a block.

First, each block is assigned to a sample. Each block has, say, 256 threads. The number of elements per thread would be 8192 / 256 = 32 elements per thread.

Each thread in the block processes 32 elements of the vector for that sample. The thread computes the sum of the squared differences for its 32 elements, then participates in a block-wide reduction to compute the total sum.

Wait, that might be manageable.

Here's a more detailed plan:

For a single sample i:

- The block has B threads (e.g., 256).

- Each thread processes (D / B) elements (assuming D is divisible by B).

- For each element in its assigned range, compute (a[i][d] - p[i][d])^2 and accumulate into a per-thread partial sum.

- Do the same for (a[i][d] - n[i][d])^2.

- Then, perform a reduction within the block to get the total norm_sq_ap and norm_sq_an.

- Then compute the distance, the term, and the loss for that sample.

The reduction within the block can be done using a parallel reduction algorithm (like the one described in CUDA documentation).

This way, each block handles one sample, and the kernel launches 32768 blocks, each with 256 threads.

But 32768 blocks might be manageable since each block is doing a lot of work. The total number of threads would be 32768 * 256 = ~8 million threads, which is within the limits.

Alternatively, perhaps use more threads per block if D is large. Wait, 256 threads per block is manageable, and 8192 elements divided by 256 threads gives 32 elements per thread, which is feasible.

Another consideration: the input tensors are of shape (32768, 8192). So each tensor is stored in row-major order, so each row is contiguous in memory. Therefore, accessing the elements of a[i][d], p[i][d], n[i][d] can be done efficiently by loading the pointers to each row.

Wait, but for a block handling sample i, it needs to access the ith row of a, p, and n. So each block would need to have the pointers to the rows of a[i], p[i], n[i].

Therefore, the kernel would take the three tensors as inputs, along with the margin.

The kernel signature would be something like:

__global__ void triplet_loss_kernel(
    const float* a_data,
    const float* p_data,
    const float* n_data,
    float margin,
    float* loss_out,
    int batch_size,
    int dim
) {

    // Each block handles a sample (i)
    int i = blockIdx.x;

    if (i >= batch_size) return;

    // Compute for sample i:

    // Compute delta_ap squared differences and sum
    float norm_sq_ap = 0.0f;
    float norm_sq_an = 0.0f;

    // Each thread in the block handles a portion of the dim elements.

    // Compute the thread index within the block
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    // Each thread processes dim / num_threads elements (approx)
    for (int d = tid; d < dim; d += num_threads) {
        float a_val = a_data[i * dim + d];
        float p_val = p_data[i * dim + d];
        float n_val = n_data[i * dim + d];

        float diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        float diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    // Perform reduction within the block to get the total norm_sq_ap and norm_sq_an
    // First, accumulate partial sums across threads in the block

    // Use shared memory for reduction
    extern __shared__ float shared[];

    float* s_norm_sq_ap = shared;
    float* s_norm_sq_an = shared + blockDim.x;

    // Each thread writes its partial sum to shared memory
    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    // Perform reduction for norm_sq_ap
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    // The final sum is in s_norm_sq_ap[0] and s_norm_sq_an[0]
    if (tid == 0) {
        float d_ap = sqrt(s_norm_sq_ap[0]);
        float d_an = sqrt(s_norm_sq_an[0]);

        float term = d_ap - d_an + margin;
        float loss_i = fmaxf(term, 0.0f);

        atomicAdd(loss_out, loss_i);
    }
}

Wait, but in this case, each block (sample) will compute its loss_i and then we need to sum all loss_i and divide by batch_size.

Wait, but in the code above, all the loss_i are being added to a single output pointer (loss_out). So loss_out would be a single float where all the contributions are accumulated via atomicAdd. But atomic operations can be slow, so this might not be efficient.

Alternatively, perhaps we can have each block compute its loss_i and write it to a per-block array, then have another kernel or a reduction step to compute the total.

Wait, here's an alternative approach:

Instead of accumulating into a single float, each block can write its loss_i to an array (e.g., an array of length batch_size), and then we can compute the mean of that array.

So, let's adjust the kernel to write each loss_i to a per-sample array, then compute the mean.

Modify the kernel to have an output array:

__global__ void triplet_loss_kernel(
    const float* a_data,
    const float* p_data,
    const float* n_data,
    float margin,
    float* loss_per_sample,
    int batch_size,
    int dim
) {

    // Each block handles a sample (i)
    int i = blockIdx.x;

    if (i >= batch_size) return;

    // Compute for sample i:

    float norm_sq_ap = 0.0f;
    float norm_sq_an = 0.0f;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int d = tid; d < dim; d += num_threads) {
        float a_val = a_data[i * dim + d];
        float p_val = p_data[i * dim + d];
        float n_val = n_data[i * dim + d];

        float diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        float diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    extern __shared__ float shared[];
    float* s_norm_sq_ap = shared;
    float* s_norm_sq_an = shared + blockDim.x;

    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float d_ap = sqrt(s_norm_sq_ap[0]);
        float d_an = sqrt(s_norm_sq_an[0]);

        float term = d_ap - d_an + margin;
        float loss_i = fmaxf(term, 0.0f);

        loss_per_sample[i] = loss_i;
    }
}

Then, after running this kernel, we have an array loss_per_sample of size batch_size. The final loss is the mean of this array.

To compute the mean, we can use a reduction kernel on the loss_per_sample array.

Alternatively, we can use atomicAdd to accumulate the sum, then divide by batch_size.

But for the purpose of optimizing, perhaps using the reduction kernel is better, but let's see.

The kernel for the reduction:

__global__ void sum_loss_kernel(float* loss_per_sample, float* total_loss, int n) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + tid;
    if (i < n) {
        sdata[tid] = loss_per_sample[i];
    } else {
        sdata[tid] = 0.0f;
    }
    __syncthreads();

    // Perform reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(total_loss, sdata[0]);
    }
}

But this requires another kernel launch, which may add overhead.

Alternatively, in the first kernel, instead of writing to loss_per_sample, we can compute the total sum directly with atomicAdd.

But atomicAdd on a single float can be a bottleneck due to contention. For 32768 samples, each performing an atomicAdd, this could be slow.

Hmm, perhaps a better approach is to have each block compute its loss_i and store it in an array, then compute the sum of the array using a built-in function like thrust::reduce, but since we are writing CUDA kernels, perhaps we can use a reduction kernel.

Alternatively, since the batch is 32768, and each sample is independent, the first kernel can store each loss_i in an array, and then we can use a second kernel to compute the sum.

This may be manageable.

So, the plan for the CUDA code is:

- Launch a kernel to compute loss_per_sample for each sample.

- Then, compute the sum of loss_per_sample and divide by batch_size to get the final loss.

The first kernel's shared memory usage must be considered. The shared memory for the first kernel:

Each block needs to store two arrays (for norm_sq_ap and norm_sq_an) of size blockDim.x each. So total shared memory per block is 2 * blockDim.x * sizeof(float). If using 256 threads per block, that's 256 * 2 * 4 bytes = 2048 bytes per block, which is acceptable.

Now, the kernel launch parameters:

For the first kernel:

blockDim = 256 threads per block.

gridDim = batch_size (32768).

The shared memory per block is 2 * blockDim.x floats, so 2 * 256 = 512 floats. Since each float is 4 bytes, that's 2048 bytes per block, which is within the device's shared memory limit (typically 48 or 96KB per SM).

But let me check the parameters:

dim = 8192.

Each thread in the block processes dim / blockDim.x elements. For dim=8192 and blockDim=256, each thread processes 32 elements (since 8192 / 256 = 32).

So the loop over d is manageable.

Now, for the kernel parameters:

The first kernel requires:

- a_data: pointer to the a tensor (shape batch_size x dim). The storage is in row-major order.

Same for p_data and n_data.

The loss_per_sample array is of size batch_size, and each block writes to loss_per_sample[i] where i is blockIdx.x.

The kernel requires that the input tensors are contiguous in memory. Assuming that the input tensors are passed as contiguous tensors, this should be okay.

Now, let's write the full code in Python with the CUDA kernel.

First, define the CUDA code for the first kernel (computing loss_per_sample):

triplet_loss_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void triplet_loss_kernel(
    const scalar_t* __restrict__ a_data,
    const scalar_t* __restrict__ p_data,
    const scalar_t* __restrict__ n_data,
    scalar_t margin,
    scalar_t* loss_per_sample,
    int batch_size,
    int dim
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    scalar_t norm_sq_ap = 0.0;
    scalar_t norm_sq_an = 0.0;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int d = tid; d < dim; d += num_threads) {
        scalar_t a_val = a_data[i * dim + d];
        scalar_t p_val = p_data[i * dim + d];
        scalar_t n_val = n_data[i * dim + d];

        scalar_t diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        scalar_t diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    extern __shared__ scalar_t shared[];
    scalar_t* s_norm_sq_ap = shared;
    scalar_t* s_norm_sq_an = shared + blockDim.x;

    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    // Reduction step for norm_sq_ap and norm_sq_an
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        scalar_t d_ap = sqrt(s_norm_sq_ap[0]);
        scalar_t d_an = sqrt(s_norm_sq_an[0]);

        scalar_t term = d_ap - d_an + margin;
        scalar_t loss_i = (term > 0.0) ? term : 0.0;

        loss_per_sample[i] = loss_i;
    }
}

// Host function to launch the kernel
at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
) {
    const int batch_size = a.size(0);
    const int dim = a.size(1);

    auto loss_per_sample = at::empty({batch_size}, a.options());
    auto total_loss = at::zeros({1}, a.options());

    // Kernel 1: Compute loss_per_sample
    int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);

    // Shared memory size per block: 2 * blockDim.x * sizeof(float)
    size_t shared_size = 2 * block_size * sizeof(float);

    triplet_loss_kernel<float><<<grid, block, shared_size, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(),
        p.data_ptr<float>(),
        n.data_ptr<float>(),
        margin,
        loss_per_sample.data_ptr<float>(),
        batch_size,
        dim
    );

    // Kernel 2: Compute sum of loss_per_sample
    auto sum_loss = at::sum(loss_per_sample);
    auto avg_loss = sum_loss / batch_size;

    return avg_loss;
}
"""

Wait, but in the code above, after computing loss_per_sample, the code uses PyTorch's at::sum function, which might launch another kernel. But the problem is that the current code is in CUDA, so perhaps we need to compute the sum using a CUDA kernel as well for better performance.

Alternatively, in the code, we can compute the sum using a reduction kernel, but that requires writing another CUDA kernel.

Alternatively, perhaps using the atomicAdd approach:

Modify the first kernel to accumulate the total_loss directly via atomicAdd.

Wait, let's adjust the kernel:

Modify the first kernel to compute the total loss directly with atomicAdd.

But this may have contention issues with the atomicAdd, but for 32768 threads, each doing an atomicAdd on a single float, the contention may be manageable? Let me think.

Wait, in the first approach, each block (sample) computes loss_i and then adds it to the total_loss using atomicAdd. The total_loss is a single float.

The number of atomic operations is 32768, which may be okay because each atomicAdd is on a single address. However, atomicAdd is an atomic operation and can have some latency, but given that there are only 32768 operations, it might be manageable.

Let me adjust the kernel accordingly.

Modified kernel code:

triplet_loss_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void triplet_loss_kernel(
    const scalar_t* __restrict__ a_data,
    const scalar_t* __restrict__ p_data,
    const scalar_t* __restrict__ n_data,
    scalar_t margin,
    scalar_t* total_loss,
    int batch_size,
    int dim
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    scalar_t norm_sq_ap = 0.0;
    scalar_t norm_sq_an = 0.0;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int d = tid; d < dim; d += num_threads) {
        scalar_t a_val = a_data[i * dim + d];
        scalar_t p_val = p_data[i * dim + d];
        scalar_t n_val = n_data[i * dim + d];

        scalar_t diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        scalar_t diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    extern __shared__ scalar_t shared[];
    scalar_t* s_norm_sq_ap = shared;
    scalar_t* s_norm_sq_an = shared + blockDim.x;

    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        scalar_t d_ap = sqrt(s_norm_sq_ap[0]);
        scalar_t d_an = sqrt(s_norm_sq_an[0]);

        scalar_t term = d_ap - d_an + margin;
        scalar_t loss_i = (term > 0.0) ? term : 0.0;

        atomicAdd(total_loss, loss_i);
    }
}

at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
) {
    const int batch_size = a.size(0);
    const int dim = a.size(1);

    auto total_loss = at::zeros({1}, a.options()).cuda();

    int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    size_t shared_size = 2 * block_size * sizeof(float);

    // Launch the kernel
    triplet_loss_kernel<float><<<grid, block, shared_size, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(),
        p.data_ptr<float>(),
        n.data_ptr<float>(),
        margin,
        total_loss.data_ptr<float>(),
        batch_size,
        dim
    );

    // Divide by batch_size
    auto avg_loss = total_loss / static_cast<float>(batch_size);

    return avg_loss;
}
"""

This way, the kernel directly accumulates the total_loss via atomicAdd, and then the average is computed on the host side by dividing by batch_size.

The problem with this approach is that atomicAdd on a single float can have contention between all the blocks. Since all 32768 blocks are writing to the same memory location, the atomic operations might serialize the accesses, leading to significant slowdown.

To reduce contention, perhaps we can use a larger block size or use a reduction tree approach.

Alternatively, let's try this code first and see if it works. If the atomicAdd is too slow, perhaps we can optimize further.

Alternatively, use a per-block storage and then do a reduction on those.

But let's proceed with this code and see.

Now, compiling this into the Python code:

We need to wrap this CUDA code into a PyTorch extension using load_inline.

First, the header for the CUDA code.

Wait, in the code above, the kernel is templated, but we need to specify the scalar_t as float.

Alternatively, maybe better to hardcode to float.

Alternatively, keep it as a template.

The code is written with templates, so in the host function, we can call it with float.

The code should be okay.

Now, the Python code:

We need to load the CUDA code.

First, define the CUDA source code as a string.

Then, compile it using load_inline.

The ModelNew class would replace the TripletMarginLoss with this custom kernel.

The original Model class uses the PyTorch's TripletMarginLoss, which is an nn.Module.

The new ModelNew class should not use that, but instead compute the loss via the custom CUDA kernel.

Thus, the new ModelNew class will have __init__ that takes the margin, and in forward, calls the CUDA kernel.

The code would look like this:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the CUDA kernel for the Triplet Loss
triplet_loss_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void triplet_loss_kernel(
    const scalar_t* __restrict__ a_data,
    const scalar_t* __restrict__ p_data,
    const scalar_t* __restrict__ n_data,
    scalar_t margin,
    scalar_t* total_loss,
    int batch_size,
    int dim
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    scalar_t norm_sq_ap = 0.0;
    scalar_t norm_sq_an = 0.0;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int d = tid; d < dim; d += num_threads) {
        scalar_t a_val = a_data[i * dim + d];
        scalar_t p_val = p_data[i * dim + d];
        scalar_t n_val = n_data[i * dim + d];

        scalar_t diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        scalar_t diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    extern __shared__ scalar_t shared[];
    scalar_t* s_norm_sq_ap = shared;
    scalar_t* s_norm_sq_an = shared + blockDim.x;

    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        scalar_t d_ap = sqrt(s_norm_sq_ap[0]);
        scalar_t d_an = sqrt(s_norm_sq_an[0]);

        scalar_t term = d_ap - d_an + margin;
        scalar_t loss_i = (term > 0.0) ? term : 0.0;

        atomicAdd(total_loss, loss_i);
    }
}

at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
) {
    const int batch_size = a.size(0);
    const int dim = a.size(1);

    auto total_loss = at::zeros({1}, a.options()).cuda();

    int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    size_t shared_size = 2 * block_size * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(a.scalar_type(), "triplet_loss_cuda", ([&] {
        triplet_loss_kernel<scalar_t><<<grid, block, shared_size, at::cuda::getCurrentCUDAStream()>>>(
            a.data_ptr<scalar_t>(),
            p.data_ptr<scalar_t>(),
            n.data_ptr<scalar_t>(),
            margin,
            total_loss.data_ptr<scalar_t>(),
            batch_size,
            dim
        );
    }));

    // Divide by batch_size
    auto avg_loss = total_loss / static_cast<float>(batch_size);

    return avg_loss;
}
"""

# The header declarations for the CUDA function
triplet_loss_header = """
at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
);
"""

# Load the CUDA code
triplet_loss_cuda = load_inline(
    name="triplet_loss_cuda",
    cpp_sources=triplet_loss_header,
    cuda_sources=triplet_loss_source,
    functions=["triplet_loss_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin

    def forward(self, anchor, positive, negative):
        return triplet_loss_cuda.triplet_loss_cuda(anchor, positive, negative, self.margin)

def get_inputs():
    scale = torch.rand(())
    return [
        torch.rand(batch_size, *input_shape)*scale,
        torch.rand(batch_size, *input_shape),
        torch.rand(batch_size, *input_shape)
    ]

def get_init_inputs():
    return [1.0]  # Default margin
```

Wait, but the original code had input_shape as (8192,), so input_shape is a tuple (8192,). So the tensors are of shape (32768, 8192).

Also, in the code above, the `dim` parameter in the kernel is the second dimension (8192), which is correct.

However, there are a few things to check:

1. The kernel uses a blockDim of 256. Since the batch size is 32768, the grid size is 32768 blocks, which should be okay as long as the device has enough SMs to handle that many blocks.

2. The use of AT_DISPATCH_FLOATING_TYPES ensures that the kernel is called with the correct scalar type (e.g., float).

3. The atomicAdd is on a single float, which may have contention. If the batch size is very large (like 32768), this could be a problem. To reduce contention, perhaps we can use a larger block size, but the blockDim is already 256. Alternatively, use a per-block partial sum and then accumulate those.

Alternatively, let's try a different approach where the kernel computes per-block partial sums and then we do a second kernel to compute the total.

Let me consider modifying the kernel to store the partial sums in an array, then compute the total with a reduction.

Modified code:

First kernel computes loss_per_sample as before, but stores each sample's loss_i in an array.

Then, a second kernel computes the sum of loss_per_sample.

Alternatively, use thrust's reduction, but if we can't use thrust, we can write a reduction kernel.

Here's an alternative approach with two kernels:

First kernel computes loss_per_sample.

Second kernel computes the sum.

The first kernel is as before, but writes to loss_per_sample.

The second kernel is a reduction kernel to compute the total.

Here's the modified code:

triplet_loss_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void triplet_loss_kernel(
    const scalar_t* __restrict__ a_data,
    const scalar_t* __restrict__ p_data,
    const scalar_t* __restrict__ n_data,
    scalar_t margin,
    scalar_t* loss_per_sample,
    int batch_size,
    int dim
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    scalar_t norm_sq_ap = 0.0;
    scalar_t norm_sq_an = 0.0;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int d = tid; d < dim; d += num_threads) {
        scalar_t a_val = a_data[i * dim + d];
        scalar_t p_val = p_data[i * dim + d];
        scalar_t n_val = n_data[i * dim + d];

        scalar_t diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        scalar_t diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    extern __shared__ scalar_t shared[];
    scalar_t* s_norm_sq_ap = shared;
    scalar_t* s_norm_sq_an = shared + blockDim.x;

    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        scalar_t d_ap = sqrt(s_norm_sq_ap[0]);
        scalar_t d_an = sqrt(s_norm_sq_an[0]);

        scalar_t term = d_ap - d_an + margin;
        scalar_t loss_i = (term > 0.0) ? term : 0.0;

        loss_per_sample[i] = loss_i;
    }
}

template <typename scalar_t>
__global__ void sum_reduction_kernel(
    scalar_t* loss_per_sample,
    scalar_t* total_loss,
    int n,
    int block_size
) {
    extern __shared__ scalar_t shared[];
    int tid = threadIdx.x;
    int i = blockIdx.x * block_size + tid;
    scalar_t sum = 0.0;

    if (i < n) {
        sum = loss_per_sample[i];
    }

    // Load into shared memory
    shared[tid] = sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = block_size / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(total_loss, shared[0]);
    }
}

at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
) {
    const int batch_size = a.size(0);
    const int dim = a.size(1);

    auto loss_per_sample = at::empty({batch_size}, a.options()).cuda();
    auto total_loss = at::zeros({1}, a.options()).cuda();

    int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    size_t shared_size = 2 * block_size * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(a.scalar_type(), "triplet_loss_cuda", ([&] {
        triplet_loss_kernel<scalar_t><<<grid, block, shared_size, at::cuda::getCurrentCUDAStream()>>>(
            a.data_ptr<scalar_t>(),
            p.data_ptr<scalar_t>(),
            n.data_ptr<scalar_t>(),
            margin,
            loss_per_sample.data_ptr<scalar_t>(),
            batch_size,
            dim
        );
    }));

    // Reduction kernel to compute total loss
    int reduction_block_size = 256;
    int reduction_grid_size = (batch_size + reduction_block_size - 1) / reduction_block_size;
    dim3 reduction_grid(reduction_grid_size);
    dim3 reduction_block(reduction_block_size);
    size_t reduction_shared_size = reduction_block_size * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(a.scalar_type(), "sum_reduction_cuda", ([&] {
        sum_reduction_kernel<scalar_t><<<reduction_grid, reduction_block, reduction_shared_size, at::cuda::getCurrentCUDAStream()>>>(
            loss_per_sample.data_ptr<scalar_t>(),
            total_loss.data_ptr<scalar_t>(),
            batch_size,
            reduction_block_size
        );
    }));

    auto avg_loss = total_loss / static_cast<float>(batch_size);

    return avg_loss;
}
"""

This code uses two kernels: the first to compute loss_per_sample, and the second to compute the sum of loss_per_sample.

The reduction kernel (sum_reduction_kernel) is a standard parallel reduction.

This approach avoids using atomicAdd for the total loss, instead using a reduction kernel which should be more efficient.

Now, compiling this into the Python code:

The header for the functions:

triplet_loss_header = """
at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
);
"""

The rest is similar.

Now, the Python code would look like:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the CUDA kernel for the Triplet Loss
triplet_loss_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void triplet_loss_kernel(
    const scalar_t* __restrict__ a_data,
    const scalar_t* __restrict__ p_data,
    const scalar_t* __restrict__ n_data,
    scalar_t margin,
    scalar_t* loss_per_sample,
    int batch_size,
    int dim
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    scalar_t norm_sq_ap = 0.0;
    scalar_t norm_sq_an = 0.0;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int d = tid; d < dim; d += num_threads) {
        scalar_t a_val = a_data[i * dim + d];
        scalar_t p_val = p_data[i * dim + d];
        scalar_t n_val = n_data[i * dim + d];

        scalar_t diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        scalar_t diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    extern __shared__ scalar_t shared[];
    scalar_t* s_norm_sq_ap = shared;
    scalar_t* s_norm_sq_an = shared + blockDim.x;

    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        scalar_t d_ap = sqrt(s_norm_sq_ap[0]);
        scalar_t d_an = sqrt(s_norm_sq_an[0]);

        scalar_t term = d_ap - d_an + margin;
        scalar_t loss_i = (term > 0.0) ? term : 0.0;

        loss_per_sample[i] = loss_i;
    }
}

template <typename scalar_t>
__global__ void sum_reduction_kernel(
    scalar_t* loss_per_sample,
    scalar_t* total_loss,
    int n,
    int block_size
) {
    extern __shared__ scalar_t shared[];
    int tid = threadIdx.x;
    int i = blockIdx.x * block_size + tid;
    scalar_t sum = 0.0;

    if (i < n) {
        sum = loss_per_sample[i];
    }

    shared[tid] = sum;
    __syncthreads();

    for (int s = block_size / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(total_loss, shared[0]);
    }
}

at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
) {
    const int batch_size = a.size(0);
    const int dim = a.size(1);

    auto loss_per_sample = at::empty({batch_size}, a.options()).cuda();
    auto total_loss = at::zeros({1}, a.options()).cuda();

    int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    size_t shared_size = 2 * block_size * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(a.scalar_type(), "triplet_loss_cuda", ([&] {
        triplet_loss_kernel<scalar_t><<<grid, block, shared_size, at::cuda::getCurrentCUDAStream()>>>(
            a.data_ptr<scalar_t>(),
            p.data_ptr<scalar_t>(),
            n.data_ptr<scalar_t>(),
            margin,
            loss_per_sample.data_ptr<scalar_t>(),
            batch_size,
            dim
        );
    }));

    // Reduction kernel to compute total loss
    int reduction_block_size = 256;
    int reduction_grid_size = (batch_size + reduction_block_size - 1) / reduction_block_size;
    dim3 reduction_grid(reduction_grid_size);
    dim3 reduction_block(reduction_block_size);
    size_t reduction_shared_size = reduction_block_size * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(a.scalar_type(), "sum_reduction_cuda", ([&] {
        sum_reduction_kernel<scalar_t><<<reduction_grid, reduction_block, reduction_shared_size, at::cuda::getCurrentCUDAStream()>>>(
            loss_per_sample.data_ptr<scalar_t>(),
            total_loss.data_ptr<scalar_t>(),
            batch_size,
            reduction_block_size
        );
    }));

    auto avg_loss = total_loss / static_cast<float>(batch_size);

    return avg_loss;
}
"""

# The header declarations for the CUDA function
triplet_loss_header = """
at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
);
"""

# Load the CUDA code
triplet_loss_cuda = load_inline(
    name="triplet_loss_cuda",
    cpp_sources=triplet_loss_header,
    cuda_sources=triplet_loss_source,
    functions=["triplet_loss_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin

    def forward(self, anchor, positive, negative):
        return triplet_loss_cuda.triplet_loss_cuda(anchor, positive, negative, self.margin)

def get_inputs():
    scale = torch.rand(())
    return [
        torch.rand(batch_size, *input_shape)*scale,
        torch.rand(batch_size, *input_shape),
        torch.rand(batch_size, *input_shape)
    ]

def get_init_inputs():
    return [1.0]  # Default margin
```

This should be the correct implementation.

However, there's a possible issue with the sum_reduction_kernel. In the kernel, when loading into shared memory, the thread may read beyond the array if i >= n. To handle that, the initial 'sum' should be initialized to 0, and only add loss_per_sample[i] if i is within bounds.

The code currently sets sum = loss_per_sample[i] if i <n else 0, but in the code:

if (i < n) {
    sum = loss_per_sample[i];
} else {
    sum = 0.0;
}

Wait, in the code above, the code is:

scalar_t sum = 0.0;

if (i < n) {
    sum = loss_per_sample[i];
}

So that's correct.

Now, testing this code, but since we can't test here, we have to make sure it's correct.

Another possible optimization: The computation of sqrt can be a bottleneck. However, it's necessary for the distance.

Alternatively, if we can find a way to avoid the sqrt, but according to the formula, the distance is L2, so sqrt is needed.

Another optimization: The computation of delta_ap and delta_an can be done once.

Alternatively, in the kernel, perhaps compute the differences in a way that reuses computations.

But I think the current code is as optimized as possible.

Thus, this should be the final code.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

triplet_loss_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void triplet_loss_kernel(
    const scalar_t* __restrict__ a_data,
    const scalar_t* __restrict__ p_data,
    const scalar_t* __restrict__ n_data,
    scalar_t margin,
    scalar_t* loss_per_sample,
    int batch_size,
    int dim
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    scalar_t norm_sq_ap = 0.0;
    scalar_t norm_sq_an = 0.0;

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int d = tid; d < dim; d += num_threads) {
        scalar_t a_val = a_data[i * dim + d];
        scalar_t p_val = p_data[i * dim + d];
        scalar_t n_val = n_data[i * dim + d];

        scalar_t diff_ap = a_val - p_val;
        norm_sq_ap += diff_ap * diff_ap;

        scalar_t diff_an = a_val - n_val;
        norm_sq_an += diff_an * diff_an;
    }

    extern __shared__ scalar_t shared[];
    scalar_t* s_norm_sq_ap = shared;
    scalar_t* s_norm_sq_an = shared + blockDim.x;

    s_norm_sq_ap[tid] = norm_sq_ap;
    s_norm_sq_an[tid] = norm_sq_an;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_norm_sq_ap[tid] += s_norm_sq_ap[tid + s];
            s_norm_sq_an[tid] += s_norm_sq_an[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        scalar_t d_ap = sqrt(s_norm_sq_ap[0]);
        scalar_t d_an = sqrt(s_norm_sq_an[0]);

        scalar_t term = d_ap - d_an + margin;
        scalar_t loss_i = (term > 0.0) ? term : 0.0;

        loss_per_sample[i] = loss_i;
    }
}

template <typename scalar_t>
__global__ void sum_reduction_kernel(
    scalar_t* loss_per_sample,
    scalar_t* total_loss,
    int n,
    int block_size
) {
    extern __shared__ scalar_t shared[];
    int tid = threadIdx.x;
    int i = blockIdx.x * block_size + tid;
    scalar_t sum = 0.0;

    if (i < n) {
        sum = loss_per_sample[i];
    }

    shared[tid] = sum;
    __syncthreads();

    for (int s = block_size / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(total_loss, shared[0]);
    }
}

at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
) {
    const int batch_size = a.size(0);
    const int dim = a.size(1);

    auto loss_per_sample = at::empty({batch_size}, a.options()).cuda();
    auto total_loss = at::zeros({1}, a.options()).cuda();

    int block_size = 256;
    dim3 grid(batch_size);
    dim3 block(block_size);
    size_t shared_size = 2 * block_size * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(a.scalar_type(), "triplet_loss_cuda", ([&] {
        triplet_loss_kernel<scalar_t><<<grid, block, shared_size, at::cuda::getCurrentCUDAStream()>>>(
            a.data_ptr<scalar_t>(),
            p.data_ptr<scalar_t>(),
            n.data_ptr<scalar_t>(),
            margin,
            loss_per_sample.data_ptr<scalar_t>(),
            batch_size,
            dim
        );
    }));

    int reduction_block_size = 256;
    int reduction_grid_size = (batch_size + reduction_block_size - 1) / reduction_block_size;
    dim3 reduction_grid(reduction_grid_size);
    dim3 reduction_block(reduction_block_size);
    size_t reduction_shared_size = reduction_block_size * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(a.scalar_type(), "sum_reduction_cuda", ([&] {
        sum_reduction_kernel<scalar_t><<<reduction_grid, reduction_block, reduction_shared_size, at::cuda::getCurrentCUDAStream()>>>(
            loss_per_sample.data_ptr<scalar_t>(),
            total_loss.data_ptr<scalar_t>(),
            batch_size,
            reduction_block_size
        );
    }));

    auto avg_loss = total_loss / static_cast<float>(batch_size);

    return avg_loss;
}
"""

triplet_loss_header = """
at::Tensor triplet_loss_cuda(
    at::Tensor a,
    at::Tensor p,
    at::Tensor n,
    float margin
);
"""

triplet_loss_cuda = load_inline(
    name="triplet_loss_cuda",
    cpp_sources=triplet_loss_header,
    cuda_sources=triplet_loss_source,
    functions=["triplet_loss_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin

    def forward(self, anchor, positive, negative):
        return triplet_loss_cuda.triplet_loss_cuda(anchor, positive, negative, self.margin)

batch_size = 32768
input_shape = (8192,)

def get_inputs():
    scale = torch.rand(())
    return [
        torch.rand(batch_size, *input_shape)*scale,
        torch.rand(batch_size, *input_shape),
        torch.rand(batch_size, *input_shape)
    ]

def get_init_inputs():
    return [1.0]  # Default margin
```