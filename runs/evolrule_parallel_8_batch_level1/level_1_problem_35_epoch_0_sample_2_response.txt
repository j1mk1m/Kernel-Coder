The code should have the same API as the original. The output code must be a complete Python script that can be run standalone. You may not use any third-party libraries. You must use torch.utils.cpp_extension.load_inline to inline embed your custom CUDA kernels. The code must not have any syntax errors. Make sure the code is compatible with Python 3.8 and CUDA 11.1. 

You may make use of existing CUDA libraries such as cuBLAS, cuDNN if you deem it appropriate. 

You may replace any number of operators, but it's recommended to replace the GroupNorm operator as it is the core operator here. 

Your goal is to maximize the speed of the forward pass, while maintaining correctness. You may optionally optimize the backward pass as well, but the focus should be on the forward. 

Please ensure your code includes all necessary imports and definitions. The code must be self-contained.



Okay, so I need to optimize the GroupNorm layer using a custom CUDA kernel. The original model uses PyTorch's nn.GroupNorm, and I need to replace that with my own implementation to get a speedup. Let me think about how GroupNorm works first.

Group Normalization divides the channels into groups and normalizes each group. The steps are: for each group, compute the mean and variance of the group, then subtract the mean and divide by the standard deviation, then scale and shift with learnable parameters. The standard implementation in PyTorch might have some overhead, so writing a custom kernel could help.

First, I should figure out the dimensions. The input is of shape (batch, features, dim1, dim2). The features are divided into num_groups groups. So each group has features / num_groups channels. Let me make sure that features is divisible by num_groups. In the given example, features is 64 and num_groups is 8, so each group has 8 features. That's okay.

The main steps in the forward pass are:

1. Reshape the input to group the channels into groups. For example, from (B, C, H, W) to (B, G, C/G, H*W), where G is the number of groups.

2. For each group, compute the mean and variance over the spatial dimensions and the channels within the group.

3. Normalize each element by subtracting the mean and dividing by sqrt(var + eps).

4. Multiply by gamma and add beta (the learnable parameters).

Hmm, but in the original model, the GroupNorm layer has parameters (gamma and beta), so I need to make sure my custom kernel uses those. Wait, in the given Model class, the parameters are part of the nn.GroupNorm instance. So when replacing with a custom implementation, I need to replicate the parameter handling. Wait, in the original code, the ModelNew needs to have the same API as the original Model. So the custom kernel should take the gamma and beta as inputs?

Wait, the original Model uses self.gn, which has its own parameters. To replicate that, my ModelNew should have parameters for gamma and beta, and the custom kernel must take them as inputs. Alternatively, I can have the parameters inside the model and pass them to the kernel. Let me see.

The original code's ModelNew would replace the GroupNorm layer with a custom function. So in the new model, perhaps the parameters (gamma and beta) are stored in the model, and the kernel uses those. So the forward function would pass those parameters into the kernel.

Alternatively, maybe the custom kernel can be designed to take the gamma and beta tensors as inputs. So in the ModelNew class, we need to have the parameters, similar to how nn.GroupNorm works.

Wait, the original code's Model has the GroupNorm layer's parameters (gamma and beta) stored in self.gn. So in the new ModelNew class, I need to replicate that. But since I'm replacing the entire layer with a custom kernel, maybe I can just take over those parameters and include them in the new model's __init__.

Wait, perhaps the best approach is to create a new module in ModelNew that has the same parameters as the original GroupNorm layer, and then use those parameters in the custom kernel.

Alternatively, perhaps the parameters can be passed as arguments to the forward function. Hmm, but in PyTorch, parameters are part of the model, so the custom kernel should probably take tensors for gamma and beta as inputs.

So the plan is:

1. Create a custom CUDA kernel that implements the GroupNorm forward pass, taking the input tensor, gamma, beta, number of groups, and epsilon (if any) as inputs, and returns the normalized tensor.

2. The ModelNew class will have parameters gamma and beta, just like the original GroupNorm layer.

Wait, but the original code's ModelNew is supposed to have the same API as the original Model. The original Model's __init__ takes num_features and num_groups as parameters. So the new ModelNew should do the same.

Wait, in the original code, the Model class has:

def __init__(self, num_features: int, num_groups: int):
    super().__init__()
    self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)

So the new ModelNew should also have those parameters, and inside its __init__, it should initialize gamma and beta parameters, just like the original.

Wait, but in PyTorch's GroupNorm, the parameters are initialized with gamma as 1 and beta as 0. So in the new model, we can create parameters similarly.

Alternatively, maybe the custom kernel can be written to take gamma and beta as tensors, so the ModelNew will have those parameters stored as attributes, and in the forward function, pass them to the kernel.

So, the steps for the new ModelNew:

- In __init__, create parameters gamma and beta of shape (num_features,).

- The forward function takes input x, applies the custom kernel which uses gamma and beta.

Now, the CUDA kernel needs to perform the group norm computation. Let me think about how to structure the kernel.

The main steps in the kernel:

For each group:

1. Compute the mean and variance over the features in the group and the spatial dimensions.

The input is of shape (batch, features, dim1, dim2). Each group has (features / num_groups) channels.

First, I can reshape the input into (batch, num_groups, group_size, spatial_size), where group_size = features / num_groups and spatial_size = dim1 * dim2.

Then, for each element in the group (along the group_size and spatial dimensions), compute the mean and variance over those dimensions.

Wait, the mean and variance are computed over the features and the spatial dimensions. So for each group, the mean and variance are computed over the channels in the group and the spatial dimensions. That is, for each group, the mean is the average over all elements in the group's channels and spatial locations.

So the dimensions to reduce are the group's channel dimension (group_size) and the spatial dimensions (dim1 and dim2). Wait, actually, the group's features are grouped, so for each group, the dimensions are group_size (number of features per group) and the spatial dimensions. So the mean is over all elements in the group except the batch dimension. So the axes to reduce are (group_size, spatial_size). Wait, more precisely, for each group, the mean is over all elements in the group's channels and spatial dimensions. So for each group, the shape is (batch, group_size, spatial_size), so the mean is over the group_size and spatial_size dimensions. Wait, actually, the spatial dimensions are part of the reduction as well. So for each group in each batch, the mean is over all the channels in the group and all spatial locations. So the reduction is over the group's channels and the spatial dimensions.

Therefore, for each group and batch, compute the mean and variance over (group_size * spatial_size) elements.

So, how to compute this efficiently in CUDA?

Approach:

Loop over each group and each batch:

1. For each group and batch, compute the sum of all elements in that group and batch. Then divide by the number of elements to get the mean.

2. Similarly, compute the squared differences, sum them, divide by the number of elements, then take sqrt( variance + eps ) to get the denominator.

But doing this for each group and batch in a kernel could be manageable.

The problem is that for each element, you need to know which group it belongs to, and which batch. Also, the spatial dimensions are part of the computation.

Alternative approach: For each element in the input tensor, compute its group index, then accumulate the sum and squared sums for each group and batch.

But this might require atomic operations, which can be slow. Alternatively, we can process in parallel per group and batch.

Let me think of the data layout. Suppose the input is a tensor of shape (B, C, H, W). The groups are along the channel dimension, so each group has C/G channels. Let me denote:

C = number of features = num_features

G = num_groups

Each group has C/G channels.

So for each group g (from 0 to G-1), the channels are from g*(C/G) to (g+1)*(C/G) -1.

The kernel could process each group and batch independently.

Let me think in terms of threads and blocks. For example:

Each thread can be responsible for a group and batch. But since the number of groups and batches can be large, the grid and block dimensions need to be set appropriately.

Alternatively, process in blocks where each thread handles a specific spatial position and channel in the group.

Hmm, this is getting a bit complicated. Maybe a better way is to use a helper function to compute the mean and variance for each group and batch.

Alternatively, here's a possible approach:

1. Reshape the input tensor into (B, G, C_per_group, H*W), where C_per_group = C/G.

2. Compute the mean and variance for each (B, G) slice over the C_per_group and H*W dimensions.

3. Normalize each element by subtracting the mean and dividing by the sqrt(var + eps).

4. Multiply by gamma and add beta.

The reshape is easy in PyTorch, so the kernel can handle that. However, the actual kernel will need to process the data in a way that allows this.

Alternatively, the kernel can process each group and batch independently. Let me think of the kernel steps:

First, the kernel needs to compute for each group and batch the mean and variance.

For each element in the group and batch:

sum += x[i]

sum_sq += x[i]^2

Then, after computing the sum and sum_sq, mean = sum / count, var = (sum_sq / count) - (mean)^2.

Once we have the mean and var, normalization is straightforward.

So the problem is how to perform these reductions efficiently in CUDA.

Perhaps, for each group and batch, we can have a thread block that processes the elements in that group and batch. Each thread in the block processes a subset of the elements, accumulating partial sums and squared sums. Then, using block-wide reductions to get the total sum and sum_sq.

This approach would be manageable.

Let me outline the steps for the kernel:

The kernel will be launched with a grid of blocks, each block corresponding to a group and a batch. So the number of blocks is B * G.

Each block will handle a group and batch. Within the block, threads process the elements of that group and batch.

The steps in the block:

1. Compute the total number of elements in the group's slice for the batch: elements_per_group = (C_per_group) * (H * W).

2. Each thread in the block reads a chunk of the elements, computes their contribution to the sum and sum_sq.

3. Use warp or block-level reduction to compute the total sum and sum_sq for the group and batch.

4. Compute the mean and variance.

5. Then, each thread in the block processes their elements, applying normalization and scaling with gamma and beta.

Wait, but after computing the mean and var, all elements in the group and batch need to be normalized. So the same mean and var are applied to all elements in the group and batch.

So the process would be:

- For each group and batch (handled by a block):

   a. Compute sum and sum_sq.

   b. Compute mean = sum / count.

   c. var = (sum_sq/count) - mean^2.

   d. Compute inv_std = 1/sqrt(var + eps).

   e. For each element in the group and batch:

      normalized = (element - mean) * inv_std

      output = normalized * gamma[g] + beta[g]

Wait, but gamma and beta are per channel? Wait no, in GroupNorm, gamma and beta are per group's channel? Wait no, in GroupNorm, gamma and beta are per channel (same as in BatchNorm), but the normalization is done per group. Wait, actually in GroupNorm, the gamma and beta are of size C (number of features), same as the input channels. Each channel is part of a group, but gamma and beta are applied per channel. Wait, no, the parameters are per channel, not per group. Let me check.

According to PyTorch's documentation, GroupNorm has two learnable parameters per channel (gamma and beta). So each channel has its own scaling and shifting. Therefore, the gamma and beta tensors have the same shape as the number of channels, i.e., gamma is of size (C,).

Therefore, for each element in the input tensor, which is in a group, after normalization (using the group's mean and var), the gamma and beta for that channel are applied.

Therefore, the gamma and beta are per channel, not per group. So for a given group, each channel in that group uses its own gamma and beta.

Therefore, in the kernel, when applying the gamma and beta, we need to index them per channel.

Hmm, this complicates things because for each element, its channel index determines which gamma and beta to use. So when processing elements in a group, the gamma and beta indices depend on the original channel.

Wait, let me think of the data layout again.

Suppose the input is (B, C, H, W). We divide the C channels into G groups, each with C/G channels.

For group g, the channels are from c_start = g*(C/G) to c_end = (g+1)*(C/G) - 1.

Each element in the group's channels must be normalized using the group's mean and var, and then scaled by gamma[c] and shifted by beta[c], where c is the original channel.

Therefore, the gamma and beta are per channel, so for each element in the group, we need to know which channel it is to index gamma and beta.

Therefore, in the kernel, for each element's position, we need to track which channel it belongs to, so that we can get the correct gamma and beta.

This complicates the kernel a bit.

Perhaps the way to handle this is:

The kernel will process elements in a way that allows tracking the original channel.

Alternatively, we can precompute for each group and channel in the group the gamma and beta values, but that might not be efficient.

Alternatively, in the kernel, for each element in the group's channels, we can compute the channel index, then use that to get gamma and beta.

Hmm. Let's see.

So, for each element in the input:

Let's suppose the element is at position (b, c, h, w). The group g for this channel c is g = c // (C_per_group).

Wait, since C_per_group = C/G, then g = c // (C_per_group). 

Therefore, for a given group g and batch b, the channels in the group are from c_start = g * C_per_group to c_end = (g+1)*C_per_group -1.

Therefore, in the kernel, for each element, we can compute its channel c, then get gamma[c] and beta[c].

But when processing in a block for a group g and batch b, all channels in that group are being processed. So for each element in those channels, we can compute their original channel and use that to index gamma and beta.

Alternatively, since gamma and beta are per channel, perhaps it's better to pass them as tensors to the kernel and use their values directly.

Now, putting this all together.

The CUDA kernel structure would be:

Each thread block corresponds to a (batch, group) pair.

Within the block, threads process all the elements in the group's channels and spatial dimensions for that batch.

First, compute the sum and sum_sq for the group and batch.

Then compute mean and variance.

Then, compute the inv_std.

Then, each thread processes its portion of the elements, normalizing and applying gamma and beta.

The steps in code:

The kernel will have:

- Input tensor (x) of shape (B, C, H, W).

- Gamma and beta tensors (each of shape (C,)).

- Number of groups (G).

- Epsilon (a small value for numerical stability, like 1e-5).

First, in the kernel:

For a given block (b, g):

   c_start = g * (C/G)

   c_end = (g+1)*(C/G) -1

   The region to process is (b, c_start ... c_end, all h, w).

   The number of elements in this region is (c_end - c_start +1) * H * W.

   So, the block will process all these elements.

   Each thread in the block can process a subset of the elements.

   First, compute the sum and sum_sq over all elements in this region.

   Then compute mean and var.

   Then, for each element, compute normalized value = (x - mean) / sqrt(var + eps).

   Then multiply by gamma[channel] and add beta[channel].

But how to structure this in CUDA:

Let me think about the kernel parameters:

The kernel will need to take pointers to the input, output, gamma, beta.

The dimensions:

B, C, H, W, G, epsilon.

So, first, in the kernel function:

__global__ void group_norm_kernel(...){

   // Determine which block this is.

   int b = blockIdx.x;

   int g = blockIdx.y;

   // Check if block is within valid range.

   if (b >= B || g >= G) return;

   // Compute the C per group.

   int C_per_group = C / G;

   int c_start = g * C_per_group;

   int c_end = (g + 1)*C_per_group -1;

   // Now, the elements to process are all (c in c_start to c_end), h, w.

   // Total elements in this region: C_per_group * H * W.

   // Each thread in the block can handle a chunk of these elements.

   // To compute sum and sum_sq, we can have each thread accumulate their portion.

   // First, compute the total number of elements in the region.

   int total_elements = C_per_group * H * W;

   // Each thread in the block can process a portion.

   // The block can have a certain number of threads, say 256.

   // Let's have each thread process a chunk.

   // But for reduction, better to use a block-wise reduction approach.

   // Let me use a shared memory array to accumulate partial sums.

   __shared__ float sdata_sum[THREADS_PER_BLOCK];

   __shared__ float sdata_sq[THREADS_PER_BLOCK];

   int tid = threadIdx.x;

   // Initialize to zero.

   sdata_sum[tid] = 0.0f;

   sdata_sq[tid] = 0.0f;

   __syncthreads();

   // Each thread processes a set of elements.

   for (int idx = tid; idx < total_elements; idx += blockDim.x){

      // compute the position in the region.

      // The element is in the (c, h, w) of the group's channels.

      // To map idx to (c, h, w):

      // The C_per_group channels are ordered first.

      int c_offset = idx / (H * W);

      int spatial_idx = idx % (H * W);

      int h = spatial_idx / W;

      int w = spatial_idx % W;

      int c = c_start + c_offset;

      // Now, the element is at (b, c, h, w).

      float x_val = x[b][c][h][w]; // Need to access the input tensor correctly.

      sdata_sum[tid] += x_val;

      sdata_sq[tid] += x_val * x_val;

   }

   __syncthreads();

   // Now perform block reduction for sum and sum_sq.

   for (int s=blockDim.x/2; s>0; s>>=1){

      if (tid < s){

          sdata_sum[tid] += sdata_sum[tid + s];

          sdata_sq[tid] += sdata_sq[tid + s];

      }

      __syncthreads();

   }

   // The block's sum and sum_sq are in sdata_sum[0] and sdata_sq[0].

   float sum = sdata_sum[0];

   float sum_sq = sdata_sq[0];

   // Compute mean and var.

   float mean = sum / total_elements;

   float var = (sum_sq / total_elements) - mean * mean;

   float inv_std = 1.0f / sqrtf(var + eps);

   // Now, write back the mean and inv_std to shared memory, so all threads can access.

   __shared__ float block_mean;

   __shared__ float block_inv_std;

   if (tid ==0){

       block_mean = mean;

       block_inv_std = inv_std;

   }

   __syncthreads();

   // Now, each thread processes their elements again to compute the normalized value.

   for (int idx = tid; idx < total_elements; idx += blockDim.x){

      int c_offset = idx / (H * W);

      int spatial_idx = idx % (H * W);

      int h = spatial_idx / W;

      int w = spatial_idx % W;

      int c = c_start + c_offset;

      float x_val = x[b][c][h][w];

      float normalized = (x_val - block_mean) * block_inv_std;

      float gamma_val = gamma[c]; // Need to access gamma tensor here.

      float beta_val = beta[c];

      float out_val = normalized * gamma_val + beta_val;

      // Write to output tensor.

      output[b][c][h][w] = out_val;

   }

}

Wait, but there are several issues here:

1. How to access the input tensor and output tensor in the kernel. Since CUDA kernels can't directly index into tensors like that. Need to use pointers and compute linear indices.

2. The parameters gamma and beta are also tensors, so their values must be accessed via pointers.

So, the kernel needs to have the data in a contiguous format, and the indices must be calculated properly.

Assuming that the input, output, gamma, beta are stored as 1D arrays (since CUDA kernels work with pointers), we need to compute the linear indices.

Let me think of the tensors as flattened arrays.

Suppose the input tensor is of type float*, and has dimensions B x C x H x W. So the linear index for (b, c, h, w) is:

index = b * (C * H * W) + c * (H * W) + h * W + w.

Similarly for gamma and beta: gamma[c] is gamma + c.

So, in the kernel code:

float* x_ptr = ...;

float* output_ptr = ...;

float* gamma_ptr = ...;

float* beta_ptr = ...;

So, in the loops above, the element's value is:

int linear_idx = b * C*H*W + c*H*W + h*W + w;

float x_val = x_ptr[linear_idx];

Similarly, gamma_val = gamma_ptr[c], beta_val = beta_ptr[c].

Therefore, the kernel code would need to compute these indices properly.

Also, the dimensions B, C, H, W, G, eps should be passed as parameters to the kernel.

Therefore, the kernel function would have parameters like:

__global__ void group_norm_kernel(const float* x, float* output,

                                  const float* gamma, const float* beta,

                                  int B, int C, int H, int W, int G,

                                  float eps,

                                  int batch_stride, int channel_stride, int height_stride,

                                  int output_batch_stride, ... etc.)

Wait, perhaps to make it easier, we can precompute the strides.

Alternatively, we can pass all the necessary strides.

Alternatively, the input tensor is in row-major order, so the strides can be computed as:

batch_stride = C * H * W;

channel_stride = H * W;

height_stride = W;

width_stride = 1;

Similarly for the output tensor.

Alternatively, to avoid passing all these strides, we can compute the linear indices as:

linear_index = b * (C * H * W) + c * (H * W) + h * W + w;

But that's assuming the input is stored in row-major (contiguous) order, which it should be if we use .contiguous() before passing.

Therefore, in the kernel, for each element's coordinates (b, c, h, w), the linear index is:

index = b * (C * H * W) + c * (H * W) + h * W + w;

Hence, the kernel code would need to compute this.

Now, let's structure the kernel accordingly.

Also, the gamma and beta are of size C, so their indices are straightforward.

Now, the kernel code steps:

First, in the kernel:

int b = blockIdx.x;

int g = blockIdx.y;

if (b >= B || g >= G) return;

int C_per_group = C / G;

int c_start = g * C_per_group;

int c_end = (g + 1)*C_per_group -1;

The total elements in this group and batch is C_per_group * H * W.

Each thread in the block processes a portion of the elements.

But the block size must be chosen such that blockDim.x divides the total_elements, or we have to loop through all elements in steps.

Alternatively, use a block of threads with a size that can handle the reduction efficiently, perhaps using 256 threads per block.

But let's see:

The block is for a given (b, g), so each block is responsible for their own group and batch.

The blockDim.x can be, say, 256 threads.

First, each thread processes some elements to accumulate sum and sum_sq.

Then, perform a block-wide reduction.

After that, each thread processes their elements again for the normalization step.

Now, the problem is that the first loop (sum and sum_sq) needs to process all elements in the group and batch, so each thread in the block must handle a portion of them.

The first loop's index can be:

for (int idx = threadIdx.x; idx < total_elements; idx += blockDim.x) {

   ... process element at idx ...

}

This way, all threads in the block contribute to the sum.

Then, after the reduction, the second loop (normalization) also does the same.

Now, the code outline:

__global__ void group_norm_kernel(const float* x,

                                 float* output,

                                 const float* gamma,

                                 const float* beta,

                                 int B, int C, int H, int W, int G,

                                 float eps) {

    int b = blockIdx.x;

    int g = blockIdx.y;

    if (b >= B || g >= G)

        return;

    int C_per_group = C / G;

    int c_start = g * C_per_group;

    int c_end = (g + 1) * C_per_group - 1;

    int total_elements = C_per_group * H * W;

    // Shared memory for partial sums

    extern __shared__ float sdata[];

    float* s_sum = sdata;

    float* s_sq = sdata + blockDim.x;

    int tid = threadIdx.x;

    s_sum[tid] = 0.0f;

    s_sq[tid] = 0.0f;

    __syncthreads();

    // First pass: compute sum and sum of squares

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {

        // Compute (c, h, w) from idx

        int c_offset = idx / (H * W);

        int spatial_idx = idx % (H * W);

        int h = spatial_idx / W;

        int w = spatial_idx % W;

        int c = c_start + c_offset;

        // Compute linear index for input

        int in_idx = b * C * H * W + c * H * W + h * W + w;

        float x_val = x[in_idx];

        s_sum[tid] += x_val;

        s_sq[tid] += x_val * x_val;

    }

    __syncthreads();

    // Reduce the sums in the block

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {

        if (tid < s) {

            s_sum[tid] += s_sum[tid + s];

            s_sq[tid] += s_sq[tid + s];

        }

        __syncthreads();

    }

    // Get the total sum and sum_sq from thread 0

    float total_sum, total_sq;

    if (tid == 0) {

        total_sum = s_sum[0];

        total_sq = s_sq[0];

    }

    __syncthreads();

    // Compute mean and variance

    float mean, variance, inv_std;

    if (tid == 0) {

        mean = total_sum / total_elements;

        variance = total_sq / total_elements - mean * mean;

        inv_std = 1.0f / sqrtf(variance + eps);

    }

    // Broadcast mean and inv_std to all threads in the block

    __shared__ float block_mean, block_inv_std;

    if (tid == 0) {

        block_mean = mean;

        block_inv_std = inv_std;

    }

    __syncthreads();

    // Second pass: compute normalized values and apply gamma/beta

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {

        // Compute c, h, w as before

        int c_offset = idx / (H * W);

        int spatial_idx = idx % (H * W);

        int h = spatial_idx / W;

        int w = spatial_idx % W;

        int c = c_start + c_offset;

        // Read x_val again (could be optimized by storing in shared memory but might not be worth it)

        int in_idx = b * C * H * W + c * H * W + h * W + w;

        float x_val = x[in_idx];

        float normalized = (x_val - block_mean) * block_inv_std;

        // Get gamma and beta for this channel

        float gamma_val = gamma[c];

        float beta_val = beta[c];

        float out_val = normalized * gamma_val + beta_val;

        // Write to output

        int out_idx = in_idx; // assuming output has the same layout

        output[out_idx] = out_val;

    }

}

Wait, but in the first step, when tid is not zero, the total_sum and total_sq are not visible. So, after the reduction, the first thread computes the mean and inv_std. Then, they broadcast to shared variables block_mean and block_inv_std. So all threads can read those values from shared memory.

That should work.

Now, the kernel requires shared memory for the partial sums and the final values. The shared memory allocation must be sufficient.

The shared memory needed is:

- 2 * blockDim.x floats for the sum and sum_sq arrays.

- 2 floats for the block_mean and block_inv_std (already included in the sdata array? Wait, in the code above, sdata is declared as extern __shared__, and then split into s_sum and s_sq. The block_mean and block_inv_std are stored in separate variables, but in the code above, they are stored in shared memory by using the first two elements? Wait no:

Wait, in the code above, block_mean and block_inv_std are stored in block-scoped shared variables:

__shared__ float block_mean, block_inv_std;

But in CUDA, __shared__ variables are allocated per block. So that's okay. But in the code I wrote earlier, I used sdata for the s_sum and s_sq. Maybe better to use a single shared memory array and allocate enough space.

Alternatively, the code I have now may have an error in the shared memory allocation.

Wait, in the code above, the extern __shared__ float sdata[] is used, and s_sum and s_sq are pointers into this array. The total required size would be 2 * blockDim.x + 2 (for block_mean and block_inv_std). Wait, no, because block_mean and block_inv_std are separate __shared__ variables. Wait, no, in the code I wrote, block_mean and block_inv_std are declared as:

__shared__ float block_mean, block_inv_std;

So that requires 2 floats of shared memory, separate from the sdata array. So total shared memory needed is:

size of sdata (2*blockDim.x floats) + 2 floats for block_mean and block_inv_std.

But in the code, the extern __shared__ is used for sdata. The total shared memory must be specified when launching the kernel.

Alternatively, perhaps the code can be restructured to use the sdata array for all variables.

Alternatively, perhaps it's better to allocate the shared memory as follows:

The shared memory required is 2 * blockDim.x (for the sum and sum_sq) plus 2 floats for the mean and inv_std.

So total shared memory needed is:

(2 * blockDim.x + 2) * sizeof(float)

Hence, when launching the kernel, we need to specify the shared memory size.

Therefore, in the kernel launch configuration:

int smem_size = (2 * blockDim.x + 2) * sizeof(float);

But I need to make sure that in the kernel, the extern __shared__ is allocated with that size.

Alternatively, the code can be adjusted to use the sdata array for storing the mean and inv_std.

Let me adjust the code:

In the kernel:

extern __shared__ float sdata[];

// s_sum and s_sq are the first blockDim.x elements each.

float* s_sum = sdata;

float* s_sq = sdata + blockDim.x;

// The last 2 elements can store the mean and inv_std.

float* block_mean = sdata + 2 * blockDim.x;

float* block_inv_std = sdata + 2 * blockDim.x + 1;

Thus, the total shared memory needed is (2 * blockDim.x + 2) * sizeof(float).

This way, we don't need separate __shared__ variables.

So modifying the code:

After the reduction:

if (tid == 0) {

    block_mean[0] = mean;

    block_inv_std[0] = inv_std;

}

Then, all threads can read from block_mean[0] and block_inv_std[0].

This way, the shared memory allocation is handled via the extern __shared__.

Thus, the kernel's extern __shared__ must be allocated to 2*blockDim.x + 2.

So when launching the kernel, the shared memory size is set to this.

Now, the kernel code would look something like:

__global__ void group_norm_kernel(const float* x,

                                 float* output,

                                 const float* gamma,

                                 const float* beta,

                                 int B, int C, int H, int W, int G,

                                 float eps) {

    int b = blockIdx.x;

    int g = blockIdx.y;

    if (b >= B || g >= G)

        return;

    int C_per_group = C / G;

    int c_start = g * C_per_group;

    int c_end = (g + 1) * C_per_group - 1;

    int total_elements = C_per_group * H * W;

    // Shared memory for partial sums and results

    extern __shared__ float sdata[];

    float* s_sum = sdata;

    float* s_sq = sdata + blockDim.x;

    float* block_mean = sdata + 2 * blockDim.x;

    float* block_inv_std = block_mean + 1;

    int tid = threadIdx.x;

    s_sum[tid] = 0.0f;

    s_sq[tid] = 0.0f;

    __syncthreads();

    // First pass: compute sum and sum of squares

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {

        // ... same as before ...

    }

    __syncthreads();

    // Reduce the sums in the block

    // ... same as before ...

    // Compute mean and variance

    if (tid == 0) {

        float total_sum = s_sum[0];

        float total_sq = s_sq[0];

        float mean = total_sum / total_elements;

        float variance = total_sq / total_elements - mean * mean;

        float inv_std = 1.0f / sqrtf(variance + eps);

        block_mean[0] = mean;

        block_inv_std[0] = inv_std;

    }

    __syncthreads();

    // Second pass: compute normalized values and apply gamma/beta

    // ... same as before, using block_mean[0] and block_inv_std[0]

}

This way, all shared memory is managed via the extern array.

Now, in the kernel launch configuration, the shared memory size is:

smem_size = (2 * blockDim.x + 2) * sizeof(float)

Hence, when launching the kernel, we pass this as the sharedMem parameter.

Now, the next step is to write the wrapper function in C++.

The wrapper function will handle the dimensions and launch the kernel.

The wrapper function in C++ would look something like:

torch::Tensor group_norm_cuda(torch::Tensor x,

                             torch::Tensor gamma,

                             torch::Tensor beta,

                             int num_groups,

                             float eps) {

    int B = x.size(0);

    int C = x.size(1);

    int H = x.size(2);

    int W = x.size(3);

    int G = num_groups;

    // Check that C is divisible by G

    int C_per_group = C / G;

    assert(C % G == 0);

    // Output tensor

    auto output = torch::empty_like(x);

    // Launch configuration

    int block_size = 256;

    dim3 blocks(B, G);

    dim3 threads(block_size);

    int smem_size = (2 * block_size + 2) * sizeof(float);

    // Launch the kernel

    group_norm_kernel<<<blocks, threads, smem_size, at::cuda::getCurrentCUDAStream()>>>(
        x.data_ptr<float>(),
        output.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        B, C, H, W, G,
        eps
    );

    return output;
}

Wait, but in PyTorch, the tensors are passed as arguments, and the kernel is called with the data pointers.

Now, the problem is that in PyTorch, the tensors need to be contiguous. The kernel assumes that the input and output are contiguous. So in the wrapper function, we can ensure that x is contiguous, and the output is created as contiguous.

Hence, in the wrapper function:

x = x.contiguous();

output = torch::empty_like(x); // should also be contiguous.

Now, putting this all together in the Python code:

In the Python code, we need to:

- Define the CUDA source code with the kernel and wrapper.

- Compile it using load_inline.

- The ModelNew class will use this custom function.

First, the CUDA code as a string:

group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void group_norm_kernel(const scalar_t* __restrict__ x,
                                 scalar_t* __restrict__ output,
                                 const scalar_t* __restrict__ gamma,
                                 const scalar_t* __restrict__ beta,
                                 int B, int C, int H, int W, int G,
                                 float eps) {

    int b = blockIdx.x;
    int g = blockIdx.y;

    if (b >= B || g >= G)
        return;

    int C_per_group = C / G;
    int c_start = g * C_per_group;
    int c_end = (g + 1) * C_per_group - 1;
    int total_elements = C_per_group * H * W;

    extern __shared__ float sdata[];

    float* s_sum = sdata;
    float* s_sq = sdata + blockDim.x;
    float* block_mean = sdata + 2 * blockDim.x;
    float* block_inv_std = block_mean + 1;

    int tid = threadIdx.x;
    s_sum[tid] = 0.0f;
    s_sq[tid] = 0.0f;

    __syncthreads();

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {
        int c_offset = idx / (H * W);
        int spatial_idx = idx % (H * W);
        int h = spatial_idx / W;
        int w = spatial_idx % W;
        int c = c_start + c_offset;

        int in_idx = b * C * H * W + c * H * W + h * W + w;
        scalar_t x_val = x[in_idx];

        s_sum[tid] += static_cast<float>(x_val);
        s_sq[tid] += static_cast<float>(x_val * x_val);
    }

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_sum[tid] += s_sum[tid + s];
            s_sq[tid] += s_sq[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total_sum = s_sum[0];
        float total_sq = s_sq[0];
        float mean = total_sum / total_elements;
        float variance = total_sq / total_elements - mean * mean;
        float inv_std = 1.0f / sqrtf(variance + eps);

        block_mean[0] = mean;
        block_inv_std[0] = inv_std;
    }

    __syncthreads();

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {
        int c_offset = idx / (H * W);
        int spatial_idx = idx % (H * W);
        int h = spatial_idx / W;
        int w = spatial_idx % W;
        int c = c_start + c_offset;

        int in_idx = b * C * H * W + c * H * W + h * W + w;
        scalar_t x_val = x[in_idx];

        float normalized = (static_cast<float>(x_val) - block_mean[0]) * block_inv_std[0];
        scalar_t gamma_val = gamma[c];
        scalar_t beta_val = beta[c];

        output[in_idx] = static_cast<scalar_t>(normalized * static_cast<float>(gamma_val) + static_cast<float>(beta_val));
    }
}

torch::Tensor group_norm_cuda(torch::Tensor x,

                             torch::Tensor gamma,

                             torch::Tensor beta,

                             int num_groups,

                             float eps) {

    int B = x.size(0);
    int C = x.size(1);
    int H = x.size(2);
    int W = x.size(3);
    int G = num_groups;

    // Check divisibility
    if (C % G != 0) {
        AT_ERROR("Number of channels must be divisible by num_groups");
    }

    auto output = torch::empty_like(x);

    const int threads = 256;
    int C_per_group = C / G;
    int total_elements_per_group = C_per_group * H * W;
    dim3 blocks(B, G);
    dim3 threadsPerBlock(threads);

    // Compute shared memory size
    int smem_size = (2 * threads + 2) * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), "group_norm_cuda", ([&] {
        group_norm_kernel<scalar_t><<<blocks, threadsPerBlock, smem_size, at::cuda::getCurrentCUDAStream()>>>(
            x.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            gamma.data_ptr<scalar_t>(),
            beta.data_ptr<scalar_t>(),
            B, C, H, W, G,
            eps);
    }));

    return output;
}
"""

group_norm_cpp_source = """
torch::Tensor group_norm_cuda(torch::Tensor x,

                             torch::Tensor gamma,

                             torch::Tensor beta,

                             int num_groups,

                             float eps);
"""

Wait, but in the kernel, I used template <typename scalar_t>, so the wrapper must dispatch to the correct type.

The AT_DISPATCH_FLOATING_TYPES macro is used to handle different data types (float, double, etc.), but since PyTorch typically uses float32, maybe it's okay.

Now, in the Python code:

We can use load_inline to compile the CUDA code.

Then, in the ModelNew class, we need to have parameters gamma and beta, just like the original GroupNorm layer.

The original Model class initializes the GroupNorm layer with num_groups and num_channels.

The new ModelNew class must replicate this.

So the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, num_features: int, num_groups: int):
        super().__init__()
        self.num_groups = num_groups
        self.num_features = num_features
        # Initialize gamma and beta parameters
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        # Load the CUDA kernel
        self.group_norm = load_inline_cuda_function

    def forward(self, x):
        # Call the CUDA kernel with the parameters
        return self.group_norm(x, self.gamma, self.beta, self.num_groups, 1e-5)

Wait, but how to load the inline CUDA function.

Wait, the load_inline function returns a module with the function.

Wait, in the example given earlier, the elementwise_add_cuda function was loaded, and then called via self.elementwise_add.elementwise_add_cuda(a,b).

So similarly, here, after compiling the group_norm_cuda function, we can get a handle to it.

First, in the Python code:

First, the CUDA source is defined as group_norm_source and group_norm_cpp_source.

Then, we compile it:

group_norm = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Then, in the ModelNew class, we can have:

class ModelNew(nn.Module):
    def __init__(self, num_features: int, num_groups: int):
        super().__init__()
        self.num_features = num_features
        self.num_groups = num_groups
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        self.group_norm = group_norm

    def forward(self, x):
        # Ensure x is contiguous
        x = x.contiguous()
        return self.group_norm.group_norm_cuda(
            x, self.gamma, self.beta, self.num_groups, 1e-5
        )

Wait, but in PyTorch, the tensors need to be on the GPU. So in the get_inputs() function, the input x is generated on CPU, so need to move it to GPU.

Wait, in the original code's get_inputs():

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

But in the problem statement, the original code's get_inputs() may not be on CUDA. However, in the example given, the elementwise_add example uses .cuda().

Wait, looking back at the original problem's code:

The original code's get_inputs() for the GroupNorm example:

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

But in the example for the elementwise_add, the inputs are generated with .cuda().

Assuming that in the problem, the inputs should be on the GPU, but the code may need to be adjusted.

But the ModelNew's forward function can handle tensors on the GPU.

Alternatively, the code may need to move the tensors to CUDA.

But in the problem, the user says "Please ensure your code includes all necessary imports and definitions. The code must be self-contained."

Hence, in the get_inputs() function, the code should generate tensors on the GPU.

Wait, in the original problem's given code:

Looking back at the problem's code for the GroupNorm example:

The get_inputs() is:

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

def get_init_inputs():
    return [features, num_groups] # num_features

So the inputs are on CPU. But in practice, the model is supposed to be run on CUDA. Hence, in the new code, the ModelNew should be on CUDA, and the inputs need to be moved to CUDA.

Hence, in the get_inputs() function, we should generate tensors on CUDA.

Therefore, modifying the get_inputs() to:

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2).cuda()
    return [x]

But in the problem's instructions, the user provided the original code's get_inputs() and get_init_inputs(), so perhaps the new code should keep the same API, but the user's code may have to be adjusted. Since the problem says the new code must be a complete Python script, including the get_inputs() and get_init_inputs(), the code should include those, with tensors on CUDA.

Thus, in the final code, the get_inputs() should generate tensors on the GPU.

Putting it all together, the complete code would be:

The Python script with the ModelNew class, the CUDA kernel definition, and the get_inputs() and get_init_inputs() functions.

Now, let's write the complete code.

First, the CUDA source code strings.

Then, the model class.

Also, the parameters gamma and beta must be initialized correctly.

Wait, in the original Model class, the GroupNorm layer is initialized with:

self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)

The gamma and beta parameters are initialized to 1 and 0, respectively. The new ModelNew's gamma and beta are initialized the same way (using nn.Parameter(torch.ones...) and torch.zeros).

Therefore, that part is correct.

Now, handling the CUDA kernel:

The group_norm_cuda function requires the input x, gamma, beta, num_groups, and eps.

In the forward function of ModelNew, x is passed, along with self.gamma and self.beta, and self.num_groups.

The epsilon value is set to 1e-5 here, as in the original GroupNorm's default.

Now, the complete code in Python:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for GroupNorm
group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void group_norm_kernel(const scalar_t* __restrict__ x,
                                 scalar_t* __restrict__ output,
                                 const scalar_t* __restrict__ gamma,
                                 const scalar_t* __restrict__ beta,
                                 int B, int C, int H, int W, int G,
                                 float eps) {

    int b = blockIdx.x;
    int g = blockIdx.y;

    if (b >= B || g >= G)
        return;

    int C_per_group = C / G;
    int c_start = g * C_per_group;
    int c_end = (g + 1) * C_per_group - 1;
    int total_elements = C_per_group * H * W;

    extern __shared__ float sdata[];

    float* s_sum = sdata;
    float* s_sq = sdata + blockDim.x;
    float* block_mean = sdata + 2 * blockDim.x;
    float* block_inv_std = block_mean + 1;

    int tid = threadIdx.x;
    s_sum[tid] = 0.0f;
    s_sq[tid] = 0.0f;

    __syncthreads();

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {
        int c_offset = idx / (H * W);
        int spatial_idx = idx % (H * W);
        int h = spatial_idx / W;
        int w = spatial_idx % W;
        int c = c_start + c_offset;

        int in_idx = b * C * H * W + c * H * W + h * W + w;
        scalar_t x_val = x[in_idx];

        s_sum[tid] += static_cast<float>(x_val);
        s_sq[tid] += static_cast<float>(x_val * x_val);
    }

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_sum[tid] += s_sum[tid + s];
            s_sq[tid] += s_sq[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total_sum = s_sum[0];
        float total_sq = s_sq[0];
        float mean = total_sum / total_elements;
        float variance = total_sq / total_elements - mean * mean;
        float inv_std = 1.0f / sqrtf(variance + eps);

        block_mean[0] = mean;
        block_inv_std[0] = inv_std;
    }

    __syncthreads();

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {
        int c_offset = idx / (H * W);
        int spatial_idx = idx % (H * W);
        int h = spatial_idx / W;
        int w = spatial_idx % W;
        int c = c_start + c_offset;

        int in_idx = b * C * H * W + c * H * W + h * W + w;
        scalar_t x_val = x[in_idx];

        float normalized = (static_cast<float>(x_val) - block_mean[0]) * block_inv_std[0];
        scalar_t gamma_val = gamma[c];
        scalar_t beta_val = beta[c];

        output[in_idx] = static_cast<scalar_t>(normalized * static_cast<float>(gamma_val) + static_cast<float>(beta_val));
    }
}

torch::Tensor group_norm_cuda(torch::Tensor x,

                             torch::Tensor gamma,

                             torch::Tensor beta,

                             int num_groups,

                             float eps) {

    int B = x.size(0);
    int C = x.size(1);
    int H = x.size(2);
    int W = x.size(3);
    int G = num_groups;

    // Check divisibility
    if (C % G != 0) {
        AT_ERROR("Number of channels must be divisible by num_groups");
    }

    auto output = torch::empty_like(x);

    const int threads = 256;
    int C_per_group = C / G;
    int total_elements_per_group = C_per_group * H * W;
    dim3 blocks(B, G);
    dim3 threadsPerBlock(threads);

    // Compute shared memory size
    int smem_size = (2 * threads + 2) * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), "group_norm_cuda", ([&] {
        group_norm_kernel<scalar_t><<<blocks, threadsPerBlock, smem_size, at::cuda::getCurrentCUDAStream()>>>(
            x.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            gamma.data_ptr<scalar_t>(),
            beta.data_ptr<scalar_t>(),
            B, C, H, W, G,
            eps);
    }));

    return output;
}
"""

group_norm_cpp_source = """
torch::Tensor group_norm_cuda(torch::Tensor x,

                             torch::Tensor gamma,

                             torch::Tensor beta,

                             int num_groups,

                             float eps);
"""

# Compile the inline CUDA code for GroupNorm
group_norm = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_cuda"],
    verbose=True,
    extra_cflags=["-DWITH_CUDA"],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int, num_groups: int):
        super().__init__()
        self.num_features = num_features
        self.num_groups = num_groups
        self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32).cuda())
        self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32).cuda())
        self.group_norm = group_norm  # The loaded CUDA module

    def forward(self, x):
        x = x.contiguous()
        return self.group_norm.group_norm_cuda(
            x, self.gamma, self.beta, self.num_groups, 1e-5
        )

batch_size = 112
features = 64
num_groups = 8
dim1 = 512
dim2 = 512

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2).cuda()
    return [x]

def get_init_inputs():
    return [features, num_groups]  # num_features
```

Now, checking for possible issues:

1. The CUDA kernel uses block.x and block.y to index batches and groups. If the number of batches or groups is larger than the maximum allowed by the grid dimensions (which is 65535 for each dimension in CUDA), it might fail. However, in the given problem's parameters, batch_size is 112 and num_groups is 8, so it's okay.

2. The shared memory calculation must be correct. The code uses (2 * blockDim.x + 2) * sizeof(float), which for block_size=256 gives 2*256 + 2 = 514 floats, which is 2056 bytes. That should be acceptable.

3. The kernel uses threadIdx.x and loops over all elements. The block size is 256, which is a common choice.

4. The parameters gamma and beta are initialized as nn.Parameters on the GPU.

5. The forward function ensures x is contiguous, which is necessary for the kernel's assumed layout.

6. The epsilon value is set to 1e-5, which matches PyTorch's default for GroupNorm.

Possible optimizations:

- Using a larger block size for better occupancy.

- Using more efficient reduction algorithms for the sum and sum_sq.

- Checking if the input tensor is contiguous and handling non-contiguous cases.

- Using cuBLAS for reductions, but for small groups it might not be worth it.

However, given the time constraints and the problem's requirement to write functional code, this should work.

Testing: The code should be tested by creating a Model and ModelNew instance and comparing outputs. But since the problem doesn't require testing code, it's omitted.

Potential issues in the code:

- The kernel might have off-by-one errors in the indices, especially in the computation of c_start and c_end.

- The shared memory size might be miscalculated.

- The use of static_cast for converting between scalar_t (e.g., float) and float might be redundant but safe.

Overall, this should be a valid implementation.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename scalar_t>
__global__ void group_norm_kernel(
    const scalar_t* __restrict__ x,
    scalar_t* __restrict__ output,
    const scalar_t* __restrict__ gamma,
    const scalar_t* __restrict__ beta,
    int B, int C, int H, int W, int G,
    float eps) {
    int b = blockIdx.x;
    int g = blockIdx.y;
    if (b >= B || g >= G) return;

    int C_per_group = C / G;
    int c_start = g * C_per_group;
    int total_elements = C_per_group * H * W;

    extern __shared__ float sdata[];
    float* s_sum = sdata;
    float* s_sq = sdata + blockDim.x;
    float* block_mean = sdata + 2 * blockDim.x;
    float* block_inv_std = block_mean + 1;

    int tid = threadIdx.x;
    s_sum[tid] = 0.0f;
    s_sq[tid] = 0.0f;
    __syncthreads();

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {
        int c_offset = idx / (H * W);
        int spatial_idx = idx % (H * W);
        int h = spatial_idx / W;
        int w = spatial_idx % W;
        int c = c_start + c_offset;

        int in_idx = b * C * H * W + c * H * W + h * W + w;
        scalar_t val = x[in_idx];
        s_sum[tid] += static_cast<float>(val);
        s_sq[tid] += static_cast<float>(val * val);
    }
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_sum[tid] += s_sum[tid + s];
            s_sq[tid] += s_sq[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float mean = s_sum[0] / total_elements;
        float var = s_sq[0] / total_elements - mean * mean;
        float inv_std = 1.0f / sqrtf(var + eps);
        block_mean[0] = mean;
        block_inv_std[0] = inv_std;
    }
    __syncthreads();

    for (int idx = tid; idx < total_elements; idx += blockDim.x) {
        int c_offset = idx / (H * W);
        int spatial_idx = idx % (H * W);
        int h = spatial_idx / W;
        int w = spatial_idx % W;
        int c = c_start + c_offset;

        int in_idx = b * C * H * W + c * H * W + h * W + w;
        scalar_t val = x[in_idx];
        float normed = (static_cast<float>(val) - block_mean[0]) * block_inv_std[0];
        scalar_t gamma_val = gamma[c];
        scalar_t beta_val = beta[c];
        output[in_idx] = static_cast<scalar_t>(normed * static_cast<float>(gamma_val) + static_cast<float>(beta_val));
    }
}

torch::Tensor group_norm_cuda(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps) {
    int B = x.size(0);
    int C = x.size(1);
    int H = x.size(2);
    int W = x.size(3);
    int G = num_groups;

    if (C % G != 0) {
        AT_ERROR("Number of channels must be divisible by num_groups");
    }

    auto output = torch::empty_like(x);
    const int threads = 256;
    dim3 blocks(B, G);
    dim3 threadsPerBlock(threads);
    int smem_size = (2 * threads + 2) * sizeof(float);

    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), "group_norm_cuda", ([&] {
        group_norm_kernel<scalar_t><<<blocks, threadsPerBlock, smem_size, at::cuda::getCurrentCUDAStream()>>>(
            x.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            gamma.data_ptr<scalar_t>(),
            beta.data_ptr<scalar_t>(),
            B, C, H, W, G, eps);
    }));

    return output;
}
"""

group_norm_cpp_source = """
torch::Tensor group_norm_cuda(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps);
"""

group_norm = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_cuda"],
    verbose=True,
    extra_cflags=["-DWITH_CUDA"],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int, num_groups: int):
        super().__init__()
        self.num_groups = num_groups
        self.num_features = num_features
        self.gamma = nn.Parameter(torch.ones(num_features, dtype=torch.float32).cuda())
        self.beta = nn.Parameter(torch.zeros(num_features, dtype=torch.float32).cuda())
        self.group_norm = group_norm

    def forward(self, x):
        x = x.contiguous()
        return self.group_norm.group_norm_cuda(
            x, self.gamma, self.beta, self.num_groups, 1e-5
        )

batch_size = 112
features = 64
num_groups = 8
dim1 = 512
dim2 = 512

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2).cuda()
    return [x]

def get_init_inputs():
    return [features, num_groups]
```