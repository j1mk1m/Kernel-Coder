Make sure to write the code in a way that the ModelNew's forward() method is as efficient as possible, and the code is valid.



Okay, I need to optimize the given Model which uses a torch.einsum operation for a 4D tensor-matrix multiplication. The goal is to replace this with a custom CUDA kernel to improve performance.

First, let me understand the computation. The einsum "bijl,lk->bijk" means that for each element in the output (b,i,j,k), it's the sum over l of A[b,i,j,l] * B[l,k]. So this is a batched matrix multiplication where each 3D slice (along the l dimension) of A is multiplied by B, and the result is a 4D tensor.

The standard way to do this without einsum might be using batch matrix multiplication functions like torch.matmul or torch.bmm. Let me check the dimensions. A has shape (b, i, j, l), and B is (l, k). If I reshape A to (b*i*j, l), then matmul with B (l, k) would give (b*i*j, k), then reshape back to (b, i, j, k). But this might be equivalent to what the einsum does, and perhaps using torch.matmul would be more efficient than einsum. However, since the user wants a custom CUDA kernel, maybe implementing this with a kernel can be faster.

Alternatively, maybe the existing torch.einsum is already optimized, but perhaps writing a custom kernel can exploit specific patterns or coalesced memory access for better performance. Let's think about the kernel structure.

The kernel needs to compute for each output element C[b,i,j,k] the sum over l of A[b,i,j,l] * B[l,k]. 

To parallelize this, each thread can compute one output element. But considering that each element of the output is a dot product between a vector from A and a vector from B. Wait, actually, for each (b,i,j,k), the computation is a sum over l. So the output has dimensions (b,i,j,k). 

Alternatively, maybe each thread can handle a single (b,i,j,k) and compute the sum over l. That would be straightforward but might have a lot of threads and some redundancy since each thread would loop over l. The problem is that for each output element, you have to loop over l elements. The l dimension is 256 in the test case, which is manageable but could be slow if done per thread.

Alternatively, we can structure the computation so that the l dimension is handled in parallel. Maybe using a tiled approach. Hmm, but for a kernel, perhaps the best approach is to have each thread compute one output element (b,i,j,k), and loop over l. 

Let me outline the kernel:

- The grid can be divided into blocks, each block handling a chunk of the output elements. The output dimensions are (b, i, j, k). Let's say we have a 3D grid or 2D grid, but for simplicity, maybe flatten all dimensions except l. 

The total number of output elements is b * i * j * k. Each thread can take one element, so threads_per_block * num_blocks needs to cover all elements. But since l is 256, the loop over l might be manageable if l isn't too big. For l=256, the loop would have 256 iterations per thread, which might be acceptable if the kernel is well-optimized.

Wait, but with l=256 and the kernel having each thread process a single output element, that's 256 iterations per thread. For example, if the output is 8*256*512*768, that's a huge number of threads. Wait, 8 * 256 *512*768 is 8*256=2048, *512=1,048,576, *768 is way too big. Wait, that's 8 * 256 *512*768 = let's compute:

Wait, 8 (b) * 256 (i) *512 (j) * 768 (k). That's 8 * 256 is 2048; 2048 *512 = 1,048,576; times 768 is 815,372,697,6 elements. That's way too many elements, so that approach might not be feasible because the number of threads can't handle that. 

Wait, perhaps I miscalculated the problem's dimensions. Let me check the test case:

In the test code, the dimensions are given as:

b = 8
i = 256
j = 512
l = 256
k = 768

So the output has shape (8,256,512,768). So the total elements are 8*256*512*768. Let me compute that:

8 * 256 = 2048; 2048 *512 = 1,048,576; 1,048,576 *768 = 815,372,697,6 elements. That's over 800 million elements, which is way too big. So a kernel that processes each element individually would require that many threads, which is impossible because the maximum threads per block is 1024, and the maximum grid dimensions are limited (like 2^31 in each dimension). So that approach isn't feasible.

Hmm, so I need a different strategy. Maybe instead of processing each output element independently, process a block of elements together. 

Alternative approach: Let's structure the computation in a way that each thread block processes a block of output elements along the l dimension. 

Wait, perhaps the problem can be viewed as a batched matrix multiply. Let's see:

The input A has shape (b, i, j, l). B has shape (l, k). The output is (b, i, j, k). So for each b, i, j, we have a vector of length l (from A's last dimension) multiplied by the matrix B (l, k), resulting in a vector of length k. So, for each (b, i, j), we have a matrix multiply of a vector (l) with matrix (l, k), giving (k). 

So the entire computation can be considered as for each (b, i, j), compute A[b,i,j,:] @ B, resulting in a vector of size k. So this is like a batched matrix multiplication where the batch dimensions are (b, i, j). 

In PyTorch, if we can reshape A and B to use batch matrix multiplication. For instance, A can be viewed as (b*i*j, l), and B as (l, k), then the product would be (b*i*j, k), and then reshaping back to (b, i, j, k). 

So using torch.matmul(A.reshape(-1, l), B).view(b, i, j, k). 

But how does this compare to the existing einsum? The einsum might be doing this under the hood, but perhaps the custom kernel can be more optimized. 

Alternatively, maybe the existing code can be optimized by using the batched matmul approach instead of einsum. Let me check:

Wait, the current forward is written as:

def forward(self, A, B):
    return torch.einsum("bijl,lk->bijk", A, B)

If we can replace this with:

return torch.matmul(A.view(-1, l), B).view(b, i, j, k)

But is that faster? Let me think. The einsum might have some optimizations, but maybe the matmul is faster. However, the problem requires using a custom CUDA kernel, so perhaps that's not sufficient. 

Alternatively, perhaps writing a custom kernel for this batched matrix multiplication can be faster. Let's proceed with that.

So the goal is to write a CUDA kernel that computes the batched matrix multiplication of A (shape BxI x J x L) and B (L x K) to produce the output of size BxI x J x K. 

To structure the kernel, perhaps we can process each (b,i,j) block as a separate matrix multiplication. Since each (b,i,j) corresponds to a vector of L elements in A, which is multiplied by B (L x K) to get a K-vector. 

So the total number of such vectors is B*I*J, so each of these is a separate matrix multiply (matrix multiply of a row vector with a matrix? Wait, no. Each vector is a row (L elements) in A, so the product is a row (K elements). 

The kernel can be structured to process each (b,i,j) in parallel. Each thread block can handle one (b,i,j) group. 

Each block can process the K elements for its (b,i,j). For each K index, we need to compute the sum over L of A[b,i,j,l] * B[l,k].

To compute this sum, each thread could handle a different k index, but then they would need to loop over L. Alternatively, use a shared memory approach to accumulate the sums.

Alternatively, use a tiled approach where each thread in a block computes a portion of the sum for a k index. 

Hmm, perhaps using a shared memory tile for the B matrix would help. Since B is (L, K), which is 256x768. 

Alternatively, since B is fixed for all (b,i,j), we can precompute it in shared memory once per block. Wait, but B is the same for all blocks, so that might not be feasible unless we can broadcast it. 

Alternatively, since B is a matrix, each block can load a chunk of B into shared memory once, then compute the products for their respective (b,i,j) vectors.

Let me think of the kernel structure. 

Suppose each block handles one (b,i,j) triplet. The block has threads for each k. 

Wait, for each (b,i,j), the output has K elements. Each element in K is the sum over L of A[b,i,j,l] * B[l,k].

So for each (b,i,j), we can compute the K elements in parallel. Let's say each thread in a block handles a single k. 

The number of threads per block would be K (768 in the test case). But 768 is a bit high for threads per block (max 1024). Maybe that's okay. 

Each thread would compute the sum over l (256 elements) for their k. 

The steps for a thread in a block:

1. For their (b,i,j,k):

   a. Read A[b,i,j,l] for each l (loop over l from 0 to L-1).

   b. Multiply by B[l,k].

   c. Accumulate the sum.

2. Then, store the result in the output at (b,i,j,k).

But how do we map the block and threads to (b,i,j,k)?

The block indices can correspond to (b,i,j), and each block has K threads (each thread handles a k). 

The total number of blocks would be B*I*J. 

The number of threads per block is K. 

Wait, K is 768 which is less than the maximum threads per block (1024), so that's okay. 

But for each block, the threads will loop over L (256) elements. 

So the kernel could look like this:

__global__ void tensor_matmul_kernel(const float* A, const float* B, float* C,
                                    int B_dim, int I_dim, int J_dim, int L_dim, int K_dim) {

    int b = blockIdx.x;
    int i = blockIdx.y;
    int j = blockIdx.z;
    int k = threadIdx.x;

    if (b >= B_dim || i >= I_dim || j >= J_dim || k >= K_dim) return;

    float sum = 0.0f;

    for (int l = 0; l < L_dim; ++l) {
        const int a_index = b * I_dim * J_dim * L_dim + i * J_dim * L_dim + j * L_dim + l;
        const int b_index = l * K_dim + k;
        sum += A[a_index] * B[b_index];
    }

    const int c_index = b * I_dim * J_dim * K_dim + i * J_dim * K_dim + j * K_dim + k;
    C[c_index] = sum;
}

Wait, but this requires that the grid is 3D (B, I, J), and the block is 1D (K threads). 

But in CUDA, the maximum block dimensions can be up to 1024 in x, but the grid dimensions can be up to 65535 in each dimension, so for B=8, I=256, J=512, the grid would have 8 *256 *512 blocks. Let me compute that: 8*256 is 2048, times 512 gives 1,048,576 blocks. That's acceptable as long as the total is under the maximum grid size. The maximum grid size in each dimension is 2^31, so that's okay.

But each block has 768 threads, which is under the 1024 limit. 

This approach would work, but the problem is that for each thread (k) in each block, they have to loop over L=256 elements. That's 256 iterations per thread. For K=768, each block has 768 threads, each doing 256 loops. 

This could be optimized by unrolling the loop or vectorizing. Alternatively, using shared memory to cache A and B elements to reduce global memory accesses. 

Alternatively, perhaps using a tiled approach where each thread processes a portion of the L dimension. Let's see. 

Suppose each block has multiple threads, and each thread handles a tile of L. 

Wait, maybe the initial approach is manageable. Let me see how the indexing works.

The kernel parameters:

- A is a 4D tensor (B, I, J, L). So the stride for each dimension is important. But in the code, the linear index calculation assumes that the data is stored in row-major order, so the formula for a_index is correct as written. 

But in practice, tensors in PyTorch are stored in contiguous memory, so the linear index can be computed as:

For A[b,i,j,l], the linear index is:

b * (I * J * L) + i * (J * L) + j * L + l

Similarly for the output C[b,i,j,k], the linear index is:

b * (I * J * K) + i * (J * K) + j * K + k

The B matrix is (L, K), so B[l][k] is stored at l*K +k.

This kernel should work. 

But let me check for possible optimizations. 

First, the loop over L can be vectorized. Since L is 256, which is a multiple of 4 (256/4=64), we can use vector types like float4 to load and multiply in chunks. 

Alternatively, using shared memory to cache the B matrix might help, but since B is (256, 768), which is a large matrix, the shared memory might not be sufficient. 

Alternatively, since B is constant for all blocks, perhaps we can precompute it in shared memory once per block. 

Wait, in this kernel approach, each block processes a (b,i,j) and needs the entire B matrix for all its K threads. Since each thread in a block is handling a different k, they all need access to B's l,k elements. 

If we can load a row of B into shared memory, but that's per block. 

Alternatively, for each block, load all the B's l,k elements for the current k's? Hmm, not sure. 

Alternatively, since B is a fixed matrix, perhaps we can transpose it once and store it in a way that allows faster access. But that might complicate things. 

Alternatively, let's think about the memory access patterns. 

For each thread in the block (processing a particular k), the loop over l would access A's elements in a contiguous manner (since l is the innermost loop), so that's good for memory access. For B, each thread's l loop accesses B[l][k], which is a stride of K each step (since B is stored in row-major as L rows of K elements). So for each l increment, the B element is K steps away. That's bad for coalescing. 

Hmm, so perhaps transposing B to be K x L (so K is the first dimension) would help. Then B would be stored as K rows of L elements. Then, for each thread's k, accessing B[k][l], which would be contiguous in memory for each l. 

Alternatively, if the B matrix is transposed, then B's storage would be K*L, and the index would be k*L + l. 

So if B is transposed, then the kernel could read B[k*L + l], which would have better memory access for the loop. 

So maybe we should transpose B before passing it to the kernel. 

Let me adjust the code accordingly. 

So in the Python code, when preparing the input, transpose B to be (K, L) instead of (L, K). 

Wait, in the original problem, B is (L, K). So in the kernel, B is (L,K), so the element at l,k is B[l*K +k]. If we transpose B to be K x L, then the element at k,l is B[k*L +l]. 

Therefore, if we transpose B, then for each thread's k, the loop over l would access consecutive memory locations. 

This would significantly improve memory access patterns, leading to better performance. 

So the steps would be:

1. In Python, transpose B to K rows, L columns (so shape K, L).

2. Modify the kernel to use the transposed B. 

So the kernel's B is now K x L. 

Then the loop over l would be:

sum += A[b,i,j,l] * transposed_B[k][l]

Wait, the transposed B's layout is (K, L), so for a given k and l, it's B[k][l], which is stored at k*L + l. 

So in the code, B_index would be k*L + l. 

That's better for memory access. 

Therefore, in the kernel code:

sum += A[a_index] * B[bk_index];

where bk_index is k * L_dim + l.

Wait, let's recast:

Original B is (L, K). Transposed becomes (K, L). 

Thus, in the kernel, B's pointer points to the transposed matrix. 

So the kernel code becomes:

float sum = 0.0f;

for (int l = 0; l < L_dim; ++l) {
    const int a_index = b * I_dim * J_dim * L_dim + i * J_dim * L_dim + j * L_dim + l;
    const int bk_index = k * L_dim + l; // since B is K x L now
    sum += A[a_index] * B[bk_index];
}

Then, the index for B is optimized. 

This change would make the memory access for B's elements contiguous for each k and l, improving cache locality and memory bandwidth. 

So this is a crucial optimization step. 

Therefore, the plan is:

- In Python, before launching the kernel, transpose B to (K, L). 

- Modify the kernel to use the transposed B. 

Another optimization: since the kernel is launched with blocks of (B, I, J), and each block has K threads, the total number of blocks is B*I*J, which could be very large. For the test case, that's 8 *256 *512 = 1,048,576 blocks. 

CUDA has a maximum grid size in 3D dimensions, but since we're using 3D grid (blockIdx.x = b, y=i, z=j), the maximum per dimension is 65535. 

8 is okay for x, 256 for y, 512 for z? Let me see:

The maximum for each dimension is 2^31-1, so that's okay. 

Alternatively, perhaps using a 1D grid and mapping the indices differently. 

Alternatively, perhaps the block dimensions can be adjusted. 

Another possible optimization is to handle multiple k indices per thread. Since K is 768, which is divisible by 4 (768 /4=192), perhaps each thread can process 4 k's. But this complicates the indexing. 

Alternatively, use a 2D block. 

Wait, perhaps the initial approach is manageable. 

Now, the next step is to code this. 

So in the Python code:

def forward(self, A, B):
    # Transpose B to (K, L)
    B_transposed = B.t().contiguous()  # shape (K, L)
    # Launch kernel
    ...

Now, the kernel code:

The kernel function would take A, B_transposed, and output C. 

The kernel's parameters must include the dimensions (B, I, J, L, K). 

In the kernel code, the dimensions are passed as parameters. 

Now, in the Python code, when calling the kernel, we need to calculate the number of blocks and threads per block. 

The number of blocks is (B, I, J). Since blockIdx is 3D, we can set gridDim3(b, i, j). 

The block dimensions are (K, 1, 1), so blockDim.x = K. 

Wait, no. The block's threads are 1D, so blockDim.x is K. 

Thus, in the launch configuration:

dim3 blocks(B_dim, I_dim, J_dim);

dim3 threads(K_dim);

kernel<<<blocks, threads>>>(...);

But in CUDA, the maximum block size is 1024. Since K is 768, that's okay. 

Now, putting it all together. 

First, the CUDA kernel code:

#include <torch/extension.h>

__global__ void tensor_matmul_kernel(
    const float* A,
    const float* B_transposed,  // shape (K, L)
    float* C,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim) {

    int b = blockIdx.x;
    int i = blockIdx.y;
    int j = blockIdx.z;
    int k = threadIdx.x;

    if (b >= B_dim || i >= I_dim || j >= J_dim || k >= K_dim) return;

    float sum = 0.0f;

    for (int l = 0; l < L_dim; ++l) {
        // Compute index in A: A[b][i][j][l]
        int a_offset = b * I_dim * J_dim * L_dim;
        a_offset += i * J_dim * L_dim;
        a_offset += j * L_dim;
        a_offset += l;
        const float a_val = A[a_offset];

        // Compute index in B_transposed: B_transposed[k][l]
        int bk_offset = k * L_dim + l;
        const float b_val = B_transposed[bk_offset];

        sum += a_val * b_val;
    }

    // Compute output index C[b][i][j][k]
    int c_offset = b * I_dim * J_dim * K_dim;
    c_offset += i * J_dim * K_dim;
    c_offset += j * K_dim;
    c_offset += k;
    C[c_offset] = sum;
}

Then the wrapper function:

torch::Tensor tensor_matmul_cuda(
    torch::Tensor A,
    torch::Tensor B_transposed,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim) {

    auto C = torch::empty({B_dim, I_dim, J_dim, K_dim}, A.options());

    const int threads = K_dim;
    const dim3 blocks(B_dim, I_dim, J_dim);
    const dim3 threads_per_block(threads, 1, 1);

    tensor_matmul_kernel<<<blocks, threads_per_block>>>(
        A.data_ptr<float>(),
        B_transposed.data_ptr<float>(),
        C.data_ptr<float>(),
        B_dim, I_dim, J_dim, L_dim, K_dim);

    return C;
}

Wait, but in the kernel launch, the blocks are 3D. However, in CUDA, the maximum grid dimensions are 3D, so this should be okay. 

Now, in the Python code, the function that compiles this kernel needs to have the CUDA source and the wrapper function. 

The problem is that the dimensions (B_dim, I_dim, J_dim, L_dim, K_dim) need to be passed as parameters. But in the forward function, these can be extracted from the inputs. 

Wait, when calling the CUDA function from Python, the dimensions can be obtained from the tensors. 

In the Python forward function:

def forward(self, A, B):
    B_transposed = B.t().contiguous()
    B_dim, I_dim, J_dim, L_dim = A.shape
    K_dim = B.size(1)  # original B is (L, K), so transposed is (K, L), so K is first dimension's size?

    Wait, B was originally (L, K), so after transpose, it's (K, L). So K_dim is B_transposed.size(0).

    K_dim = B.size(1)  # original B's second dimension is K. 

    So in code:

    B_dim, I_dim, J_dim, L_dim = A.shape
    K_dim = B.size(1)

    Then call the CUDA function with these parameters. 

Wait, but in the kernel function's parameters, B_dim is the first dimension of A, which is the batch size. 

So the kernel function requires B_dim as the first dimension of A, so that's correct. 

Now, the Python code would have to include the CUDA source and the wrapper function. 

Putting it all together, the code for ModelNew would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the CUDA kernel
tensor_matmul_source = """
#include <torch/extension.h>

__global__ void tensor_matmul_kernel(
    const float* A,
    const float* B_transposed,
    float* C,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim) {

    int b = blockIdx.x;
    int i = blockIdx.y;
    int j = blockIdx.z;
    int k = threadIdx.x;

    if (b >= B_dim || i >= I_dim || j >= J_dim || k >= K_dim) return;

    float sum = 0.0f;

    for (int l = 0; l < L_dim; ++l) {
        int a_offset = b * I_dim * J_dim * L_dim;
        a_offset += i * J_dim * L_dim;
        a_offset += j * L_dim;
        a_offset += l;
        const float a_val = A[a_offset];

        int bk_offset = k * L_dim + l;
        const float b_val = B_transposed[bk_offset];

        sum += a_val * b_val;
    }

    int c_offset = b * I_dim * J_dim * K_dim;
    c_offset += i * J_dim * K_dim;
    c_offset += j * K_dim;
    c_offset += k;
    C[c_offset] = sum;
}

torch::Tensor tensor_matmul_cuda(
    torch::Tensor A,
    torch::Tensor B_transposed,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim) {

    auto C = torch::empty({B_dim, I_dim, J_dim, K_dim}, A.options());

    const int threads = K_dim;
    const dim3 blocks(B_dim, I_dim, J_dim);
    const dim3 threads_per_block(threads, 1, 1);

    tensor_matmul_kernel<<<blocks, threads_per_block>>>(
        A.data_ptr<float>(),
        B_transposed.data_ptr<float>(),
        C.data_ptr<float>(),
        B_dim, I_dim, J_dim, L_dim, K_dim);

    return C;
}
"""

# The header for the C++ code
tensor_matmul_cpp_source = """
torch::Tensor tensor_matmul_cuda(
    torch::Tensor A,
    torch::Tensor B_transposed,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim);
"""

# Compile the CUDA code
tensor_matmul = load_inline(
    name="tensor_matmul",
    cpp_sources=tensor_matmul_cpp_source,
    cuda_sources=tensor_matmul_source,
    functions=["tensor_matmul_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()

    def forward(self, A, B):
        B_transposed = B.t().contiguous()
        B_dim, I_dim, J_dim, L_dim = A.shape
        K_dim = B.size(1)
        return tensor_matmul.tensor_matmul_cuda(
            A,
            B_transposed,
            B_dim, I_dim, J_dim, L_dim, K_dim
        )

Wait, but in the Python code, the kernel function's parameters are A, B_transposed, B_dim, I_dim, etc. These need to be correctly passed. 

Also, in the forward function, when we call the CUDA function, the dimensions are extracted from A and B. 

Testing the code:

Suppose A has shape (8,256,512,256), B is (256, 768). 

Then B_transposed is (768,256). 

B_dim is 8, I_dim 256, J_dim 512, L_dim 256, K_dim 768. 

So the code should work. 

Potential issues:

- The grid dimensions for blockIdx: For B=8, I=256, J=512, the blockIdx is 3D. The maximum grid size in each dimension is 2^31, so 8, 256, 512 are all within limits. 

- Threads per block: K is 768, which is less than 1024. 

- The loop over L (256 iterations per thread) might be acceptable, but could be optimized further. 

Another possible optimization is to use shared memory for the B matrix. Since B is (K, L), but each block is processing a particular k. Wait, no, each block is processing a (b,i,j) group, and each thread in the block is handling a k. 

Alternatively, for each block, the B matrix for the current k's can be loaded into shared memory. However, since each block has a different k (each thread in the block has a different k), that may not be feasible. 

Alternatively, since B is fixed for all blocks, we could load the entire B matrix into shared memory once per block, but the size is K*L = 768*256 = 196,608 floats, which is about 786KB. The shared memory per block is limited (e.g., 48KB for some GPUs), so this would not be possible. 

Thus, this approach is the best we can do without more complex optimizations. 

Another thing to note is that in the kernel, the loops are unrolled in a way that may not be optimal, but given that it's a simple loop, it's manageable. 

Potential improvements for the kernel:

- Use vector types for the accumulation. For example, using float4 to process 4 elements of l at a time. 

- Use warp-level parallelism to reduce the number of loops. 

Alternatively, the current kernel may be sufficient for a starting point. 

Thus, the code provided above should be a valid and efficient custom CUDA kernel replacing the einsum operation.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the CUDA kernel for tensor-matrix multiplication
tensor_matmul_source = """
#include <torch/extension.h>

__global__ void tensor_matmul_kernel(
    const float* A,
    const float* B_transposed,
    float* C,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim) {

    int b = blockIdx.x;
    int i = blockIdx.y;
    int j = blockIdx.z;
    int k = threadIdx.x;

    if (b >= B_dim || i >= I_dim || j >= J_dim || k >= K_dim) return;

    float sum = 0.0f;

    for (int l = 0; l < L_dim; ++l) {
        int a_offset = b * I_dim * J_dim * L_dim;
        a_offset += i * J_dim * L_dim;
        a_offset += j * L_dim;
        a_offset += l;
        const float a_val = A[a_offset];

        int bk_offset = k * L_dim + l;
        const float b_val = B_transposed[bk_offset];

        sum += a_val * b_val;
    }

    int c_offset = b * I_dim * J_dim * K_dim;
    c_offset += i * J_dim * K_dim;
    c_offset += j * K_dim;
    c_offset += k;
    C[c_offset] = sum;
}

torch::Tensor tensor_matmul_cuda(
    torch::Tensor A,
    torch::Tensor B_transposed,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim) {

    auto C = torch::zeros({B_dim, I_dim, J_dim, K_dim}, A.options());

    const int threads = K_dim;
    const dim3 blocks(B_dim, I_dim, J_dim);
    const dim3 threads_per_block(threads, 1, 1);

    tensor_matmul_kernel<<<blocks, threads_per_block>>>(
        A.data_ptr<float>(),
        B_transposed.data_ptr<float>(),
        C.data_ptr<float>(),
        B_dim, I_dim, J_dim, L_dim, K_dim);

    return C;
}
"""

tensor_matmul_cpp_source = """
torch::Tensor tensor_matmul_cuda(
    torch::Tensor A,
    torch::Tensor B_transposed,
    int B_dim,
    int I_dim,
    int J_dim,
    int L_dim,
    int K_dim);
"""

# Compile the CUDA kernel
tensor_matmul = load_inline(
    name="tensor_matmul",
    cpp_sources=tensor_matmul_cpp_source,
    cuda_sources=tensor_matmul_source,
    functions=["tensor_matmul_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()

    def forward(self, A, B):
        B_transposed = B.t().contiguous()  # Transpose B to (K, L) for efficient access
        B_dim, I_dim, J_dim, L_dim = A.shape
        K_dim = B.size(1)
        return tensor_matmul.tensor_matmul_cuda(
            A,
            B_transposed,
            B_dim, I_dim, J_dim, L_dim, K_dim
        )
```