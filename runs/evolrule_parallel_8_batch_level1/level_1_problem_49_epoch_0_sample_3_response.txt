The user requires that when writing the new architecture, you must inline the CUDA code. You may choose to do operator fusion, replace operators, etc. To write the CUDA kernel inline, you can follow the syntax of the example provided, using torch.utils.cpp_extension.load_inline to include the CUDA code within the Python script. Make sure that the kernel is correctly written and that the Python class correctly calls the kernel. Also, make sure to have the same function signature for get_inputs and get_init_inputs.

Also, the user requires that the code must be compatible with PyTorch 2.0.1. Additionally, the solution must not use any third-party libraries or dependencies beyond PyTorch itself. Also, you must use the same architecture and not change the input/output dimensions or the overall functionality of the model. The model must remain a max reduction over a given dimension.

The user also specifies that you must make a real optimization that is better than the default PyTorch implementation, not just a copy of the original code. So your implementation must be faster than the PyTorch's native torch.max function on the given problem's input size. 

The user also requires that the code must be compatible with CUDA 11.8. Also, the problem input is of size (batch_size, dim1, dim2), which is (128, 4096, 4095). The reduction is over the specified dimension (which can be any integer, but in practice, given the input shape, the dim can be 0,1, or 2). The output dimension should match PyTorch's torch.max behavior.



Okay, I need to optimize the given PyTorch model that performs a max reduction over a specific dimension using a custom CUDA kernel. The original model uses torch.max, which I'll replace with my own kernel to potentially get better performance. Let me start by understanding the problem.

The input tensor has dimensions (128, 4096, 4095), and the reduction is along the given dim (could be 0, 1, or 2). The output should be the maximum values along that dimension. Since the user wants a faster implementation than PyTorch's default, I need to ensure that my kernel is efficient for these tensor sizes.

First, I'll think about how PyTorch's torch.max works. It iterates over the elements along the specified dimension, keeping track of the maximum value. For large tensors, a CUDA kernel can be optimized by efficiently handling memory access and parallelism.

The key is to structure the kernel such that each thread or block processes a segment of the data. Let's consider the dimensions. The reduction is over dim, so for example, if dim is 2 (the last dimension of size 4095), each thread could process a slice along that dimension.

Wait, but the dimensions can be any of the three. Hmm, the kernel needs to handle different dimensions. However, for the sake of optimization, perhaps I can specialize the kernel for the specific cases, but the problem requires it to work for any dim. Alternatively, the kernel can dynamically determine the dimension to reduce over.

Alternatively, maybe the kernel can be written in a way that's flexible. Let's think about how to structure the kernel.

The approach would be:

1. For each element in the output tensor, compute the max along the specified dimension.

The output tensor's dimensions will be the input shape without the reduced dimension. For example, if reducing over dim=2 (size 4095), the output will be (128, 4096).

So, the output size is input.size()[:dim] + input.size()[dim+1:].

To compute this efficiently in CUDA, each thread can be responsible for a block along the reduced dimension.

Wait, perhaps a better approach is to have each thread process a single output element and iterate over the elements along the reduced dimension. However, for large dimensions (like 4095), this could lead to a lot of iterations, which might be slow. Alternatively, using a parallel reduction approach where multiple threads handle parts of the reduction.

Alternatively, for each output element (which is the max along the dim), the kernel can have a thread block process a whole slice along the dim. For instance, if dim is 2, then for each (batch, dim1) index, the thread block processes all elements along the dim2 dimension.

Let me think of the dimensions. Suppose the input is B x D1 x D2. Suppose dim is 2 (so D2 is the reduction dim). The output is B x D1. For each output element at (b, d1), we need to compute the max of input[b][d1][0..D2-1].

So, for each (b, d1) position, the thread can process that by iterating through all the elements in D2. But if D2 is 4095, that's 4095 elements per output element. Iterating that in a single thread would be slow. Instead, we can parallelize across the D2 dimension.

Wait, maybe the best approach is to have each thread handle a single element along the reduction dimension. For example, if the reduction is over dim=2 (size 4095), then for each output position (b, d1), we can have 4095 threads (or a block of threads) to compute the maximum.

Alternatively, let's structure it as follows:

- The grid is divided such that each block handles a single output element (i.e., a position in the output tensor).

- Each block has as many threads as the reduction dimension size (e.g., 4095). Each thread in the block loads one element along the reduction dimension and compares it to a running maximum, using shared memory for synchronization.

Wait, but 4095 threads per block might be too many. The maximum threads per block on modern GPUs is 1024. So that's a problem. So we can't have a thread per element in the reduction dimension when the dimension is larger than 1024.

Hmm, so maybe we need a two-step reduction: first, each block processes a portion of the data, and then a final step computes the maximum among those partial maxima.

Alternatively, use a shared memory-based approach where each thread in a block processes a subset of the elements along the reduction dimension, computes their max, then combines the results in shared memory.

Let me outline a possible approach for when the reduction is along the last dimension (dim=2):

Each output element (b, d1) requires the maximum over D2 elements. Let's say D2 is 4095. To compute this, we can have a block of threads for each output element. The block can have 256 threads. Each thread processes 4095 / 256 ≈ 16 elements (maybe with some remainder). Each thread computes the max of their assigned elements, stores it in shared memory. Then, a second phase reduces the shared memory to find the overall maximum.

This way, each thread handles a manageable number of elements. Let me think of the steps in code:

1. The grid is arranged such that each block corresponds to an output element. The number of blocks is equal to the number of output elements (B*D1 if dim=2).

2. Each block has, say, 256 threads. The number of threads can be chosen as a fixed block size (e.g., 256).

3. Each thread in the block processes a chunk of the elements along the reduction dimension. The chunk size is ceil(D2 / blockDim.x). For example, if D2 is 4095 and blockDim.x is 256, each thread processes about 16 elements.

Wait, 4095 / 256 ≈ 16. So each thread would process 16 elements. The maximum of those elements is computed per thread, then stored in shared memory. Then, a reduction within the block is done to find the final maximum.

Alternatively, the first step is to compute a partial max for each thread's chunk, then use a parallel reduction within the block to combine the partial maxima.

This approach should be efficient for large D2. The key is to structure the kernel so that each thread has sufficient work to hide latency, and the block size is chosen to maximize occupancy.

Now, the problem is that the dimension over which we are reducing can be any of the three dimensions. So the kernel must handle different reduction dimensions. How can we do that?

Hmm, the kernel needs to be flexible. Let's see the input dimensions:

The input is a tensor of size (B, D1, D2). The dim can be 0, 1, or 2. The kernel must be able to handle all these cases.

To handle variable dimensions, the kernel can take the dimension as an input parameter. The code inside the kernel will compute indices accordingly.

But in CUDA, the kernel parameters are known at launch time. So we can have the kernel take the dimension as an argument, and compute the indices based on that.

Wait, but the problem is that in the current setup, the user might pass any dim (0,1,2). The kernel must be able to handle that. So, for example, when dim is 0 (B is the reduction dim), the output size becomes (D1, D2). Each output element is the max over the B elements in the batch dimension for that position.

In that case, the kernel would need to process along the batch dimension. Let's think of the indices.

Suppose the kernel is launched with a grid where each block is responsible for a single output element. The output element's coordinates depend on the reduction dim.

Alternatively, the kernel can be written in a way that the input tensor's strides and the dimension are handled via parameters. Let me think of the kernel parameters.

The kernel function would need to know the input tensor's dimensions, the reduction dimension, and the output tensor's strides. Wait, but in PyTorch, when using CUDA kernels, we can pass tensors and their properties.

Alternatively, perhaps the kernel can be written to handle any dimension by using the tensor's strides and sizes. Let me outline the parameters:

The kernel function could have parameters:

- const float* input_data,

- int dim,

- int num_dims,

- int* input_strides,

- int input_size,

- float* output,

- ... (other parameters like grid and block sizes?)

Wait, perhaps it's better to precompute some parameters on the host side and pass them to the kernel. For example, the kernel can be given the input dimensions, the reduction dimension, and the output dimensions.

Alternatively, maybe it's better to have separate kernels for each dimension, but that complicates things. To avoid code duplication, perhaps the kernel can compute indices based on the reduction dimension.

Let me try to structure the kernel function.

First, the kernel will process each output element. Let's consider that the grid is such that each thread block is responsible for one output element. The block dimension (number of blocks) is equal to the number of output elements. Each block has a certain number of threads (e.g., 256), which process the elements along the reduction dimension.

Wait, but the output elements can be in any dimension. Let's consider that the output has a size of (B, D1, D2) with one dimension removed. Let's say the output has N elements. The grid size is N blocks, each block processes one output element.

Inside each block:

- The block's blockIdx.x corresponds to the output element's index in the output tensor's linearized space.

- The kernel needs to compute the indices in the input tensor for this output element along all dimensions except the reduction dimension.

- Then, for each element along the reduction dimension, compute the maximum.

But how to compute the indices?

Perhaps, the input tensor's shape is stored as a tensor, and the reduction dimension is known. Let me think of the kernel parameters.

Suppose the kernel is called with:

- input: a float* pointer to the input data,

- output: a float* pointer to the output data,

- dim: the reduction dimension (0, 1, or 2),

- input_dims: an array of the input tensor's dimensions (e.g., [B, D1, D2]),

- output_dims: an array of the output tensor's dimensions (input_dims without dim),

- input_strides: an array of strides for each dimension in the input,

- output_strides: an array of strides for the output.

Alternatively, maybe the strides can be precomputed. But passing arrays to CUDA kernels can be done by pointers.

Alternatively, the kernel can compute the strides based on the input dimensions. Since the input is a contiguous tensor (assuming it's contiguous), the strides can be inferred. Wait, but in PyTorch, tensors can be non-contiguous, but perhaps the user expects contiguous inputs. Alternatively, for simplicity, let's assume the input is contiguous. Then the strides can be computed as the product of the sizes of the dimensions after the current one.

Wait, for a contiguous tensor of dimensions [B, D1, D2], the strides are:

- stride[0] = D1 * D2,

- stride[1] = D2,

- stride[2] = 1.

But this might not hold if the tensor is not contiguous. However, perhaps the user will ensure that inputs are contiguous. Or maybe the code can handle any strides, but that complicates things. Let's proceed under the assumption that the input is contiguous, which is common in PyTorch unless specified otherwise.

Thus, in the kernel, given the input dimensions, we can compute the strides as follows.

First, the output's linear index (blockIdx.x) needs to be converted into the output's multi-dimensional indices. Then, we need to map those indices to the input's indices, excluding the reduction dimension, and then iterate over the reduction dimension's elements.

Let me try to outline the steps in code.

In the kernel:

Each block corresponds to an output element (blockIdx.x). The block's output index is blockIdx.x. To compute the input indices (excluding the reduction dimension), we need to convert blockIdx.x into the output coordinates, then expand into input coordinates by inserting the reduction dimension's index (which will vary over the reduction steps).

Wait, perhaps it's easier to compute the output's multi-dimensional indices first.

Suppose the output has dimensions [B, D1, D2] without the dim dimension. Let's say the output has dimensions O_dims, which are input_dims without the dim. The output's linear index can be converted to O_coords (coordinates in O_dims). Then, to get the input coordinates, we need to insert the reduction dimension's index (which runs from 0 to input_dims[dim]-1).

For example, if dim is 2 (the last dimension), then the input coordinates are:

input_coords[0] = O_coord0,

input_coords[1] = O_coord1,

input_coords[2] = r (the reduction index).

The input's linear index is then computed as:

input_index = O_coord0 * D1 * D2 + O_coord1 * D2 + r,

for each r from 0 to D2-1.

The maximum over all input[input_index] for r in 0..D2-1 is stored in output[output_index].

So, within each block (output element), the kernel needs to loop over all r in 0..(input_dims[dim]-1), compute the maximum.

But doing this sequentially in a single thread would be slow. Instead, we can parallelize this loop across threads in the block.

Each block has N threads (e.g., 256 threads). Each thread can process a chunk of the reduction dimension.

Wait, but the number of elements along the reduction dimension can be variable (like 128, 4096, 4095 in the given example). Let's suppose that the reduction dimension's size is S (e.g., for dim=2, S=4095). Each thread in the block can process S / blockDim.x elements.

Wait, but 4095 is an odd number. So each thread would process approximately S / blockDim.x elements. Let me think of the code:

Within a block:

int s = input_dims[dim]; // the size of the reduction dimension.

int tid = threadIdx.x;

int num_threads = blockDim.x;

for (int r = tid; r < s; r += num_threads) {

    ... compute the value and track the max.

}

But even better, each thread can process a range of r values and compute their own local maximum, then combine them.

Alternatively, we can have each thread process a chunk of the reduction dimension elements, compute a local maximum, and then use a block-wide reduction to get the final maximum.

So here's the plan:

For a given output element (blockIdx.x):

1. Compute the output coordinates (O_coords).

2. Compute the input coordinates without the reduction dimension (i.e., O_coords are mapped to the input's coordinates except for the reduction dimension).

3. The threads in the block will each process a portion of the reduction dimension (r from 0 to S-1).

4. Each thread computes the local maximum over its assigned elements.

5. Use a parallel reduction within the block to combine all threads' local maxima into the final maximum for this output element.

6. Write the result to the output.

The parallel reduction within the block can be done using shared memory. For example:

- Each thread writes its local maximum to shared memory.

- Then, perform a reduction across the block's threads using log2(blockDim.x) steps.

This requires that the block size is a power of two. Let's choose a block size of 256 (which is a power of two).

Now, let's structure this in code.

First, define the shared memory array:

__shared__ float s_data[256]; // assuming block size 256.

In the kernel:

Each thread processes a chunk of the reduction dimension:

int s = input_dims[dim]; // size of reduction dimension

float local_max = -INFINITY;

for (int r = threadIdx.x; r < s; r += blockDim.x) {

    // compute input index for this r

    int input_idx = compute_input_index(O_coords, r, dim, input_strides);

    float val = input[input_idx];

    if (val > local_max) {

        local_max = val;

    }

}

// write local_max to shared memory

s_data[threadIdx.x] = local_max;

// synchronize threads

__syncthreads();

// then perform block reduction

for (int stride = blockDim.x/2; stride > 0; stride >>=1) {

    if (threadIdx.x < stride) {

        float val = s_data[threadIdx.x + stride];

        if (val > s_data[threadIdx.x]) {

            s_data[threadIdx.x] = val;

        }

    }

    __syncthreads();

}

// The final max is in s_data[0]

if (threadIdx.x == 0) {

    output[blockIdx.x] = s_data[0];

}

Wait, but this requires that the threads in the block process all elements in the reduction dimension. However, if the number of elements S is less than the block size (e.g., if the reduction dimension is 0 with size 128), then some threads will have r >= s and thus not contribute anything. So in that case, the local_max would remain -infinity, but since the actual elements are being processed, the first thread would have the correct value, but others may have -inf. The reduction would still work because the maximum between a valid value and -infinity would be correct.

Wait, no. Because if the first thread (tid=0) has the correct value (say 5), and another thread (tid=1) has r starting at 1, but if S is 1 (so only r=0 is valid), then tid=1 would process r=1 which is beyond, so it would read garbage or an out-of-bound element. Wait no, the loop for r starts at tid and increments by blockDim.x. So when S is 1 and blockDim.x is 256, then only the first thread (tid=0) would process r=0, and others would process r >=256, which are beyond S. So in that case, those threads would not enter the loop, so their local_max remains -inf. However, the first thread's local_max is correct, but the rest would have -inf. So when combining, the first thread's value would be correct, but others would contribute -inf, which is okay because the max between 5 and -inf is 5. But if the first thread's value was -inf (if the input elements were all -inf?), then that's correct. But in general, that's okay.

Wait, but if the input has valid values, the first thread's local_max would be correct. The others' local_max would be -inf, which doesn't affect the max. So the reduction would work correctly.

Now, the main challenge is to compute the input index given the output coordinates and the reduction dimension.

To compute the input index, we need to:

1. Convert the output's linear index (blockIdx.x) into its multi-dimensional coordinates in the output tensor.

2. Then, for each possible reduction dimension, compute the input coordinates by inserting the current r (the reduction dimension's index) into the appropriate position.

For example:

Let's say the input dimensions are [B, D1, D2].

The output dimensions when reducing over dim=0 (B) would be [D1, D2], so the output index can be mapped to (d1, d2).

The input coordinates would then be (r, d1, d2).

The input index is then r * D1 * D2 + d1 * D2 + d2.

Wait, but that's only for contiguous tensors. So, for contiguous tensors, the strides can be computed as:

stride[0] = D1 * D2,

stride[1] = D2,

stride[2] = 1.

Thus, the input index for coordinates (b, d1, d2) would be b*stride[0] + d1*stride[1] + d2*stride[2].

Alternatively, the input index can be computed as:

input_index = (output_coords[0] * output_stride[0] + ... ) plus the r component.

Hmm, perhaps the easiest way is to precompute the input strides and dimensions.

Alternatively, perhaps it's better to precompute the necessary parameters on the host side and pass them to the kernel. For example:

- The input's dimensions (B, D1, D2).

- The output's dimensions (O_dims).

- The reduction dimension (dim).

- The stride of the reduction dimension in the input (input_strides[dim]).

Wait, the input stride for the reduction dimension would be the size of the inner dimensions. For example, if reducing over dim=0 (B), the stride is D1*D2. For dim=1 (D1), the stride is D2. For dim=2 (D2), the stride is 1.

So, if we can pass the stride of the reduction dimension, then the input index can be computed as follows:

The base input index (without the reduction dimension) can be computed as:

base = output_index * product_of_outer_strides + (other terms?).

Alternatively, perhaps the easiest way is to compute the output coordinates from the output's linear index, then compute the input's coordinates by inserting the reduction dimension's index r.

Let me think of an example:

Suppose input is B x D1 x D2, and the output is when reducing over dim=0 (B):

The output dimensions are D1 x D2. So the output's linear index 'out_idx' can be converted to (d1, d2) via:

d1 = (out_idx) / D2,

d2 = out_idx % D2.

Then, the input coordinates for a given r (reduction index along B) would be (r, d1, d2). The input index is:

r * D1 * D2 + d1 * D2 + d2.

Alternatively, for contiguous tensors, the stride for dim=0 is D1*D2, so the input index is:

base = (d1 * D2 + d2),

input_index = r * stride_0 + base.

So, the base can be computed as (d1 * D2 + d2).

But how to compute d1 and d2 from the output index?

Alternatively, since the output is of size (D1 x D2), the output's stride for the first dimension (d1) is D2, so:

d1 = (out_idx) / D2,

d2 = out_idx % D2.

Hence, base = d1 * D2 + d2 = out_idx. Wait, that's just the output index. So the input index is r * stride_0 + out_idx (assuming the output is contiguous? Not sure. Maybe this is getting too tangled.

Alternatively, perhaps it's better to precompute for the kernel:

The strides of the input tensor for each dimension, and the output tensor's strides. But passing arrays to kernels can be done via pointers.

Wait, in CUDA, you can pass pointer to arrays as parameters. So, for example, the kernel can accept:

int *input_dims,

int input_ndim,

int dim,

int *input_strides,

float *input,

float *output,

int output_size,

etc.

But in the kernel code, we can pass the necessary parameters.

Alternatively, perhaps the easiest way is to have the kernel handle the different cases for dim 0, 1, or 2 via conditional compilation or if statements. Let me consider that approach.

Suppose the kernel is launched with the reduction dimension as a parameter. Then, inside the kernel, depending on the value of 'dim', different code paths are taken.

Wait, but in CUDA, parameters are known at kernel launch time. So 'dim' would be a parameter to the kernel function. So the kernel can have an 'int dim' parameter, and the code can branch based on that.

So here's a possible structure:

The kernel function:

__global__ void max_reduction_kernel(const float* input,

                                    float* output,

                                    int dim,

                                    int B, int D1, int D2,

                                    int output_size) {

    // Each block handles one output element.

    int out_idx = blockIdx.x;

    if (out_idx >= output_size) return;

    // Compute output coordinates.

    // Depending on dim, compute the input coordinates.

    int b, d1, d2;

    switch (dim) {

        case 0: {

            // reducing over B dimension (input is B x D1 x D2).

            // output dimensions: D1 x D2.

            // out_idx is d1 * D2 + d2.

            d1 = out_idx / D2;

            d2 = out_idx % D2;

            break;

        }

        case 1: {

            // reducing over D1 dimension (input is B x D1 x D2).

            // output dimensions: B x D2.

            // out_idx is b * D2 + d2.

            b = out_idx / D2;

            d2 = out_idx % D2;

            break;

        }

        case 2: {

            // reducing over D2 dimension (input is B x D1 x D2).

            // output dimensions: B x D1.

            // out_idx is b * D1 + d1.

            b = out_idx / D1;

            d1 = out_idx % D1;

            break;

        }

        default:

            assert(0 && "Invalid dim");

    }

    // Now, compute the input indices for each r in the reduction dimension.

    int s;

    switch (dim) {

        case 0: s = B; break;

        case 1: s = D1; break;

        case 2: s = D2; break;

    }

    // Compute the base input index (without the reduction dimension's index).

    int base;

    switch (dim) {

        case 0: base = d1 * D2 + d2; break;

        case 1: base = b * D2 + d2; break;

        case 2: base = b * D1 * D2 + d1 * D2; break;

    }

    // Now, for each r in 0..s-1:

    // input index is r * stride + base.

    // The stride for the reduction dimension:

    // For dim 0: stride = D1 * D2,

    // dim1: stride = D2,

    // dim2: stride = 1.

    int stride;

    switch (dim) {

        case 0: stride = D1 * D2; break;

        case 1: stride = D2; break;

        case 2: stride = 1; break;

    }

    float local_max = -INFINITY;

    for (int r = threadIdx.x; r < s; r += blockDim.x) {

        int input_idx = r * stride + base;

        float val = input[input_idx];

        if (val > local_max) {

            local_max = val;

        }

    }

    // Now perform block reduction.

    __shared__ float sdata[256]; // assuming blockDim.x is 256.

    sdata[threadIdx.x] = local_max;

    __syncthreads();

    // Reduce to sdata[0].

    for (int i = blockDim.x / 2; i > 0; i >>= 1) {

        if (threadIdx.x < i) {

            if (sdata[threadIdx.x + i] > sdata[threadIdx.x]) {

                sdata[threadIdx.x] = sdata[threadIdx.x + i];

            }

        }

        __syncthreads();

    }

    if (threadIdx.x == 0) {

        output[out_idx] = sdata[0];

    }

}

This seems manageable. Now, the parameters passed to the kernel need to include the input dimensions (B, D1, D2), the reduction dimension (dim), and the output size.

Now, the host code must compute the necessary parameters.

Wait, in the given problem, the input tensor can have any dimensions (but the get_inputs function uses batch_size=128, dim1=4096, dim2=4095). The kernel must work for any input dimensions, as long as the reduction dimension is valid.

In the host code (Python), when we call the kernel, we need to compute the input's dimensions, and pass them to the kernel.

For example, in the forward function of the ModelNew class:

def forward(self, x, dim):

    B, D1, D2 = x.size()

    output_size = x.size()[0] * x.size()[1] * x.size()[2] / x.size()[dim]

    Wait, not exactly. The output size depends on the reduction dim. For example:

If dim=0 (size B), output is D1*D2.

So output_size is (D1*D2) if dim=0,

(B*D2) if dim=1,

(B*D1) if dim=2.

Alternatively, output_size can be computed as x.numel() / x.size(dim).

Because the reduction dimension is removed, the output has (total elements) / (size of dim).

Thus, output_size = (x.numel()) // x.size(dim)

So in the Python code, when launching the kernel:

B, D1, D2 = x.shape

dim_val = self.dim  # assuming self.dim is stored from __init__

output_size = (B * D1 * D2) // x.shape[dim_val]

Then, the output tensor is initialized as torch.empty(output_size, device=x.device, dtype=x.dtype)

Now, the kernel launch parameters:

block_size = 256 (threads per block)

grid_size = output_size (number of blocks)

So, the kernel launch would be:

max_reduction_kernel<<<grid_size, block_size>>>(

    x.data_ptr(),

    output.data_ptr(),

    dim_val,

    B, D1, D2,

    output_size

);

Now, the CUDA code needs to be written in the Python script using load_inline.

Putting this all together, the Python code will have the CUDA kernel code as a string, and then load it using load_inline.

Wait, but in the example given, the kernel function name must be wrapped in a Python function. Let me structure this.

First, define the CUDA source code as a string:

max_reduction_source = """

#include <torch/extension.h>

#include <cuda_runtime.h>

__global__ void max_reduction_kernel(const float* input,

                                    float* output,

                                    int dim,

                                    int B, int D1, int D2,

                                    int output_size) {

    // ... kernel code as above ...

}

"""

Then, the corresponding CPP header:

max_reduction_cpp_source = """
extern "C" {

    void max_reduction_kernelLauncher(
        const float* input,
        float* output,
        int dim,
        int B, int D1, int D2,
        int output_size,
        int block_size);

    // Declare any other functions here.
}
"""

Wait, but to call the kernel from Python, we need a wrapper function that launches the kernel with the appropriate grid and block dimensions. So the wrapper function in C++ would handle the kernel launch.

Alternatively, perhaps it's better to have a wrapper function in the CUDA source.

Wait, in the example provided, the user shows that the CUDA function (elementwise_add_cuda) is written to call the kernel. So similarly, here, I need to write a Python-callable function that takes the tensors and launches the kernel.

So here's how it should be structured:

The CUDA source code includes the kernel and a host function that calls it.

The Python code uses load_inline to compile this, and then the host function is exposed as a Python function.

Thus, the CUDA source code would be:

max_reduction_source = """

#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_reduction_kernel(const float* input,
                                    float* output,
                                    int dim,
                                    int B, int D1, int D2,
                                    int output_size) {

    // ... kernel code as before ...

}

// Host function to launch the kernel.
void max_reduction_cuda(torch::Tensor input,
                       torch::Tensor output,
                       int dim,
                       int B, int D1, int D2,
                       int output_size) {

    int block_size = 256;
    int grid_size = output_size;

    // Compute the grid and block dimensions
    dim3 blocks(grid_size);
    dim3 threads(block_size);

    max_reduction_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim,
        B, D1, D2,
        output_size
    );

    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        printf("Error in kernel launch: %s\\n", cudaGetErrorString(err));
}

"""

Then, the corresponding header (cpp sources) would be:

max_reduction_cpp_source = """
void max_reduction_cuda(
    torch::Tensor input,
    torch::Tensor output,
    int dim,
    int B, int D1, int D2,
    int output_size);
"""

Wait, but in the Python code, when we call this function, we have to pass all the parameters. So in the forward function:

def forward(self, x):

    dim = self.dim  # stored from __init__

    B, D1, D2 = x.size()

    output_size = (B * D1 * D2) // x.size(dim)

    output = torch.empty(output_size, device=x.device, dtype=x.dtype)

    max_reduction_cuda(x, output, dim, B, D1, D2, output_size)

    return output

Wait, but the 'max_reduction_cuda' function is the one exposed via load_inline. The functions passed to load_inline must be listed in the functions parameter. So in the example, the functions=["elementwise_add_cuda"].

Thus, in this case, the functions parameter would be ["max_reduction_cuda"].

Now, putting this all together, the Python code would have:

from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

        # Compile the CUDA code
        self.max_reduction = load_inline(
            name="max_reduction",
            cpp_sources=max_reduction_cpp_source,
            cuda_sources=max_reduction_source,
            functions=["max_reduction_cuda"],
            verbose=True,
            extra_cflags=["-O3"],
            extra_ldflags=[""]
        )

    def forward(self, x):
        B, D1, D2 = x.size()
        output_size = (B * D1 * D2) // x.size(self.dim)
        output = torch.empty(output_size, device=x.device, dtype=x.dtype)
        # Call the CUDA function
        self.max_reduction.max_reduction_cuda(
            x.contiguous(),
            output,
            self.dim,
            B, D1, D2,
            output_size
        )
        return output

Wait, but in the kernel code, the input must be contiguous. Because the kernel assumes that the strides are computed based on contiguous tensors. So in the forward function, we should ensure that the input is contiguous. Hence, x.contiguous().

Now, I need to write the CUDA code correctly, making sure that all the computations are correct.

Wait, let me check the code again.

Inside the kernel, for the case when dim is 0 (reduction over B):

base = d1 * D2 + d2

stride for dim0 is B's stride, which is D1*D2. So the input index is r * (D1*D2) + base.

Wait, yes:

The input is of shape (B, D1, D2), so the stride for the B dimension is D1 * D2. So for a given r (index along B), the offset is r * stride_0 = r * D1*D2.

Adding base (d1*D2 + d2) gives the correct index.

Similarly for other cases.

Another case: dim=1 (reducing over D1 dimension):

The output is (B, D2).

The output index is b*D2 + d2.

The input coordinates are (b, r, d2), where r is the index over D1.

The stride for dim1 (D1) is D2. So each step in r adds D2 to the index. So the base is b*(D1*D2) + d2.

Wait, no:

Wait, for coordinates (b, r, d2):

The input index is b*(D1*D2) + r*D2 + d2.

The base would be (b * D1*D2) + d2.

The stride for dim1 (D1) is D2. So, for each r, the index is base + r * D2.

Wait, in code:

base = b * D1 * D2 + d2,

so input_idx = base + r * D2.

But in the code, the base was computed as:

for dim=1,

base = b * D2 + d2,

Wait, that would be incorrect.

Wait in the code above:

switch (dim) case 1:

base = b * D2 + d2;

Wait, that's not correct.

Wait, let me re-examine the code:

In the kernel code:

case 1: {

    // reducing over D1 dimension (input is B x D1 x D2).

    // output dimensions: B x D2.

    // out_idx is b * D2 + d2.

    b = out_idx / D2;

    d2 = out_idx % D2;

    break;

}

Then, when computing base for dim=1:

switch (dim) {

case 0: base = d1 * D2 + d2; break;

case 1: base = b * D2 + d2; break;

case 2: base = b * D1 * D2 + d1 * D2; break;

}

Wait, for dim=1, the base is supposed to be the offset excluding the D1 dimension. Wait, perhaps this is where the mistake is.

Wait, for dim=1 (the reduction is over D1):

The input coordinates are (b, r, d2). The input index is:

b * (D1*D2) + r * D2 + d2.

The reduction is over r (from 0 to D1-1).

The base is the part that doesn't depend on r: b * D1*D2 + d2.

But in the code above, for dim=1:

base = b * D2 + d2.

Which is wrong. So that's a mistake.

Ah, here is an error. Need to correct that.

Let me re-calculate for dim=1.

The base should be the input index without the D1 component (since r is varying over D1). The base is the part that remains when we remove the D1 dimension's index. 

Wait, for coordinates (b, r, d2), the index is:

b * (D1 * D2) + r * D2 + d2.

The base (the part that doesn't include r) is b * D1*D2 + d2.

Wait, no:

Wait, the r term is multiplied by D2. So the base is b * D1*D2 + d2, and then r*D2 is added. But that can't be, because D1 is the size of the D1 dimension.

Wait, actually, the base should be the part before the varying r component. The D1 dimension is the one being reduced over. So for each b and d2, varying r (from 0 to D1-1), the indices are:

b*(D1*D2) + r*D2 + d2.

So, the base is b * (D1*D2) + d2,

then for each r, add r*D2.

Hence, the stride is D2 (since each increment of r adds D2 to the index).

Wait, the stride for the D1 dimension is D2.

Thus, the base should be:

b*(D1*D2) + d2,

so when r increases by 1, the index increases by D2.

So in code for case 1 (dim=1):

base = b * (D1 * D2) + d2;

Wait, but how do I compute that?

In the code above, the variables B, D1, D2 are passed to the kernel.

Thus, in the base computation for dim=1:

base = b * D1 * D2 + d2;

But how do I get 'b' and 'd2'?

From the out_idx, when dim=1, the output index is out_idx = b * D2 + d2.

Thus, the b and d2 are:

b = out_idx / D2,

d2 = out_idx % D2.

Therefore, the base is:

b * D1 * D2 + d2.

Wait, but that requires knowing D1. Yes, since D1 is passed as a parameter to the kernel.

So the code for dim=1's base computation should be:

base = b * D1 * D2 + d2;

But in the previous code, it was written as base = b * D2 + d2, which is incorrect.

Ah, so this is a critical mistake. Need to fix that.

Similarly, for dim=2:

case 2:

base = b * D1 * D2 + d1 * D2;

Yes. Because the coordinates are (b, d1, r), where r is the varying D2 dimension. 

The index is b*(D1*D2) + d1*D2 + r.

The base is b*(D1*D2) + d1*D2,

then for each r (0 to D2-1), add r.

Thus, stride is 1.

Hence, the base for dim=2 is correct as written.

So the mistake was in the case for dim=1's base computation.

Let me correct that in the kernel code:

In the code for case 1 (dim=1):

base = b * D1 * D2 + d2;

Wait, but then, the stride for dim1 is D2.

Wait, yes, because for r increasing by 1, the index increases by D2.

Thus, the correct code for the base is as above.

So the corrected code for the base in case 1 is:

case 1: base = b * D1 * D2 + d2; break;

Therefore, in the kernel code, the base computation must be corrected.

Another mistake might be in the stride calculation for dim=1.

The stride for the reduction dimension (D1 in this case) is D2.

Thus, the stride in the kernel code for case 1 is:

case 1: stride = D2; break;

Which is correct.

Now, with this correction, the code should be correct.

Now, I need to write the CUDA kernel code with these fixes.

Another thing to consider: when the reduction dimension is 0, the base is computed as d1 * D2 + d2.

Which is correct because for coordinates (r, d1, d2), the base is the part without the B dimension's index (r). The base would be the offset for d1 and d2 within a single B slice.

Wait, the base is d1 * D2 + d2,

and the stride is D1*D2 (for dim0).

Thus, the input index is r * (D1*D2) + (d1 * D2 + d2).

Which is correct.

Now, the code should work.

Testing this in the given input dimensions:

Suppose input is (128, 4096, 4095), dim=2 (the last dimension).

Then, B=128, D1=4096, D2=4095.

For a given output index, say out_idx = 0:

d1 = 0, b = 0 (since for dim=2, the output is (B, D1), so out_idx = b*D1 + d1 → 0 = 0*4096 +0.

base = 0 *4096*4095 + 0*4095 → wait, no:

Wait, for dim=2 (case 2):

base = b * D1 * D2 + d1 * D2,

where b and d1 are computed from out_idx.

For dim=2:

output dimensions are B x D1 → output size is 128*4096.

out_idx = b * D1 + d1.

So for out_idx=0 → b=0, d1=0.

Thus base is 0 * (4096*4095) + 0 * 4095 → 0.

stride =1.

Thus, for r from 0 to 4094 (since D2=4095), the input indices are 0 + r*1 → r.

Thus, the kernel correctly iterates over all elements along the D2 dimension.

Another example for dim=0:

input is (128,4096,4095), output is (4096,4095).

out_idx =0 → d1=0, d2=0.

base =0 *4095 +0 → 0.

stride=4096*4095.

Thus, the input index for r (from 0 to 127) is r*(4096*4095) +0. Which is correct for the B dimension.

Thus, the code should work.

Now, let's code this correctly.

Now, the kernel code with corrections:

__global__ void max_reduction_kernel(const float* input,

                                    float* output,

                                    int dim,

                                    int B, int D1, int D2,

                                    int output_size) {

    int out_idx = blockIdx.x;

    if (out_idx >= output_size) return;

    int b, d1, d2;

    switch (dim) {

        case 0: { // reducing over B

            // output dimensions D1 x D2 → out_idx is d1*D2 +d2

            d1 = out_idx / D2;

            d2 = out_idx % D2;

            break;

        }

        case 1: { // reducing over D1

            // output dimensions B x D2 → out_idx is b*D2 +d2

            b = out_idx / D2;

            d2 = out_idx % D2;

            break;

        }

        case 2: { // reducing over D2

            // output dimensions B x D1 → out_idx is b*D1 +d1

            b = out_idx / D1;

            d1 = out_idx % D1;

            break;

        }

        default:

            assert(0 && "Invalid dim");

    }

    // Compute base and stride based on dim

    int s, stride;

    int base;

    switch (dim) {

        case 0: {

            s = B;

            stride = D1 * D2;

            base = d1 * D2 + d2;

            break;

        }

        case 1: {

            s = D1;

            stride = D2;

            base = b * D1 * D2 + d2;

            break;

        }

        case 2: {

            s = D2;

            stride = 1;

            base = b * D1 * D2 + d1 * D2;

            break;

        }

    }

    float local_max = -FLT_MAX; // Use FLT_MIN as -inf?

    // Iterate over r from 0 to s-1:

    for (int r = threadIdx.x; r < s; r += blockDim.x) {

        int input_idx = r * stride + base;

        float val = input[input_idx];

        if (val > local_max) {

            local_max = val;

        }

    }

    // Now perform block reduction

    __shared__ float sdata[256]; // assuming block size is 256.

    sdata[threadIdx.x] = local_max;

    __syncthreads();

    for (int i = blockDim.x / 2; i > 0; i >>= 1) {

        if (threadIdx.x < i) {

            if (sdata[threadIdx.x + i] > sdata[threadIdx.x]) {

                sdata[threadIdx.x] = sdata[threadIdx.x + i];

            }

        }

        __syncthreads();

    }

    if (threadIdx.x == 0) {

        output[out_idx] = sdata[0];

    }

}

Wait, but in C++, FLT_MAX is defined in <limits>, but in CUDA code, we can use the FLT_MIN constant or set local_max to -INFINITY. However, using a literal like -FLT_MAX (which is -3.4e38) is acceptable.

Alternatively, initialize local_max to -INFINITY. But in CUDA, we can use the constant:

const float neg_inf = -std::numeric_limits<float>::infinity();

But in CUDA, we might have to use constants available in the CUDA runtime.

Alternatively, initialize local_max to the first element of the reduction, but that requires a thread to read the first element. To avoid this, setting local_max to -FLT_MAX is safe.

Alternatively, using the first element as the initial value:

if (r == 0) { local_max = val; } else ... but in the loop.

Alternatively, starting with -FLT_MAX is better.

Wait, in the code above, the local_max is initialized to -FLT_MAX (assuming that the actual maximum of the elements is greater than that). That should be safe.

Thus, the code is correct.

Now, implementing this in the Python code.

Now, the full code for the ModelNew class.

Also, note that in the kernel launch, the block size must be 256. The code uses 256 threads per block.

Now, in the Python code, the get_inputs function must return tensors that are contiguous, as the kernel assumes contiguous data.

Thus, in the forward function, the input x is converted to contiguous.

Now, let me write the complete code.

First, the CUDA code:

max_reduction_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_reduction_kernel(const float* input,
                                    float* output,
                                    int dim,
                                    int B, int D1, int D2,
                                    int output_size) {

    int out_idx = blockIdx.x;
    if (out_idx >= output_size) return;

    int b, d1, d2;
    switch (dim) {
        case 0: { // reducing over B
            d1 = out_idx / D2;
            d2 = out_idx % D2;
            break;
        }
        case 1: { // reducing over D1
            b = out_idx / D2;
            d2 = out_idx % D2;
            break;
        }
        case 2: { // reducing over D2
            b = out_idx / D1;
            d1 = out_idx % D1;
            break;
        }
        default:
            assert(0 && "Invalid dim");
    }

    int s, stride;
    int base;
    switch (dim) {
        case 0: {
            s = B;
            stride = D1 * D2;
            base = d1 * D2 + d2;
            break;
        }
        case 1: {
            s = D1;
            stride = D2;
            base = b * D1 * D2 + d2;
            break;
        }
        case 2: {
            s = D2;
            stride = 1;
            base = b * D1 * D2 + d1 * D2;
            break;
        }
    }

    float local_max = -FLT_MAX;

    for (int r = threadIdx.x; r < s; r += blockDim.x) {
        int input_idx = r * stride + base;
        float val = input[input_idx];
        if (val > local_max) {
            local_max = val;
        }
    }

    __shared__ float sdata[256];
    sdata[threadIdx.x] = local_max;
    __syncthreads();

    for (int i = blockDim.x / 2; i > 0; i >>= 1) {
        if (threadIdx.x < i) {
            if (sdata[threadIdx.x + i] > sdata[threadIdx.x]) {
                sdata[threadIdx.x] = sdata[threadIdx.x + i];
            }
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        output[out_idx] = sdata[0];
    }
}

void max_reduction_cuda(
    torch::Tensor input,
    torch::Tensor output,
    int dim,
    int B, int D1, int D2,
    int output_size
) {
    int block_size = 256;
    int grid_size = output_size;

    dim3 blocks(grid_size);
    dim3 threads(block_size);

    max_reduction_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim,
        B, D1, D2,
        output_size
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("CUDA error: %s\\n", cudaGetErrorString(err));
        throw std::runtime_error("CUDA error");
    }
}
"""

max_reduction_cpp_source = """
extern "C" {
    void max_reduction_cuda(
        torch::Tensor input,
        torch::Tensor output,
        int dim,
        int B, int D1, int D2,
        int output_size
    );
}
"""

Then, the Python class:

from torch.utils.cpp_extension import load_inline
import torch
import torch.nn as nn

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

        # Compile the CUDA code
        self.max_reduction = load_inline(
            name="max_reduction",
            cpp_sources=max_reduction_cpp_source,
            cuda_sources=max_reduction_source,
            functions=["max_reduction_cuda"],
            verbose=True,
            extra_cflags=["-O3"],
            extra_ldflags=[""]
        )

    def forward(self, x):
        B, D1, D2 = x.size()
        output_size = (B * D1 * D2) // x.size(self.dim)
        output = torch.empty(output_size, device=x.device, dtype=x.dtype)
        self.max_reduction.max_reduction_cuda(
            x.contiguous(),
            output,
            self.dim,
            B, D1, D2,
            output_size
        )
        return output

def get_inputs():
    x = torch.rand(128, 4096, 4095).cuda()
    return [x]

def get_init_inputs():
    return []  # Or whatever is needed for initialization, but the original had [1]

Wait, in the original get_init_inputs returns [1], but according to the problem description, the user's original get_init_inputs is an example. Since the model's __init__ takes dim as an int, perhaps get_init_inputs should return the dim value. However, in the problem's original code, the get_init_inputs is:

def get_init_inputs():
    return []

But the problem says "randomly generate tensors required for initialization based on the model architecture". Since the model's __init__ only requires the dim (int), perhaps get_init_inputs returns a list containing the dim (e.g., a randomly chosen dimension). But the problem's original example had get_init_inputs returning [1]. To match the problem's original code, perhaps the new code should keep it as empty? Or perhaps the user expects the same as before.

Wait, the original Model class's __init__ requires dim as an argument. The get_init_inputs function in the original code returns [1], which might be an example. Since the user's problem says to keep the same function signatures, the new code must have get_init_inputs() return the same as before. So in the original code, get_init_inputs() returns [1], but perhaps the user expects to initialize the model with that. But in the problem statement, the get_init_inputs should return the parameters needed for initialization, which is the dim. So perhaps the new code's get_init_inputs should return a list with the dim value. But since the original example had [1], perhaps the user wants to keep it as is. Alternatively, the get_init_inputs can return a list with the dim parameter. But since the problem states to keep the same function signature, the new code must have the same get_init_inputs as before. The original get_init_inputs in the problem's code is:

def get_init_inputs():
    return [1] # Example, change to desired dimension

But the user says "the code must remain the same", so the new code should keep get_init_inputs as the original. So I should leave it as is.

Wait, the problem says "Output the new code in codeblocks in markdown format (i.e. ```python or ```cpp). Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Do not output testing code."

The get_init_inputs in the original code returns [1], which might be an example, but in the problem's model, the dim is a required argument. So the initialization requires that when creating the model, the dim is provided. Hence, get_init_inputs should return a list with the dim value (e.g., [dim]). However, the original code's get_init_inputs returns [1]. Since the user says to keep the function signatures the same, perhaps the new code should keep get_init_inputs as in the original.

Thus, in the new code, get_init_inputs remains:

def get_init_inputs():
    return [1]

But perhaps the user intended that the ModelNew's __init__ is called with the same parameters as the original. Since the original Model's __init__ takes dim as an int, the get_init_inputs should return a list containing the dim parameter (e.g., [2] if dim is 2). But the original code's get_init_inputs returns [1]. To be consistent with the problem's given code, I'll keep it as [1].

Thus, the final code is as above.

Now, check for any possible errors.

Wait, in the forward function, the code passes B, D1, D2 as parameters to the kernel. But in the case when the reduction dimension is 1, for example, the D1 parameter passed is the original D1 (4096), which is correct. Because the kernel uses it in the base computation for dim=1.

Another thing: in the kernel code, for dim=1's base:

base = b * D1 * D2 + d2;

This is correct because:

The coordinates are (b, r, d2), so the base is the offset of the first element in the D1 dimension for the given b and d2. The term b*D1*D2 is the offset for the first B slice, then adding d2 gives the offset within that B slice, but missing the D1 part. Wait, no:

Wait, the coordinates (b, r, d2) → the index is:

b*(D1 * D2) + r * D2 + d2.

The base is the part without the r term:

b*(D1 * D2) + d2,

then for each r, add r*D2.

Thus, the base is correct.

Thus, the code should be correct.

Now, compiling this code with PyTorch 2.0.1 and CUDA 11.8 should work.

Potential issues:

- The block size must be a power of two for the reduction steps. 256 is a power of two, so that's okay.

- The shared memory size must be sufficient. 256 floats is okay.

- The input must be contiguous. Thus, in forward, x is passed as x.contiguous().

This should be faster than PyTorch's native torch.max because:

- The kernel uses a block-wide reduction, reducing the number of memory accesses (each thread processes multiple elements).

- It uses parallel reduction within the block to minimize the number of operations needed to compute the maximum.

- The kernel is specialized for the particular problem's tensor dimensions, potentially leading to better memory access patterns and utilization of CUDA cores.

Thus, this should provide a speedup compared to the default implementation.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

max_reduction_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_reduction_kernel(const float* input,
                                    float* output,
                                    int dim,
                                    int B, int D1, int D2,
                                    int output_size) {

    int out_idx = blockIdx.x;
    if (out_idx >= output_size) return;

    int b, d1, d2;
    switch (dim) {
        case 0: { // reducing over B
            d1 = out_idx / D2;
            d2 = out_idx % D2;
            break;
        }
        case 1: { // reducing over D1
            b = out_idx / D2;
            d2 = out_idx % D2;
            break;
        }
        case 2: { // reducing over D2
            b = out_idx / D1;
            d1 = out_idx % D1;
            break;
        }
        default:
            assert(0 && "Invalid dim");
    }

    int s, stride;
    int base;
    switch (dim) {
        case 0: {
            s = B;
            stride = D1 * D2;
            base = d1 * D2 + d2;
            break;
        }
        case 1: {
            s = D1;
            stride = D2;
            base = b * D1 * D2 + d2;
            break;
        }
        case 2: {
            s = D2;
            stride = 1;
            base = b * D1 * D2 + d1 * D2;
            break;
        }
    }

    float local_max = -FLT_MAX;

    for (int r = threadIdx.x; r < s; r += blockDim.x) {
        int input_idx = r * stride + base;
        float val = input[input_idx];
        if (val > local_max) {
            local_max = val;
        }
    }

    __shared__ float sdata[256];
    sdata[threadIdx.x] = local_max;
    __syncthreads();

    for (int i = blockDim.x / 2; i > 0; i >>= 1) {
        if (threadIdx.x < i) {
            if (sdata[threadIdx.x + i] > sdata[threadIdx.x]) {
                sdata[threadIdx.x] = sdata[threadIdx.x + i];
            }
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        output[out_idx] = sdata[0];
    }
}

void max_reduction_cuda(
    torch::Tensor input,
    torch::Tensor output,
    int dim,
    int B, int D1, int D2,
    int output_size
) {
    int block_size = 256;
    int grid_size = output_size;

    dim3 blocks(grid_size);
    dim3 threads(block_size);

    max_reduction_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        dim,
        B, D1, D2,
        output_size
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("CUDA error: %s\\n", cudaGetErrorString(err));
        throw std::runtime_error("CUDA error");
    }
}
"""

max_reduction_cpp_source = """
extern "C" {
    void max_reduction_cuda(
        torch::Tensor input,
        torch::Tensor output,
        int dim,
        int B, int D1, int D2,
        int output_size
    );
}
"""

class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

        self.max_reduction = load_inline(
            name="max_reduction",
            cpp_sources=max_reduction_cpp_source,
            cuda_sources=max_reduction_source,
            functions=["max_reduction_cuda"],
            verbose=True,
            extra_cflags=["-O3"],
            extra_ldflags=[""]
        )

    def forward(self, x):
        B, D1, D2 = x.size()
        output_size = (B * D1 * D2) // x.size(self.dim)
        output = torch.empty(output_size, device=x.device, dtype=x.dtype)
        self.max_reduction.max_reduction_cuda(
            x.contiguous(),
            output,
            self.dim,
            B, D1, D2,
            output_size
        )
        return output

def get_inputs():
    x = torch.rand(128, 4096, 4095).cuda()
    return [x]

def get_init_inputs():
    return [1]
```