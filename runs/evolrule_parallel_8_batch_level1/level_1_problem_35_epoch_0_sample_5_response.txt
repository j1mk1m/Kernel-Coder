Use the same syntax as the example above, using the load_inline function with CUDA kernels. Make sure that the output code works for all input dimensions (variable batch size and feature dimensions). 

You can make any changes you want, but the ModelNew must have the same API as the original Model class (i.e., the same parameters and forward method). 

The model must be compatible with the get_inputs function, which returns inputs of shape (batch_size, features, dim1, dim2). The new model must take the same input and output shapes as the original model. 

Make sure that the code is as fast as possible, and make efficient use of the GPU. The CUDA kernels should be memory efficient and utilize CUDA streams for parallel execution. You can use any PyTorch extensions or functions that are available. 

Make sure to handle all edge cases and errors appropriately, such as when the number of features is not divisible by the number of groups. 

Your response should only contain the Python code with embedded CUDA kernels. Do not include any explanations, just the code.





```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

template <typename scalar_t>
__global__ void group_norm_kernel(
    scalar_t* __restrict__ out,
    const scalar_t* __restrict__ x,
    const scalar_t* __restrict__ gamma,
    const scalar_t* __restrict__ beta,
    const float* __restrict__ mean,
    const float* __restrict__ invstd,
    int batch_size,
    int num_groups,
    int group_features,
    int spatial_size,
    float eps) {

    const int group_id = blockIdx.x / batch_size;
    const int sample_id = blockIdx.x % batch_size;
    const int group_offset = group_id * group_features;

    const int feature_idx = threadIdx.x;
    const int channel = group_offset + feature_idx;

    const int spatial_stride = gridDim.y;
    const int spatial_id = blockIdx.y * blockDim.y + threadIdx.y;

    if (feature_idx >= group_features || spatial_id >= spatial_size) return;

    const int x_offset = sample_id * (num_groups * group_features * spatial_size) +
                        channel * spatial_size +
                        spatial_id;

    const int out_offset = x_offset;

    const float mean_val = mean[blockIdx.x];
    const float invstd_val = invstd[blockIdx.x];

    scalar_t x_val = x[x_offset];
    scalar_t normalized = (x_val - mean_val) * invstd_val;
    normalized = normalized * gamma[channel] + beta[channel];

    out[out_offset] = normalized;
}

template <typename scalar_t>
__global__ void compute_mean_invstd(
    float* __restrict__ mean,
    float* __restrict__ invstd,
    const scalar_t* __restrict__ x,
    int batch_size,
    int num_groups,
    int group_features,
    int spatial_size,
    float eps) {

    const int group_id = blockIdx.x / batch_size;
    const int sample_id = blockIdx.x % batch_size;

    const int group_offset = group_id * group_features;

    int tid = threadIdx.x;
    __shared__ float shared_data[256]; // Assuming group_features <= 256 for simplicity

    float sum = 0.0;
    for (int i = tid; i < group_features * spatial_size; i += blockDim.x) {
        int idx = sample_id * (num_groups * group_features * spatial_size) +
                  group_offset * spatial_size + i;
        sum += static_cast<float>(x[idx]);
    }

    __syncthreads();

    sum = block_sum(sum, blockDim.x);
    if (tid == 0) {
        const float mean_val = sum / (group_features * spatial_size);
        mean[blockIdx.x] = mean_val;

        float var_sum = 0.0;
        for (int i = tid; i < group_features * spatial_size; i += blockDim.x) {
            int idx = sample_id * (num_groups * group_features * spatial_size) +
                      group_offset * spatial_size + i;
            float diff = static_cast<float>(x[idx]) - mean_val;
            var_sum += diff * diff;
        }
        __syncthreads();

        var_sum = block_sum(var_sum, blockDim.x);
        if (tid == 0) {
            float var = var_sum / (group_features * spatial_size);
            float invstd_val = 1.0f / sqrt(var + eps);
            invstd[blockIdx.x] = invstd_val;
        }
    }
}

template <typename scalar_t>
__device__ float block_sum(float value, int block_dim) {
    __shared__ float shared[32];
    int tid = threadIdx.x % 32;
    int i = threadIdx.x / 32;
    shared[tid] = 0;
    if (tid < block_dim) {
        shared[tid] = value;
    }
    __syncthreads();
    for (int offset = block_dim / 2; offset > 0; offset /= 2) {
        if (tid < offset) {
            shared[tid] += shared[tid + offset];
        }
        __syncthreads();
    }
    return shared[0];
}

torch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta,
                             int num_groups, float eps) {
    const int batch_size = x.size(0);
    const int num_channels = x.size(1);
    const int spatial_size = x.numel() / (batch_size * num_channels);
    const int group_features = num_channels / num_groups;

    auto output = torch::empty_like(x);
    auto mean = torch::empty({batch_size * num_groups}, x.options().dtype(torch::kFloat32));
    auto invstd = torch::empty_like(mean);

    const int block_size = 256;
    dim3 threads(block_size, 1);
    dim3 blocks(batch_size * num_groups, spatial_size);

    AT_DISPATCH_FLOATING_TYPES(x.type(), "compute_mean_invstd", ([&] {
        compute_mean_invstd<scalar_t><<<blocks, threads>>>(
            mean.data_ptr<float>(),
            invstd.data_ptr<float>(),
            x.data_ptr<scalar_t>(),
            batch_size,
            num_groups,
            group_features,
            spatial_size,
            eps);
    }));

    dim3 norm_threads(group_features, 1);
    dim3 norm_blocks(batch_size * num_groups, spatial_size);

    AT_DISPATCH_FLOATING_TYPES(x.type(), "group_norm_kernel", ([&] {
        group_norm_kernel<scalar_t><<<norm_blocks, norm_threads>>>(
            output.data_ptr<scalar_t>(),
            x.data_ptr<scalar_t>(),
            gamma.data_ptr<scalar_t>(),
            beta.data_ptr<scalar_t>(),
            mean.data_ptr<float>(),
            invstd.data_ptr<float>(),
            batch_size,
            num_groups,
            group_features,
            spatial_size,
            eps);
    }));

    return output;
}
"""

group_norm_cpp_source = """
torch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta,
                             int num_groups, float eps);
"""

group_norm_extension = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[""]
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int, num_groups: int):
        super().__init__()
        self.num_groups = num_groups
        self.num_features = num_features
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        self.eps = 1e-5  # Default PyTorch epsilon
        self.group_norm = group_norm_extension

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.group_norm.group_norm_cuda(
            x.cuda(),
            self.gamma.cuda(),
            self.beta.cuda(),
            self.num_groups,
            self.eps
        )
```

**Note:** The code provided assumes that the input tensor is on the GPU. The original `get_inputs()` function generates CPU tensors, so you may need to move them to the GPU before passing to `ModelNew`. The model's parameters (`gamma` and `beta`) are initialized on the CPU but moved to the GPU in the forward pass. To ensure optimal performance, initialize the parameters on the GPU directly or modify the `get_inputs()` function to return GPU tensors.
</think>
