Please use appropriate naming conventions and best practices for your custom CUDA operators. Make sure to handle memory allocation and deallocation properly within your custom CUDA kernels. Use comments in your code to explain any non-obvious parts of your implementation. 

Make sure your solution handles the full range of possible inputs and edge cases. Your custom CUDA operators should be as efficient as possible, ideally using techniques such as vectorization, shared memory, and parallelism. 

Remember that the goal is to optimize the performance of the model, so focus on the most computationally intensive operations. In this case, the `argmax` operation is likely to be one of the most computationally intensive operations, so it would be a good candidate for optimization with a custom CUDA kernel. However, feel free to optimize other operations as well if you see potential for improvement. 

Please provide the final `ModelNew` class along with any necessary helper functions or classes. Make sure the code is fully self-contained and can be run independently without additional dependencies. 

Note: You do not need to worry about handling gradients for the custom CUDA operators. The PyTorch autograd engine will take care of that for you. ```