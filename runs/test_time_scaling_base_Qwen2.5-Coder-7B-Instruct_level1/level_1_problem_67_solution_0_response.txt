Please use PyTorch's `torch.utils.cpp_extension.load_inline` function to compile and load the custom CUDA kernels. You should aim to achieve at least 2x speedup over the original implementation for the forward pass of the Model. You can target any specific parts of the operation that would benefit most from custom CUDA kernels, such as the convolution operation itself. 

Remember to provide the full implementation of the `ModelNew` class, including the necessary imports and any additional helper functions or classes you might need. Make sure to document any changes you make to improve performance.