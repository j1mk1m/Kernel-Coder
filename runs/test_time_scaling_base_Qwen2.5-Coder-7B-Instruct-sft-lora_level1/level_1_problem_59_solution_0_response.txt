### Constraints:

- You can use any CUDA features and libraries.
- You must implement the entire forward pass using custom CUDA kernels where possible.
- Your implementation should be efficient and handle large batch sizes and feature maps.
- You should aim to reduce the computational overhead compared to PyTorch's default implementation.
- Your solution should be generalizable to different values of in_channels, out_channels, kernel_size, and other parameters.
- Make sure to include any necessary imports at the beginning of your code.
- Ensure that the code is well-commented to explain key parts of the implementation.
- Avoid using pre-built PyTorch operators in your custom kernels unless they are unavoidable (e.g., for data movement).
- Consider operator fusion and algorithmic optimizations to further improve performance.
- Handle edge cases such as non-uniform batch sizes or varying kernel sizes.

### Note:

- The goal is to demonstrate how custom CUDA kernels can be used to optimize deep learning models and achieve significant speedups.
- Feel free to experiment with different kernel designs and optimization techniques to find the most effective solution.
- The provided test code will help you verify that your implementation works correctly and efficiently.
```




















s
