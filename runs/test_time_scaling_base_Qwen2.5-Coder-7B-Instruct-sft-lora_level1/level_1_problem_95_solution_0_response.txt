```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation
cross_entropy_loss_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss
__global__ void cross_entropy_loss_kernel(const float* predictions, const int* targets, float* loss, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float log_prob = predictions[idx * num_classes + target_class];
        loss[idx] = -log_prob; // Negative log-likelihood
    }
}

torch::Tensor cross_entropy_loss_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), batch_size, num_classes);

    return loss.mean(); // Average loss over the batch
}
"""

cross_entropy_loss_cpp_source = (
    "torch::Tensor cross_entropy_loss_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation
cross_entropy_loss = load_inline(
    name="cross_entropy_loss",
    cpp_sources=cross_entropy_loss_cpp_source,
    cuda_sources=cross_entropy_loss_source,
    functions=["cross_entropy_loss_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss = cross_entropy_loss

    def forward(self, predictions, targets):
        return self.cross_entropy_loss.cross_entropy_loss_cuda(predictions, targets)
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick
cross_entropy_loss_logsumexp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick
__global__ void cross_entropy_loss_logsumexp_kernel(const float* predictions, const int* targets, float* loss, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood
    }
}

torch::Tensor cross_entropy_loss_logsumexp_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), batch_size, num_classes);

    return loss.mean(); // Average loss over the batch
}
"""

cross_entropy_loss_logsumexp_cpp_source = (
    "torch::Tensor cross_entropy_loss_logsumexp_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick
cross_entropy_loss_logsumexp = load_inline(
    name="cross_entropy_loss_logsumexp",
    cpp_sources=cross_entropy_loss_logsumexp_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_source,
    functions=["cross_entropy_loss_logsumexp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp = cross_entropy_loss_logsumexp

    def forward(self, predictions, targets):
        return self.cross_entropy_loss_logsumexp.cross_entropy_loss_logsumexp_cuda(predictions, targets)
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the batch and gradient scale
}
"""

cross_entropy_loss_logsumexp_grad_scale_cpp_source = (
    "std::tuple<torch::Tensor, torch::Tensor> cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets);"
)

# Compile the inline CUDA code for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale = load_inline(
    name="cross_entropy_loss_logsumexp_grad_scale",
    cpp_sources=cross_entropy_loss_logsumexp_grad_scale_cpp_source,
    cuda_sources=cross_entropy_loss_logsumexp_grad_scale_source,
    functions=["cross_entropy_loss_logsumexp_grad_scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cross_entropy_loss_logsumexp_grad_scale = cross_entropy_loss_logsumexp_grad_scale

    def forward(self, predictions, targets):
        loss, grad_scale = self.cross_entropy_loss_logsumexp_grad_scale.cross_entropy_loss_logsumexp_grad_scale_cuda(predictions, targets)
        return loss, grad_scale
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for cross-entropy loss computation using logsumexp trick and gradient scaling
cross_entropy_loss_logsumexp_grad_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel for computing cross-entropy loss using logsumexp trick and gradient scaling
__global__ void cross_entropy_loss_logsumexp_grad_scale_kernel(const float* predictions, const int* targets, float* loss, float* grad_scale, int batch_size, int num_classes) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_class = targets[idx];
        float max_logit = predictions[idx * num_classes];
        float exp_sum = 0.0f;
        for (int i = 0; i < num_classes; ++i) {
            float exp_val = exp(predictions[idx * num_classes + i] - max_logit);
            exp_sum += exp_val;
        }
        float log_exp_sum = log(exp_sum);
        float log_prob = predictions[idx * num_classes + target_class] - max_logit - log_exp_sum;
        loss[idx] = -log_prob; // Negative log-likelihood

        // Compute gradient scale
        float exp_target = exp(predictions[idx * num_classes + target_class] - max_logit);
        float sum_exp = exp_target + exp_sum - exp_target;
        grad_scale[idx] = exp_target / sum_exp;
    }
}

torch::Tensor cross_entropy_loss_logsumexp_grad_scale_cuda(torch::Tensor predictions, torch::Tensor targets) {
    auto batch_size = predictions.size(0);
    auto num_classes = predictions.size(1);
    auto loss = torch::zeros({batch_size}, predictions.options());
    auto grad_scale = torch::zeros({batch_size}, predictions.options());

    const int block_size = 256;
    const int num_blocks = (batch_size + block_size - 1) / block_size;

    cross_entropy_loss_logsumexp_grad_scale_kernel<<<num_blocks, block_size>>>(predictions.data_ptr<float>(), targets.data_ptr<int>(), loss.data_ptr<float>(), grad_scale.data_ptr<float>(), batch_size, num_classes);

    return {loss.mean(), grad_scale}; // Average loss over the