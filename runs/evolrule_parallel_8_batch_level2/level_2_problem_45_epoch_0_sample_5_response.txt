        Make sure to:
        - Replace the minimum number of operators to get the maximum speedup. For example, avoid rewriting operators that are already well-optimized in PyTorch (e.g., matmul).
        - Use the same architecture structure (same layers, etc.) but replace some of the PyTorch operators with custom CUDA implementations.
        - Only replace operators that have a significant speedup opportunity (e.g., avoid replacing simple operations like addition if PyTorch's implementation is already as fast as possible).
        - Include all necessary imports and helper functions, but exclude any code not related to the model definition and the get_inputs functions.
        - Ensure that the forward method of ModelNew has the same mathematical behavior as the original Model.
        - All custom CUDA kernels must be implemented inline using the torch.utils.cpp_extension.load_inline function, similar to the example provided.
        - The code must be compatible with PyTorch 1.13.1 and CUDA 12.1.

        Good luck! Here's the optimized ModelNew code with custom CUDA kernels for sigmoid and logsumexp operations, which are known to have significant optimization potential. Matrix multiplications (linear layers) are left as is since theyâ€™re well-optimized in PyTorch:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        
        # Inline CUDA kernel for Sigmoid activation
        sigmoid_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void sigmoid_kernel(const float* input, float* output, int size) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx < size) {
                output[idx] = 1.0f / (1.0f + expf(-input[idx]));
            }
        }

        torch::Tensor sigmoid_cuda(torch::Tensor input) {
            auto size = input.numel();
            auto output = torch::empty_like(input);
            
            const int block_size = 256;
            const int num_blocks = (size + block_size - 1) / block_size;

            sigmoid_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
            return output;
        }
        """
        
        # Inline CUDA kernel for LogSumExp
        logsumexp_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>
        #include <limits>

        __global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int features) {
            extern __shared__ float shared[];
            
            int sample = blockIdx.x;
            int tid = threadIdx.x;
            
            // Load data into shared memory
            for (int i = tid; i < features; i += blockDim.x) {
                shared[i] = input[sample * features + i];
            }
            __syncthreads();
            
            // Reduce using warp-level operations
            float max_val = -std::numeric_limits<float>::infinity();
            for (int i = tid; i < features; i += blockDim.x) {
                if (shared[i] > max_val) {
                    max_val = shared[i];
                }
            }
            __shared__ float block_max;
            if (tid == 0) {
                block_max = max_val;
                for (int i = 1; i < blockDim.x; i++) {
                    if (block_max < shared[i]) block_max = shared[i];
                }
                output[sample] = block_max;
            }
            __syncthreads();
            
            max_val = block_max;
            float sum = 0.0;
            for (int i = tid; i < features; i += blockDim.x) {
                sum += expf(shared[i] - max_val);
            }
            
            __shared__ float block_sum;
            if (tid == 0) {
                block_sum = 0.0;
                for (int i = 0; i < blockDim.x; i++) {
                    block_sum += shared[i];
                }
                output[sample] = logf(block_sum) + max_val;
            }
        }
        """
        
        # Compile the sigmoid kernel
        sigmoid_cpp = "torch::Tensor sigmoid_cuda(torch::Tensor input);"
        self.sigmoid = load_inline(
            name="sigmoid",
            cpp_sources=sigmoid_cpp,
            cuda_sources=sigmoid_source,
            functions=["sigmoid_cuda"],
            verbose=True
        )
        
        # Compile the logsumexp kernel
        logsumexp_cpp = "torch::Tensor logsumexp_cuda(torch::Tensor input, int batch_size, int features);"
        self.logsumexp = load_inline(
            name="logsumexp",
            cpp_sources=logsumexp_cpp,
            cuda_sources=logsumexp_source,
            functions=["logsumexp_cuda"],
            verbose=True
        )

    def forward(self, x):
        x = self.linear1(x)
        x = self.sigmoid.sigmoid_cuda(x)  # Custom sigmoid
        x = self.linear2(x)
        batch_size, features = x.size()
        return self.logsumexp.logsumexp_cuda(x, batch_size, features)  # Custom logsumexp

# Ensure get_inputs and get_init_inputs match original
def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, output_size]
```

Wait, I see an error in the logsumexp kernel code. The logsumexp implementation has some logic issues in the shared memory reduction. Let me fix that. Also, the kernel launch parameters might need adjustment. Let me correct the shared memory usage and the reduction steps properly.

Here's the corrected version with proper reduction steps using shared memory and warp-level operations:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        
        # Inline CUDA kernel for Sigmoid activation
        sigmoid_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void sigmoid_kernel(const float* input, float* output, int size) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx < size) {
                output[idx] = 1.0f / (1.0f + expf(-input[idx]));
            }
        }

        torch::Tensor sigmoid_cuda(torch::Tensor input) {
            auto size = input.numel();
            auto output = torch::empty_like(input);
            
            const int block_size = 256;
            const int num_blocks = (size + block_size - 1) / block_size;

            sigmoid_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
            return output;
        }
        """
        
        # Corrected inline CUDA kernel for LogSumExp
        logsumexp_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>
        #include <limits>

        __global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int features) {
            extern __shared__ float shared[];
            
            int sample = blockIdx.x;
            int tid = threadIdx.x;
            int block_size = blockDim.x;
            
            // Load data into shared memory
            for (int i = tid; i < features; i += block_size) {
                shared[i] = input[sample * features + i];
            }
            __syncthreads();
            
            // Compute max value
            float max_val = -std::numeric_limits<float>::infinity();
            for (int i = tid; i < features; i += block_size) {
                if (shared[i] > max_val) {
                    max_val = shared[i];
                }
            }
            
            // Block-level max reduction
            for (int s = block_size / 2; s > 0; s >>= 1) {
                if (tid < s) {
                    max_val = max(max_val, __shfl_down_sync(0xFFFFFFFF, max_val, s));
                }
                __syncthreads();
            }
            
            // All threads now have the block max
            if (tid == 0) {
                output[sample] = max_val;
            }
            __syncthreads();
            
            // Compute sum(exp(x - max))
            float sum = 0.0f;
            for (int i = tid; i < features; i += block_size) {
                sum += expf(shared[i] - max_val);
            }
            
            // Block-level sum reduction
            for (int s = block_size / 2; s > 0; s >>= 1) {
                sum += __shfl_down_sync(0xFFFFFFFF, sum, s);
            }
            
            if (tid == 0) {
                output[sample] = logf(sum) + max_val;
            }
        }
        """
        
        # Compile sigmoid kernel
        sigmoid_cpp = "torch::Tensor sigmoid_cuda(torch::Tensor input);"
        self.sigmoid = load_inline(
            name="sigmoid",
            cpp_sources=sigmoid_cpp,
            cuda_sources=sigmoid_source,
            functions=["sigmoid_cuda"],
            verbose=True
        )
        
        # Compile logsumexp kernel
        logsumexp_cpp = "torch::Tensor logsumexp_cuda(torch::Tensor input, int batch_size, int features);"
        self.logsumexp = load_inline(
            name="logsumexp",
            cpp_sources=logsumexp_cpp,
            cuda_sources=logsumexp_source,
            functions=["logsumexp_cuda"],
            verbose=True
        )

    def forward(self, x):
        x = self.linear1(x)
        x = self.sigmoid.sigmoid_cuda(x)
        x = self.linear2(x)
        batch_size, features = x.size()
        return self.logsumexp.logsumexp_cuda(x, batch_size, features)

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, output_size]
``` Your code has some errors in the logsumexp kernel implementation. Let me help you fix them. First, in the logsumexp kernel, the shared memory allocation is not properly calculated. The shared memory size must be at least the number of features per sample. Also, the kernel launch parameters for the logsumexp kernel should include the shared memory size. Additionally, the reduction steps for max and sum can be optimized using warp-level operations. Let me provide a corrected version of the logsumexp kernel with proper shared memory usage and reduction logic.

Also, the sigmoid kernel can be optimized by using fast approximate math functions if allowed, but since you requested correctness, we'll keep it as is. Here's the corrected version with proper shared memory handling and reduction steps:


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        
        # Inline CUDA kernel for Sigmoid activation
        sigmoid_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void sigmoid_kernel(const float* input, float* output, int size) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx < size) {
                output[idx] = 1.0f / (1.0f + expf(-input[idx]));
            }
        }

        torch::Tensor sigmoid_cuda(torch::Tensor input) {
            auto size = input.numel();
            auto output = torch::empty_like(input);
            
            const int block_size = 256;
            const int num_blocks = (size + block_size - 1) / block_size;

            sigmoid_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
            return output;
        }
        """
        
        # Corrected inline CUDA kernel for LogSumExp with proper shared memory and reduction
        logsumexp_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>
        #include <limits>

        __global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int features) {
            extern __shared__ float shared[];
            
            int sample = blockIdx.x;
            int tid = threadIdx.x;
            int block_size = blockDim.x;
            
            // Load data into shared memory
            for (int i = tid; i < features; i += block_size) {
                shared[i] = input[sample * features + i];
            }
            __syncthreads();
            
            // Compute max value using parallel reduction
            float max_val = -std::numeric_limits<float>::infinity();
            for (int i = tid; i < features; i += block_size) {
                if (shared[i] > max_val) {
                    max_val = shared[i];
                }
            }
            
            for (int s = block_size / 2; s > 0; s >>= 1) {
                if (tid < s) {
                    max_val = max(max_val, __shfl_down_sync(0xFFFFFFFF, max_val, s));
                }
                __syncthreads();
            }
            
            if (tid == 0) {
                output[sample] = max_val;
            }
            __syncthreads();
            
            // Compute sum(exp(x - max))
            float sum = 0.0f;
            for (int i = tid; i < features; i += block_size) {
                sum += expf(shared[i] - max_val);
            }
            
            for (int s = block_size / 2; s > 0; s >>= 1) {
                sum += __shfl_down_sync(0xFFFFFFFF, sum, s);
            }
            
            if (tid == 0) {
                output[sample] = logf(sum) + max_val;
            }
        }
        """
        
        # Compile sigmoid kernel
        sigmoid_cpp = "torch::Tensor sigmoid_cuda(torch::Tensor input);"
        self.sigmoid = load_inline(
            name="sigmoid",
            cpp_sources=sigmoid_cpp,
            cuda_sources=sigmoid_source,
            functions=["sigmoid_cuda"],
            verbose=True
        )
        
        # Compile logsumexp kernel with shared memory calculation
        logsumexp_cpp = "torch::Tensor logsumexp_cuda(torch::Tensor input, int features);"
        self.logsumexp = load_inline(
            name="logsumexp",
            cpp_sources=logsumexp_cpp,
            cuda_sources=logsumexp_source,
            functions=["logsumexp_cuda"],
            verbose=True
        )

    def forward(self, x):
        x = self.linear1(x)
        x = self.sigmoid.sigmoid_cuda(x)
        x = self.linear2(x)
        batch_size, features = x.size()
        # Calculate shared memory size as features * sizeof(float)
        shared_mem = features * sizeof(float)
        # Note: The function signature in the CUDA kernel requires features to compute shared memory
        return self.logsumexp.logsumexp_cuda(x, features)

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, output_size]
```

Wait, there are several critical errors here that need fixing to make the code functional and correct:

1. **Shared Memory Calculation**: The shared memory allocation in the kernel launch is missing. The `logsumexp_cuda` function needs to pass the shared memory size when launching the kernel.

2. **Function Signature Mismatch**: The CUDA kernel `logsumexp_kernel` expects `batch_size` and `features`, but the Python wrapper function's signature in the code above changed to only take `features`, which breaks the interface.

3. **Data Type for Shared Memory**: The line `shared_mem = features * sizeof(float)` uses `sizeof`, which is a C++ operator but not available in Python. This should be calculated using `torch.cuda.FloatTensor().element_size()` or similar.

4. **Kernel Launch Parameters**: The kernel launch in the Python wrapper must specify the shared memory size using the `stream` parameter or directly in the kernel call.

5. **Thread Block Configuration**: The block size should be chosen to fit within the maximum threads per block and shared memory limits.

Here's the corrected version with proper handling of shared memory and kernel parameters:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        
        # Sigmoid kernel remains unchanged
        sigmoid_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void sigmoid_kernel(const float* input, float* output, int size) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx < size) {
                output[idx] = 1.0f / (1.0f + expf(-input[idx]));
            }
        }

        torch::Tensor sigmoid_cuda(torch::Tensor input) {
            auto size = input.numel();
            auto output = torch::empty_like(input);
            
            const int block_size = 256;
            const int num_blocks = (size + block_size - 1) / block_size;

            sigmoid_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
            return output;
        }
        """
        
        # Corrected LogSumExp kernel with proper shared memory handling
        logsumexp_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>
        #include <limits>

        __global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int features) {
            extern __shared__ float shared[];
            
            int sample = blockIdx.x;
            int tid = threadIdx.x;
            int block_size = blockDim.x;
            
            // Load data into shared memory
            for (int i = tid; i < features; i += block_size) {
                shared[i] = input[sample * features + i];
            }
            __syncthreads();
            
            // Compute max value using parallel reduction
            float max_val = -std::numeric_limits<float>::infinity();
            for (int i = tid; i < features; i += block_size) {
                if (shared[i] > max_val) {
                    max_val = shared[i];
                }
            }
            
            for (int s = block_size / 2; s > 0; s >>= 1) {
                if (tid < s) {
                    max_val = max(max_val, __shfl_down_sync(0xFFFFFFFF, max_val, s));
                }
                __syncthreads();
            }
            
            if (tid == 0) {
                output[sample] = max_val;
            }
            __syncthreads();
            
            // Compute sum(exp(x - max))
            float sum = 0.0f;
            for (int i = tid; i < features; i += block_size) {
                sum += expf(shared[i] - max_val);
            }
            
            for (int s = block_size / 2; s > 0; s >>= 1) {
                sum += __shfl_down_sync(0xFFFFFFFF, sum, s);
            }
            
            if (tid == 0) {
                output[sample] = logf(sum) + max_val;
            }
        }
        """
        
        # Compile sigmoid kernel
        sigmoid_cpp = "torch::Tensor sigmoid_cuda(torch::Tensor input);"
        self.sigmoid = load_inline(
            name="sigmoid",
            cpp_sources=sigmoid_cpp,
            cuda_sources=sigmoid_source,
            functions=["sigmoid_cuda"],
            verbose=True
        )
        
        # Compile logsumexp kernel with proper parameters
        logsumexp_cpp = "torch::Tensor logsumexp_cuda(torch::Tensor input, int batch_size, int features);"
        self.logsumexp = load_inline(
            name="logsumexp",
            cpp_sources=logsumexp_cpp,
            cuda_sources=logsumexp_source,
            functions=["logsumexp_cuda"],
            verbose=True
        )

    def forward(self, x):
        x = self.linear1(x)
        x = self.sigmoid.sigmoid_cuda(x)
        x = self.linear2(x)
        batch_size, features = x.size()
        # Calculate shared memory needed: features * sizeof(float)
        shared_mem_bytes = features * torch.cuda.FloatTensor().element_size()
        # Launch kernel with shared memory
        output = torch.empty(batch_size, dtype=x.dtype, device=x.device)
        self.logsumexp.logsumexp_cuda(x, batch_size, features, output, shared_mem_bytes)
        return output

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, output_size]
```

Wait, there's still an issue with the kernel launch in the forward pass. The Python wrapper for the CUDA kernel should handle the kernel launch parameters, including shared memory. Let me correct this by properly defining the wrapper function in the CUDA code and adjusting the kernel call:

Final Corrected Code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        
        # Sigmoid kernel
        sigmoid_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void sigmoid_kernel(const float* input, float* output, int size) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx < size) {
                output[idx] = 1.0f / (1.0f + expf(-input[idx]));
            }
        }

        torch::Tensor sigmoid_cuda(torch::Tensor input) {
            auto size = input.numel();
            auto output = torch::empty_like(input);
            
            const int block_size = 256;
            const int num_blocks = (size + block_size - 1) / block_size;

            sigmoid_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
            return output;
        }
        """
        
        # Corrected LogSumExp kernel with proper shared memory and wrapper
        logsumexp_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>
        #include <limits>

        __global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int features) {
            extern __shared__ float shared[];
            
            int sample = blockIdx.x;
            int tid = threadIdx.x;
            int block_size = blockDim.x;
            
            // Load data into shared memory
            for (int i = tid; i < features; i += block_size) {
                shared[i] = input[sample * features + i];
            }
            __syncthreads();
            
            // Compute max value
            float max_val = -std::numeric_limits<float>::infinity();
            for (int i = tid; i < features; i += block_size) {
                if (shared[i] > max_val) {
                    max_val = shared[i];
                }
            }
            
            for (int s = block_size / 2; s > 0; s >>= 1) {
                if (tid < s) {
                    max_val = max(max_val, __shfl_down_sync(0xFFFFFFFF, max_val, s));
                }
                __syncthreads();
            }
            
            if (tid == 0) {
                output[sample] = max_val;
            }
            __syncthreads();
            
            // Compute sum(exp(x - max))
            float sum = 0.0f;
            for (int i = tid; i < features; i += block_size) {
                sum += expf(shared[i] - max_val);
            }
            
            for (int s = block_size / 2; s > 0; s >>= 1) {
                sum += __shfl_down_sync(0xFFFFFFFF, sum, s);
            }
            
            if (tid == 0) {
                output[sample] = logf(sum) + max_val;
            }
        }

        torch::Tensor logsumexp_cuda(torch::Tensor input, int batch_size, int features) {
            auto output = torch::empty({batch_size}, input.options());
            const int block_size = 256; // Threads per block
            const size_t shared_mem = features * sizeof(float);
            
            logsumexp_kernel<<<batch_size, block_size, shared_mem>>>(
                input.data_ptr<float>(), 
                output.data_ptr<float>(), 
                batch_size, 
                features
            );
            return output;
        }
        """
        
        # Compile sigmoid kernel
        sigmoid_cpp = "torch::Tensor sigmoid_cuda(torch::Tensor input);"
        self.sigmoid = load_inline(
            name="sigmoid",
            cpp_sources=sigmoid_cpp,
            cuda_sources=sigmoid_source,
            functions=["sigmoid_cuda"],
            verbose=True
        )
        
        # Compile logsumexp kernel
        logsumexp_cpp = "torch::Tensor logsumexp_cuda(torch::Tensor input, int batch_size, int features);"
        self.logsumexp = load_inline(
            name="logsumexp",
            cpp_sources=logsumexp_cpp,
            cuda_sources=logsumexp_source,
            functions=["logsumexp_cuda"],
            verbose=True
        )

    def forward(self, x):
        x = self.linear1(x)
        x = self.sigmoid.sigmoid_cuda(x)
        x = self.linear2(x)
        batch_size, features = x.size()
        return self.logsumexp.logsumexp_cuda(x, batch_size, features)

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, output_size]
```