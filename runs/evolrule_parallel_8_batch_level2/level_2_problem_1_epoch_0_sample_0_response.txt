Make sure to import necessary modules, and define all the helper functions and kernels required. 

You should make sure that the code is as efficient as possible, and that you use all the techniques you can think of to get the best possible performance. 

You may choose to implement a custom CUDA kernel for the entire forward pass of the model (convolution + ReLU + bias addition) in a single kernel, or break it into multiple kernels. Operator fusion is recommended for better performance.

You may also consider algorithmic changes, such as using a more efficient algorithm for convolution, or using online softmax (though softmax is not present here). 

In your kernel, make sure to use shared memory, threads and blocks appropriately. 

You may also use PyTorch's in-place operations or other optimizations to minimize memory usage.

When possible, try to combine multiple operations into a single CUDA kernel to reduce memory bandwidth and kernel launch overhead. 

Make sure that the output of your optimized model matches the original model's output (up to numerical precision errors). 

Make sure the kernel is compatible with the input shapes and the model's architecture. 

The problem is to implement an optimized version of the given Model with custom CUDA operators. 

Start by analyzing the operations in the forward pass:

The original forward pass is:

x = self.conv(x)  
x = torch.relu(x)  
x = x + self.bias  

The goal is to replace these operations with custom CUDA kernels to achieve better performance. 

The key operators here are:

1. 2D convolution (nn.Conv2d). 
2. ReLU non-linearity. 
3. Bias addition. 

The challenge is to implement these efficiently in a single kernel or multiple fused kernels.

First, implementing a custom convolution is non-trivial. However, fusing convolution with ReLU and bias addition could be beneficial.

Alternatively, maybe replacing each operator with custom CUDA kernels (convolution, ReLU, bias_add) but fused into a single kernel.

The original code uses nn.Conv2d, which involves parameters (weight and bias). However, in the given Model, the bias is already added via self.bias, but note that the Conv2d module has its own bias. Wait, in the current code:

Wait, looking at the original Model's __init__:

The Conv2d is initialized with its own bias? Because the default for Conv2d is bias=True. Let me check:

Wait, the code says:

self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

The default for bias in Conv2d is True. So the Conv2d has its own bias. But in the forward, the user adds another self.bias. Wait, that's redundant. 

Wait, the problem says: "adds a bias term." 

Wait, the original model's forward is:

x = self.conv(x)  # this includes the convolution and the conv's own bias
x = torch.relu(x)
x = x + self.bias  # adding an additional bias term after ReLU?

Wait that seems incorrect. Because typically, after convolution, you apply ReLU, but the convolution already has a bias. Adding another bias term after ReLU is not standard. Maybe it's part of the problem's setup. 

But regardless, the code is as written, so we need to replicate the same behavior in the custom kernel.

First, the problem is to optimize the forward pass which is:

convolution (with its own weight and bias) → ReLU → add self.bias.

We need to implement this sequence in an optimized way with custom CUDA kernels.

Approach:

Option 1: Implement a fused kernel that does convolution (including its own bias), ReLU, and then adds the additional bias term (self.bias). 

This is possible, but requires implementing a 2D convolution from scratch in CUDA, which is a bit involved but manageable.

Alternatively, we can try to fuse the convolution with the subsequent operations, but first we need to understand the steps.

First, let's outline the computation steps:

For each output pixel (spatial location):

Let W be the convolution weights (out_channels x in_channels x kernel_size x kernel_size)

Let B_conv be the convolution's bias (out_channels)

Then the convolution output is:

out_conv = conv2d(x) + B_conv

Then ReLU: out_relu = max(0, out_conv)

Then adding the additional self.bias (which is of shape (out_channels, 1, 1)), so it's a channel-wise bias added to each spatial position.

Therefore, the total output is:

out_final = ReLU(conv2d(x) + B_conv) + self.bias

Wait, but the self.bias is a tensor of shape (out_channels, 1, 1), so when added to the ReLU output (which is of shape (batch, out_channels, H', W'), it is broadcasted over the spatial dimensions.

So the steps are:

1. Compute the convolution, including the conv's own bias.

2. Apply ReLU.

3. Add the additional bias term (per-channel).

Therefore, the fused kernel can compute all three steps in one go.

Implementing this in CUDA requires writing a custom convolution kernel that:

- Computes the convolution output (including the conv's bias).

- Applies ReLU.

- Adds the additional per-channel bias.

This can be done in a single kernel.

However, implementing a 2D convolution from scratch in CUDA is quite involved. Let's consider the steps:

The convolution is a sliding window operation. For each output spatial position (h, w), and for each output channel, we need to compute the dot product between the kernel weights and the input region, then add the bias.

The key challenges are:

- Efficiently handling the spatial dimensions and the kernel window.

- Managing memory accesses (input, weights, bias).

- Using shared memory to reduce memory transactions.

- Thread organization (how threads are mapped to output positions and channels).

Since the problem requires the fastest possible implementation, we need to optimize for performance, using shared memory, coalesced memory access, etc.

Alternatively, perhaps we can use the existing PyTorch's convolution implementation but then fuse the ReLU and bias addition? But since PyTorch's convolutions are already highly optimized, maybe fusing those post-conv operations would give the most benefit.

Wait, the standard PyTorch's conv2d already includes the bias addition. So the first step (convolution + conv's bias) is already handled by the conv2d function. The problem's model adds another bias after ReLU.

Thus, the main computation steps are:

1. conv2d with bias (already done by PyTorch's conv2d).

2. ReLU activation.

3. Add an additional bias (self.bias).

Therefore, the main optimization is fusing steps 2 and 3 into a single kernel.

Alternatively, even better: fusing all three steps into a single kernel (conv + ReLU + add self.bias).

But implementing a custom convolution kernel is a big task.

Alternatively, perhaps replacing only the ReLU and bias addition with a fused kernel.

Let me think:

The original code's steps after convolution:

x = torch.relu(x)  
x = x + self.bias  

These two steps can be combined into a single kernel, which for each element does:

out[i] = max(0, x[i]) + bias[i_channel]

Since the self.bias is a per-channel bias, but it's of shape (out_channels, 1, 1), so when adding, each channel's value is added to all spatial positions in that channel.

Therefore, the per-element computation is:

out[i] = max(0, x[i]) + bias[channel]

where channel is the channel index of the element.

This can be implemented in a single element-wise kernel that takes x, bias, and produces the result.

This is simpler than reimplementing convolution.

Therefore, perhaps the most feasible optimization is to fuse ReLU and bias addition into a single kernel.

Alternatively, even the ReLU and bias addition can be done with PyTorch's built-in operations, but in practice, combining them into a single kernel can save some overhead.

Let me check the performance implications.

First, the ReLU is an element-wise operation, and adding the bias is also element-wise (since the bias is per-channel, it can be broadcasted).

So if we can do both in a single kernel, we can avoid a memory copy between ReLU and the addition, which might help.

Therefore, let's consider writing a kernel that does both ReLU and adding the per-channel bias.

The steps for this kernel:

For each element in the input tensor (x), compute:

output = max(0, x) + bias[channel]

where 'channel' is the channel index of the element.

To do this, we need to:

- Iterate over all elements of the input tensor.

- For each element, determine its channel index.

- Access the corresponding bias value.

- Apply ReLU and add the bias.

This is straightforward to implement.

Moreover, since the bias is per-channel and stored in a tensor of shape (out_channels, 1, 1), when passed to the kernel, we can load the bias values into shared memory or registers for faster access.

Alternatively, since the bias tensor is of shape (C, 1, 1), each element in the bias tensor corresponds to a channel, so we can precompute the bias values and store them in an array.

Therefore, the kernel can be written as follows:

__global__ void fused_relu_bias_add(
    const float* input, const float* bias, float* output,
    int batch_size, int channels, int height, int width) {

    // Calculate the index
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * height * width)
        return;

    int w = idx % width;
    idx /= width;
    int h = idx % height;
    idx /= height;
    int c = idx % channels;
    int n = idx / channels;

    // Compute the value
    float val = input[idx];
    val = fmaxf(val, 0.0f);  // ReLU
    val += bias[c];          // add per-channel bias

    output[idx] = val;
}

This would require launching with appropriate grid and block dimensions.

This kernel can be faster than separate ReLU and add operations because it reduces kernel launches and memory transactions.

Now, regarding the convolution: the existing PyTorch's Conv2d is already highly optimized. Unless we can find a way to fuse the convolution with the subsequent operations, it might not be worth reimplementing the convolution.

However, the problem allows us to choose which operators to replace. So perhaps the best approach is to keep the convolution as is, and just fuse ReLU and bias addition into a single kernel.

Alternatively, maybe we can also fuse the convolution's bias addition with the ReLU and the additional bias. Wait, the convolution's own bias is already added in the conv2d function. So the ReLU is applied to the output of conv2d plus its bias. The additional self.bias is added after ReLU.

So the only two operations we can fuse are ReLU and the additional bias addition.

Therefore, the fused kernel for ReLU and add is the main optimization here.

Another thing to consider: the self.bias is a per-channel bias. Since it's of shape (out_channels, 1, 1), when adding to the output of conv (which is (batch, out_channels, H', W')), the addition is broadcasted over the spatial dimensions. So the per-channel bias can be considered as a 1D tensor of length out_channels, where each element is added to all spatial positions of that channel.

Therefore, in the kernel, the bias can be stored as a 1D array, so for channel c, bias[c] is the value to add.

Thus, in the kernel code, the bias is a 1D array of length out_channels, which can be loaded into shared memory for faster access.

Wait, but for small numbers of channels (like 128), it might be better to just load the bias into registers or global memory. Shared memory might be overkill.

Alternatively, the kernel can take the bias tensor, which is of shape (C, 1, 1), and for a given channel c, the bias value is at position (c, 0, 0). So in the kernel, the code can compute the bias value as bias[c * stride], but perhaps more straightforwardly, since the bias tensor is contiguous, the index can be directly accessed as bias[c].

Wait, if the bias is stored as a 1D tensor (flattened), then yes, but the original code's self.bias is of shape (out_channels, 1, 1). So when passed to the kernel, perhaps we can convert it to a 1D tensor first.

Alternatively, in the kernel, given the bias tensor of shape (C, 1, 1), the element for channel c is at position c * (1 * 1) + 0 * 1 + 0 = c.

Therefore, to get the bias value for channel c, we can simply access bias[c].

Therefore, in the kernel, the bias can be treated as a 1D array.

Thus, the fused kernel can take the input tensor (output of the convolution), the bias tensor (self.bias), and compute the ReLU and addition.

Therefore, this kernel is manageable and should give a performance improvement over separate ReLU and add operations.

Additionally, we can look at the convolution's bias addition. The convolution's own bias is already included in the conv2d computation, so we don't need to touch that.

Therefore, the plan is:

- Keep the convolution layer as is (using PyTorch's nn.Conv2d).

- Replace the ReLU and bias addition with a fused CUDA kernel.

Therefore, the new ModelNew will:

1. Compute the convolution using PyTorch's Conv2d.

2. Apply the fused ReLU + add self.bias via the custom kernel.

This should give a speedup by reducing the number of operations and memory transactions.

Now, implementing this:

First, write the CUDA kernel for fused_relu_bias_add.

Then, in the ModelNew class, replace the ReLU and add operations with a call to this kernel.

Additionally, ensure that the kernel is properly launched with the right grid and block dimensions.

Let me proceed step by step.

First, the code structure would be:

Import necessary modules.

Define the CUDA kernel source code.

Compile the kernel using load_inline.

Define the ModelNew class, which replaces the forward pass with the custom kernel.

Now, writing the kernel:

The kernel needs to take:

- Input tensor (output of the conv, shape (batch, C, H', W'))

- Bias tensor (shape (C, 1, 1))

- Output tensor.

The kernel function will perform element-wise ReLU and add the bias.

To handle the indices correctly, we need to map the linear index to the 4D tensor dimensions.

Alternatively, the kernel can treat the input as a 1D array, and the user is responsible for ensuring that the input is contiguous and the dimensions are passed correctly.

The kernel code would be something like:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_add(
    const float* input, const float* bias, float* output,
    int batch_size, int channels, int height, int width) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * height * width)
        return;

    // Compute the indices
    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % channels;
    int n = idx / (channels * width * height);

    float val = input[idx];
    val = fmaxf(val, 0.0f);  // ReLU
    val += bias[c];          // add per-channel bias

    output[idx] = val;
}

Then, the wrapper function in Python:

torch::Tensor fused_relu_bias_add_cuda(torch::Tensor input, torch::Tensor bias) {
    // Ensure the input is contiguous
    input = input.contiguous();
    bias = bias.contiguous();

    // Get dimensions
    int batch_size = input.size(0);
    int channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    // Output tensor
    auto output = torch::empty_like(input);

    // Calculate grid and block dimensions
    const int num_elements = batch_size * channels * height * width;
    const int threads_per_block = 256;
    const int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;

    fused_relu_bias_add<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}

Wait, but in the kernel, the bias is a 3D tensor (C, 1, 1). When accessing bias[c], we need to make sure that the stride allows that. Since in PyTorch, tensors are stored in row-major order, so the first dimension (channels) is the outermost. Therefore, bias[c] would access the first element of the (c, 0, 0) position.

Alternatively, to simplify, we can reshape the bias tensor to a 1D tensor of size C before passing it to the kernel. So in the wrapper function:

auto bias_1d = bias.view({-1});

Then, in the kernel, the bias pointer is cast to a float*, and access via bias[c].

Wait, in the kernel, the bias is passed as a 1D array, so the code in the kernel can be:

val += bias[c];

This should work as long as the bias tensor is reshaped to 1D.

Therefore, in the wrapper function:

bias_1d = bias.view({-1});

But in the code above, the wrapper function is in C++ (CUDA), so the view can be done there.

Alternatively, in the Python code, before calling the CUDA function, we can reshape the bias.

But the wrapper function is written in C++, so perhaps better to handle it there.

Thus, in the wrapper function:

// Reshape bias to 1D
auto bias_1d = bias.view({-1});

Then, pass bias_1d.data_ptr<float>() to the kernel.

Alternatively, in the kernel, since the bias tensor is of shape (C, 1, 1), the stride for the first dimension is 1*1 (since the next dimensions are 1 and 1). Therefore, accessing bias[c] is equivalent to the first element of the c-th channel.

Therefore, in the kernel, the code is okay.

Thus, the code should work.

Now, compiling this kernel.

The source code for the CUDA kernel is as follows.

Then, in Python:

elementwise_add_cpp_source = (
    "torch::Tensor fused_relu_bias_add_cuda(torch::Tensor input, torch::Tensor bias);"
)

Then, using load_inline to compile it.

Now, integrating into the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_relu_bias_add = fused_relu_bias_add  # the loaded CUDA module

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_relu_bias_add.fused_relu_bias_add_cuda(x, self.bias)
        return x

Wait, but in the code example given earlier, the elementwise_add was a module loaded with load_inline, and the function is accessed as elementwise_add.elementwise_add_cuda.

Therefore, similarly, the fused_relu_bias_add would be a loaded module with a function called fused_relu_bias_add_cuda.

Therefore, in the code:

self.fused_relu_bias_add = fused_relu_bias_add

Then, in forward:

x = self.fused_relu_bias_add.fused_relu_bias_add_cuda(x, self.bias)

This should work.

Now, considering the input shapes.

In the get_inputs function, the input is a tensor of shape (batch_size, in_channels, height, width) = (128, 64, 128, 128). The output after convolution would be (batch_size, out_channels, H', W'), where H' and W' depend on the padding and stride. Since the original code didn't specify padding or stride, the default for Conv2d is stride=1 and padding=0. So the output spatial dimensions would be (128 - kernel_size + 1, 128 - kernel_size + 1) → (128 - 3 +1 = 126). So output shape is (128, 128, 126, 126).

The self.bias is of shape (out_channels, 1, 1) → (128, 1, 1).

Therefore, the kernel should handle these dimensions correctly.

Now, considering the CUDA kernel's parameters:

The kernel requires batch_size, channels, height, width.

These can be obtained from the input tensor in the wrapper function.

The wrapper function in C++:

torch::Tensor fused_relu_bias_add_cuda(torch::Tensor input, torch::Tensor bias) {
    // Ensure the input is contiguous
    input = input.contiguous();
    bias = bias.contiguous();

    int batch_size = input.size(0);
    int channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    // Check that bias has shape (channels, 1, 1)
    TORCH_CHECK(bias.sizes() == torch::IntArrayRef({channels, 1, 1}), "Bias shape mismatch");

    auto output = torch::empty_like(input);

    const int num_elements = batch_size * channels * height * width;
    const int threads_per_block = 256;
    const int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;

    fused_relu_bias_add<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}

This should handle the input dimensions.

Now, putting all together in code.

Also, note that PyTorch's CUDA tensors are stored in float32 (assuming the default), so the kernel uses float pointers.

Now, handling the case where the input is on the GPU.

The input tensors (x) and self.bias are already on the GPU, since the original get_inputs() returns tensors on CUDA (if using .cuda())?

Wait, looking at the original get_inputs() function:

Wait in the given architecture, the get_inputs() is:

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

Wait, but in the example given earlier, the original model's get_inputs() used .cuda(). But in the problem's given architecture, the get_inputs() does not include .cuda(). However, since the user is expected to run on CUDA, we need to make sure that the tensors are on the GPU.

Assuming that the code is supposed to run on CUDA, the model's forward function expects the inputs to be on the GPU. Therefore, in the kernel code, we can assume that the tensors are on the same device.

Therefore, the code should work as long as the input and bias are on the same GPU device.

Now, compiling the kernel.

Now, writing the complete Python code with the CUDA kernel.

The code would be as follows:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused ReLU and bias add CUDA kernel
fused_relu_bias_add_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_add(
    const float* input, const float* bias, float* output,
    int batch_size, int channels, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * height * width) {
        return;
    }

    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % channels;
    int n = idx / (channels * width * height);

    float val = input[idx];
    val = fmaxf(val, 0.0f);  // ReLU
    val += bias[c];          // add per-channel bias

    output[idx] = val;
}

torch::Tensor fused_relu_bias_add_cuda(torch::Tensor input, torch::Tensor bias) {
    input = input.contiguous();
    bias = bias.contiguous();

    // Check dimensions
    int batch_size = input.size(0);
    int channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    // Check bias shape
    auto bias_size = bias.sizes();
    TORCH_CHECK(bias_size[0] == channels && bias_size[1] == 1 && bias_size[2] == 1,
                "Bias tensor must be of shape (", channels, ", 1, 1)");

    auto output = torch::empty_like(input);

    int num_elements = batch_size * channels * height * width;
    const int threads_per_block = 256;
    int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;

    fused_relu_bias_add<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}
"""

fused_relu_bias_add_cpp_source = (
    "torch::Tensor fused_relu_bias_add_cuda(torch::Tensor input, torch::Tensor bias);"
)

# Compile the fused kernel
fused_relu_bias_add = load_inline(
    name="fused_relu_bias_add",
    cpp_sources=fused_relu_bias_add_cpp_source,
    cuda_sources=fused_relu_bias_add_source,
    functions=["fused_relu_bias_add_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))  # shape (out_channels, 1, 1)
        self.fused_relu_bias_add = fused_relu_bias_add  # the loaded CUDA module

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_relu_bias_add.fused_relu_bias_add_cuda(x, self.bias)
        return x

# The get_inputs and get_init_inputs functions remain the same as in the original architecture
# (They are not part of the ModelNew class, so no need to redefine them here unless required)
# Assuming that the user will use the original get_inputs and get_init_inputs functions.
```

Now, let's verify the code.

First, the CUDA kernel's fused_relu_bias_add function takes input and bias tensors, checks their shapes, and computes the output.

The wrapper function in C++ ensures that input and bias are contiguous.

The check for bias shape ensures that it's (C,1,1).

The kernel's indexing is correct, as it breaks down the linear index into n, c, h, w.

The ReLU and bias addition are applied per element.

The grid and block dimensions are calculated appropriately.

In the ModelNew class:

- The convolution is the same as before.

- The forward pass uses the fused kernel after the convolution.

This should produce the same result as the original model.

Potential issues to check:

- The bias tensor must be of shape (out_channels, 1, 1), which is handled by the check in the C++ code.

- The kernel's bias access via bias[c] is correct because the bias tensor is contiguous and stored in row-major order. For the first element of each channel, this is correct.

- The output tensor is created as empty_like(input), which should have the same shape and device.

Now, considering performance:

This fused kernel replaces two separate operations (ReLU and add) with one kernel launch. This reduces kernel launch overhead and memory transactions.

Additionally, since the computation is done in a single pass over the data, it may have better cache locality.

However, the convolution itself is handled by PyTorch's optimized implementation, so there is no need to reimplement it unless there's a way to fuse it with the subsequent steps.

Another possible optimization: fusing the convolution's bias addition with the ReLU and the self.bias addition.

Wait, the convolution's own bias is already added in the Conv2d operation. The ReLU is applied to the result, then the self.bias is added.

Thus, the only steps that can be fused are ReLU and self.bias addition.

Hence, the above code should be correct and performant.

Therefore, this is the optimized ModelNew class.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused ReLU and bias add CUDA kernel
fused_relu_bias_add_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_relu_bias_add(
    const float* input, const float* bias, float* output,
    int batch_size, int channels, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * height * width) {
        return;
    }

    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % channels;
    int n = idx / (channels * width * height);

    float val = input[idx];
    val = fmaxf(val, 0.0f);  // ReLU
    val += bias[c];          // add per-channel bias

    output[idx] = val;
}

torch::Tensor fused_relu_bias_add_cuda(torch::Tensor input, torch::Tensor bias) {
    input = input.contiguous();
    bias = bias.contiguous();

    // Check dimensions
    int batch_size = input.size(0);
    int channels = input.size(1);
    int height = input.size(2);
    int width = input.size(3);

    // Check bias shape
    auto bias_size = bias.sizes();
    TORCH_CHECK(bias_size[0] == channels && bias_size[1] == 1 && bias_size[2] == 1,
                "Bias tensor must be of shape (", channels, ", 1, 1)");

    auto output = torch::empty_like(input);

    int num_elements = batch_size * channels * height * width;
    const int threads_per_block = 256;
    int num_blocks = (num_elements + threads_per_block - 1) / threads_per_block;

    fused_relu_bias_add<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}
"""

fused_relu_bias_add_cpp_source = (
    "torch::Tensor fused_relu_bias_add_cuda(torch::Tensor input, torch::Tensor bias);"
)

# Compile the fused kernel
fused_relu_bias_add = load_inline(
    name="fused_relu_bias_add",
    cpp_sources=fused_relu_bias_add_cpp_source,
    cuda_sources=fused_relu_bias_add_source,
    functions=["fused_relu_bias_add_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))  # shape (out_channels, 1, 1)
        self.fused_relu_bias_add = fused_relu_bias_add  # the loaded CUDA module

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_relu_bias_add.fused_relu_bias_add_cuda(x, self.bias)
        return x
```