Please do not use any PyTorch native CUDA extensions (e.g., torch.nn.functional, etc.) in your custom CUDA kernels. Only use the PyTorch C++ API and CUDA runtime API in your custom CUDA kernels.

You may also need to create helper functions in C++ if needed. Make sure you follow the same structure as the example provided. 

When defining the kernels, make sure the parameters are correctly mapped to the inputs and outputs. 

Make sure that the inputs and outputs of your custom kernels are compatible with the original architecture's forward pass.

        Now, your turn to optimize this architecture. I will give you an example of how to do it step by step.

        Let me think... the original architecture has Gemm (linear layer), batch normalization, scaling, and softmax. 

        First, maybe I can fuse some of these operations into a single kernel. For example, the linear layer (matrix multiplication) followed by batch norm, scaling, and softmax. Combining these into a single kernel could reduce memory overhead and kernel launch overhead.

        The linear layer computes x = W*x + b. Then batch norm is applied, which involves normalizing the output, scaling by gamma, adding beta. Then scaling by self.scale, then softmax. 

        Let me think about the steps:

        Original forward pass:
        1. Gemm: x = W * x + b
        2. BatchNorm: x = (x - mean)/std * gamma + beta
        3. Scaling: x = scale * x
        4. Softmax: x = exp(x)/sum(exp(x))

        Maybe these can be fused into a single kernel. But need to consider that during inference, batch norm uses running stats (mean and variance stored), so during forward, it's not calculating mean and variance each time, but uses the stored ones. 

        So for inference, the batch norm can be simplified to: x = (x - running_mean)/sqrt(running_var + eps) * gamma + beta

        Then scaling is x = scale * x. 

        So combining Gemm, batch norm, scaling into a single kernel, then softmax. Alternatively, even combine all into one kernel?

        However, implementing softmax might be a bit tricky because it requires the sum of exponentials, which needs a reduction step. 

        Maybe first combine the Gemm + batch norm + scaling into one kernel, then handle softmax separately with a custom kernel.

        Let me think of the Gemm part first. The Gemm is a matrix multiplication of input x (batch_size x in_features) with weight W (out_features x in_features) plus bias b (out_features). 

        Then, the batch norm uses the running_mean (shape out_features), running_var (shape out_features), gamma (shape out_features), beta (shape out_features). The formula is:

        x_normalized = (x - running_mean) / sqrt(running_var + eps)

        x_bn = x_normalized * gamma + beta

        Then scaling is x_scaled = scale * x_bn

        So the scaling is a multiplication by a scalar (if scale_shape is (1,)), so just a scalar multiplication.

        So putting this together, the combined computation can be expressed as:

        x = (W * x + b - running_mean) / sqrt(running_var + eps) * gamma + beta

        x = scale * x

        Then, the softmax is applied.

        So, the entire computation could be written as a single kernel for the Gemm, batch norm, scaling, but the softmax requires an element-wise exponential and a row-wise sum.

        Alternatively, we can split into two fused kernels:

        1. Fused Gemm + BatchNorm + Scaling into one kernel, producing the intermediate tensor before softmax.

        2. Fused Softmax into another kernel, since the softmax requires a reduction step.

        However, for maximum optimization, combining everything into a single kernel might be better, but it might be complicated.

        Let's first tackle the first part: fusing Gemm + BatchNorm + Scaling.

        The Gemm operation is a matrix multiplication: out = W * x + b. The matrix dimensions are:

        x: (batch_size, in_features)

        W: (out_features, in_features)

        The result is (batch_size, out_features). 

        Then, for each element in the out_features dimension, we apply the batch norm and scaling.

        Since all these operations are element-wise along the out_features dimension, perhaps we can compute the entire thing in a single kernel.

        Let me think about the kernel structure.

        The Gemm part can be implemented as a matrix multiplication. To compute W * x efficiently, we can use a tiled approach, but that might be complex. Alternatively, since we are writing a custom kernel, perhaps it's better to use the standard approach, but then combine it with the other operations.

        Alternatively, since the batch norm and scaling are element-wise, after the Gemm, we can perform the batch norm and scaling in the same kernel.

        So the idea is to compute each element of the output tensor in a single kernel, combining the Gemm, batch norm, and scaling steps.

        Wait, but the Gemm is a matrix multiplication, which requires summing over the in_features dimension. So each output element is the dot product of the weight vector with the input vector, plus the bias. 

        To compute this efficiently in a CUDA kernel, we can structure the kernel to compute each output element (for each sample in the batch) in parallel. 

        Let me outline the steps:

        For each sample in the batch (batch_size) and each output feature (out_features), compute:

        - Compute the dot product of the input row (in_features) and the weight row (in_features). 

        - Add the bias term.

        - Subtract the running_mean, divide by sqrt(running_var + eps), multiply by gamma, add beta.

        - Multiply by scale.

        This computation for each (batch, out) element can be done in a CUDA kernel. 

        The problem is that for each output element, the dot product requires iterating over all in_features elements. 

        So for in_features = 8192, this is a lot of computation per output element, but since in_features and out_features are both 8192, this could be computationally intensive. 

        Alternatively, perhaps it's better to implement the matrix multiplication in a tiled manner using shared memory to improve memory access and performance, but that requires a more complex kernel.

        Alternatively, maybe we can split the work into blocks and threads such that each thread handles one output element (for a batch sample), but then each thread would have to loop over the in_features elements.

        Let me think of the kernel structure:

        We can have a grid of blocks, each block handling a batch sample. Each thread in a block handles an output feature. 

        For example:

        - Number of blocks = batch_size (each block handles one sample)

        - Number of threads per block = out_features (each thread handles one output feature)

        Then, for each thread in block i (sample i), thread j (output feature j):

        1. Compute the dot product of input x[i] (row) with weight[j] (row). 

        2. Add the bias[j].

        3. Apply batch norm and scaling steps.

        The problem is that the dot product requires accessing all in_features elements for each output feature. 

        Since in_features is 8192, this would be 8192 multiplications and additions per thread. 

        For 8192 output features, each block (sample) would have 8192 threads, each doing 8192 computations. 

        This may be too much, leading to a high number of operations and potential memory bandwidth issues.

        Alternatively, maybe the matrix multiplication can be optimized using shared memory and tiled approach, but that's more complex. 

        Alternatively, perhaps we can proceed with this approach and see if the code can be written.

        Let me proceed.

        First, the weights and bias are parameters of the model, so in the custom kernel, they need to be passed as arguments. 

        The batch norm parameters (running_mean, running_var, gamma, beta) are also parameters of the model's batch norm layer. 

        The scale is a parameter (a scalar in this case).

        So, the kernel will need to have access to these parameters. 

        However, in PyTorch, when we write a custom CUDA kernel, the parameters have to be passed as arguments to the kernel. 

        So, in the forward function of the new model, when we call the custom kernel, we have to pass all these parameters as tensors.

        Let me think about the kernel code.

        Let's define the fused kernel as follows:

        The kernel takes as inputs:

        - input tensor x (shape: batch_size x in_features)

        - weight tensor (shape: out_features x in_features)

        - bias tensor (shape: out_features)

        - running_mean (shape: out_features)

        - running_var (shape: out_features)

        - gamma (shape: out_features)

        - beta (shape: out_features)

        - scale (shape: 1)

        The output tensor will be of shape batch_size x out_features, which is the result after Gemm, batch norm, scaling.

        Then, after that, we need to apply the softmax.

        So the steps are:

        1. For each element (i, j):

            a. Compute the dot product of input[i] and weight[j], add bias[j]

            b. Subtract running_mean[j], divide by sqrt(running_var[j] + eps), multiply by gamma[j], add beta[j]

            c. Multiply by scale[0]

        Then, perform the softmax on the rows.

        Let's first tackle the fused kernel for steps 1-3 (Gemm, BN, scaling).

        The kernel code would need to compute each element's value as per above.

        Let me write the kernel code in CUDA.

        First, define the kernel function:

        __global__ void fused_gemm_bnorm_scale(
            const float* __restrict__ input,
            const float* __restrict__ weight,
            const float* __restrict__ bias,
            const float* __restrict__ running_mean,
            const float* __restrict__ running_var,
            const float* __restrict__ gamma,
            const float* __restrict__ beta,
            const float* __restrict__ scale,
            float* __restrict__ output,
            int batch_size,
            int in_features,
            int out_features,
            float eps
        ) {
            int sample_idx = blockIdx.x;
            int out_idx = threadIdx.x;

            if (sample_idx >= batch_size || out_idx >= out_features)
                return;

            // Compute the dot product for this output feature
            float dot_product = 0.0f;
            for (int in_idx = 0; in_idx < in_features; ++in_idx) {
                dot_product += input[sample_idx * in_features + in_idx] * weight[out_idx * in_features + in_idx];
            }
            dot_product += bias[out_idx];

            // Apply batch norm
            float mean = running_mean[out_idx];
            float var = running_var[out_idx];
            float inv_std = 1.0f / sqrt(var + eps);
            float normalized = (dot_product - mean) * inv_std;
            float bn_out = normalized * gamma[out_idx] + beta[out_idx];

            // Apply scaling
            float scaled = bn_out * scale[0];

            // Write to output
            output[sample_idx * out_features + out_idx] = scaled;
        }

        This is a naive implementation where each thread computes one output element. However, for in_features=8192, this will require 8192 iterations per thread, which is computationally expensive and may not be efficient. 

        To optimize this, perhaps we can use shared memory and tile the computation. 

        Alternatively, use matrix multiplication primitives. However, since we need to combine with batch norm and scaling, perhaps we can do the matrix multiplication first, then apply the other operations.

        Alternatively, maybe it's better to separate the Gemm into a separate kernel and then perform the other operations in the same kernel.

        Let me think again. Maybe first compute the matrix multiplication (Gemm) in a separate kernel, then apply batch norm and scaling in another kernel. But that would split into two kernels, but still better than four separate operations.

        Alternatively, perhaps the matrix multiplication can be done with cuBLAS, but the user instruction says not to use PyTorch native CUDA extensions. However, cuBLAS is part of CUDA, so maybe it's allowed?

        Wait the problem says: "Please do not use any PyTorch native CUDA extensions (e.g., torch.nn.functional, etc.) in your custom CUDA kernels. Only use the PyTorch C++ API and CUDA runtime API in your custom CUDA kernels."

        So we can use CUDA Runtime API, which includes cuBLAS. So using cuBLAS is allowed?

        Let me check: the user says to use only the PyTorch C++ API and CUDA runtime API. So if we use cuBLAS (which is part of CUDA), that should be allowed.

        If that's the case, then for the matrix multiplication, we can use cuBLAS's gemm function, which would be more efficient than a naive CUDA kernel.

        Then, the batch norm, scaling, and softmax can be done in a single kernel.

        Let me outline this approach:

        1. Use cuBLAS to compute the matrix multiplication (W * x) + bias. 

        2. Then, apply batch norm, scaling, and softmax in a single kernel.

        However, the problem is that batch norm requires per-element operations, so after the matrix multiplication, the rest can be done in a single kernel.

        Let me proceed with this approach.

        First, the Gemm part:

        In C++, using cuBLAS:

        // Assume handle is a cublasHandle_t initialized.

        float alpha = 1.0f;
        float beta = 1.0f;  // since we add the bias.

        // The matrix multiplication is: out = W * x + bias.

        // The dimensions are:

        // W is (out_features, in_features)

        // x is (batch_size, in_features), so transpose to (in_features, batch_size)

        // The multiplication: W (out x in) * x^T (in x batch) = out x batch, then transpose back to batch x out.

        // So to compute W * x as a batched GEMM?

        Hmm, perhaps it's better to use batched GEMM.

        Alternatively, let me think of the dimensions:

        The matrix multiplication can be represented as:

        out[i, j] = sum_{k=0}^{in_features-1} W[j][k] * x[i][k] + bias[j]

        To compute this for all i, j.

        Using cuBLAS, the GEMM parameters are:

        - opA: transpose of W (since W is out x in, so opA = 't' to make it in x out)

        - opB: x is batch_size x in_features, so opB = 'n'

        The dimensions for GEMM are:

        m = out_features (rows of opA)

        n = batch_size (columns of opB)

        k = in_features (columns of opA, rows of opB)

        So the output matrix will be out_features x batch_size. 

        Then, adding the bias to each row. 

        The bias is a vector of size out_features, so adding it to each row of the matrix.

        Then, we need to transpose the result to get batch_size x out_features.

        Alternatively, arrange the data so that the result is batch x out.

        This requires careful handling.

        Let me see the code for this:

        // Assume W is stored as a float* pointing to the data.

        // x is input tensor (batch, in_features)

        // The output of GEMM will be out_features x batch_size.

        // So:

        cublasHandle_t handle;
        cublasCreate(&handle);

        float* d_x = input.data<float>();
        float* d_W = weight.data<float>();
        float* d_out = output.data<float>();
        float* d_bias = bias.data<float>();

        // The operation is: out = W * x + bias

        // To compute W * x:

        // The GEMM parameters:

        // opA is 't' (transpose W: in x out)

        // opB is 'n'

        // m = out_features

        // n = batch_size

        // k = in_features

        // alpha = 1.0

        // beta = 0.0 (since we will add bias later)

        // So:

        cublasSgemm(
            handle,
            CUBLAS_OP_T,
            CUBLAS_OP_N,
            out_features,
            batch_size,
            in_features,
            &alpha,
            d_W,
            in_features,  // leading dimension of A (W is stored as row-major, so leading dim is in_features)
            d_x,
            in_features,
            &beta,
            d_out,
            out_features // leading dimension of C is out_features (since it's stored as row-major)
        );

        // Then add bias to each row of d_out (each row corresponds to a feature)

        for (int j = 0; j < out_features; ++j) {
            for (int i = 0; i < batch_size; ++i) {
                d_out[j * batch_size + i] += d_bias[j];
            }
        }

        // Then transpose the result to batch_size x out_features.

        However, doing this with a loop in host code would be slow. Instead, we can perform the bias addition in a CUDA kernel and then transpose.

        Alternatively, arrange the output in the desired order without transposing, but that might complicate things.

        Alternatively, maybe it's better to proceed with the initial approach of a fused kernel for the entire process, but using cuBLAS for the matrix multiplication part.

        Alternatively, let me see if using cuBLAS is manageable.

        Once the matrix multiplication is done with cuBLAS, then the rest can be done in a CUDA kernel.

        Let me proceed step by step.

        First, the fused kernel for the matrix multiplication (using cuBLAS) and then the rest.

        However, in the custom CUDA kernel code, we need to call cuBLAS functions. But when writing a PyTorch extension, we can do that.

        Wait, the user says: "Only use the PyTorch C++ API and CUDA runtime API in your custom CUDA kernels." So using cuBLAS via the CUDA runtime API is allowed.

        So in the C++ code, we can use cublas functions.

        Let me outline the steps in the custom C++ code:

        The fused function would first perform the matrix multiplication with bias using cuBLAS, then perform the batch norm, scaling, and softmax.

        However, softmax requires a reduction step to compute the exponentials and the sum.

        Alternatively, let's first handle the first three steps (Gemm, batch norm, scaling) using cuBLAS and a CUDA kernel, then handle softmax with another kernel.

        Let's proceed.

        First, the first part:

        The function would look like:

        torch::Tensor fused_forward(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, torch::Tensor running_mean, torch::Tensor running_var, torch::Tensor gamma, torch::Tensor beta, torch::Tensor scale, float eps) {

            // Compute Gemm (matrix multiply + bias)
            int batch_size = input.size(0);
            int in_features = input.size(1);
            int out_features = weight.size(0);

            // Output tensor for the result after Gemm, batch norm, scaling
            torch::Tensor gemm_out = torch::empty({batch_size, out_features}, device);

            // Perform W * x + bias using cuBLAS
            cublasHandle_t handle;
            cublasCreate(&handle);

            float alpha = 1.0f;
            float beta = 0.0f;

            // W is (out_features, in_features), so transpose for cublas
            cublasSgemm(
                handle,
                CUBLAS_OP_T,
                CUBLAS_OP_N,
                out_features,
                batch_size,
                in_features,
                &alpha,
                weight.data_ptr<float>(),
                in_features,
                input.data_ptr<float>(),
                in_features,
                &beta,
                gemm_out.data_ptr<float>(),
                out_features
            );

            // Add bias to each row (each feature)
            // The bias is a vector of size out_features, so each row (feature) in gemm_out (which is stored as row-major) has the same bias.
            // To do this in a kernel:

            // Launch a kernel to add bias:
            // For each element (sample, feature), add bias[feature]

            // Kernel for adding bias
            dim3 grid( (batch_size * out_features + 255) / 256 );
            dim3 block(256);

            add_bias_kernel<<<grid, block>>>(
                gemm_out.data_ptr<float>(),
                bias.data_ptr<float>(),
                batch_size,
                out_features
            );

            // Then apply batch norm, scaling:

            // Compute batch norm parameters:
            // For each feature j:
            // mean = running_mean[j]
            // var = running_var[j]
            // inv_std = 1/sqrt(var + eps)
            // normalized = (x - mean) * inv_std
            // bn_out = normalized * gamma + beta
            // scaled_out = bn_out * scale

            // Launch another kernel to apply batch norm and scaling
            apply_bnorm_scale_kernel<<<grid, block>>>(
                gemm_out.data_ptr<float>(),
                running_mean.data_ptr<float>(),
                running_var.data_ptr<float>(),
                gamma.data_ptr<float>(),
                beta.data_ptr<float>(),
                scale.data_ptr<float>(),
                eps,
                batch_size,
                out_features
            );

            // Now, gemm_out has the result after Gemm + batch norm + scaling.

            // Next, apply softmax. The softmax requires computing the exponential of each element, then dividing by the sum over the features.

            // The softmax kernel needs to compute for each sample:
            // For each feature j:
            // exp(x_j) / sum(exp(x_i) for i in features)

            // To compute this, first compute the exponentials, then compute the sum per sample.

            // Let's first compute the exponentials and store them in an intermediate tensor.
            torch::Tensor exp_vals = torch::empty_like(gemm_out);
            torch::Tensor sums = torch::empty(batch_size, torch::dtype(torch::kFloat32).device(gemm_out.device()));

            // Compute exponentials:
            exp_kernel<<<grid, block>>>(
                gemm_out.data_ptr<float>(),
                exp_vals.data_ptr<float>(),
                batch_size,
                out_features
            );

            // Compute the sum for each sample:
            compute_sums_kernel<<<grid, block>>>(
                exp_vals.data_ptr<float>(),
                sums.data_ptr<float>(),
                batch_size,
                out_features
            );

            // Then compute the softmax by dividing each exponential by the sum:
            softmax_divide_kernel<<<grid, block>>>(
                exp_vals.data_ptr<float>(),
                sums.data_ptr<float>(),
                batch_size,
                out_features
            );

            return exp_vals; // which now holds the softmax results
        }

        This approach breaks down the computation into multiple kernels, but may be manageable.

        However, writing all these kernels would be time-consuming, but let's try to code this step by step.

        First, the add_bias_kernel:

        __global__ void add_bias_kernel(
            float* out,
            const float* bias,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            int sample = idx / out_features;
            int feature = idx % out_features;
            out[idx] += bias[feature];
        }

        Then, the apply_bnorm_scale kernel:

        __global__ void apply_bnorm_scale_kernel(
            float* x,
            const float* running_mean,
            const float* running_var,
            const float* gamma,
            const float* beta,
            const float* scale,
            float eps,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            int feature = idx % out_features;
            float mean = running_mean[feature];
            float var = running_var[feature];
            float inv_std = 1.0f / sqrtf(var + eps);
            float normalized = (x[idx] - mean) * inv_std;
            float bn = normalized * gamma[feature] + beta[feature];
            x[idx] = bn * scale[0]; // assuming scale is a scalar (scale_shape is (1,))
        }

        Then, the exponential kernel:

        __global__ void exp_kernel(
            const float* x,
            float* exp_vals,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            exp_vals[idx] = expf(x[idx]);
        }

        The sum computation kernel requires a reduction. To compute the sum over each sample's features:

        To compute the sum for each sample, we can use a block per sample, and have threads in the block compute the sum for their assigned features.

        Let me define the kernel:

        __global__ void compute_sums_kernel(
            const float* exp_vals,
            float* sums,
            int batch_size,
            int out_features
        ) {
            extern __shared__ float shared[];
            int sample = blockIdx.x;
            int tid = threadIdx.x;

            float sum = 0.0f;

            for (int i = tid; i < out_features; i += blockDim.x) {
                sum += exp_vals[sample * out_features + i];
            }

            // Perform parallel reduction in block
            for (int s = blockDim.x / 2; s > 0; s >>= 1) {
                __syncthreads();
                if (tid < s) {
                    sum += shared[tid + s];
                }
            }

            if (tid == 0) {
                sums[sample] = sum;
            }
        }

        Wait, but shared memory needs to be allocated. Alternatively, a better approach is to use a block per sample and each thread computes a partial sum, then reduces within the block.

        Alternatively, use a shared array for the partial sums:

        __global__ void compute_sums_kernel(
            const float* exp_vals,
            float* sums,
            int batch_size,
            int out_features
        ) {
            int sample = blockIdx.x;
            int tid = threadIdx.x;
            __shared__ float shared[256]; // assuming blockDim.x <= 256

            float sum = 0.0f;
            for (int i = tid; i < out_features; i += blockDim.x) {
                sum += exp_vals[sample * out_features + i];
            }

            shared[tid] = sum;
            __syncthreads();

            // Perform reduction in shared memory
            for (int s = blockDim.x / 2; s > 0; s >>= 1) {
                if (tid < s) {
                    shared[tid] += shared[tid + s];
                }
                __syncthreads();
            }

            if (tid == 0) {
                sums[sample] = shared[0];
            }
        }

        The block size should be chosen such that blockDim.x divides out_features or can handle it. For example, if blockDim.x is 256 and out_features is 8192, then each thread handles 8192 / 256 = 32 elements.

        Finally, the softmax divide kernel:

        __global__ void softmax_divide_kernel(
            float* exp_vals,
            const float* sums,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            int sample = idx / out_features;
            exp_vals[idx] /= sums[sample];
        }

        Now, putting all together in the C++ code.

        Additionally, the user's original model requires that the inputs to the model are the parameters (weight, bias, etc.) as part of the model's parameters. So in the custom kernel, we need to pass all these tensors as arguments.

        However, in the PyTorch model, these parameters are stored in the model's attributes (e.g., self.gemm.weight, self.bn.running_mean, etc.). 

        In the new model (ModelNew), we need to replicate the parameters.

        The original Model has:

        - self.gemm: a Linear layer with weight and bias.

        - self.bn: a BatchNorm1d layer with running_mean, running_var, gamma (weight), beta (bias).

        - self.scale: a Parameter of shape (1,).

        So in the ModelNew class, we need to have these parameters as attributes.

        Therefore, the ModelNew will have to duplicate the parameters:

        class ModelNew(nn.Module):
            def __init__(self, in_features, out_features, bn_eps, bn_momentum, scale_shape):
                super().__init__()
                self.weight = nn.Parameter(torch.empty(out_features, in_features))
                self.bias = nn.Parameter(torch.empty(out_features))
                self.running_mean = nn.Parameter(torch.empty(out_features), requires_grad=False)
                self.running_var = nn.Parameter(torch.empty(out_features), requires_grad=False)
                self.gamma = nn.Parameter(torch.empty(out_features))  # BatchNorm's weight
                self.beta = nn.Parameter(torch.empty(out_features))   # BatchNorm's bias
                self.scale = nn.Parameter(torch.ones(scale_shape))
                # Initialize parameters from the original model's parameters
                # (this would be handled when copying from the original model)

            def forward(self, x):
                # Call the custom CUDA kernel function here
                return self.custom_forward(x)

        However, the problem requires that the new model's initialization is compatible with the original get_init_inputs function, which returns [in_features, out_features, bn_eps, bn_momentum, scale_shape].

        The original Model's __init__ takes in_features, out_features, bn_eps, bn_momentum, scale_shape.

        Therefore, the ModelNew should also take these parameters in __init__ and initialize the parameters accordingly.

        However, for the parameters like running_mean, running_var, gamma, beta, these are initialized by the original BatchNorm layer. To replicate them, we can copy the parameters from the original model. But since the problem says to generate the new code from scratch, perhaps we need to initialize them manually.

        Alternatively, in the new model's __init__, we can initialize the parameters similar to how PyTorch's nn.Linear and nn.BatchNorm1d do it.

        For example:

        self.weight = nn.Parameter(torch.empty(out_features, in_features).normal_(0, 1))
        self.bias = nn.Parameter(torch.zeros(out_features))
        self.running_mean = nn.Parameter(torch.zeros(out_features), requires_grad=False)
        self.running_var = nn.Parameter(torch.ones(out_features), requires_grad=False)
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        self.scale = nn.Parameter(torch.ones(scale_shape))

        But to exactly match the original model's initialization, perhaps it's better to use the same initializers.

        However, since the problem doesn't require maintaining the same initialization, just the same architecture, this should be acceptable.

        Now, putting all together.

        The custom CUDA kernel will need to take all these parameters as inputs when called.

        In the ModelNew's forward method, the inputs are:

        - input tensor x

        - weight, bias, running_mean, running_var, gamma, beta, scale, eps

        The eps is a scalar, which is part of the batch norm's parameters (bn_eps).

        So the forward function would be:

        def forward(self, x):
            return self.fused_forward(
                x,
                self.weight,
                self.bias,
                self.running_mean,
                self.running_var,
                self.gamma,
                self.beta,
                self.scale,
                bn_eps  # assuming bn_eps is passed as an argument to the model
            )

        Wait, but bn_eps is a parameter of the model? In the original Model, bn_eps is passed to the BatchNorm1d constructor. In the new model, since we are managing the batch norm parameters manually, we can store bn_eps as a buffer or a parameter.

        So, adding to ModelNew:

        self.bn_eps = bn_eps  # stored as an attribute

        So, in the __init__:

        self.bn_eps = bn_eps

        Then, in the forward:

        eps = self.bn_eps

        Now, the C++ function (the fused_forward function) will take these tensors and the eps as arguments.

        Now, putting all the code together.

        First, the CUDA kernels and the fused_forward function.

        Here is the code:

        First, the CUDA kernels and the fused_forward function.

        The CUDA code:

        // fused_gemm_bnorm_scale_softmax.cu

        #include <torch/extension.h>
        #include <cuda.h>
        #include <cuda_runtime.h>
        #include <cublas_v2.h>
        #include <math.h>

        __global__ void add_bias_kernel(
            float* out,
            const float* bias,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            int sample = idx / out_features;
            int feature = idx % out_features;
            out[idx] += bias[feature];
        }

        __global__ void apply_bnorm_scale_kernel(
            float* x,
            const float* running_mean,
            const float* running_var,
            const float* gamma,
            const float* beta,
            const float* scale,
            float eps,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            int feature = idx % out_features;
            float mean = running_mean[feature];
            float var = running_var[feature];
            float inv_std = 1.0f / sqrtf(var + eps);
            float normalized = (x[idx] - mean) * inv_std;
            float bn = normalized * gamma[feature] + beta[feature];
            x[idx] = bn * scale[0]; // assuming scale is a scalar (scale_shape is (1,))
        }

        __global__ void exp_kernel(
            const float* x,
            float* exp_vals,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            exp_vals[idx] = expf(x[idx]);
        }

        __global__ void compute_sums_kernel(
            const float* exp_vals,
            float* sums,
            int batch_size,
            int out_features
        ) {
            int sample = blockIdx.x;
            int tid = threadIdx.x;
            __shared__ float shared[256]; // assuming blockDim.x <= 256

            float sum = 0.0f;
            for (int i = tid; i < out_features; i += blockDim.x) {
                sum += exp_vals[sample * out_features + i];
            }

            shared[tid] = sum;
            __syncthreads();

            for (int s = blockDim.x / 2; s > 0; s >>= 1) {
                if (tid < s) {
                    shared[tid] += shared[tid + s];
                }
                __syncthreads();
            }

            if (tid == 0) {
                sums[sample] = shared[0];
            }
        }

        __global__ void softmax_divide_kernel(
            float* exp_vals,
            const float* sums,
            int batch_size,
            int out_features
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * out_features)
                return;
            int sample = idx / out_features;
            exp_vals[idx] /= sums[sample];
        }

        torch::Tensor fused_forward(
            torch::Tensor input,
            torch::Tensor weight,
            torch::Tensor bias,
            torch::Tensor running_mean,
            torch::Tensor running_var,
            torch::Tensor gamma,
            torch::Tensor beta,
            torch::Tensor scale,
            float eps
        ) {
            // Ensure all tensors are on the same device
            auto device = input.device();
            TORCH_CHECK(weight.device() == device);
            TORCH_CHECK(bias.device() == device);
            TORCH_CHECK(running_mean.device() == device);
            TORCH_CHECK(running_var.device() == device);
            TORCH_CHECK(gamma.device() == device);
            TORCH_CHECK(beta.device() == device);
            TORCH_CHECK(scale.device() == device);

            int batch_size = input.size(0);
            int in_features = input.size(1);
            int out_features = weight.size(0);

            // Step 1: Compute Gemm (W * x + bias) using cuBLAS
            torch::Tensor gemm_out = torch::empty({batch_size, out_features}, input.options());

            cublasHandle_t handle;
            cublasCreate(&handle);

            float alpha = 1.0f;
            float beta_val = 0.0f; // No initial value added

            cublasSgemm(
                handle,
                CUBLAS_OP_T,
                CUBLAS_OP_N,
                out_features,
                batch_size,
                in_features,
                &alpha,
                weight.data_ptr<float>(),
                in_features,
                input.data_ptr<float>(),
                in_features,
                &beta_val,
                gemm_out.data_ptr<float>(),
                out_features
            );

            cublasDestroy(handle);

            // Step 2: Add