The following are the steps you need to follow to generate the code:

First, identify which operators in the original Model's forward function can be replaced with custom CUDA kernels or fused into a single kernel. For each chosen operator, describe briefly why you choose them and how you plan to replace them (in one or two sentences). 

Second, generate the Python code for ModelNew with the required imports, and the inline CUDA code for the custom kernel(s). The code must be compilable. 

Third, if you are using operator fusion, also provide a separate CUDA kernel that combines multiple operations. 

Fourth, the code must be compatible with PyTorch and use the load_inline function. 

Fifth, ensure that the new ModelNew has the same input and output as the original Model, and that it can be initialized with the same parameters via get_init_inputs(). 

Remember to include all the necessary imports. 

Now, proceed step by step as described, starting with the analysis, followed by the code. 

---

Step 1: Analysis of the original Model's forward pass:

The forward function consists of the following operations: 

1. 3D convolution (self.conv)
2. Division by a constant (x / self.divisor)
3. Max pooling (self.max_pool)
4. Global average pooling (self.global_avg_pool)
5. Adding a bias (x = x + self.bias)
6. Sum along a specific dimension (torch.sum(...))

Now, identify which operators can be optimized with custom CUDA kernels or fused.

First, the division by a constant (operator 2) is a simple element-wise operation, which can be implemented in a custom CUDA kernel for potential speedup, especially for large tensors. 

Second, the addition of a bias (operator 5) is also an element-wise operation. Since it follows the global average pooling, which reduces dimensions, the tensor shape here might be smaller, but combining it with prior operations could reduce memory overhead.

Third, the sum along a dimension (operator 6) is another reduction operation. It can be implemented with a custom kernel to possibly avoid the overhead of PyTorch's default implementation.

However, the most significant potential for optimization lies in fusing multiple operations. For instance, combining the 3D convolution (operator 1) with the subsequent division (operator 2) into a single kernel might save time by avoiding an intermediate write to memory. Similarly, after the max pooling (operator 3), the global average pooling (operator 4) could be combined with the addition of the bias (operator 5), and finally, the sum (operator 6). 

However, convolution is a compute-intensive operation and may not easily fuse with other operations due to its complexity. Alternatively, fusing division, max pooling, global average pooling, bias addition, and the sum into a single kernel might be feasible. However, the convolution would still remain a separate step.

Alternatively, let's consider each step's computational cost. The convolution is likely the most expensive part, followed by the pooling operations. The division and bias addition are relatively cheap but could be optimized for latency.

Alternatively, perhaps the division and addition of bias can be fused into a single kernel, since they are both element-wise operations. Similarly, the max pooling and global average pooling could be combined into a single kernel, but that might complicate the implementation. 

Alternatively, the global average pooling is a reduction operation that could be combined with the final sum. 

Alternatively, the sum over a specific dimension (operator 6) might be implemented more efficiently as a custom kernel. Let me think about each operation's potential.

Let me think of possible fusions:

1. Convolution + division: Since convolution outputs are immediately divided by a constant, this can be done in the convolution kernel itself by scaling the output. This might reduce memory bandwidth, as we can perform the division during the convolution computation.

2. Max pooling + global average pooling: Max pooling is applied, then global average pooling. However, global average pooling is an average over all spatial dimensions. If the max pooling is followed by global average pooling, maybe it's possible to compute both in a single step? Probably not straightforward, as max pooling is a non-linear operation and the global average is a separate step. However, if the max pooling's output is then globally averaged, perhaps we can combine the two steps into a single kernel. Wait, the max pool is a reduction over local regions, and the global average is a reduction over the entire spatial dimensions. So, after max pooling, the spatial dimensions are reduced, and then the global average would average over those reduced dimensions. So, maybe the two can be combined, but it's unclear if that's beneficial. 

Alternatively, perhaps after the convolution and division, the max pooling and then global average can be handled in a single kernel. However, this might complicate the code.

Alternatively, the final operations: after global average pooling, we add a bias and then sum along a specific dimension. The global average pooling reduces the spatial dimensions to 1, so the tensor becomes (N, C, 1, 1, 1). Adding a bias of shape (C,1,1,1) is element-wise, then summing along dimension 1 (the channel dimension) would produce a tensor of (N, 1, 1, 1), which is then squeezed? Or maybe the sum is along another dimension.

Wait, let's look at the parameters: 

The bias_shape is (out_channels, 1, 1, 1). After the global average pooling, the output is (batch_size, out_channels, 1, 1, 1). Adding the bias (which has shape (out_channels, 1,1,1)) is done via broadcasting. So the result is still (batch_size, out_channels, 1,1,1). 

Then, summing along dim=self.sum_dim (which is 1, the channel dimension) would produce a tensor of shape (batch_size, 1, 1, 1, 1)? Wait no: when you sum along dimension 1, the size of dimension 1 becomes 1 (since it's summed out). So the result would be (batch_size, 1, 1, 1, 1), but perhaps the trailing dimensions are kept as 1. 

Therefore, the final sum is a reduction over the channel dimension. 

Perhaps combining the bias addition and the sum into a single kernel could be efficient. Since the bias is added first, but since it's element-wise, and then summed over the channels, perhaps we can compute the sum directly while adding the bias. That is, instead of adding the bias and then summing, we can do the summation of (x + bias) over the channel dimension, which could be done in a single step. This would save an intermediate addition step. 

Similarly, perhaps the division can be done during the convolution. 

So, the plan is:

1. Fuse the convolution with the division. 

2. Then, perform the max pooling and global average pooling as separate steps, but maybe those are too complex to fuse. Alternatively, maybe the max pooling followed by global average can be optimized, but perhaps it's easier to keep them as separate steps unless there's a clear way to combine. 

3. The final bias addition and sum can be combined into a single kernel. 

Alternatively, perhaps the global average pooling is already a reduction, but the max pooling is another reduction. Let me think: 

The forward steps are:

conv -> divide by constant -> max pool -> global avg pool -> add bias -> sum over dim 1. 

The key is to see if any of these can be combined into a single kernel.

The convolution is the first step, which is compute-heavy. Fusing it with the division is straightforward and would save a memory write. 

The max pooling is a local spatial reduction, followed by a global average over the remaining spatial dimensions. The combination of max pooling and global average might be done in a single step, but that requires computing both the max over local regions and then averaging across the entire output. It might not be straightforward. 

Alternatively, the global average after max pooling is equivalent to averaging over the pooled spatial dimensions. 

Perhaps, after the convolution and division, the max pooling and global average can be done in a single kernel. For example, during the max pooling, we can track the maximum values in each local region, and then compute the average over the entire spatial dimensions. However, this would require storing both the max values and their counts, which might complicate the implementation. 

Alternatively, perhaps it's better to leave the pooling steps as separate, but use optimized PyTorch functions for them. 

The final two steps (adding bias and summing) can be combined into a single kernel, as they operate element-wise and then reduce over a dimension. 

So, the steps to implement would be:

- Replace the convolution with a fused convolution and division kernel. 

- Combine the bias addition and summation into a single kernel. 

Alternatively, perhaps the division and the bias addition can be combined with other operations, but let's proceed step by step.

First, the convolution + division:

The standard convolution computes y = conv(x) and then y = y / divisor. To fuse this, we can modify the convolution kernel to divide each output element by the divisor during the computation. 

Second, the addition of the bias and summation over channels:

The current code does:

x = x + self.bias  # element-wise addition
x = torch.sum(x, dim=self.sum_dim)

We can combine this into a single kernel that adds the bias and accumulates the sum over the channels. This would save one memory write (the intermediate after adding the bias). 

The sum over channels can be computed in parallel by each thread handling a channel and accumulating into a shared memory buffer, then reducing across threads. 

Now, the max pooling and global average pooling steps:

PyTorch's max_pool3d and adaptive_avg_pool3d might already be optimized, but if we can find a way to combine them, perhaps into a single kernel, that might help. However, this requires understanding their implementation. The max_pool3d computes the maximum over local regions, and the adaptive_avg_pool3d computes the average over the entire spatial dimensions (since it's set to (1,1,1)), which in effect averages over all remaining spatial dimensions after max pooling. 

Wait, the global average pooling (AdaptiveAvgPool3d((1,1,1))) will take the input after max pooling and compute the average over all spatial dimensions. Suppose after max pooling, the spatial dimensions are reduced, then the global average would average those. 

But maybe it's possible to compute the average over the original spatial dimensions combined with the max pooling in a single step? Probably not straightforward, since max pooling is a non-linear operation. 

Alternatively, perhaps the max pooling can be done in a way that also computes the average, but that doesn't make sense. 

Alternatively, the adaptive average pooling after max pooling is a separate step. 

Therefore, perhaps it's best to leave the pooling steps as PyTorch functions unless there is a way to combine them with other operations. 

Therefore, the main kernels to implement are:

1. Fused convolution and division (conv_divide)

2. Fused bias addition and summation over channels (sum_with_bias)

Now, proceed to implement these.

First, let's start with the fused convolution and division.

However, implementing a 3D convolution kernel from scratch would be time-consuming and error-prone. Given that PyTorch already provides optimized convolution implementations, perhaps it's better to instead fuse the division into the existing PyTorch convolution by scaling the weights or the output. However, modifying the convolution's weights would require scaling them, but the division is a constant, so the convolution's output can be divided by the divisor. 

Wait, the division is a constant, so if we can compute the convolution as usual and then divide by the divisor, it's a simple element-wise operation. But if we can do this in a fused kernel with the convolution, perhaps the memory access can be optimized. However, implementing a custom 3D convolution is non-trivial. 

Given that the user has to provide code that is compilable and functional, perhaps it's better to avoid re-implementing the convolution, as it's a complex operation. Therefore, perhaps focus on the element-wise operations and the final summation.

Therefore, perhaps the following steps:

- Replace the division by a constant with a custom kernel (element-wise division)

- Replace the final bias addition and summation with a single kernel. 

Alternatively, the division is a simple operation, but for a 5D tensor, it's just element-wise. However, PyTorch's division is already optimized, so perhaps not much gain here. 

The bias addition and summation over channels: 

The bias is added element-wise, then the sum over channels. Since the global average pool reduces the spatial dimensions to 1, the tensor before adding the bias has shape (batch, channels, 1,1,1). Adding the bias (shape (channels,1,1,1)) results in the same shape. Summing over dim=1 (the channel dimension) would produce a tensor of shape (batch, 1, 1, 1, 1). 

Therefore, the final step reduces the tensor to a single channel dimension. 

Implementing a custom kernel for this could combine the bias addition and the sum. 

Thus, the plan is:

1. Replace the final bias addition and summation with a fused kernel.

2. Possibly replace the division with a custom kernel, but it may not be necessary. 

Alternatively, perhaps the division is too trivial to warrant a custom kernel. Let's check the steps again:

The forward path:

convolution (PyTorch) -> element-wise division (by a scalar) -> max_pool (PyTorch) -> global_avg_pool (PyTorch) -> add bias (element-wise) -> sum over dim.

The division and the addition are both element-wise. The sum is a reduction. 

So, perhaps the main candidates for optimization are the element-wise division and the final combination of add bias and sum. 

First, the element-wise division by a constant is a simple operation. In PyTorch, this is already vectorized and optimized. However, if we can combine it with another operation, it might save a step. But since it's a single operation, perhaps it's not worth writing a custom kernel. 

The final steps (add bias + sum over channels) can be fused into a single kernel. 

Therefore, the main custom kernel is for the final two steps. 

Thus, the steps to implement are:

- Fused kernel for adding the bias and summing over channels.

Additionally, perhaps the global average pooling can be optimized, but since it's a reduction, maybe the default implementation is already efficient. 

Therefore, the code will:

- Keep the convolution, division, max pool, and global average pool as PyTorch functions.

- Replace the final add bias and sum with a custom kernel. 

Now, let's proceed to implement this.

First, the fused kernel for the final steps.

The inputs to the kernel will be:

- The input tensor after global_avg_pool (shape: (batch, C, 1,1,1))

- The bias tensor (shape: (C,1,1,1))

The kernel needs to compute: (input + bias).sum(dim=1)

The output is a tensor of shape (batch, 1, 1,1,1) (assuming dim=1 is summed over).

To implement this efficiently, we can have each thread handle a batch element and process all channels, accumulating the sum. 

Alternatively, we can have a grid where each block corresponds to a batch element. Each thread in the block processes a channel, adds the bias, and accumulates into a shared memory array, then performs a reduction within the block to compute the sum for that batch element.

The steps for the kernel:

For each element in the batch:

- Initialize sum to 0.

- For each channel in C:

    - value = input[batch, channel, ...] + bias[channel, ...]

    - sum += value

- Store the sum in the output at batch position.

This can be parallelized per batch element, with threads handling different channels. 

However, the spatial dimensions after global average pool are all 1, so the input is effectively 2D: (batch, C, 1,1,1) is equivalent to (batch, C). 

Therefore, the actual computation can treat the input as (batch, C), and the bias as (C). 

Thus, the kernel can process each batch element independently. 

The CUDA kernel structure:

We can have a grid of blocks, each block handling one batch element. The number of blocks is equal to the batch size. Each block will process all the channels for that batch element. 

Inside each block, we can have threads. Suppose we have a thread per channel. 

Wait, but if C is large (e.g., 16), then the number of threads per block can be 16, which is small. Alternatively, we can have multiple threads per block and use shared memory for reduction. 

Alternatively, for each batch element, the summation over C channels can be done in parallel using a block of threads. 

Here's a possible approach:

- Each block handles one batch element.

- Each thread in the block is assigned to process a subset of the channels.

- The threads compute their partial sums and accumulate them into shared memory, then perform a reduction within the block to get the total sum for that batch.

This would allow parallel processing of the channels within a block.

Let's outline the code:

__global__ void fused_sum_with_bias_kernel(
    const float* input,
    const float* bias,
    float* output,
    int batch_size,
    int channels,
    int spatial_size // which is 1*1*1, so 1
) {

    // Each block handles one batch element
    int batch_idx = blockIdx.x;

    // Load input for this batch
    const float* input_row = input + batch_idx * channels * spatial_size;
    const float* bias_ptr = bias;

    // Shared memory for partial sums
    extern __shared__ float shared[];

    int thread_id = threadIdx.x;
    int tid = threadIdx.x;

    float sum = 0.0f;

    // Each thread processes a channel
    for (int c = tid; c < channels; c += blockDim.x) {
        float val = input_row[c * spatial_size] + bias_ptr[c];
        sum += val;
    }

    // Write to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Perform block reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        // Write the result to output
        output[batch_idx * spatial_size] = shared[0];
    }
}

The spatial_size is 1, so we can ignore it. 

The parameters would be:

- input: tensor of shape (batch, C, 1,1,1)

- bias: tensor of shape (C, 1,1,1) (but we can treat it as (C,))

- output: tensor of shape (batch, 1, 1,1,1)

The number of blocks is batch_size. The block size can be chosen as min(threads_per_block, channels), but need to handle that. 

Wait, the block size can be up to the number of channels, but for efficiency, it's better to use a block size that divides the number of channels. However, this can be dynamic. 

Alternatively, the block size can be 256, and for channels less than 256, it's okay. 

In the kernel call, the shared memory size needed is blockDim.x * sizeof(float). 

The code for the kernel function in CUDA would be as above.

Now, in Python, the function would be:

def fused_sum_with_bias(input, bias):

    batch_size = input.size(0)
    channels = input.size(1)
    spatial_size = input.size(2) * input.size(3) * input.size(4)  # which is 1

    output = torch.zeros(batch_size, 1, 1, 1, 1, device=input.device)

    block_size = 256  # or some other value, but needs to be chosen properly
    # block_size should be at least the number of channels? Not necessarily, since threads loop over channels in steps.

    # The grid size is batch_size, block size is block_size (threads per block)
    # The shared memory per block is block_size * sizeof(float)
    shared_mem = block_size * sizeof(float)

    # Need to calculate the block size properly. Alternatively, set block size to 1024 or 512, but the exact value may depend on channels.

    # Alternatively, block_size = min(1024, channels) but this complicates things.

    # For simplicity, let's choose a fixed block size, say 256, and see.

    # Launch the kernel
    fused_sum_with_bias_kernel<<<batch_size, block_size, shared_mem>>>(input.data_ptr(), bias.data_ptr(), output.data_ptr(), batch_size, channels, spatial_size)

    return output

Wait, but the bias has to be reshaped or treated as a 1D array. Since the bias tensor is (C,1,1,1), we can view it as (C,), so in the kernel, the bias is accessed as bias[c].

Now, considering that in PyTorch, the tensors may be contiguous, but we need to ensure that the input and bias are in the correct format.

Now, compiling this kernel with load_inline. 

Additionally, the code for the fused kernel needs to be written correctly.

Now, moving to code generation.

First, the ModelNew will need to import necessary modules and define the custom kernel.

Second, the code for the fused_sum_with_bias kernel.

Additionally, perhaps the division and the addition can also be fused, but as I thought earlier, maybe the division can be left as is, unless it's beneficial.

Wait, let's check the division step:

x = x / self.divisor

This is an element-wise division by a scalar. PyTorch's implementation is likely optimized, so perhaps writing a custom kernel for this is not necessary. However, if we can combine it with another operation, it might save some time. But if not, perhaps it's better to leave it as is.

Therefore, the only custom kernel needed is the fused_sum_with_bias.

Proceeding with this plan.

Now, writing the code:

The ModelNew will replace the final two operations (add bias and sum) with the custom kernel.

The rest of the operations remain as in the original code, using PyTorch's functions.

Therefore, the code for ModelNew:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused kernel code for adding bias and summing over channels
fused_sum_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void fused_sum_with_bias_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch_size,
    int channels,
    int spatial_size) {
    int batch_idx = blockIdx.x;
    const scalar_t* input_row = input + batch_idx * channels * spatial_size;
    const scalar_t* bias_ptr = bias;

    extern __shared__ scalar_t shared[];
    int tid = threadIdx.x;
    scalar_t sum = 0.0;

    // Each thread processes a channel in steps of blockDim.x
    for (int c = tid; c < channels; c += blockDim.x) {
        scalar_t val = input_row[c * spatial_size] + bias_ptr[c];
        sum += val;
    }

    // Write partial sum to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[batch_idx * spatial_size] = shared[0];
    }
}

std::tuple<torch::Tensor> fused_sum_with_bias(
    torch::Tensor input,
    torch::Tensor bias) {
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int spatial_size = input.size(2) * input.size(3) * input.size(4);
    const int output_size = spatial_size;

    torch::Tensor output = torch::zeros({batch_size, 1, 1, 1, 1}, input.options());

    const int block_size = 256;
    const int shared_size = block_size * sizeof(float);

    // Launch the kernel
    AT_DISPATCH_FLOATING_TYPES(input.type(), "fused_sum_with_bias", ([&] {
        fused_sum_with_bias_kernel<scalar_t><<<batch_size, block_size, shared_size>>>(
            input.data_ptr<scalar_t>(),
            bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            channels,
            spatial_size
        );
    }));

    return std::make_tuple(output);
}
"""

fused_sum_cpp_source = R"(
std::tuple<torch::Tensor> fused_sum_with_bias(
    torch::Tensor input,
    torch::Tensor bias);
)"

# Compile the fused kernel
fused_sum = load_inline(
    name="fused_sum",
    cpp_sources=fused_sum_cpp_source,
    cuda_sources=fused_sum_source,
    functions=["fused_sum_with_bias"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.divisor = divisor
        self.max_pool = nn.MaxPool3d(pool_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.sum_dim = sum_dim
        self.fused_sum = fused_sum

    def forward(self, x):
        x = self.conv(x)
        x = x / self.divisor
        x = self.max_pool(x)
        x = self.global_avg_pool(x)
        # Instead of x = x + self.bias and then sum, use the fused kernel
        output = self.fused_sum.fused_sum_with_bias(x, self.bias)
        return output[0]  # Since it's a tuple from the kernel

Wait, but the output from the kernel is a tuple with one element, so accessing [0] is correct.

Wait in the kernel's function declaration, it returns a tuple<torch::Tensor>, so in Python, it returns a tuple, hence [0].

But let's check the code again:

In the fused_sum_with_bias function in C++, it returns std::make_tuple(output). So in Python, when you call fused_sum.fused_sum_with_bias(a, b), it returns a tuple (output,), so you need to index [0] to get the tensor.

Now, let's verify the parameters:

The input to the fused_sum_with_bias function must be:

input: the tensor after global_avg_pool (shape: batch, C, 1,1,1)

bias: the self.bias (shape: (C,1,1,1))

The kernel treats the bias as a 1D array (since the spatial dimensions are 1), so accessing bias[c] is okay.

The spatial_size is calculated as input.size(2)*input.size(3)*input.size(4) which is 1.

The output is initialized as (batch_size, 1,1,1,1).

This should match the original model's output.

Now, check if the sum is along the correct dimension. The original code summed along self.sum_dim = 1 (the channel dimension). The fused kernel sums over all channels, so yes, it is equivalent.

Therefore, this code should work.

Additionally, we need to ensure that the fused kernel is correctly implemented and that the block size and shared memory are handled properly.

Another possible optimization is to handle the spatial dimensions correctly, but since they are all 1, it's okay.

Potential issues:

- The block size must be a power of 2? Not necessarily, but the reduction loop assumes that it's handled properly. Since we're using blockDim.x, which can be any value, but the reduction steps are done in powers of two.

Wait, the reduction loop:

for (int s = blockDim.x / 2; s > 0; s >>= 1) {

This works if blockDim.x is a power of two, but if it's not, then s may not cover all cases. For example, if blockDim.x is 255, then s starts at 127, then 63, etc., which may not fully reduce all threads. 

To handle non-power-of-two block sizes, we need a different reduction approach. However, for simplicity, we can choose a block size that is a power of two (e.g., 256) and set the shared memory accordingly. 

In the code above, block_size is 256, which is a power of two. So the reduction loop should work correctly.

Another thing to check is the input and bias being contiguous. The kernel assumes that input and bias are contiguous in memory. Since PyTorch tensors may not be contiguous by default, we might need to ensure they are. 

In the code, the global_avg_pool output should be contiguous. The bias is a parameter and should be contiguous as well. 

Alternatively, in the kernel, the input is accessed as input_row[c * spatial_size], which is correct if the tensor is stored in row-major order. 

Thus, the code should be okay.

Therefore, the final code with the fused kernel for the final steps is as above.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused kernel code for adding bias and summing over channels
fused_sum_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void fused_sum_with_bias_kernel(
    const scalar_t* __restrict__ input,
    const scalar_t* __restrict__ bias,
    scalar_t* __restrict__ output,
    int batch_size,
    int channels,
    int spatial_size) {
    int batch_idx = blockIdx.x;
    const scalar_t* input_row = input + batch_idx * channels * spatial_size;
    const scalar_t* bias_ptr = bias;

    extern __shared__ scalar_t shared[];
    int tid = threadIdx.x;
    scalar_t sum = 0.0;

    // Each thread processes a channel in steps of blockDim.x
    for (int c = tid; c < channels; c += blockDim.x) {
        scalar_t val = input_row[c * spatial_size] + bias_ptr[c];
        sum += val;
    }

    // Write partial sum to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[batch_idx * spatial_size] = shared[0];
    }
}

std::tuple<torch::Tensor> fused_sum_with_bias(
    torch::Tensor input,
    torch::Tensor bias) {
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int spatial_size = input.size(2) * input.size(3) * input.size(4);
    const int output_size = spatial_size;

    torch::Tensor output = torch::zeros({batch_size, 1, 1, 1, 1}, input.options());

    const int block_size = 256;
    const int shared_size = block_size * sizeof(float);

    // Launch the kernel
    AT_DISPATCH_FLOATING_TYPES(input.type(), "fused_sum_with_bias", ([&] {
        fused_sum_with_bias_kernel<scalar_t><<<batch_size, block_size, shared_size>>>(
            input.data_ptr<scalar_t>(),
            bias.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            batch_size,
            channels,
            spatial_size
        );
    }));

    return std::make_tuple(output);
}
"""

fused_sum_cpp_source = R"""std::tuple<torch::Tensor> fused_sum_with_bias(
    torch::Tensor input,
    torch::Tensor bias);
"""

# Compile the fused kernel
fused_sum = load_inline(
    name="fused_sum",
    cpp_sources=fused_sum_cpp_source,
    cuda_sources=fused_sum_source,
    functions=["fused_sum_with_bias"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.divisor = divisor
        self.max_pool = nn.MaxPool3d(pool_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.sum_dim = sum_dim
        self.fused_sum = fused_sum

    def forward(self, x):
        x = self.conv(x)
        x = x / self.divisor
        x = self.max_pool(x)
        x = self.global_avg_pool(x)
        output = self.fused_sum.fused_sum_with_bias(x, self.bias)
        return output[0]  # Extract the tensor from the tuple
```