The following are the requirements and constraints for your solution:

You can choose to replace any operators or function calls in the original architecture, but you must reimplement at least the following operations with custom CUDA kernels: the combination of the Gemm (Linear layer), batch normalization, scaling, and Softmax. You may choose to combine these operations into a single kernel or split them into multiple kernels. However, all of these operations must be implemented using your custom CUDA kernels. You may also choose to implement other operations with custom CUDA kernels if you want.

You may use any PyTorch extensions, including PyTorch's CUDA extension APIs, or ATen. You can also use any standard CUDA programming techniques (e.g., shared memory, thread synchronization, etc.).

The model's parameters (weights and biases of the Linear layer, BatchNorm parameters) should be kept as PyTorem model parameters so that they can be optimized via backpropagation. The model should still be trainable after your modifications. 

You must ensure that your code is compatible with PyTorch's autograd system. This means that your custom CUDA kernels must correctly compute gradients if they are differentiable. You can use PyTorch's autograd by registering backward functions or by using the autograd.Function wrapper.

You must make sure that your code runs correctly with the given get_inputs and get_init_inputs functions. Your code should not crash when these functions are called.

You are allowed to use any PyTorch built-in operators except for the ones you are replacing with custom CUDA kernels. For example, if you replace the Gemm operator with a custom kernel, you cannot use torch.matmul in your custom kernel implementation.

You may choose to write the kernels in either C++/CUDA or pure Python (using torch.utils.cpp_extension.load_inline). However, the provided example uses the latter, so you should follow the same approach here.

You are allowed to use any of PyTorch's CUDA-accelerated functions in your kernels except for the ones you are explicitly replacing. For instance, if you are replacing the Gemm, you can use other CUDA functions like atomicAdd, but not torch.add or similar functions.

The input tensors to your model (as generated by get_inputs) are on the CPU. However, in the get_inputs function provided above, the inputs are generated on the CPU. Wait, in the original problem statement's example, the inputs were generated on the CPU, but in the example code, the inputs were generated on the CUDA device. Wait, in the user's problem, the get_inputs function for the given architecture returns tensors on the CPU. But in the example given by the user, the inputs were generated on the CUDA device. So need to make sure that the code handles inputs generated on the CPU correctly. Wait, the user says in the problem statement: 

Wait, looking back at the problem statement, the user says: "You are given the following architecture: " which includes the get_inputs function which returns tensors on the CPU: 

def get_inputs():
    return [torch.rand(batch_size, in_features)]

But in the example given by the user (the first example where they replaced a + b), the inputs were generated on the CUDA device. So in this problem, the user's given architecture's get_inputs returns tensors on CPU. So when writing the new code, the model should work with inputs on the CPU, but in practice, when the model is used, the tensors will be moved to CUDA if the model is on CUDA. Wait, but the model's forward function will be run on CUDA if the model is on CUDA. So the code needs to handle that the inputs are generated on CPU, but the model is expected to run on CUDA. So perhaps the user expects that the model is moved to CUDA, and the inputs are moved to CUDA as well. But the get_inputs function as given returns CPU tensors. So in the code, when testing, one would have to move the inputs to CUDA. However, the user says that the code should not crash when get_inputs is called, but the code is supposed to run correctly. So in the code, the model's forward should take tensors on whatever device they are on. Therefore, the custom CUDA kernels should handle being on CUDA devices. Therefore, in the code, the model must be placed on CUDA, and inputs must be moved to CUDA. But the get_inputs function returns CPU tensors. Therefore, in the code, when using get_inputs, the tensors would be on CPU, but when passed to the model, which is on CUDA, they need to be moved. However, the problem states "You must ensure that your code runs correctly with the given get_inputs and get_init_inputs functions. Your code should not crash when these functions are called." Therefore, the code must accept inputs generated on CPU. However, the custom CUDA kernels must be run on the GPU, so the tensors must be on GPU. Therefore, the code must move the inputs to GPU before processing. Wait, but in PyTorch, when you move the model to GPU, the forward function will expect the inputs to be on GPU as well. Therefore, the code must ensure that inputs are on the same device as the model. So when writing the model's forward function, the inputs can be moved to the device of the model's parameters. Alternatively, the user might expect that the code is written in a way that the inputs are on the correct device. However, given that the problem allows the code to work with get_inputs, which returns CPU tensors, the code must handle that. But the custom CUDA kernels can only run on CUDA. Therefore, the inputs must be moved to CUDA before passing to the kernels. Therefore, in the forward function, the inputs can be moved to the device. However, this might be inefficient, but it's necessary to comply with the problem constraints. Alternatively, perhaps the user expects that the model is run on CUDA and the inputs are moved to CUDA before being passed to the model. Since the problem states "Your code should not crash when these functions are called", but doesn't specify the device, perhaps the code can assume that the model is on CUDA and inputs are moved there. But the problem says the inputs are generated on CPU, so the code must handle that. Hmm, this is a bit ambiguous, but proceeding with the assumption that the model is on CUDA and the inputs are moved to CUDA before processing. Therefore, in the forward function, perhaps the inputs are moved to the device of the model's parameters. Alternatively, in the custom CUDA kernels, we can assume that the inputs are on CUDA. Therefore, the code must ensure that the tensors are on CUDA. 

Now, back to the problem. The user requires that the Gemm (Linear layer), batch normalization, scaling, and Softmax are implemented with custom CUDA kernels. They can be combined into one kernel or split into multiple. The model must remain trainable, so the parameters (weights, bias, BN parameters) must be part of the model's parameters. The kernels must be compatible with autograd.

The approach here is to combine the Linear (Gemm), BatchNorm, scaling, and Softmax into a single fused kernel. This is because combining multiple operations into a single kernel can reduce memory traffic and thread synchronization overhead. However, implementing the backward pass for such a fused kernel would be complex. Alternatively, split into multiple kernels but still fuse some parts. 

Alternatively, separate Gemm and BN into one kernel and then scaling and softmax into another. 

First, let's think of the operations:

1. Gemm: x = W * x + b (where W is the weight, b is the bias)
2. BatchNorm: x = (x - mean) / sqrt(var + eps) * gamma + beta (but since it's a BatchNorm1d, the mean and variance are computed over the batch dimension and channels)
Wait, BatchNorm1d applies normalization over the C dimension (the second dimension in (N,C)), so for each channel, compute mean and variance across the batch. Therefore, for each feature (dimension 1), compute mean and var over all samples in the batch. 

However, in training mode, the batch statistics are used. In evaluation mode, the running mean and var are used. But the problem doesn't specify the mode, so the code must handle both. However, implementing a custom BatchNorm kernel that supports both training and evaluation modes requires more work. 

However, since the problem allows us to choose the operators to replace, but requires replacing these, so we need to handle batch normalization in the kernel. 

But integrating all these operations (gemm, bn, scaling, softmax) into a single kernel would be quite complex, especially the backward pass. 

Alternatively, separate Gemm and BatchNorm into a fused kernel, then scaling and softmax into another. 

Alternatively, let's think of the steps:

The forward pass for the model is:

x = Gemm(x)  --> W * x + b
x = BatchNorm(x) --> normalize along the channels
x = scale * x
x = softmax(x)

The backward pass would require gradients for each of these steps.

To make it manageable, perhaps we can fuse Gemm and BatchNorm into one kernel, then scaling and softmax into another. 

But let's see:

First, let's see the dimensions:

Input x is (batch_size, in_features). 

The Linear layer (Gemm) has weight of shape (out_features, in_features) and bias (out_features). The output after Gemm is (batch_size, out_features). 

Then, the BatchNorm is over the out_features dimension (BatchNorm1d). 

Scaling is a parameter of shape (1, ), so it's element-wise multiplication. 

Then, Softmax over the features (dim=1). 

Therefore, the operations can be written as:

x = matmul(W, x) + b
x = (x - mean) / sqrt(var + eps) * gamma + beta
x = scale * x
x = softmax(x)

The challenge is to fuse these operations into one or more kernels. 

First, the batch normalization step requires computing the mean and variance across the batch dimension. So for each channel (out_features), compute the mean of the current mini-batch. 

This computation is data-dependent and requires reduction over the batch dimension. 

Implementing this in a CUDA kernel would require per-channel reductions. 

Given that the problem allows us to replace the operators with custom CUDA kernels, we can proceed.

Let me think of the steps:

First, the fused kernel for Gemm + BatchNorm.

The Gemm (matrix multiplication) can be implemented with a standard GEMM kernel, but fused with the batch normalization.

However, the batch normalization requires computing the mean and variance over the batch for each channel. 

Therefore, in the forward pass, the steps would be:

1. Compute the output of the linear layer: Y_linear = X * W^T + b (assuming the linear layer is implemented as X * W^T + b, since PyTorch's Linear layer is in_features x out_features, and the input is batch_size x in_features, so the multiplication is X * W^T + b).

Wait, PyTorch's Linear layer is defined as: 

out = input @ weight^T + bias, where weight is (out_features, in_features), so the dimensions are correct.

So the forward pass of the Linear layer is Y_linear = matmul(X, W) + b, where W is (in_features, out_features) ?

Wait, no. Let's clarify:

Suppose the input is (B, in_features), and the weight matrix of the Linear layer is (out_features, in_features). Then, the output is computed as:

output = input.matmul(weight.t()) + bias.view(1, -1)

Therefore, the matrix multiplication is between (B, in_features) and (in_features, out_features), resulting in (B, out_features).

Therefore, to compute the linear layer in a CUDA kernel, we can do:

for each sample in batch, for each output feature, compute the dot product between the input sample and the corresponding row of the weight matrix, then add the bias.

But doing this in a kernel is possible but may be slower than using cuBLAS. However, since we have to replace it with a custom kernel, we have to proceed.

Alternatively, using cuBLAS in the custom kernel is allowed as long as we are not using the PyTorch's matmul operator. Since cuBLAS is a CUDA library, and the problem allows using CUDA functions except the ones being replaced. Since we are replacing the Gemm operator, we can't use torch.matmul, but we can use cuBLAS's functions. However, cuBLAS is part of CUDA, so it's allowed.

Wait, the problem says: "You may choose to write the kernels in either C++/CUDA or pure Python (using torch.utils.cpp_extension.load_inline). However, the provided example uses the latter, so you should follow the same approach here."

Therefore, writing a CUDA kernel in C++ code via load_inline is acceptable. 

Therefore, in the custom kernel, using cuBLAS for the matrix multiplication would be efficient, but perhaps the problem wants us to implement it from scratch. However, since cuBLAS is a CUDA library function, it's allowed. The problem says: "You may use any PyTorch built-in operators except for the ones you are replacing with custom CUDA kernels. For example, if you replace the Gemm operator with a custom kernel, you cannot use torch.matmul in your custom kernel implementation."

Therefore, using cuBLAS is allowed because it's not a PyTorch operator. Therefore, the Gemm can be implemented via cuBLAS. 

Therefore, perhaps it's better to use cuBLAS for the matrix multiplication part to achieve better performance, and then proceed with the rest.

Alternatively, implement the matrix multiplication manually in the kernel. But for large matrices, cuBLAS would be better.

Assuming we can use cuBLAS, here's a plan:

Fusion of Gemm + BatchNorm:

The fused kernel would:

1. Compute the linear layer using cuBLAS (matrix multiplication and bias addition).
2. Compute the batch normalization (mean, var, normalize, scale by gamma, shift by beta).
3. Then multiply by the scale parameter.
4. Then apply the softmax.

But integrating all these steps into a single kernel might be complex, but let's see.

Alternatively, split into two fused kernels:

- First, fuse Gemm + BatchNorm + scaling into one kernel, then Softmax into another.

But the Softmax requires the exponential and normalization across features.

Alternatively, fuse all steps except Softmax, and use PyTorch's Softmax. However, the problem requires that the Softmax is replaced with a custom kernel. Therefore, all must be included.

Alternatively, the entire sequence Gemm + BN + scale + Softmax into one kernel.

The steps in forward:

Input: x (batch_size, in_features)

Output: 

Step 1: Gemm (W, bias)

y_linear = x @ W + bias

Step 2: BatchNorm: compute mean and var over the batch for each feature.

mean = mean(y_linear, dim=0)

var = var(y_linear, dim=0)

y_bn = (y_linear - mean) / sqrt(var + eps) * gamma + beta

Step 3: scale * y_bn

y_scaled = scale * y_bn

Step 4: Softmax: exp(y_scaled) / sum(exp(y_scaled), dim=1)

The challenge is to compute all these steps in a fused CUDA kernel.

The backward pass requires gradients for each step, which can be complex.

Alternatively, separate into Gemm + BN as one kernel, and scaling + Softmax as another. But the problem requires replacing all four.

Let me think of the kernel structure.

First, the forward kernel must:

- Compute the linear layer (using cuBLAS?)

But if we use cuBLAS, we can call it from the kernel? Wait no. cuBLAS is a library that must be called from host code. Therefore, in a CUDA kernel, we can't call cuBLAS functions directly. Therefore, to use cuBLAS, we need to launch it from the host code, not from within the kernel.

Therefore, perhaps we need to implement the matrix multiplication manually.

Alternatively, separate the Gemm into a cuBLAS call in the host code, then proceed with the rest in kernels. But that would require not fusing the Gemm with the subsequent steps, but since the problem requires replacing the Gemm operator, we can't use PyTorch's Linear layer, so we have to implement it ourselves. 

Therefore, to implement the Gemm in a CUDA kernel, let's consider that.

First, the matrix multiplication:

The input is (B, in_features), weights (out_features, in_features), so the output is (B, out_features).

The kernel would process each output element (i.e., for each sample, each output feature).

The computation for each output element is sum_{k=0}^{in_features-1} input[i][k] * weight[j][k], plus the bias[j].

Therefore, for each output element (i,j), compute the dot product of the input's row and the weight's row.

This is a standard GEMM, but implemented in a CUDA kernel.

The kernel would have:

Each thread block processes a block of output elements, or each thread computes one element.

The grid dimensions would be (B * out_features) threads, each thread computes one element.

However, for large dimensions, this might be inefficient due to the high number of threads. Alternatively, using a tiled approach with shared memory could be better, but that's more complex.

Alternatively, using a kernel that processes one row of the input (i.e., one sample) and computes all output features for that sample.

Each thread block can handle one sample, with each thread computing one output feature.

But the number of output features is 8192, which is quite large. So each thread block would need a large number of threads, which is not feasible (max threads per block is 1024).

Alternatively, use a 2D grid where each block computes a block of output features for all samples.

Hmm, this is getting complicated. Let's proceed with a simple kernel first, even if it's not the most optimized, but functional.

Alternatively, use the cuBLAS gemm in the host code, but then the rest must be in kernels.

Wait, perhaps the approach is:

- The Gemm is implemented using cuBLAS in host code (since it's allowed as not a PyTorch operator), then the rest (BN, scaling, softmax) are implemented in a kernel. But since the problem requires replacing the Gemm operator with a custom kernel, using cuBLAS in host code might be acceptable as long as it's part of the custom kernel's implementation.

Wait, the Gemm operator is part of the Linear layer. So replacing the Linear layer's computation requires implementing its computation. Since the Linear layer is replaced by the custom kernel, using cuBLAS in the custom kernel's host code is allowed.

Therefore, here's the plan:

Implement a fused kernel that:

1. Performs the matrix multiplication using cuBLAS (host code part)
2. Computes the batch normalization (host code for mean/var computation, but actually, the kernel would need to compute mean and var for each feature across the batch)
Wait, the mean and var are computed across the batch. 

Wait, the batch normalization requires, for each feature (dim=1), compute the mean over the batch dimension (dim=0).

Therefore, for each feature j, mean_j = (1/B) * sum_{i=0}^{B-1} x[i][j]

Similarly for variance.

Therefore, the computation of mean and variance is a reduction over the batch dimension for each feature.

This can be done in a CUDA kernel by:

- Each thread computes partial sums for a block of features, then using a reduction.

Alternatively, use atomic operations, but that's not efficient.

Perhaps a better approach is to first compute the sum over the batch for each feature.

Let me think of steps:

Forward pass:

1. Compute the linear layer (x @ W + b) via cuBLAS.

2. Compute the batch normalization:

   a. Compute mean and variance for each feature.

   b. Normalize the data: (x - mean) / sqrt(var + eps)

   c. Scale and shift with gamma and beta.

3. Apply scaling by the scale parameter (element-wise multiplication).

4. Apply Softmax.

All steps 2-4 can be implemented in a single kernel, but step 2 requires reduction across the batch dimension for each feature.

Therefore, the kernel would need to first compute the means and variances.

However, in CUDA, reductions across large arrays (e.g., batch_size=1024 and out_features=8192) would need to be handled efficiently.

Alternatively, compute the mean and var first, then proceed with the normalization.

Therefore, the steps in code would be:

- After the matrix multiplication, we have y_linear tensor.

- Compute the mean of y_linear across dim 0 (batch).

- Compute the variance (mean of squares minus square of mean).

- Then, compute the normalized tensor.

- Then, apply gamma, beta, scaling, and softmax.

However, all these steps must be implemented in a custom kernel, without using PyTorch's batch norm operator.

The challenge is to compute the mean and variance efficiently.

Another approach is to first compute the mean in a kernel:

Mean computation kernel:

Each thread block handles one feature.

Each thread in the block processes a batch element.

The block computes the sum for its feature.

Then, after all blocks have computed their sums, a second kernel reduces the per-block sums into a global mean.

Wait, this requires multiple steps. Alternatively, use a reduction kernel.

Alternatively, use a kernel that for each feature j:

sum = 0.0

for each i in 0..B-1:

sum += y_linear[i][j]

Then mean[j] = sum / B

Similarly for variance.

But doing this in parallel across features.

Each thread can process a feature.

For each thread j (feature index):

sum = 0.0

for i in 0..B-1:

sum += y_linear[i][j]

mean[j] = sum / B

But with B=1024, and out_features=8192, this would require 1024 * 8192 = ~8 million operations. But in parallel across 8192 threads.

Wait, the number of threads would be equal to the number of features (8192). Each thread handles one feature.

Each thread would have to loop over B elements (1024 iterations). So 1024 iterations per thread.

This could be slow. Therefore, it's better to have more threads per feature.

Alternatively, use a 2D grid where each block handles a group of features, and each thread in the block handles a batch element.

For example:

Let the number of blocks be equal to the number of features (out_features). Each block is responsible for a single feature.

Within each block, the number of threads is equal to the batch size (1024). Each thread handles one batch element.

The block's threads can accumulate their sums into shared memory, then compute the total sum.

Wait, but the batch size is 1024, which is the maximum threads per block (if using 1024 threads per block). So:

Each block has 1024 threads, each thread corresponds to a batch index.

Each block is responsible for one feature (j).

Each thread (i) in the block computes y_linear[i][j], and they sum these across threads in the block.

Therefore, per block:

sum = sum over i of y_linear[i][j]

Then, after all blocks have computed their sums, divide by B to get the mean for each feature.

This would require 8192 blocks, each with 1024 threads. 

The total number of threads is 8192 * 1024 = ~8 million, which is manageable.

Similarly for variance.

This approach is feasible.

Therefore, the steps would be:

First, compute the linear layer (y_linear).

Then, compute the mean for each feature via a kernel.

Then, compute the variance via another kernel (or same as mean, but with squared values).

Then, compute the normalized tensor.

Then apply gamma, beta, scaling, and softmax.

However, all of these steps have to be implemented in CUDA kernels, and the backward pass must be handled.

This is getting quite involved.

Alternatively, fuse as much as possible into a single kernel to minimize kernel launches and memory copies.

Alternatively, proceed step by step.

First, let's outline the code structure.

The ModelNew class will replace the Linear, BN, scaling, and Softmax with a custom CUDA kernel.

The parameters (W, b, gamma, beta, scale) must remain as PyTorch parameters so that they can be optimized.

Therefore, in the ModelNew class, we'll have parameters:

- weight: from the Linear layer
- bias: from the Linear layer
- gamma (weight) and beta (bias) from the BatchNorm
- scale: the scaling parameter

These must be stored as nn.Parameters so that they are part of the model's parameters.

The custom kernel must take these parameters as inputs.

Now, for the forward pass:

We need to implement the entire computation in a custom CUDA kernel.

The custom kernel will:

1. Compute the linear layer (matrix multiplication + bias)

2. Compute the batch normalization (mean, var, normalize, gamma, beta)

3. Multiply by scale parameter

4. Apply softmax.

All in a single kernel? Or multiple kernels?

Let's first try to implement each part step by step.

First, the matrix multiplication.

Implementing the matrix multiplication (Gemm):

Input: x (B, in_features)

Weight: (out_features, in_features)

Bias: (out_features)

Output: y_linear (B, out_features)

The kernel for this would need to compute y_linear[i][j] = sum_{k} x[i][k] * weight[j][k] + bias[j]

Each thread can compute one element of y_linear.

Total elements: B * out_features = 1024 * 8192 = ~8.3 million elements. 

Each thread can handle one element.

Block size: 256 threads per block.

Number of blocks: ceil(8388608 / 256) = 32768 blocks. 

This may be a lot, but CUDA can handle it.

Alternatively, use a tiled approach with shared memory to coalesce memory accesses.

But for simplicity, let's proceed with the straightforward kernel.

Then, compute the mean and variance.

As discussed earlier, using a kernel that, for each feature j, computes the mean over batch.

Then, compute variance similarly.

Then, compute the normalized tensor.

Then, apply gamma, beta, scale, and softmax.

The Softmax requires computing the exponential of each element, then dividing by the sum over features.

The exponential and division can be done in a kernel.

Putting it all together, the code would have multiple kernel calls.

But the problem requires that the operations are implemented with custom CUDA kernels, so this is acceptable.

Now, handling the backward pass.

The custom kernel must be differentiable. To do this, we can use PyTorch's autograd by creating a custom autograd.Function that wraps the forward kernel and implements the backward kernel.

Therefore, the steps are:

1. Define a custom autograd.Function, e.g., FusedOp, which contains the forward and backward kernels.

2. In the forward method of the function, perform the computation steps (Gemm, BN, scaling, Softmax) using custom CUDA kernels.

3. In the backward method, compute the gradients with respect to the inputs and the parameters.

However, implementing the backward pass is quite involved, especially for the fused operations.

Alternatively, perhaps it's better to first write the forward pass correctly, then handle the backward.

But let's proceed.

First, the forward pass:

Implementing the forward kernels.

Let me write the code step by step.

First, the model class:

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):
        super().__init__()
        # Define parameters
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias = nn.Parameter(torch.empty(out_features))
        self.gamma = nn.Parameter(torch.empty(out_features))  # BatchNorm weight (gamma)
        self.beta = nn.Parameter(torch.empty(out_features))   # BatchNorm bias (beta)
        self.scale = nn.Parameter(torch.ones(scale_shape))
        # Initialize parameters
        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in)
        torch.nn.init.uniform_(self.bias, -bound, bound)
        torch.nn.init.ones_(self.gamma)
        torch.nn.init.zeros_(self.beta)
        # Compile the custom CUDA kernels here

    def forward(self, x):
        # Use the custom CUDA kernels here, passing the parameters
        # Return the output
        ...

The parameters are initialized similarly to the original model.

Next, the custom CUDA kernels.

The code must be written using load_inline, as in the example.

First, the forward pass:

The forward computation will involve several steps, each implemented in a CUDA kernel.

Alternatively, combine all steps into a single kernel.

Let me first outline the CUDA kernels needed.

Kernel 1: Compute the linear layer (Gemm).

Kernel 2: Compute batch normalization (mean, var, normalize, gamma, beta).

Kernel 3: Apply scaling and softmax.

Wait, but the order is:

After linear layer comes BN, then scaling, then softmax.

Alternatively, combine all steps into a single kernel for efficiency.

Let's try to write a fused kernel that does all steps.

The fused kernel would process each element of the input x (B, in_features), compute the linear layer output, then compute the mean/var across batch, normalize, apply gamma/beta, scale, then softmax.

But the mean and variance are computed across the batch, so they are per-feature.

Therefore, the kernel needs to first compute the linear layer, then compute the means and variances, then proceed.

However, this requires that the linear layer output is stored in memory, then the mean/var are computed from it.

Alternatively, the kernel can first compute the linear layer, compute the means and vars, then proceed.

But in CUDA, the kernel execution is synchronous, so the steps can be done in sequence.

Therefore, the forward pass would be:

1. Compute y_linear using a kernel.

2. Compute mean and var for each feature.

3. Normalize and apply gamma/beta.

4. Multiply by scale.

5. Apply softmax.

Each of these steps can be a separate kernel.

Now, implementing each step.

First, the linear layer kernel:

The kernel for the linear layer:

Each thread computes one element of the output y_linear.

The kernel would look like this:

__global__ void gemm_kernel(
    const float* __restrict__ input,  // (B, in_features)
    const float* __restrict__ weight, // (out_features, in_features)
    const float* __restrict__ bias,   // (out_features)
    float* __restrict__ output,       // (B, out_features)
    int B, int in_features, int out_features
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * out_features) return;
    int i = idx / out_features;  // batch index
    int j = idx % out_features;  // output feature index

    float sum = 0.0;
    for (int k = 0; k < in_features; ++k) {
        sum += input[i * in_features + k] * weight[j * in_features + k];
    }
    output[idx] = sum + bias[j];
}

This is a straightforward implementation, but may be slow for large matrices.

Alternatively, using shared memory for tiling.

But for simplicity, proceed with this.

Next, compute the mean and variance.

First, compute mean:

The mean is a vector of size out_features, where each element is the mean of the corresponding feature across the batch.

The kernel for computing the mean would need to compute for each feature j, sum over all samples i.

As discussed earlier, we can have a kernel where each block handles a feature j.

Each block has B threads (since B=1024).

Each thread i in the block computes input[i][j], then the block reduces the sum.

The kernel would look like this:

__global__ void compute_mean(
    const float* __restrict__ input, // (B, out_features)
    float* __restrict__ mean,        // (out_features)
    int B, int out_features
) {
    int j = blockIdx.x; // feature index
    if (j >= out_features) return;

    // Each thread in the block handles one sample
    int i = threadIdx.x;
    if (i >= B) return;

    float val = input[i * out_features + j];
    // Use shared memory for reduction
    extern __shared__ float sdata[];
    sdata[threadIdx.x] = val;
    __syncthreads();

    // Perform reduction in shared memory
    for (int s = blockDim.x/2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        mean[j] = sdata[0] / B;
    }
}

This kernel uses a block per feature, with B threads. The shared memory size is B floats. 

But for B=1024, this requires 4KB of shared memory per block (since 1024 * 4 bytes = 4KB), which is within the limits (max 48KB for many GPUs). However, with 8192 features, the number of blocks is 8192, which may be a lot. However, CUDA can handle it.

Similarly for variance:

The variance is computed as:

var[j] = (sum_{i} (input[i][j] - mean[j])^2 ) / B

So first compute the squared difference, then sum over samples.

The variance kernel would be similar to the mean kernel, but using (input[i][j] - mean[j]) squared.

So:

__global__ void compute_variance(
    const float* __restrict__ input,
    const float* __restrict__ mean,
    float* __restrict__ variance,
    int B, int out_features
) {
    int j = blockIdx.x;
    if (j >= out_features) return;

    int i = threadIdx.x;
    if (i >= B) return;

    float val = input[i * out_features + j] - mean[j];
    val *= val;

    extern __shared__ float sdata[];
    sdata[threadIdx.x] = val;
    __syncthreads();

    for (int s = blockDim.x/2; s > 0; s >>=1) {
        if (threadIdx.x < s) {
            sdata[threadIdx.x] += sdata[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        variance[j] = sdata[0] / B;
    }
}

Next, normalize the data:

The normalized output is (input - mean) / sqrt(var + eps) 

Then multiplied by gamma and added beta.

This can be done in a kernel:

__global__ void batch_norm(
    const float* __restrict__ input,
    const float* __restrict__ mean,
    const float* __restrict__ variance,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float eps,
    float* __restrict__ output,
    int B, int out_features
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * out_features) return;
    int i = idx / out_features;
    int j = idx % out_features;

    float inv_std = 1.0f / sqrt(variance[j] + eps);
    float val = (input[i * out_features + j] - mean[j]) * inv_std;
    val = val * gamma[j] + beta[j];
    output[idx] = val;
}

Then apply the scaling:

scale is a scalar (assuming scale_shape is (1,)), so multiply each element by scale.

__global__ void apply_scale(
    const float* __restrict__ input,
    const float scale,
    float* __restrict__ output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = input[idx] * scale;
    }
}

Finally, compute the softmax.

The softmax is exp(x) / sum(exp(x)), along the features (dim=1).

To compute this, we need to:

For each sample i:

1. Compute the max value of x[i] to prevent overflow.

2. Subtract max from each element.

3. Compute exp of each element.

4. Sum the exps along features.

5. Divide each element by the sum.

This can be done in a kernel.

First, compute the max for each sample.

__global__ void compute_max(
    const float* __restrict__ input,
    float* __restrict__ max_vals,
    int B, int out_features
) {
    int i = blockIdx.x;
    if (i >= B) return;

    float max_val = -FLT_MAX;
    for (int j = 0; j < out_features; ++j) {
        float val = input[i * out_features + j];
        if (val > max_val) {
            max_val = val;
        }
    }
    max_vals[i] = max_val;
}

Then, compute the exp and the sum:

__global__ void compute_softmax(
    const float* __restrict__ input,
    const float* __restrict__ max_vals,
    float* __restrict__ output,
    int B, int out_features
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * out_features) return;
    int i = idx / out_features;
    int j = idx % out_features;

    float max_val = max_vals[i];
    float numerator = exp(input[i * out_features + j] - max_val);

