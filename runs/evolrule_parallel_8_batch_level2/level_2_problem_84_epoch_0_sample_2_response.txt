Make sure you use the same variable names as the original code, and the forward function must have the same signature as the original. The only difference is that your implementation uses your custom CUDA kernels to replace some of the operators.

Additionally, use the same input and output tensor shapes as the original architecture.

You can assume that all inputs are on the same device (either CPU or CUDA). 

You can choose to replace any subset of the operators (Gemm, BatchNorm, scaling, softmax), or combine multiple operators into a single kernel for efficiency. 

Your goal is to achieve the fastest possible inference speed while maintaining correctness. 

If you choose to use CUDA, you can assume that the input tensors are already on the GPU. 

The given architecture uses PyTorch's nn.Linear, nn.BatchNorm1d, and nn.Softmax. You can replace any of these with custom CUDA kernels. You may also choose to keep some operators as is (e.g., if they are already fast enough). 

You can also choose to fuse multiple operators into a single kernel. For example, fusing the Gemm and BatchNorm into a single kernel could reduce memory bandwidth usage and kernel launch overhead.

Please optimize for CUDA. The input tensors are on CUDA devices.

Now, write the new architecture ModelNew with custom CUDA operators. Make sure the code is fully functional and compiles.

Okay, let's tackle optimizing the given Model architecture using custom CUDA kernels. The original model has four main steps: a linear layer (Gemm), batch normalization (BN), scaling, and softmax. My goal is to fuse as many of these operations into a single CUDA kernel to minimize memory transfers and kernel launch overhead.

First, I'll look at the individual operations:

1. **Gemm (Linear Layer)**: This is a matrix multiplication followed by an addition of a bias. The linear layer computes y = x * weight^T + bias.

2. **BatchNorm**: It normalizes the input by subtracting the mean and dividing by the standard deviation, then scales by gamma and adds beta. Since this is during inference, we can use the running mean and variance stored in the batch norm layer, so the computation becomes: (x - running_mean) / sqrt(running_var + eps) * gamma + beta.

3. **Scaling**: A simple element-wise multiplication by a scalar (the 'scale' parameter).

4. **Softmax**: Applies the softmax function to each row of the input tensor.

The key here is to fuse these steps into a single kernel. Let's see:

The Gemm, BN, scaling, and softmax can all be combined into one kernel. Let's outline the steps in sequence:

- Compute Gemm (matrix multiply and add bias)
- Apply batch norm (normalize, scale by gamma, add beta)
- Multiply by the scale parameter
- Apply softmax

However, the softmax can be tricky because it requires computing the exponential of each element and then normalizing. But perhaps we can combine all steps except softmax into one kernel, then handle softmax separately. Alternatively, maybe even include the softmax in the kernel if possible.

Wait, but softmax requires computing the exponential and the sum over the features. That might complicate things because each thread would need to compute partial sums. Maybe it's better to separate softmax for now. Alternatively, perhaps fuse everything except softmax into one kernel, then apply softmax with a separate optimized kernel.

Alternatively, let me see the order of operations again:

Gemm produces an output, which is then batch normalized, scaled, and then passed through softmax. Since the operations are all element-wise after the initial Gemm, perhaps we can combine the BN, scaling, and softmax into one kernel after the Gemm. Wait, but the Gemm itself is a matrix multiply, which is not element-wise.

So the main compute-heavy parts are the matrix multiply (Gemm) and the softmax. The batch norm and scaling are element-wise and could be fused with the other steps.

Alternatively, fuse the Gemm with the subsequent batch norm, scaling, and softmax into a single kernel.

But the matrix multiply is a large operation. Let's see:

The linear layer (Gemm) has a weight matrix of size (out_features, in_features). For each input sample (batch_size x in_features), we do a matrix multiply to get a batch_size x out_features tensor. This is O(batch_size * in_features * out_features) operations. Given that in_features and out_features are both 8192, that's 8k x 8k per sample, which is 64 million operations per sample, and with batch_size 1024, that's 65 billion operations. So this is the most compute-intensive part.

Batch norm is O(batch_size * out_features) operations. Scaling is the same, and softmax is O(batch_size * out_features) for the exponentials, plus a sum over features for each sample.

Thus, the Gemm is the dominant term. The other operations are smaller but can be fused.

Therefore, the best approach might be to combine the Gemm with the subsequent batch norm, scaling, and softmax into a single kernel. However, the matrix multiply is a different operation. To combine them, perhaps we can perform the matrix multiply, then immediately apply the batch norm, scaling, and softmax in the same kernel.

Alternatively, perhaps the batch norm can be fused into the Gemm's computation. Let's think:

The batch norm formula is:

y = (x - mean) / sqrt(var + eps) * gamma + beta

But during inference, mean and variance are fixed (running_mean and running_var). So substituting into the Gemm's output, suppose after the Gemm, the output is z = Wx + b. Then batch norm would be:

bn_out = (z - running_mean) / sqrt(running_var + eps) * gamma + beta

Then scaled by scale:

scaled = scale * bn_out

Then softmax:

softmax(scaled)

So if we can compute all of this in a single kernel after the matrix multiply, that could save memory transfers. However, the matrix multiply is a different operation. To fuse the matrix multiply with the subsequent steps, we can compute the matrix multiply first, then in the same kernel or another kernel, perform the element-wise operations.

Alternatively, perhaps split into two fused kernels:

1. A fused Gemm + batch norm + scaling, then a separate softmax kernel. But the softmax is still a separate step, but maybe we can optimize that.

Alternatively, the Gemm can be done with the batch norm parameters already incorporated, but that might not save much.

Wait, let's see:

The batch norm computation after the Gemm can be rewritten:

Let me denote the linear layer output as z = Wx + b.

Then, batch norm:

bn_z = (z - running_mean) / sqrt(running_var + eps) * gamma + beta

Scaling:

scaled_z = scale * bn_z

Then softmax:

softmax(scaled_z)

So the entire sequence can be written as:

softmax( scale * [ (Wx + b - running_mean) / sqrt(running_var + eps) ) * gamma + beta ] )

But combining all these into a single kernel might be possible, but the matrix multiply (Gemm) is a matrix operation, so it's necessary to first compute Wx + b, then apply the element-wise operations.

Alternatively, can the batch norm's parameters be incorporated into the weight and bias of the linear layer?

Wait, let's see:

Suppose after computing z = Wx + b,

the batch norm is:

bn_z = (z - running_mean) / sqrt(running_var + eps) * gamma + beta

Let me rearrange this:

Let's define:

denom = sqrt(running_var + eps)

bn_z = ( (Wx + b) - running_mean ) * (gamma / denom ) + ( beta - running_mean * gamma / denom )

So this can be written as:

bn_z = (W * x) * (gamma / denom) + ( (b - running_mean) * (gamma / denom) + beta )

Wait, no, let's re-express:

bn_z = (Wx + b - running_mean) * (gamma / denom) + beta

Wait:

(z - running_mean) / denom * gamma + beta = [ (Wx + b - running_mean) * gamma / denom ] + beta.

So that's equivalent to:

gamma/(denom) * (Wx + b) - gamma/(denom)*running_mean + beta

So this can be rewritten as:

gamma/(denom) * Wx + (gamma/(denom)*(b) - gamma/(denom)*running_mean + beta )

So, the entire expression can be written as:

bn_z = ( (gamma / denom) ) * Wx + [ (gamma/(denom)*(b - running_mean) ) + beta ]

So, this effectively becomes a linear layer with weight (gamma/denom)*W and bias [ (gamma/(denom)*(b - running_mean) ) + beta ]

Therefore, if we can precompute these terms, then the batch norm can be incorporated into the weight and bias of the linear layer. Then, the scaling and the subsequent steps can be added.

This would eliminate the need to compute the batch norm during inference. That's a big optimization!

Wait, that's a key insight. If we can precompute the adjusted weights and biases, then the batch norm can be folded into the linear layer's parameters. Then the entire computation becomes:

z = (W_new) * x + (b_new)

then scaled by scale, then softmax.

That would reduce the computational steps and eliminate the batch norm during inference.

Yes! This is a common optimization technique for batch normalization during inference. So instead of applying batch norm at runtime, we can precompute the adjusted weights and biases and eliminate the batch norm operation entirely.

So this reduces the computation steps to:

linear layer (with adjusted weights and bias) -> scale -> softmax.

That's a huge simplification.

So, first, let's precompute the batch norm's effect on the linear layer's parameters.

Let me formalize this:

Let:

- W be the original weight matrix of the linear layer (out_features x in_features)

- b is the bias vector (out_features)

- gamma (gamma parameter of batch norm, shape (out_features,)), beta (beta parameter, shape (out_features,))

- running_mean (shape (out_features,)), running_var (shape (out_features,)), and eps.

Then, after batch norm:

The output after batch norm is:

bn_output = ( (Wx + b) - running_mean ) / sqrt(running_var + eps) * gamma + beta

Which can be rewritten as:

bn_output = (gamma / sqrt(running_var + eps)) * (Wx + b - running_mean) + beta

Let me denote scale_factor = gamma / sqrt(running_var + eps)

Then,

bn_output = scale_factor * (Wx + (b - running_mean)) + beta

= scale_factor * Wx + scale_factor*(b - running_mean) + beta

Therefore, this can be represented as:

new_weight = scale_factor * W

new_bias = scale_factor * (b - running_mean) + beta

Therefore, the batch norm can be incorporated into the linear layer's parameters. Thus, during inference, we can precompute new_weight and new_bias and replace the linear layer with these, eliminating the batch norm computation.

This is a critical optimization. So the first step is to precompute the new weights and biases. Since in the given code, the batch norm is part of the model's parameters, during initialization, we can compute these new parameters and then proceed.

Therefore, the linear layer's parameters can be adjusted, and the batch norm is no longer needed. This reduces the computation to:

linear_layer (with new weights and bias) -> scaling -> softmax.

So that's two element-wise steps after the linear layer.

Now, the scaling step is multiplying by the 'scale' parameter (which is a tensor of shape (1, ) or something else? The code says scale_shape=(1,), so it's a scalar.

Wait, in the given code:

scale_shape = (1, )

So the scale is a scalar. So scaling is simply multiplying each element of the linear layer's output (after the batch norm is folded into the linear layer) by that scalar.

So now, the computation is:

z = linear(x) = new_weight * x + new_bias

then scaled_z = scale * z

then softmax(scaled_z)

So the main steps after the linear layer are scaling and softmax.

Now, the linear layer is the main compute part. The scaling and softmax are element-wise, so they can be fused into a single kernel with the linear layer's computation?

Wait, the linear layer (matrix multiply) is a different operation from the element-wise scaling and softmax. However, perhaps we can compute the matrix multiply and then immediately apply the scaling and softmax in the same kernel, avoiding a memory copy.

Alternatively, compute the matrix multiply, then perform the scaling and softmax in a second kernel. Let's see:

The linear layer's output is z = W_new x + b_new.

Then scaling is z * scale, then softmax over each row.

Alternatively, since the scaling is a scalar, we can compute the scaling and the softmax in a single kernel.

So the plan is:

1. Precompute the new linear layer parameters (weight and bias) by folding the batch norm.

2. Implement a fused kernel that computes the linear layer (matrix multiply plus bias), then applies scaling and softmax in the same kernel.

Wait, but the matrix multiply is a separate operation. Let's think about the kernel structure.

The matrix multiply (Gemm) is a large operation. The batch size is 1024, input features 8192, output features 8192.

The matrix multiply is O(1024 * 8192 * 8192) operations, which is about 68 billion operations. That's a lot, but the GPU can handle it.

The problem is that the matrix multiply is a dense computation. The element-wise operations after that are much cheaper, but we can combine them in the same kernel.

Wait, the matrix multiply produces an output tensor of size (1024, 8192). To compute the element-wise scaling and softmax, each element can be processed in parallel. So perhaps:

- Compute the matrix multiply (linear layer), but instead of storing the intermediate z = Wx + b, we can compute each element as part of the same kernel, and then immediately apply scaling and softmax.

Wait, no. The matrix multiply is a separate step. The linear layer's output is needed as an intermediate to compute the scaling and then the softmax. However, perhaps the matrix multiply can be structured such that after computing each row, we can apply the scaling and then compute the softmax in the same kernel.

Alternatively, let's think of the kernel as follows:

Each thread block processes a row of the output (a sample in the batch). For each row (output features of 8192 elements):

- Compute the matrix multiply for that row (each element is the dot product of input row and weight column)

Wait, but matrix multiply is a different operation. Let me think in terms of CUDA kernels.

The standard matrix multiply (GEMM) can be implemented in a tiled manner, with each thread block handling a tile of the output matrix. However, integrating the scaling and softmax into this kernel would be challenging because the scaling is straightforward, but the softmax requires computing the exponential of each element and then the sum over the row, which is a reduction operation.

Alternatively, after the matrix multiply, we can have a kernel that does:

For each row (sample):

1. Apply scaling: z_scaled[i][j] = scale * (W_new[i][j] * x + b_new[j])

Wait, no, scaling is applied to the entire z (the output of the linear layer).

Wait, scaling is scale * z, where z is the output of the linear layer. So scaling is element-wise.

Then, the softmax is applied per row, so for each row (sample), compute exponentials of each element, sum them, then divide each element by the sum.

The key point is that the scaling and softmax can be done in a single kernel that processes each row (sample) as a block.

So the plan would be:

- First, perform the matrix multiply (linear layer with new weights and bias). This is the most compute-heavy part.

- Then, apply scaling and compute the softmax in a second kernel.

Alternatively, if we can fuse the scaling and softmax into a single kernel that processes each row (each row is a sample), then that's better.

So let's outline the steps:

1. Precompute the new weights and biases by folding the batch norm parameters. This is done once during initialization.

2. The forward pass is:

   a. Compute z = new_weight * x + new_bias (matrix multiply + bias addition)

   b. Compute scaled_z = scale * z (element-wise multiplication)

   c. Compute softmax(scaled_z)

The matrix multiply is a large operation, but the scaling and softmax can be done efficiently in a separate kernel.

The problem is that step a (the matrix multiply) is the most expensive, but perhaps there's no way around it. However, we can try to optimize the matrix multiply itself.

Wait, PyTorch's nn.Linear already uses optimized CUDA kernels for the matrix multiply and bias addition, so replacing that with a custom kernel may not give much gain. Unless we can fuse the matrix multiply with the scaling and the softmax steps.

Alternatively, perhaps the matrix multiply is better left to PyTorch's implementation, and we focus on fusing the scaling and softmax.

Wait, let's think of the following: the matrix multiply is already as optimized as it can get via PyTorch's implementation. So perhaps the main gains can be had by fusing the scaling and softmax into a single kernel, which would reduce memory traffic.

Alternatively, maybe the batch norm folding can save us some parameters, and the scaling and softmax can be fused.

Let me proceed step by step.

First, implement the folding of the batch norm into the linear layer. This can be done once during initialization.

In the original code, the Model's __init__ has:

self.gemm = nn.Linear(in_features, out_features)

self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)

To fold the batch norm into the linear layer, during initialization, we can compute the new weight and bias for the linear layer.

So in the ModelNew's __init__:

self.gemm = nn.Linear(in_features, out_features)

Then, after initialization, we can compute the new parameters.

Wait, but in the original code, the batch norm's parameters (gamma, beta) and the running mean/var are already stored. So in the ModelNew's __init__:

We can compute the new weight and bias by:

gamma = self.bn.weight

beta = self.bn.bias

running_mean = self.bn.running_mean

running_var = self.bn.running_var

eps = self.bn.eps

scale_factor = gamma / torch.sqrt(running_var + eps)

new_weight = scale_factor.view(-1, 1) * self.gemm.weight

new_bias = scale_factor * (self.gemm.bias - running_mean) + beta

Then, replace the gemm's parameters with new_weight and new_bias.

Wait, but the self.gemm.bias might be None if the linear layer is initialized without a bias. Wait, in the original code, the Linear layer has a bias by default? The nn.Linear has a bias by default unless specified otherwise. So yes, assuming that the linear layer has a bias.

Wait, in PyTorch's Linear layer, the default is to have a bias. So yes.

Therefore, in the ModelNew's __init__:

We can compute the new weight and bias and set them into the gemm's parameters. This way, the batch norm is folded into the linear layer, eliminating that step.

Additionally, we can then remove the batch norm layer from the model, as it's no longer needed.

So the new model would have:

- A Linear layer (with adjusted weights and bias)

- A scaling parameter (scale)

- A softmax layer.

Now, the next step is to fuse the scaling and softmax into a single kernel.

The scaling is simply multiplying each element of the linear layer's output by the scale parameter (which is a scalar). Then the softmax is applied across each row.

The softmax function for a row is:

softmax(x_i) = exp(x_i) / sum(exp(x_j) for all j in row)

Thus, for each row, we need to compute the exponentials of all elements, sum them, and then divide each exponential by the sum.

The key here is to implement a kernel that, for each row, computes the scaled value (scale * z), then computes the exponential, keeps a running sum for the row, and then divides each element by the sum.

This can be done in a CUDA kernel where each thread block handles a row, and each thread in the block handles an element of the row.

The steps for the kernel would be:

1. For each element in the row:

   a. Compute scaled_value = scale * z[i]

   b. Compute exp(scaled_value)

   c. Accumulate the sum of all exp(scaled_value) in the row.

   d. Store the exp(scaled_value) in a temporary array.

2. Once the sum is known for the row, each thread computes the output as exp(scaled_value) / sum.

This can be done with a block per row, using shared memory to accumulate the sum.

Alternatively, using a single kernel with a block per row:

- Each thread in the block is responsible for a chunk of the elements.

- Compute the exponentials and partial sums, then use a block-wide reduction to get the total sum.

- Then, each thread computes the final value by dividing the exponential by the sum.

This approach can be implemented in CUDA.

Therefore, the plan is:

1. Precompute the new linear layer parameters by folding the batch norm (done once in __init__).

2. Compute the scaled linear output via PyTorch's Linear layer (since it's already optimized), then apply the scaling and softmax in a custom kernel.

Wait, but the scaling is just a scalar multiplication. So scaling can be done as:

scaled_z = self.scale * z

Then, the softmax is applied. However, the softmax can be done with a custom kernel that also handles the scaling to avoid an extra step.

Alternatively, the custom kernel can take the z (output of the linear layer), multiply by the scale, then compute the softmax.

Thus, the kernel can take the input tensor (z), the scale parameter, and compute the scaled softmax in one step.

Therefore, the steps in the forward pass would be:

x -> linear layer (with adjusted weights/bias) -> custom kernel (scale and softmax) -> output.

Now, implementing the custom kernel for scaling and softmax.

The kernel needs to handle a batch_size x out_features tensor. Each row is processed independently.

Let me outline the CUDA kernel structure.

First, the kernel will process each row (sample) as a block. Each block corresponds to a row. Each thread in the block processes a portion of the row's elements.

Parameters needed for the kernel:

- Input tensor (z) of shape (batch_size, out_features)

- Scale parameter (scalar)

- Output tensor (softmax result)

The kernel will:

For each row (block):

   1. Each thread loads a portion of the row's elements, applies scaling (multiply by scale), computes the exponential.

   2. The block reduces the sum of all exponentials in the row.

   3. Each thread then divides its exponential by the sum to get the softmax value.

This requires:

- Each thread in the block can process a number of elements. For example, with out_features = 8192, and a block size of 256, each thread would process 32 elements (8192 / 256 = 32.5, but we can round).

Wait, 8192 elements per row. Let's choose a block size of 256 threads. 8192 / 256 = 32. So each thread can handle 32 elements.

But in CUDA, the block size must be a power of two. 256 is a common choice.

The steps in the kernel:

__global__ void scale_softmax_kernel(const float* input, float scale, float* output, int batch_size, int features, const float* scale_param) {

   // Each block handles a row (sample)
   int row = blockIdx.x;
   if (row >= batch_size) return;

   // Each thread in the block processes a portion of the features
   int tid = threadIdx.x;

   __shared__ float shared_data[256]; // Or a larger array for partial sums?

   // Step 1: Compute exponentials and partial sums
   float sum = 0.0f;
   for (int i = tid; i < features; i += blockDim.x) {
       float val = input[row * features + i];
       float scaled = val * scale;
       float exp_val = expf(scaled);
       output[row * features + i] = exp_val; // temporary storage?
       sum += exp_val;
   }

   // Now perform a block-wide reduction to compute the total sum for this row
   // Using shared memory to accumulate partial sums
   __shared__ float partial_sums[256]; // Since blockDim.x is 256
   partial_sums[tid] = sum;
   __syncthreads();

   // Do a reduction in shared memory
   for (int s=blockDim.x/2; s>0; s>>=1) {
       if (tid < s) {
           partial_sums[tid] += partial_sums[tid + s];
       }
       __syncthreads();
   }

   float total_sum = partial_sums[0]; // The total sum is at tid 0

   // Now, each thread computes the result by dividing exp_val by total_sum
   for (int i = tid; i < features; i += blockDim.x) {
       float exp_val = output[row * features + i]; // fetched again?
       output[row * features + i] = exp_val / total_sum;
   }
}

Wait, but in this code, the output is written twice: first when storing exp_val, then again after dividing. That's okay, but we need to ensure that the first write is visible to the second read. Since the block is processing one row, and all threads in the block are handling the same row, the writes to the output array can be done in a way that is atomic? Or, perhaps it's better to first compute all exponentials into shared memory, then compute the sum, then write the results.

Alternatively, using shared memory to store the exponentials would be better, but for features=8192, that's too large for shared memory (since 8192 floats is about 32KB, and shared memory per block is limited, but 32KB is within the limit if using a 48KB block).

Wait, each block is processing one row. The number of threads in the block is 256, so each thread can process 32 elements (for 8192 elements). So the first loop:

Each thread computes 32 elements' exponentials and accumulates their sum into 'sum'.

The shared memory array can be used to store each thread's partial sum. Then, after reduction, the total sum is stored in partial_sums[0].

Then, each thread can read their original exp_val (but where?) Or perhaps the exp_vals are stored in shared memory as well.

Wait, maybe a better approach is:

1. Each thread computes its portion of the exponentials and stores them in shared memory, along with their sum.

Wait, let me rework the kernel.

First, each thread processes a chunk of elements in the row.

Let me see:

Each block has 256 threads, processing 8192 elements. Each thread processes 8192 / 256 = 32 elements.

The steps:

- Each thread loads its 32 elements from global memory, scales them, computes exponentials, and accumulates their sum.

- The sum is stored in a shared variable.

Wait, but the shared variable can be a single float per block, but each thread's partial sum needs to be added to the total.

Alternatively, use a parallel reduction approach.

Let me try this:

Shared memory can hold an array of size blockDim.x (256) to hold each thread's partial sum.

Then, after each thread computes their partial sum (summing over their 32 elements' exponentials), they write to the shared array.

Then, do a block reduction using shared memory.

Here's a better approach:

__global__ void scale_softmax_kernel(
    const float* input, 
    float scale, 
    float* output, 
    int batch_size, 
    int features) {

    int row = blockIdx.x;
    if (row >= batch_size) return;

    __shared__ float shared_sums[256]; // blockDim.x

    int tid = threadIdx.x;

    // Compute partial sums for this thread's portion
    float local_sum = 0.0f;
    for (int i = tid; i < features; i += blockDim.x) {
        float val = input[row * features + i];
        float scaled_val = val * scale;
        float exp_val = expf(scaled_val);
        local_sum += exp_val;
        // Store the exp_val in output for later use
        output[row * features + i] = exp_val;
    }

    // Write to shared memory
    shared_sums[tid] = local_sum;
    __syncthreads();

    // Now reduce the shared_sums to get total_sum
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sums[tid] += shared_sums[tid + s];
        }
        __syncthreads();
    }

    float total_sum = shared_sums[0];

    // Now compute the output values by dividing by total_sum
    for (int i = tid; i < features; i += blockDim.x) {
        int index = row * features + i;
        output[index] /= total_sum;
    }
}

This way, each thread first computes their portion of the exponentials and the partial sum, stores the exponentials in the output array (since they are needed again later), then after the reduction, each thread divides their exponentials by the total_sum and writes the result back to the output.

This should work.

Now, the kernel needs to be launched with one block per row (batch_size blocks), and each block has 256 threads (assuming blockDim.x = 256).

Now, in the ModelNew's forward function:

After the linear layer, we call this kernel.

But the linear layer's output is a tensor. So the steps are:

def forward(self, x):
    # Compute the linear layer (already has the folded BN)
    z = self.gemm(x)

    # Compute scale * z and apply softmax via custom kernel
    output = torch.empty_like(z).cuda() # Not sure, maybe use same device.

    # Call the kernel
    # Need to handle the parameters: scale is a scalar parameter of the model.

    # Get the scale parameter value
    scale_val = self.scale.item()  # Assuming self.scale is a scalar tensor.

    # Launch the kernel
    # Determine block and grid dimensions
    block_size = 256
    num_blocks = x.size(0)  # batch_size

    scale_softmax_kernel<<<num_blocks, block_size>>>(
        z.contiguous().data_ptr(),
        scale_val,
        output.data_ptr(),
        x.size(0), 
        z.size(1)
    )

    return output

Wait, but in PyTorch, you need to use the load_inline function to compile the CUDA kernel as before.

Therefore, the code structure would be:

First, define the CUDA source for the scale_softmax kernel.

Then, in the ModelNew class, after folding the batch norm parameters into the linear layer, implement the forward to call this kernel.

Additionally, we must ensure that the linear layer's parameters are properly set.

Putting this all together.

First, the folding of the batch norm parameters:

In ModelNew's __init__:

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):
        super(ModelNew, self).__init__()
        # Initialize the linear layer (Gemm)
        self.gemm = nn.Linear(in_features, out_features, bias=True)
        # Initialize the batch norm layer (to get its parameters for folding)
        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)
        # The scale parameter
        self.scale = nn.Parameter(torch.ones(scale_shape))

        # Precompute the folded weights and biases for the linear layer
        # After initialization, we need to compute the new weight and bias by incorporating the batch norm parameters.

        # Get the batch norm parameters
        gamma = self.bn.weight
        beta = self.bn.bias
        running_mean = self.bn.running_mean
        running_var = self.bn.running_var
        eps = self.bn.eps

        # Compute scale factor
        denom = torch.sqrt(running_var + eps)
        scale_factor = gamma / denom

        # New weight and bias for the linear layer
        new_weight = scale_factor.view(-1, 1) * self.gemm.weight
        new_bias = scale_factor * (self.gemm.bias - running_mean) + beta

        # Update the linear layer's parameters
        self.gemm.weight.data = new_weight
        self.gemm.bias.data = new_bias

        # We can now remove the batch norm layer, but since it's part of the module, we can just not use it in forward.

Wait, but in PyTorch, if we don't use the batch norm layer in the forward pass, it's okay, but we might need to ensure that the parameters are not part of the computation graph anymore. Alternatively, perhaps we can remove it from the model's parameters, but maybe it's not necessary. Since the batch norm's parameters are already used in the folded weights and biases, they can be left as is, but their gradients won't be used because we are not using the batch norm layer in forward.

However, during training, if this model is part of a larger network, this could cause issues, but since the problem states it's for inference, we can proceed.

Now, implementing the custom kernel for scale and softmax.

The CUDA code for the kernel:

scale_softmax_source = """
#include <torch/extension.h>
#include <math.h>
#include <cuda_runtime.h>

template <typename T>
__global__ void scale_softmax_kernel(
    const T* input,
    T scale,
    T* output,
    int batch_size,
    int features
) {
    int row = blockIdx.x;
    if (row >= batch_size) return;

    __shared__ T shared_sums[256]; // blockDim.x is 256

    int tid = threadIdx.x;

    T local_sum = 0;
    for (int i = tid; i < features; i += blockDim.x) {
        int idx = row * features + i;
        T val = input[idx];
        T scaled_val = val * scale;
        T exp_val = exp(scaled_val);
        local_sum += exp_val;
        output[idx] = exp_val; // Store intermediate exponentials
    }

    shared_sums[tid] = local_sum;
    __syncthreads();

    // Block reduction to compute total sum
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sums[tid] += shared_sums[tid + s];
        }
        __syncthreads();
    }

    T total_sum = shared_sums[0];

    // Divide each exp_val by total_sum
    for (int i = tid; i < features; i += blockDim.x) {
        int idx = row * features + i;
        output[idx] /= total_sum;
    }
}

// Define the function for CUDA execution
void scale_softmax_cuda(torch::Tensor input, torch::Tensor scale_param, torch::Tensor output) {
    float scale = scale_param.item<float>();
    int batch_size = input.size(0);
    int features = input.size(1);

    const int block_size = 256;
    const dim3 grid(batch_size);
    const dim3 block(block_size);

    // Launch the kernel with float specialization
    scale_softmax_kernel<float><<<grid, block>>>(
        input.data_ptr<float>(),
        scale,
        output.data_ptr<float>(),
        batch_size,
        features
    );
    cudaDeviceSynchronize(); // Ensure kernel completion
}
"""

Then, the corresponding header:

scale_softmax_cpp = """
void scale_softmax_cuda(torch::Tensor input, torch::Tensor scale_param, torch::Tensor output);
"""

Then, compile this kernel:

scale_softmax = load_inline(
    name="scale_softmax",
    cpp_sources=scale_softmax_cpp,
    cuda_sources=scale_softmax_source,
    functions=["scale_softmax_cuda"],
    verbose=True
)

Wait, but in PyTorch's load_inline, the functions parameter is a list of the names of the functions to be exposed. The kernel function is inside a template, but the exposed function is scale_softmax_cuda.

Now, in the forward function of ModelNew:

def forward(self, x):
    # Compute the linear layer (already includes folded batch norm)
    z = self.gemm(x)

    # Get the scale parameter (a scalar)
    scale = self.scale

    # Create output tensor
    output = torch.empty_like(z).cuda()  # Ensure it's on the same device as z

    # Call the custom CUDA kernel
    scale_softmax.scale_softmax_cuda(z.contiguous(), scale, output)

    return output

Wait, but the scale is a PyTorch tensor, and the kernel expects a scalar. So in the kernel function, we extract the scale as scale_param.item<float>().

This should work.

Putting all together, the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):
        super(ModelNew, self).__init__()
        # Initialize the linear layer (Gemm)
        self.gemm = nn.Linear(in_features, out_features, bias=True)
