The user has a powerful GPU with compute capability 8.0. The user wants maximum performance with minimal memory usage. Your code must use PyTorch extensions (e.g., torch.utils.cpp\_extension.load\_inline) to inline the CUDA kernels. You can choose to replace any subset of the operators with custom kernels (e.g., Conv2d, +, *, sigmoid, group norm). 

        When writing the kernels, the following tips are important: 

1. **Kernel Fusion**: Combine multiple operations into a single CUDA kernel. For example, the element-wise operations (add bias, multiply scale, apply sigmoid) can be fused into a single kernel to reduce memory traffic and kernel launch overhead. Similarly, the group norm operation can be fused with the sigmoid or other operations.
2. **Memory Efficiency**: Use in-place operations where possible to reduce memory usage. For example, perform the bias addition and scaling directly in the output tensor of the convolution.
3. **Algorithmic Optimizations**: The sigmoid function can be optimized using lookup tables (LUT) or polynomial approximations for faster computation with acceptable precision loss.
4. **Kernel Launch Overhead**: Minimize the number of CUDA kernel launches by fusing operations. Each kernel launch has overhead, so combining steps reduces this.
5. **Thread Block Configuration**: Use optimal thread block sizes (e.g., 256 or 512 threads per block) and grid sizes for best performance on the GPU architecture (compute capability 8.0).
6. **Shared Memory Usage**: For group norm, use shared memory to store the group statistics to avoid redundant calculations and reduce global memory access.
7. **Use Tensor Cores**: For convolution, ensure that the CUDA kernel is written to utilize Tensor Cores for FP16 operations if possible, but the problem requires FP32 precision unless otherwise specified. Wait, the original model uses torch.sigmoid which is in FP32. So we need to maintain FP32 unless user allows otherwise. The user didn’t specify, so assume FP32.
8. **Avoid Redundant Memory Allocations**: Pre-allocate memory where possible and reuse buffers between operations.
9. **Use Fast Math Intrinsics**: For mathematical functions like sigmoid, use CUDA’s fast math intrinsics (e.g., __expf) for speed if precision permits.
10. **Parallelize GroupNorm Calculations**: For group normalization, parallelize the computation of mean and variance across threads in a block, using shared memory for efficient reduction.

**Constraints**:

- All custom CUDA code must be inlined using `torch.utils.cpp_extension.load_inline`.
- The new ModelNew must have the same input/output behavior as the original Model.
- You may not use any third-party libraries or extensions beyond PyTorch and CUDA.
- All tensors must be on the GPU (cuda) and contiguous if necessary.
- The code must be compatible with PyTorch 1.13+.

Alright, let's tackle this problem. The goal is to optimize the given PyTorch model using custom CUDA kernels to maximize performance while minimizing memory usage. The original model consists of a convolution followed by element-wise operations (add bias, multiply scale, apply sigmoid), and then group normalization. 

First, I need to analyze which parts can be fused into a single kernel to reduce memory traffic and kernel launches. The element-wise operations (add, multiply, sigmoid) are good candidates for fusion since they can be done in-place after the convolution. Also, group normalization might be fused with some of these steps if possible.

Starting with the element-wise operations: Adding bias, scaling, and applying sigmoid can all be done in a single kernel. Let's see:

1. **Convolution**: The standard Conv2d operator can be left as is unless there's a way to fuse it with subsequent steps, but convolution is a complex operation and might not be straightforward to fuse with element-wise operations. So perhaps leave it as a separate step but ensure that the output is stored in a pre-allocated buffer to avoid extra allocations.

2. **Fusing Bias Add, Scaling, and Sigmoid**: These three operations are all element-wise and can be combined into a single kernel. This reduces the number of memory reads/writes and kernel launches. For example, the kernel can take the convolution output, add the bias, multiply by the scale, then apply the sigmoid function all in one pass.

3. **Group Normalization**: GroupNorm involves calculating mean and variance for each group, then normalizing. This can be done in parallel. However, fusing this with the sigmoid might not be beneficial because normalization typically requires the input before activation. Since the sigmoid is applied before group norm in the original model, they are sequential and can't be fused. Wait, looking back at the original model's forward:

Original forward sequence:
- Conv2d → Add Bias → Multiply Scale → Sigmoid → GroupNorm

Wait, no! Wait the original forward is:
x = self.conv(x)
x = x + self.bias
x = x * self.scale
x = torch.sigmoid(x)
x = self.group_norm(x)

So the order is: after the conv, add bias, multiply by scale, apply sigmoid, then group norm. 

Therefore, the sigmoid is applied before the group norm. So group norm must operate on the post-sigmoid tensor. Therefore, group norm can't be fused with the sigmoid, since it comes after. 

Hmm. So the sigmoid is part of the processing before group norm. Therefore, the element-wise operations (bias, scale, sigmoid) can be fused into a single kernel, and then the group norm is a separate step. 

Alternatively, perhaps the group norm can be optimized with shared memory and optimized reductions. Let's see.

First, let's focus on fusing the element-wise operations. Let's see:

The three operations are:

- x += bias (element-wise addition across channels)
- x *= scale (element-wise multiplication across channels)
- x = sigmoid(x)

These are all per-element operations, so a single kernel can do all three steps. 

So the plan is:

1. Run the convolution as usual (since it's a standard op and probably optimized already, but maybe we can pre-allocate the output to save memory)
2. Run a custom fused kernel for bias, scale, and sigmoid.
3. Run the group norm using a custom kernel optimized with shared memory for mean/variance calculation.

Alternatively, perhaps even fuse group norm into the same kernel as the element-wise steps, but that might complicate things because group norm requires per-group statistics. Let's see.

Wait, group norm is applied after the sigmoid. So after the sigmoid, group norm needs to compute for each group of channels the mean and variance. So perhaps the group norm can be implemented with a custom kernel that is optimized for performance, using shared memory for the reductions. 

So first, let's tackle the element-wise fusion.

**Element-wise Fusion Kernel:**

The fused kernel will take the convolution output, add the bias, multiply by the scale, and apply the sigmoid. The inputs are:

- Input tensor (convolution output)
- Bias tensor (shape (out_channels, 1, 1))
- Scale tensor (shape (out_channels, 1, 1))

The output is the same as the input (can be in-place?), but PyTorch might require a new tensor. Wait, but if the convolution output is in a pre-allocated buffer, maybe we can write directly to it. However, PyTorch's Conv2d returns a new tensor, so perhaps we need to handle that.

Wait, perhaps the convolution can be in-place, but PyTorch's Conv2d doesn't support in-place operations. So the convolution output is a new tensor. Then, the fused kernel can operate on that tensor, modifying it in-place? Or perhaps we can have the fused kernel write to the same memory. 

Alternatively, to save memory, perhaps the fused kernel can overwrite the convolution's output directly. Let me think:

Suppose the convolution is computed into a tensor 'conv_out', then the fused kernel can take 'conv_out', and perform the operations in-place, so that 'conv_out' is modified to hold the result after bias, scale, and sigmoid. That way, we avoid creating an intermediate tensor. So the steps would be:

x = self.conv(x) → this creates a new tensor (conv_out)
then, apply the fused kernel to conv_out, overwriting it → so after that, conv_out is the result after bias, scale, sigmoid.

Then, pass that to group norm. 

This would save memory, as the intermediate tensor is reused. 

Now, the fused kernel needs to:

For each element in the tensor:

result[i] = sigmoid( (conv_out[i] + bias) * scale )

Wait, but the bias and scale are per channel. So for each spatial location (height and width), the bias and scale are the same across the spatial dimensions, varying only per channel. 

So the bias has shape (out_channels, 1, 1), same with scale. So for a given channel c, and position (h,w), the value is:

(conv_out[c,h,w] + bias[c,0,0]) * scale[c,0,0]

Wait, no: The bias is added element-wise to each channel. Since the bias is (out_channels,1,1), it's broadcasted over the spatial dimensions. So the same bias value for each channel is added across all pixels in that channel. Same for scale.

So the operations can be written as:

for each element (c, h, w):

temp = x[c,h,w] + bias[c,0,0]

temp *= scale[c,0,0]

result[c,h,w] = 1/(1 + exp(-temp))

Wait, but implementing the sigmoid efficiently is important. The standard sigmoid is 1/(1 + exp(-x)), but perhaps using a faster approximation, like a polynomial or a lookup table. However, the user's original code uses torch.sigmoid, which is precise. The problem states to maintain the same behavior, so we can't lose precision. So perhaps use the fast math approximation but check if it's accurate enough. Alternatively, use the standard formula but with CUDA's fast math intrinsics. 

Wait, the user's requirement says "the new ModelNew must have the same input/output behavior as the original Model". So we can't use approximations unless they are bit-accurate. So perhaps better to use the standard formula. 

Alternatively, we can use the fast math intrinsics (__expf) but ensure that the results are correct enough. Hmm. Let's see: 

The sigmoid function can be written as:

sigmoid(x) = 1 / (1 + exp(-x))

In CUDA, using __expf(x) would be faster than the Python implementation. So replacing the Python's torch.sigmoid with a kernel that uses __expf would be faster and maintain precision.

So in the fused kernel, we can compute the sigmoid using __expf(-x) and then compute the division. 

Now, for the fused kernel's code:

We need to loop over all elements. To parallelize this, each thread can handle a single element. 

The input tensor is of shape (batch, out_channels, height, width). 

We can use a 3D grid and block structure, but for simplicity, flatten the indices. 

The kernel would:

__global__ void fused_ops(const float* input, const float* bias, const float* scale, float* output, int batch, int channels, int height, int width) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx >= batch * channels * height * width) return;

    // Compute indices
    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % channels;
    int b = idx / (channels * height * width);

    // Load bias and scale for channel c
    float b_val = bias[c];
    float s_val = scale[c];

    // Compute the value
    float temp = input[idx] + b_val;
    temp *= s_val;

    // Compute sigmoid
    float exp_neg_temp = __expf(-temp);
    output[idx] = 1.0f / (1.0f + exp_neg_temp);

}

Wait, but the input and output are the same tensor? Or is the output a separate tensor?

Actually, in the fused kernel, we can process the input and write back to the same memory. But need to ensure that the computation is correct. Since the operations are element-wise and sequential, overwriting the input is okay.

Wait, in the original code, the input to the fused steps is the output of the convolution. So the fused kernel can overwrite that tensor. So the output of the convolution is passed into the kernel, and the kernel modifies it in-place. 

Therefore, the kernel can write back to the input tensor. So input and output pointers can be the same.

So in the kernel code:

output[idx] = ... 

But in the code above, the input and output are separate pointers. To make it in-place, we can set output = input.

Alternatively, the kernel can be written to modify the input directly. 

Wait, but the kernel's signature must accept the input and output as separate tensors to allow in-place operation. Wait no, if the user wants to perform in-place, then the kernel can have input and output as the same tensor. 

So in code, the fused_ops kernel can take the input, and write to the same memory.

Therefore, the kernel can be written as:

__global__ void fused_ops(const float* input, const float* bias, const float* scale, int batch, int channels, int height, int width) {

    // ... same as above, but output is input:

    output = input; 

Wait, no, the kernel would need to write to the input. Wait, no, the pointers are passed. To overwrite the input, the output pointer must be the same as the input pointer. 

Wait, in the kernel function, if the output is the same as the input, then:

float* output = (float*)input; // same address

But in the kernel code, we can have:

output[idx] = ... 

But in the kernel's parameters, the output is a separate parameter. So in the function signature:

__global__ void fused_ops(const float* input, const float* bias, const float* scale, float* output, ...) {

    ... 

}

Then, when launching, we can set output = input to perform in-place.

Therefore, in the Python code, when calling the kernel, we can pass input and output as the same tensor.

So that's manageable. 

Now, the parameters to the kernel: batch, channels, height, width. These are the dimensions of the input tensor. 

The bias and scale tensors are of shape (channels, 1, 1). So in the kernel, the bias and scale are 1D arrays (since they are accessed via c), because we can store them as 1D arrays. 

Wait, in the kernel code, when we have:

bias is a 1D array (since it's (out_channels, 1, 1), so flattened, the first dimension is channels, and the rest are 1. So for any c, bias[c] gives the correct value.

Similarly for scale.

Therefore, when passing the bias and scale tensors to the kernel, they should be flattened to 1D arrays. 

Therefore, in the Python code, before launching the kernel, we need to ensure that the bias and scale tensors are contiguous and in the correct format. 

So in the elementwise kernel, the parameters would be:

input: the conv output tensor
bias: a 1D tensor of shape [out_channels]
scale: a 1D tensor of shape [out_channels]

Wait, but the original model has bias and scale as parameters with shape (out_channels, 1, 1). So to flatten them into 1D tensors, we can do:

bias_flat = bias.view(-1)
scale_flat = scale.view(-1)

Then pass those to the kernel.

This requires that in the kernel code, the bias and scale are 1D arrays.

Therefore, the fused_ops kernel can proceed as described.

Now, the kernel's thread and block configuration:

The total number of elements is batch * channels * height * width. 

We can calculate the grid and block size as before. Let's use a block size of 256 or 512, as per the tips.

For example:

block_size = 256

num_elements = batch * channels * height * width

num_blocks = (num_elements + block_size - 1) // block_size

This is standard.

Now, moving on to the group norm.

GroupNorm is a more complex operation. Let's think about how to optimize it.

The standard group norm requires:

For each group:

- Compute the mean and variance over the channels in the group, across the spatial dimensions and batch.

Wait, group norm computes the mean and variance for each group across the channels in that group, for each spatial position and batch element. Wait, no: the mean and variance are computed over the channels in the group, for each spatial location and batch instance. 

Wait, the formula for group norm:

Given a group g (split into num_groups groups), for each group, the mean μ and variance σ² are computed over the channels in the group, across the spatial dimensions (H and W) and across the batch.

Wait, actually, group norm's computation is:

For each group g, and for each position (h, w), compute the mean and variance over the channels in the group and over the batch?

Wait, no: the mean and variance are computed across the batch, spatial dimensions, and the channels in the group. 

Wait, the exact computation is:

For each group g (split into groups along the channel dimension):

For each sample in the batch, and each spatial location (h, w), compute the mean and variance over the channels in the group.

Wait, no, actually, the mean and variance are computed across all channels in the group, over all spatial positions and all samples in the batch. Wait no, the standard definition is that group norm normalizes each group of channels, considering all spatial locations and all samples. 

Wait according to PyTorch's documentation:

GroupNorm divides the channels into groups and computes within each group the mean and variance for normalization. 

The computation for each group is:

For each group, compute the mean and variance over the channels in the group, across the entire batch and spatial dimensions. 

Wait, actually, the mean and variance are computed over the (C/group) channels in the group, across the batch and spatial dimensions. 

Specifically, for each group:

mean = (1/(N * H * W * (C/groups))) * sum_{n, h, w, c_in_group} x_n,c,h,w

variance = (1/(N * H * W * (C/groups))) * sum_{n, h, w, c_in_group} (x_n,c,h,w - mean)^2

Therefore, the mean and variance are scalars per group, not per spatial location or per sample. 

Therefore, the group norm computation can be optimized by:

For each group:

- Compute the sum and sum of squares across all elements in the group (across all samples, channels in the group, and spatial dimensions)
- Compute mean and variance from these sums
- Normalize each element in the group using the mean and variance

This requires a reduction over all elements in the group, which can be parallelized using threads and shared memory.

The steps to compute group norm:

1. For each group:

   a. Compute sum and sum_squares over all elements in the group.

   b. Compute mean = sum / (total_elements_in_group)

      variance = sum_squares / total_elements_in_group - mean^2

   c. Compute the inverse of sqrt(variance + eps), where eps is a small value (PyTorch default is 1e-5).

   d. For each element in the group: (x - mean) * inv_std

2. Multiply by gamma and add beta (if affine). In the original problem, the group norm does not have affine parameters since it's initialized as GroupNorm(num_groups, out_channels), so gamma and beta are learnable. Wait, no, in PyTorch's GroupNorm, it has affine parameters by default. Wait the code says:

self.group_norm = nn.GroupNorm(num_groups, out_channels)

By default, PyTorch's GroupNorm has affine parameters (so they are included). Therefore, after computing the normalized tensor, we need to multiply by the gamma parameter and add beta. 

Wait in the original model's code:

The group norm is applied as:

x = self.group_norm(x)

Therefore, the group norm includes the affine transformation (gamma and beta) by default. So we need to include those in our custom kernel.

Therefore, the full group norm computation is:

normalized = (x - mean) / sqrt(variance + eps)

y = normalized * gamma + beta

Where gamma and beta are learnable parameters of the group norm layer, of shape [out_channels].

Therefore, the group norm kernel needs to handle these steps.

To implement this efficiently, we can structure the kernel as follows:

- Each thread block is responsible for a single group. 

- Within the block, threads can compute the sum and sum_squares for their portion of the group's data, then use shared memory to perform a reduction to get the total sum and sum_squares.

- Once the mean and variance are computed, each thread can compute the normalized value and apply the affine transformation.

This approach requires that the group's data can be processed in parallel by a block. Let's see:

Let’s suppose we have a group size (channels per group) = C / groups.

For each group:

- The group has channels Cg = C / groups

- The number of elements in the group is N (batch) * H * W * Cg

Each block processes one group. 

The block can have a number of threads, say 256, and split the data among them. 

Each thread would process a chunk of the data to compute partial sums. 

But for very large tensors (like the given problem has batch 128, H=256, W=256, out_channels=32, so groups=8 → channels per group=4), so per group, the elements are 128 * 256 * 256 * 4 = 33,554,432 elements per group. Wait, no, group is 32 /8 = 4 channels. So per group, the number of elements is batch * H * W * channels_per_group = 128 *256*256 *4 = 33,554,432 elements.

That's a lot, so a single block processing this would be inefficient. Alternatively, perhaps process multiple groups in a block, but that might complicate.

Alternatively, each block can process a single group, but with a large number of threads. However, the maximum block size is 1024 for compute capability 8.0, but even that might not be enough.

Alternatively, process the data in chunks using multiple blocks per group. Hmm, this is getting complicated. 

Alternatively, use a grid-stride loop approach where each thread processes multiple elements. 

Alternatively, since the elements per group are 33 million, which is manageable with shared memory for partial reductions. 

Wait, the problem requires to use as much shared memory as needed, but we have to be careful not to exceed the shared memory per block (which is 96KB for CC8.0, but in practice, each block can have up to 48KB or 96KB depending on configuration). 

The total size of the partial sums can be manageable. 

Let me think of the steps for the group norm kernel:

The kernel will process all elements in a group. Each block is assigned to a group. 

For a given group:

- Each thread in the block processes a chunk of the group's data. 

- Each thread computes the sum and sum_squares of its chunk.

- Use shared memory to accumulate the total sum and sum_squares across all threads in the block.

- Once the total sum and sum_squares are known, compute the mean and variance.

- Compute the inverse of sqrt(variance + eps).

- Then, each thread processes its chunk again to apply the normalization and affine transformation.

This approach requires two passes over the data: one for the reduction, and one for the normalization.

Alternatively, compute the mean and variance first, then apply the normalization in a second kernel pass, but that would require two kernel launches, which we want to avoid. 

Alternatively, compute the mean and variance in shared memory, then each thread can compute the normalized value on the fly. 

Here's the detailed plan:

Kernel Structure for GroupNorm:

Each block is responsible for a single group.

The block has T threads (e.g., 256 threads).

The total number of elements in the group is N = batch * height * width * channels_per_group.

Each thread processes N_threads = N / T elements (approximately).

Each thread loads its chunk of data from global memory, computes the sum and sum_squares of its chunk, stores partial sums in shared memory.

After reduction, compute mean and variance in shared memory.

Then, each thread again processes its chunk, loads the data, subtracts mean, multiplies by inv_std, and applies affine (gamma, beta), then writes back to global memory.

This approach requires two passes over the data, but since it's done in a single kernel, it might be more efficient than separate kernels.

Now, the code structure:

__global__ void group_norm_kernel(

    const float* input,
    const float* gamma,
    const float* beta,
    float* output,
    int batch,
    int channels,
    int height,
    int width,
    int num_groups,
    float eps
) {

    // Determine which group this block is processing
    int group_idx = blockIdx.x;
    if (group_idx >= num_groups) return;

    // Compute group's parameters
    int channels_per_group = channels / num_groups;
    int group_start = group_idx * channels_per_group;
    int group_end = group_start + channels_per_group;

    // Shared memory for partial sums and results
    extern __shared__ float shared[];

    // Allocate space in shared memory for partial sums and the final mean/var
    // Each thread will compute partial sum and sum_sq
    // Need 2 floats per thread for partial sums
    int tid = threadIdx.x;
    float* partial_sums = shared + tid * 2;

    // Initialize partial sums to zero
    partial_sums[0] = 0.0f;  // sum
    partial_sums[1] = 0.0f;  // sum_sq

    // Compute the total number of elements in the group
    int total_elements = batch * height * width * channels_per_group;

    // Each thread processes a chunk of the elements
    for (int i = tid; i < total_elements; i += blockDim.x) {
        // Compute the position in the input tensor
        // Need to map the element index to the tensor indices
        // Let's see: for group elements, the channels are from group_start to group_end-1

        // Compute the offset for this element in the group's data
        // The element's position can be decomposed as:

        int c_in_group = i % channels_per_group;  // channel within the group
        int spatial_offset = i / channels_per_group;

        // spatial_offset is batch * H * W + (spatial dimensions)

        // Let's compute batch, h, w:

        int w = spatial_offset % width;
        int h = (spatial_offset / width) % height;
        int b = spatial_offset / (width * height);

        // The actual channel is group_start + c_in_group

        // Compute the index in the input tensor:
        int input_idx = (
            b * channels + 
            (group_start + c_in_group) * height * width + 
            h * width + w
        );

        float val = input[input_idx];

        atomicAdd(&partial_sums[0], val);
        atomicAdd(&partial_sums[1], val * val);
    }

    // Synchronize to ensure all partial sums are accumulated
    __syncthreads();

    // Now perform reduction in shared memory to get total_sum and total_sum_sq
    // Use a parallel reduction here
    // For simplicity, let's use a warp-based reduction

    if (tid == 0) {
        float total_sum = 0.0f;
        float total_sum_sq = 0.0f;
        for (int i = 0; i < blockDim.x; i++) {
            total_sum += partial_sums[i*2];
            total_sum_sq += partial_sums[i*2+1];
        }
        shared[0] = total_sum;
        shared[1] = total_sum_sq;
    }
    __syncthreads();

    // Now compute mean and variance
    float total_sum = shared[0];
    float total_sum_sq = shared[1];

    float mean = total_sum / total_elements;
    float variance = (total_sum_sq / total_elements) - mean*mean;

    float inv_std = 1.0f / sqrt(variance + eps);

    // Now compute the normalized values and apply affine transformation
    // Again, each thread processes a chunk of elements

    for (int i = tid; i < total_elements; i += blockDim.x) {
        int c_in_group = i % channels_per_group;
        int spatial_offset = i / channels_per_group;

        int w = spatial_offset % width;
        int h = (spatial_offset / width) % height;
        int b = spatial_offset / (width * height);

        int c = group_start + c_in_group;

        int input_idx = (
            b * channels + c * height * width + 
            h * width + w
        );

        float val = input[input_idx];

        // Compute normalized value
        float normalized = (val - mean) * inv_std;
        // Apply affine transformation (gamma and beta)
        // Gamma and beta are per channel. Gamma and beta are of shape [channels]
        float gamma_val = gamma[c];
        float beta_val = beta[c];

        float out_val = normalized * gamma_val + beta_val;

        output[input_idx] = out_val;
    }
}

Wait, but the gamma and beta are parameters of the group norm layer. In the original model's code, the group norm layer's parameters (gamma and beta) are stored as nn.Parameter. Therefore, in the code, when we replace the group norm with a custom kernel, we need to access these parameters. 

Therefore, in the PyTorch model, the group norm's gamma and beta are stored in self.group_norm.weight and self.group_norm.bias. So when writing the kernel, we need to pass these as parameters. 

Therefore, in the kernel's parameters, we have:

gamma: pointer to the gamma tensor (shape [out_channels])

beta: pointer to the beta tensor (shape [out_channels])

Additionally, the kernel requires the number of groups, the total channels, batch, height, width, and epsilon (default 1e-5).

Now, the shared memory size needed:

The shared memory for the partial sums is 2 floats per thread, so for a block size of T threads, that's 2*T floats. 

Plus, the shared[0] and shared[1] for total_sum and total_sum_sq, which are already included in the partial_sums area if the first thread writes there. 

Wait in the code above, after the first loop, each thread has their partial sums in their part of shared memory. Then, if tid == 0, they accumulate all the partial sums from the threads. 

The total shared memory needed is:

Each thread needs 2 floats for partial sums → 2 * block_size floats.

Plus, the first thread will store the total_sum and total_sum_sq in shared[0] and shared[1]. So total shared memory is 2 * block_size + 2 floats.

The total bytes would be (2 * block_size + 2) * 4 bytes. 

Assuming a block size of 256, that's (2*256 +2)*4 = (512 +2)*4 = 2056 bytes. Well under the shared memory limit.

Now, the kernel's shared memory must be allocated dynamically. The user must specify the shared memory size when launching the kernel. 

The kernel is launched as:

int block_size = 256;
int num_groups = 8;

dim3 grid(num_groups);
dim3 block(block_size);

int shared_size = (block_size * 2 + 2) * sizeof(float);

group_norm_kernel<<<grid, block, shared_size>>>(...);

Wait, the shared_size should be computed as (block_size * 2) + 2 for the partial_sums and the totals. But actually, the first part is 2 per thread, so block_size * 2. The last two are stored in the first two elements of shared. So total is block_size * 2 + 2? Or is that overlapping?

Actually, in the code:

extern __shared__ float shared[];

The partial_sums array is allocated per thread as shared + tid*2.

Therefore, the total size needed for partial_sums is block_size * 2 floats.

Then, the first thread writes to shared[0] and shared[1], which are part of the allocated shared memory. 

Therefore, the total required shared memory is (block_size * 2 + 2) floats. 

Wait, no. The partial_sums are stored starting from each thread's tid*2, so for block_size threads, it needs block_size * 2 floats. Then the first thread is writing to the first two elements, which are already part of the allocated space. So the total needed is block_size * 2 floats. 

The first thread can write to shared[0] and shared[1] which are part of the first thread's partial_sums space? 

Wait no, because:

partial_sums = shared + tid*2

For tid=0, partial_sums starts at shared[0], so the first two elements are for tid=0's partial sums. 

The total_sum and total_sum_sq are stored in shared[0] and shared[1], which would overwrite the first thread's partial_sums. 

Wait, that's a mistake. Because in the code:

if (tid ==0):

    float total_sum = 0.0f;

    for (i=0 to blockDim.x-1):

        total_sum += partial_sums[i*2]

    shared[0] = total_sum;

Similarly for sum_sq.

But partial_sums[i*2] refers to the partial sums stored at shared[i*2], which for each thread's partial_sums. 

Wait, for thread i, the partial_sums are at shared[i*2], so the loop over i is correct. 

However, after that, when storing the totals in shared[0], that will overwrite the first thread's partial_sums. But since after the first loop, all threads have already read their partial sums into shared memory, so overwriting them is okay, since we don't need them anymore. 

Alternatively, the totals can be stored in a separate area. 

Alternatively, the total_sum and total_sum_sq can be stored in the first two locations of shared memory, outside the partial_sums area. 

But in any case, the total shared memory required is:

block_size * 2 (for partial_sums) + 2 (for total_sum and total_sum_sq) → total of (block_size *2 +2) floats. 

Therefore, the shared memory size must be (block_size *2 + 2) * sizeof(float).

Now, in code:

shared_size = (block_size * 2 + 2) * 4; // 4 bytes per float

Therefore, when launching the kernel, we have to calculate this.

Now, for the group norm kernel's parameters, we need to pass the following:

- input: the tensor to normalize (output of the sigmoid step)
- output: the result tensor (could be in-place with input?)
- gamma and beta: the parameters of the group norm layer
- batch, channels, height, width, num_groups, eps (default 1e-5)

Now, the group norm kernel can be written as above.

Now, putting this all together into the ModelNew class:

The steps in the forward pass are:

1. Run the convolution: x = self.conv(x)

2. Run the fused kernel on x to apply bias, scale, and sigmoid. This can be done in-place, overwriting x.

3. Run the group norm kernel on x, producing the output.

Now, the convolution is a standard PyTorch op, but to ensure memory efficiency, perhaps pre-allocate the output tensor for the convolution. However, PyTorch's Conv2d already manages memory efficiently, so maybe not necessary. 

Now, implementing this in Python with inline CUDA:

First, we need to define the fused kernel and the group norm kernel.

Let's start with the fused kernel:

Element-wise Fusion Kernel (Bias + Scale + Sigmoid):

The fused kernel code:

elementwise_fusion_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_ops_kernel(
    const float* input, 
    const float* bias, 
    const float* scale,
    float* output,
    int batch, 
    int channels, 
    int height, 
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels * height * width)
        return;

    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % channels;
    int b = idx / (channels * height * width);

    const float b_val = bias[c];
    const float s_val = scale[c];

    float temp = input[idx] + b_val;
    temp *= s_val;

    // Compute sigmoid using fast math
    float exp_neg_temp = __expf(-temp);
    output[idx] = 1.0f / (1.0f + exp_neg_temp);
}

torch::Tensor fused_ops_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    torch::Tensor scale
) {
    // Ensure input is contiguous
    auto input_