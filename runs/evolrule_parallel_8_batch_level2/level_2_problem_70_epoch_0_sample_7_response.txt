The code must be completely self-contained, and should not require any external files to be compiled. The code must be compatible with PyTorch 2.3.0, CUDA 12.1, and Python 3.10. 

The code must not use any libraries other than the ones already imported in the original code. You may import torch, torch.nn, torch.nn.functional, and torch.utils.cpp_extension, but nothing else.

You are free to use any PyTorch native functions and CUDA features. You can use any optimization techniques, including but not limited to: operator fusion (combining multiple operations into a single kernel), algorithmic optimizations, shared memory, thread coalescing, loop unrolling, etc. You can also use any CUDA kernel features like __syncthreads(), __ldg(), etc.

You can replace any number of operators in the original code with custom CUDA kernels. You may also choose to leave some operators as-is if you think that replacing them won't provide a significant speedup. 

Please make sure your code is correct (i.e., matches the output of the original code) and also as fast as possible.

I will compile and run your code to verify its correctness and performance. If your code does not work, your attempt will be considered incorrect. 

Now, write your optimized architecture with custom CUDA operators.




<think>

Okay, let's tackle this problem. The original model has a forward pass that does a linear layer (Gemm), applies a sigmoid activation, multiplies by a scaling factor, then adds the original input (residual add). The goal is to optimize this with custom CUDA kernels for speed.

First, I need to figure out which parts can be optimized. The linear layer (gemm) is already a PyTorch operation, but maybe it can be fused with subsequent steps. The sigmoid, scaling, and residual add are all element-wise operations that could be combined into a single kernel to reduce memory traffic and kernel launch overhead.

Looking at the code, the steps after the linear layer are:
1. Compute x = torch.sigmoid(x)
2. x = x * scaling_factor
3. x = x + original_x

These three steps can definitely be fused into one kernel. Since they all operate element-wise, a single kernel can handle all three operations in sequence per element. That would save three separate kernel launches and reduce memory copies.

So the plan is:
- Keep the gemm (linear layer) as is because it's a large matrix operation and PyTorch's implementation is already optimized. However, if we can fuse it with the next steps, that might help more, but fusing gemm (a matrix multiply) with element-wise operations might be tricky. Let's see.

Wait, the gemm is a matrix multiplication (input_size x hidden_size weights applied to the input). The subsequent steps are all element-wise. So fusing them into a single kernel after the gemm would still save a lot of time. The gemm itself is a big operation, but the element-wise steps are small and could be done in the same pass.

So the first step is to compute the gemm (linear), then immediately apply sigmoid, scaling, and residual add in a single kernel. Alternatively, maybe even combine the gemm's output computation with the residual add? Hmm, the residual add is adding the original output of the gemm (original_x) to the scaled sigmoid. So after computing the gemm result, we need to keep that value (original_x) and then process it through sigmoid and scaling, then add the original_x back.

Wait, the residual add is x = x (after sigmoid and scaling) + original_x. So original_x is the raw output of the gemm. So the steps are:

original_x = gemm(x)
x = sigmoid(gemm_output) * scaling_factor
x = x + original_x

So the element-wise operations after gemm can be done in one kernel. Let me think about how to implement that.

The custom kernel would take the gemm output, apply sigmoid, multiply by scaling, then add the original (so the same as the original gemm output). Wait, but the original is the same as the gemm's output, so the kernel can take the gemm output and process it in-place or with a temporary.

Alternatively, the kernel can take the gemm's output, compute the transformed version (sigmoid, scaling), then add the original in one step. So the formula for each element would be:

output[i] = (sigmoid(gemm_out[i]) * scaling_factor) + gemm_out[i]

But the original gemm_out is stored in a tensor. So the kernel needs to access both the gemm_out and process it. Since the gemm is a separate operation, we need to first compute gemm_out, then pass that tensor into the fused kernel. The fused kernel would read gemm_out, compute the transformed value, then add the original gemm_out to get the final result.

So the steps would be:

1. Compute gemm_out = self.gemm(x)
2. Compute final = fused_sigmoid_scale_add(gemm_out)

The fused kernel would process each element as follows:

element = gemm_out[i]
sigmoid_val = 1 / (1 + exp(-element))
scaled = sigmoid_val * scaling_factor
result = scaled + element

Thus, the kernel can do all these steps in a single pass.

This approach reduces three operations (sigmoid, scaling, add) into a single kernel, which is better for performance. The gemm itself is a big operation, but since it's a standard linear layer, PyTorch's implementation is probably already optimized, so replacing it with a custom kernel might not be worth it unless there's a specific optimization (like fusing with the next steps). But since the gemm's output is needed for the residual add, it's better to keep it as is and then process the rest in a fused kernel.

Now, writing the CUDA kernel for this fused operation. Let's see:

The kernel needs to take the input tensor (gemm_out), apply the sigmoid, scaling, and add the original value. The scaling factor is a constant.

The CUDA kernel would look something like this:

__global__ void fused_sigmoid_scale_add_kernel(
    const float* input,  // gemm_out
    float scaling_factor,
    float* output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx];
        float sigmoid_val = 1.0f / (1.0f + expf(-val));
        float scaled = sigmoid_val * scaling_factor;
        output[idx] = scaled + val;
    }
}

Wait, but this would require creating a new output tensor. Alternatively, can we write directly back to the input? Probably not, because the input is the gemm_out, and if we overwrite it, then the residual add would lose the original value. Wait, in the original code, the original_x is stored as a separate variable. So in the kernel, the output needs to be computed as (sigmoid(gemm_out) * scaling) + gemm_out.

Wait, the formula is:

x (after sigmoid and scaling) + original_x (which is gemm_out). So the total is (sigmoid(gemm_out)*scaling) + gemm_out. So the kernel can compute that in one step.

Therefore, the kernel can process each element as (sigmoid(in) * scaling) + in, where in is the input (gemm_out). Thus, the kernel doesn't need any intermediate storage except for the input and output.

Wait, but the input is gemm_out. So the kernel can take the input tensor and compute the transformed value and store in output. The output would be the final result.

Therefore, the kernel needs the input tensor (gemm_out), the scaling factor (a constant), and the output tensor. The size is the number of elements.

In code, in the forward pass:

def forward(self, x):
    gemm_out = self.gemm(x)
    # then call the fused kernel on gemm_out
    return self.fused_sigmoid_scale_add(gemm_out)

So the fused kernel would process gemm_out and produce the final result.

This approach combines the three steps into a single kernel, reducing overhead and memory accesses.

Now, to implement this in PyTorch with a custom CUDA kernel. Let's structure the code.

First, define the CUDA kernel code. The scaling factor is a parameter of the model, so in the kernel, it can be passed as a function parameter. Since it's a constant, we can pass it as a float to the kernel.

The kernel function would be:

#include <torch/extension.h>
#include <math.h>

__global__ void fused_sigmoid_scale_add_kernel(
    const float* input,
    float scaling_factor,
    float* output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx];
        float sigmoid_val = 1.0f / (1.0f + expf(-val));
        float scaled = sigmoid_val * scaling_factor;
        output[idx] = scaled + val;
    }
}

The wrapper function in C++ would need to take the input tensor and scaling factor, compute the output tensor, and launch the kernel.

The wrapper function:

torch::Tensor fused_sigmoid_scale_add(
    torch::Tensor input,
    float scaling_factor
) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    fused_sigmoid_scale_add_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        size
    );
    return output;
}

Wait, but in the model, the scaling_factor is a parameter. So in the model's __init__, we need to pass it to the kernel.

Wait, in the original model, the scaling_factor is a parameter passed during initialization. So in the new model, the scaling_factor is stored as an attribute, so in the kernel, we can access it from the model instance.

Therefore, when creating the fused kernel function, the scaling_factor must be passed as a parameter to the kernel function. Since in the model's forward, the scaling_factor is known (self.scaling_factor), we can pass it to the kernel.

Now, compiling this kernel using load_inline. The code would be similar to the example given. Let me structure the code.

First, define the CUDA source code as a string:

fused_sigmoid_scale_add_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_sigmoid_scale_add_kernel(
    const float* input,
    float scaling_factor,
    float* output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx];
        float sigmoid_val = 1.0f / (1.0f + expf(-val));
        float scaled = sigmoid_val * scaling_factor;
        output[idx] = scaled + val;
    }
}

torch::Tensor fused_sigmoid_scale_add(
    torch::Tensor input,
    float scaling_factor
) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    fused_sigmoid_scale_add_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        size
    );
    return output;
}
"""

Then, the CPP header:

fused_sigmoid_scale_add_cpp = (
    "torch::Tensor fused_sigmoid_scale_add(torch::Tensor input, float scaling_factor);"
)

Then, compile it with load_inline.

Wait, but in the example, they used load_inline with functions=["elementwise_add_cuda"], so here, the function is fused_sigmoid_scale_add, so functions=["fused_sigmoid_scale_add"]

Thus, in the code:

fused_sigmoid_scale_add = load_inline(
    name="fused_sigmoid_scale_add",
    cuda_sources=fused_sigmoid_scale_add_source,
    cpp_sources=fused_sigmoid_scale_add_cpp,
    functions=["fused_sigmoid_scale_add"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Wait, but the function is in the CUDA code, but the wrapper is in the .cpp part? Or does the CUDA code include both the kernel and the wrapper?

Wait, in the example, the CUDA source code includes both the kernel and the wrapper function. The cpp_sources are just the declarations. So in this case, the fused_sigmoid_scale_add function is implemented in the CUDA code. So the CUDA source has the wrapper, which is a CPU function that calls the CUDA kernel.

Therefore, the code should be okay.

Now, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scaling_factor):
        super().__init__()
        self.gemm = nn.Linear(input_size, hidden_size)
        self.scaling_factor = scaling_factor
        # Load the fused kernel
        self.fused_sigmoid_scale_add = fused_sigmoid_scale_add

    def forward(self, x):
        gemm_out = self.gemm(x)
        x = self.fused_sigmoid_scale_add.fused_sigmoid_scale_add(gemm_out, self.scaling_factor)
        return x

Wait, but the scaling_factor is a float stored in the model. So in the forward, we pass self.scaling_factor as the second argument to the fused kernel function.

This should work.

Now, checking for correctness:

The original code does:

x = self.gemm(x)
original_x = x
x = torch.sigmoid(x)
x = x * self.scaling_factor
x = x + original_x

The new code computes x as (sigmoid(gemm_out) * scaling) + gemm_out, which is the same as the original steps. So the output is correct.

Potential optimizations in the kernel:

The sigmoid function can be optimized. The current code uses expf(-val), which might be slow. Perhaps using an approximation or using a lookup table? But for simplicity, let's stick with the standard calculation for correctness first. Maybe there's a faster way, but unless needed, perhaps not.

Another thing: The input and output must be on the same device. Since the model is on CUDA (as the original get_inputs() uses .cuda()), the kernel must be launched on the GPU. The PyTorch tensors are on CUDA, so the data pointers are correct.

Another point: The gemm (linear layer) is a standard PyTorch operation which is already optimized, so keeping that as is is fine.

So putting it all together, the code would look like:

The full code with the fused kernel. Also, note that in the original code, the Model is initialized with input_size, hidden_size, scaling_factor. The new ModelNew must have the same __init__ parameters.

Now, let's write the full code:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernel for sigmoid, scaling, and residual add
fused_sigmoid_scale_add_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_sigmoid_scale_add_kernel(
    const float* input,
    float scaling_factor,
    float* output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx];
        float sigmoid_val = 1.0f / (1.0f + expf(-val));
        float scaled = sigmoid_val * scaling_factor;
        output[idx] = scaled + val;
    }
}

torch::Tensor fused_sigmoid_scale_add(
    torch::Tensor input,
    float scaling_factor
) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    fused_sigmoid_scale_add_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        size
    );
    return output;
}
"""

# The C++ header for the fused function
fused_sigmoid_scale_add_cpp = (
    "torch::Tensor fused_sigmoid_scale_add(torch::Tensor input, float scaling_factor);"
)

# Compile the fused kernel
fused_sigmoid_scale_add = load_inline(
    name="fused_sigmoid_scale_add",
    cuda_sources=fused_sigmoid_scale_add_source,
    cpp_sources=fused_sigmoid_scale_add_cpp,
    functions=["fused_sigmoid_scale_add"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scaling_factor):
        super().__init__()
        self.gemm = nn.Linear(input_size, hidden_size)
        self.scaling_factor = scaling_factor
        self.fused_sigmoid_scale_add = fused_sigmoid_scale_add  # Load the fused kernel

    def forward(self, x):
        gemm_out = self.gemm(x)
        x = self.fused_sigmoid_scale_add.fused_sigmoid_scale_add(gemm_out, self.scaling_factor)
        return x

# The input functions remain the same as in the original code
batch_size = 1024
input_size = 8192
hidden_size = 8192
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, input_size, device='cuda')]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]

Wait, but the original get_inputs() didn't have .cuda(), but in the problem statement, the user's example used .cuda(). However, in the given code, the original get_inputs() returns tensors on CPU. But in the problem's code, the get_inputs() is:

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]

Wait, but in the problem's code, the user might expect that the model runs on CPU? But since the original example used .cuda(), perhaps the code is intended for CUDA. Wait, the user's problem statement says that the inputs are generated based on the model's architecture, but in the example given, the get_inputs() used .cuda(). However, in the current problem's code, the original code's get_inputs() returns CPU tensors. Hmm, but the user might expect the model to run on GPU. Since the example used .cuda(), maybe the code should generate inputs on CUDA. Let me check.

The user's problem code for the Model has:

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]

But in the example given in the problem (the elementwise add example), the get_inputs() used .cuda(). So perhaps the user expects that the optimized code will be on CUDA. Therefore, in the ModelNew, the tensors should be on CUDA. So the get_inputs() in the new code should return CUDA tensors, but the problem says to keep the code as per the original, but in the problem's code's get_inputs(), it's on CPU. Wait, the user says "Your code must be compatible with PyTorch 2.3.0, CUDA 12.1, and Python 3.10." So likely, the model is supposed to run on CUDA.

Hence, in the code above, when defining get_inputs(), I added .cuda() to the tensor. But the original code didn't have that, so maybe the user expects the get_inputs() to stay as is. Wait, the problem says: "You may replace any number of operators in the original code with custom CUDA kernels. You may also choose to leave some operators as-is if you think that replacing them won't provide a significant speedup."

The get_inputs() is part of the code provided, so to stay compatible, perhaps we should leave get_inputs() as per the original, but the model would be on CUDA? Or maybe the user expects that the model's inputs are on CUDA. Hmm.

Wait, the problem's given code for the Model has the get_inputs() returning tensors on CPU, but the example code (the elementwise add) used .cuda() in get_inputs(). The user says that the code must be compatible with CUDA 12.1, so likely, the model is intended to run on GPU. So in the new code, the get_inputs() should return CUDA tensors, so adding .cuda() to the tensor creation.

Therefore, the code for get_inputs() should be:

def get_inputs():
    return [torch.rand(batch_size, input_size, device='cuda')]

But the original code's get_inputs() didn't have that. The user's problem says that the code must be completely self-contained and compatible with the given setup, so perhaps we should modify get_inputs() to generate CUDA tensors, as the original example did. Alternatively, maybe the original code's get_inputs() is on CPU, but the model is supposed to run on GPU. So in the ModelNew, the tensors would be moved to GPU automatically if the model is on GPU. Wait, but the model's parameters (like the Linear layer) are on CPU unless specified otherwise.

Hmm, this might be a problem. In the original code's Model, the Linear layer is created with default device (CPU). But in the new code, the Linear layer would also be on CPU unless moved. To have the model on GPU, we need to call model.cuda() or similar. However, the user's problem says that the code must be compatible with CUDA, so perhaps the code should explicitly move to GPU.

Wait, but the problem's instructions state that the code must be self-contained. So maybe in the new code, the Linear layer is initialized with device='cuda'? Or perhaps the code uses .cuda() in the get_inputs() function. Alternatively, the model's __init__ could set the device, but that's more complicated.

Alternatively, perhaps the user expects that the tensors are passed to the model, which is on CUDA. To ensure that, in the get_inputs() function, we can generate CUDA tensors. So I'll adjust the get_inputs() to use .cuda(). But since the problem says to replace the Model, not the get_inputs(), perhaps the user expects that the get_inputs() remains as in the original. But since in the example given by the user, the get_inputs() used .cuda(), maybe we should follow that.

Alternatively, maybe the code should not modify get_inputs() and get_init_inputs() at all, just replace the Model with ModelNew. The problem says to output the new architecture, so perhaps the get_inputs() and get_init_inputs() remain as in the original, but the code should work with CUDA.

Wait, the problem says to output the new code in codeblocks, so the full code should be provided. The user's original code for get_inputs() is:

def get_inputs():
    return [torch.rand(batch_size, input_size)]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]

But to run on CUDA, the inputs should be on CUDA. So perhaps the new code should modify get_inputs() to return CUDA tensors. The user's example did that, so probably it's okay.

Therefore, in the new code, the get_inputs() will have .cuda() added.

Thus, the full code would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernel for sigmoid, scaling, and residual add
fused_sigmoid_scale_add_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_sigmoid_scale_add_kernel(
    const float* input,
    float scaling_factor,
    float* output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx];
        float sigmoid_val = 1.0f / (1.0f + expf(-val));
        float scaled = sigmoid_val * scaling_factor;
        output[idx] = scaled + val;
    }
}

torch::Tensor fused_sigmoid_scale_add(
    torch::Tensor input,
    float scaling_factor
) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    fused_sigmoid_scale_add_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        size
    );
    return output;
}
"""

# The C++ header for the fused function
fused_sigmoid_scale_add_cpp = (
    "torch::Tensor fused_sigmoid_scale_add(torch::Tensor input, float scaling_factor);"
)

# Compile the fused kernel
fused_sigmoid_scale_add = load_inline(
    name="fused_sigmoid_scale_add",
    cuda_sources=fused_sigmoid_scale_add_source,
    cpp_sources=fused_sigmoid_scale_add_cpp,
    functions=["fused_sigmoid_scale_add"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scaling_factor):
        super().__init__()
        self.gemm = nn.Linear(input_size, hidden_size)
        self.scaling_factor = scaling_factor
        self.fused_sigmoid_scale_add = fused_sigmoid_scale_add  # Load the fused kernel

    def forward(self, x):
        gemm_out = self.gemm(x)
        x = self.fused_sigmoid_scale_add.fused_sigmoid_scale_add(gemm_out, self.scaling_factor)
        return x

# Maintaining the original get_inputs and get_init_inputs functions
batch_size = 1024
input_size = 8192
hidden_size = 8192
scaling_factor = 2.0

def get_inputs():
    # Generate inputs on CUDA as in the example
    return [torch.rand(batch_size, input_size, device='cuda')]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]

Wait, but in the original problem's code, get_init_inputs() returns the parameters needed to initialize the model, which are input_size, hidden_size, scaling_factor. The new ModelNew's __init__ takes those parameters, so this is okay.

Now, checking for correctness:

When the model is initialized with input_size, etc., and the forward pass uses the gemm (linear layer), which is a dense matrix multiply, followed by the fused kernel.

The kernel's output is the same as the original code's steps, so the result should be the same.

Another possible optimization is to see if the gemm can be fused with the fused kernel. But the gemm is a matrix multiply and the fused kernel is element-wise, so it's not straightforward. The gemm's output is a matrix, and the fused kernel operates on each element. Since the gemm is a standard operation, and PyTorch's implementation is already optimized (using cuBLAS), it's better to leave it as is.

Another thing to check is the kernel's block and grid size. The block_size is set to 256, which is a common choice. The grid size is computed as (size + block_size -1)/block_size, which is correct.

Potential improvements for the kernel:

- Using shared memory to cache the input elements, but since the input is read once and the computation is per-element, it might not help.
- Using warp-level parallelism or unrolling loops, but the kernel is already per-element.
- The sigmoid function can be optimized. For example, using a polynomial approximation of the sigmoid function could be faster but less accurate. However, the problem requires correctness, so we can't compromise on accuracy here.
- Using __ldg() if the input is in constant memory, but that might not be applicable here.

Alternatively, the kernel could be vectorized using CUDA's built-in types like float4, but that might complicate things.

Another possible optimization: the fused_sigmoid_scale_add function could be in-place if possible, but the output needs to be the result of (sigmoid(input)*scaling) + input. Since this is an addition of the original input, the output can't be stored in-place into the input tensor because that would overwrite the original input before the addition. So an output tensor is necessary.

Therefore, the current approach is correct.

Testing for correctness:

Suppose input is a tensor. The gemm produces gemm_out. The fused kernel computes:

output[i] = (sigmoid(gemm_out[i]) * scaling) + gemm_out[i]

Which is exactly what the original code does:

x = torch.sigmoid(gemm_out)
x = x * scaling
x = x + gemm_out

So the output is the same. Thus, the code is correct.

Performance-wise, this should be faster because three operations (sigmoid, scaling, add) are done in a single kernel launch instead of three separate ones. Also, memory accesses are optimized as they are done once.

Therefore, this should be a valid and effective optimization.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernel for sigmoid, scaling, and residual add
fused_sigmoid_scale_add_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_sigmoid_scale_add_kernel(
    const float* input,
    float scaling_factor,
    float* output,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = input[idx];
        float sigmoid_val = 1.0f / (1.0f + expf(-val));
        float scaled = sigmoid_val * scaling_factor;
        output[idx] = scaled + val;
    }
}

torch::Tensor fused_sigmoid_scale_add(
    torch::Tensor input,
    float scaling_factor
) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    fused_sigmoid_scale_add_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        scaling_factor,
        output.data_ptr<float>(),
        size
    );
    return output;
}
"""

# The C++ header for the fused function
fused_sigmoid_scale_add_cpp = (
    "torch::Tensor fused_sigmoid_scale_add(torch::Tensor input, float scaling_factor);"
)

# Compile the fused kernel
fused_sigmoid_scale_add = load_inline(
    name="fused_sigmoid_scale_add",
    cuda_sources=fused_sigmoid_scale_add_source,
    cpp_sources=fused_sigmoid_scale_add_cpp,
    functions=["fused_sigmoid_scale_add"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scaling_factor):
        super().__init__()
        self.gemm = nn.Linear(input_size, hidden_size)
        self.scaling_factor = scaling_factor
        self.fused_sigmoid_scale_add = fused_sigmoid_scale_add  # Load the fused kernel

    def forward(self, x):
        gemm_out = self.gemm(x)
        x = self.fused_sigmoid_scale_add.fused_sigmoid_scale_add(gemm_out, self.scaling_factor)
        return x

# Maintaining the original get_inputs and get_init_inputs functions
batch_size = 1024
input_size = 8192
hidden_size = 8192
scaling_factor = 2.0

def get_inputs():
    # Generate inputs on CUDA
    return [torch.rand(batch_size, input_size, device='cuda')]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]
```