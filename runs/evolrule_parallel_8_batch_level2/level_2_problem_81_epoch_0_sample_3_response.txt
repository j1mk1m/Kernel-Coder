You can write multiple CUDA kernels. You can also fuse operators in the forward pass. For instance, the matmul and swish can be fused into a single kernel. 

Make sure to import all necessary modules. The output should be a single Python file with all necessary code. Your kernels can use any PyTorch extensions (e.g., ATen). 

Your optimized model should have a function get_init_inputs() that returns the same parameters as the original (for model initialization). 

The output code must run on PyTorch 2.3.0. 

Also, ensure that the get_inputs() function in your new code is the same as the original.

The original code uses torch.sigmoid and torch.tanh and torch.clamp operators. Your custom CUDA kernels must be faster than the PyTorch defaults.



Okay, let's see. The user wants me to optimize the given PyTorch model by replacing some operators with custom CUDA kernels. The original model has a linear layer followed by Swish, divide, clamp, tanh, and another clamp. The goal is to make this faster using custom CUDA code.

First, I need to look at which operations can be fused or optimized. The Swish activation is x * sigmoid(x), which could be fused with the gemm (matrix multiplication) output. The subsequent operations like division, clamping, tanh, and another clamp might also be candidates for fusion into a single kernel to reduce memory transfers and kernel launches.

Let me start by considering fusing the gemm and Swish. The linear layer's forward is a matrix multiply plus bias (if present). Since the model's bias is set to True, the gemm includes the bias addition. But when fusing with Swish, the bias is added first before applying the activation. So the kernel would need to handle the matrix multiply, add the bias, then compute x * sigmoid(x) all in one step.

Then after Swish, there's division by 2, two clamps, tanh, and another clamp. Maybe these can be combined into another kernel. Let's see: after Swish, the steps are:

1. x / 2.0
2. clamp between -1 and 1
3. tanh(x)
4. clamp between -1 and 1 again

These can be done sequentially in a single kernel. So maybe two fused kernels: one for gemm+swish, and another for the post-processing steps (divide, clamps, tanh, clamps).

Alternatively, maybe all steps can be fused into a single kernel after the gemm. Let's think:

The entire forward pass after the gemm is:

x = gemm_out (after bias)
x = x * sigmoid(x)  # Swish
x = x / 2
x = clamp(x, -1, 1)
x = tanh(x)
x = clamp(x, -1, 1)

So if we can do all of that in a single kernel after the gemm, that would be better. But first, the gemm itself is a matrix multiply with bias. The matrix multiply is a big operation, so maybe fusing that with the activation and subsequent steps might not be efficient, because the gemm is a large computation. Alternatively, the gemm is already a PyTorch operator, perhaps it's optimized enough, but the following steps could be fused.

Wait, the user's example replaced a simple add operation with a custom kernel, but in this case, maybe the Swish is a good candidate. Let's see: the Swish is x * sigmoid(x). The sigmoid is 1/(1+exp(-x)), which is an element-wise operation, so combining that with the element-wise multiplication (x * sigmoid(x)) into a single kernel would save some computation steps.

So first, let's consider fusing the Swish activation into the gemm's output. The gemm's output is already computed, so perhaps the kernel can take the result of the gemm, apply Swish, then proceed with the rest.

Alternatively, if the gemm is part of the nn.Linear layer, which uses PyTorch's optimized matmul, maybe it's better not to replace that, but instead focus on fusing the activation and the subsequent steps.

Alternatively, perhaps the entire sequence after the gemm can be done in a single kernel. Let's outline the steps again:

After gemm (which is x = linear(x)):

1. Swish: x = x * sigmoid(x)
2. Divide by 2
3. Clamp to [-1, 1]
4. Tanh
5. Clamp again to [-1,1]

These are all element-wise operations, so they can be done in a single kernel. That might be a good approach because combining them reduces the number of kernel launches and memory accesses. Let me think about the steps:

The kernel would take the input (the output of the gemm), and then perform all these operations in sequence for each element. So for each element:

y = x * sigmoid(x)  # Swish
y = y / 2.0
y = clamp(y, -1, 1)
y = tanh(y)
y = clamp(y, -1, 1)

But implementing this in CUDA requires writing the element-wise operations efficiently. Let's see if that's feasible.

Alternatively, maybe the Swish and the divide can be combined. Since Swish is x * 1/(1 + exp(-x)), then divided by 2, so that's (x / 2) * sigmoid(x). Not sure if that helps, but the division can be incorporated into the computation.

Now, the clamps before and after tanh. Since tanh already outputs between -1 and 1, the second clamp might be redundant. Wait, the first clamp after division is between -1 and 1, then tanh is applied. Since tanh's output is between -1 and 1, the second clamp is redundant. But the user's code includes it, so perhaps the intention is to enforce the clamp after tanh. Maybe there's a reason, so we must include it as per the original code.

Wait, let me check:

The first clamp after dividing by 2. Let's say after dividing by 2, x could still be beyond [-1,1]? Let's see. Suppose the Swish output before dividing by 2 is up to some value. The Swish activation can have outputs up to infinity, but after dividing by 2, it's possible that some elements could be larger than 1 or smaller than -1. So the first clamp is necessary to bring it into [-1,1]. Then tanh is applied, which will map that to between -1 and 1 again. Then the second clamp is again between -1 and 1, which would be redundant, but perhaps the original code wants to ensure it. So we need to keep it.

Therefore, the sequence is important, so the kernel must process all steps in order.

So, the plan is to write a CUDA kernel that takes the gemm output and applies all these element-wise operations in a single pass. Let's outline that:

The kernel would process each element as follows:

float x_in = input[i]
float swish = x_in * (1.0 / (1.0 + exp(-x_in)))
float after_div = swish / 2.0
float clamped1 = min(max(after_div, -1.0), 1.0)
float tanh_out = tanh(clamped1)
float final = min(max(tanh_out, -1.0), 1.0)
output[i] = final

Wait, but exp is an expensive function. However, in CUDA, we can use the __expf intrinsic for faster computation. Also, the sigmoid can be computed as 1/(1+exp(-x)), so the kernel would need to compute that.

Alternatively, maybe we can approximate some functions, but the user requires that the kernels are faster than PyTorch's default. Since PyTorch's default implementations are optimized, especially for element-wise operations, perhaps fusing them into a single kernel can save the overhead of multiple kernel launches, which might be more efficient overall even if each computation is similar.

Alternatively, maybe some of these steps can be optimized. For instance, the tanh is applied after a clamp to [-1,1], so the tanh of a value between -1 and 1 is within the same range. The second clamp is redundant but must be kept as per the original code. However, in the code, the first clamp is after dividing by 2, so maybe it's necessary.

Another thing to note is that the original code uses torch.sigmoid, which is an element-wise operation. Similarly, tanh is element-wise. So combining all of these into a single kernel would eliminate the need for multiple kernel launches, which can save time, especially for small tensors.

Wait, the input dimensions here are batch_size=1024 and in_features=8192. So the gemm output is (1024, 8192). That's a large tensor (around 8 million elements). So even element-wise operations over this can take time. So fusing all the steps into a single kernel could be beneficial.

Therefore, I'll proceed to create a single kernel that takes the output of the gemm and applies all the subsequent operations in one pass.

Now, the steps to implement this:

1. Create a CUDA kernel that processes each element of the input tensor (the gemm output) and applies all the steps.

The code for the kernel would look something like this:

__global__ void fused_activation_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= size) return;
    
    float x = input[idx];
    
    // Swish: x * sigmoid(x)
    float sigmoid_x = 1.0f / (1.0f + expf(-x));
    float swish = x * sigmoid_x;
    
    // Divide by 2
    float after_div = swish / 2.0f;
    
    // Clamp between -1 and 1
    float clamped1 = fminf(fmaxf(after_div, -1.0f), 1.0f);
    
    // Tanh
    float tanh_out = tanhf(clamped1);
    
    // Clamp again between -1 and 1
    output[idx] = fminf(fmaxf(tanh_out, -1.0f), 1.0f);
}

Then, wrap this in a C++ function that calls the kernel.

But before that, we need to handle the gemm. The original model uses a nn.Linear layer, which is a matrix multiplication plus a bias. The Linear layer's forward is already optimized in PyTorch, so replacing that with a custom kernel may not be necessary unless we can fuse it with the Swish. However, the gemm (matrix multiply) is a large operation, and fusing it with element-wise operations might not be worth it. The matrix multiply is O(N^3), while the element-wise steps are O(N). So for large matrices, the matrix multiply is the dominant term, so maybe it's better to leave it as is, and only optimize the element-wise steps.

Therefore, the plan is:

- Keep the Linear layer as is (so the gemm is handled by PyTorch's optimized implementation).
- Create a custom CUDA kernel that fuses Swish, division, clamps, tanh, and final clamp.

This way, the gemm remains fast, and the rest of the operations are done in a single kernel, reducing overhead.

Now, in the model, the forward function would be:

x = self.gemm(x)  # This is the linear layer
x = self.fused_activation(x)

So the custom kernel takes the output of the linear layer and applies all the steps.

Therefore, the code structure would involve:

- Writing the CUDA kernel for the fused operations.
- Compiling it using load_inline in PyTorch.
- Modifying the ModelNew class to use this kernel.

Now, let's think about the kernel implementation in detail.

The kernel function:

The input tensor is the output from the linear layer (gemm), which is a 2D tensor of shape (batch_size, out_features). We need to process each element. The CUDA kernel should loop over each element.

In the kernel code, the size is the number of elements (batch_size * out_features).

The functions used here are expf, tanhf, fmaxf, fminf. CUDA has these functions available.

Now, the Python wrapper function would take a Tensor input and return the output Tensor.

Next, the code for the CUDA kernel and the wrapper.

Let me write the CUDA code as a string for load_inline.

elementwise_fused_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_activation_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        
        // Swish: x * sigmoid(x)
        float sigmoid_x = 1.0f / (1.0f + expf(-x));
        float swish = x * sigmoid_x;
        
        // Divide by 2
        float after_div = swish / 2.0f;
        
        // Clamp between -1 and 1
        float clamped1 = fminf(fmaxf(after_div, -1.0f), 1.0f);
        
        // Tanh
        float tanh_out = tanhf(clamped1);
        
        // Clamp again between -1 and 1
        output[idx] = fminf(fmaxf(tanh_out, -1.0f), 1.0f);
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    fused_activation_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), output.data_ptr<float>(), size
    );
    
    return output;
}
"""

Then the header:

elementwise_fused_cpp_source = "torch::Tensor fused_activation_cuda(torch::Tensor input);"

We can then load this using load_inline.

In the ModelNew class, we would have:

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.fused_activation = fused_activation  # the loaded module

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_activation.fused_activation_cuda(x)
        return x

Wait, but the fused_activation_cuda function is the one that wraps the kernel. The loaded module's function is called with the input tensor.

Now, the original get_init_inputs function returns [in_features, out_features], so the ModelNew must have the same parameters in __init__. The original code's Model uses in_features and out_features in __init__.

Thus, the ModelNew's __init__ must take those parameters and pass them to the Linear layer, same as the original.

Now, check if this code would compile. The CUDA code uses standard functions, and the load_inline should work.

Another thing to consider: the sigmoid computation could be optimized. For example, in CUDA, there might be faster approximations, but the user requires that the kernels are faster than PyTorch's default. The standard PyTorch sigmoid is already optimized, so replacing it with the same computation in a fused kernel may not be faster unless the fused kernel reduces overhead.

Alternatively, the PyTorch's sigmoid is implemented in a way that may be as fast as this kernel. However, by fusing all steps into a single kernel, we avoid multiple kernel launches. Let's think: the original code has:

x = x * torch.sigmoid(x) --> two operations (sigmoid and multiply)
then divide, two clamps, tanh, another clamp.

Each of these is a separate kernel launch (or at least, PyTorch might optimize them, but for small tensors, the overhead can add up). For large tensors like 8192 elements, perhaps the overhead is negligible, but for 1024x8192 elements, the element count is 8 million, so processing all in one kernel should be efficient.

Therefore, this approach should be faster.

Another point: The original code uses torch.sigmoid which is an element-wise op. Similarly, the division is an in-place or element-wise op, etc. Fusing them into one kernel reduces the number of memory accesses and kernel launches, which can save time.

Now, let's write the complete code.

Also, need to import the necessary modules. The example included from torch.utils.cpp_extension import load_inline. So adding that.

The full code would look like:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

elementwise_fused_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_activation_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        
        // Compute Swish: x * sigmoid(x)
        float sigmoid_x = 1.0f / (1.0f + expf(-x));
        float swish = x * sigmoid_x;
        
        // Divide by 2
        float after_div = swish / 2.0f;
        
        // Clamp between -1 and 1
        float clamped1 = fminf(fmaxf(after_div, -1.0f), 1.0f);
        
        // Compute tanh
        float tanh_out = tanhf(clamped1);
        
        // Final clamp (though redundant, as per original code)
        output[idx] = fminf(fmaxf(tanh_out, -1.0f), 1.0f);
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int64_t size = input.numel();
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    fused_activation_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), output.data_ptr<float>(), size
    );
    
    return output;
}
"""

elementwise_fused_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor input);"
)

# Compile the CUDA code
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=elementwise_fused_cpp_source,
    cuda_sources=elementwise_fused_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_cuda_cflags=["-std=c++14"],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.fused_activation = fused_activation  # The loaded module

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_activation.fused_activation_cuda(x)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features]

Wait, but in the original code, get_inputs uses torch.rand but without .cuda(), but the original's get_inputs returns tensors on CPU? Wait no, looking back:

Original get_inputs:

def get_inputs():
    return [torch.rand(batch_size, in_features)]

But in the example provided by the user, the original code had get_inputs with tensors on CUDA. Wait, in the example given, the original Model had get_inputs generating tensors on CUDA (because in the example, a and b are .cuda()). But in the problem's given code, the original get_inputs returns tensors on CPU? Let me check:

The user's provided code for the original has:

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]

Wait, but in the problem's given code, the Model is initialized with in_features and out_features, which are defined as batch_size=1024, in_features=8192, out_features=8192. So the get_inputs() returns a tensor on CPU. However, in the example the user provided, the custom model's get_inputs() had tensors on CUDA. So in the problem's original code, the inputs are on CPU. But in PyTorch, the model's parameters are on the device where they were initialized. The original code's Model uses nn.Linear, which by default is on CPU unless moved. So perhaps in the optimized code, the model should be placed on GPU, so the inputs need to be on GPU as well. However, the user's instruction says that the new code must have the same get_inputs as original. So the get_inputs() in the new code must return tensors on CPU, unless the original's get_inputs is different.

Wait, in the problem's given code's get_inputs() returns [torch.rand(batch_size, in_features)], which is on CPU. Therefore, the new code's get_inputs must return the same, so we need to keep that. However, in the example provided by the user, their custom model's get_inputs() had .cuda(). So the user's original code might have different get_inputs() based on the architecture. But according to the problem statement, the new code's get_inputs() must be the same as the original. Therefore, in the new code, the get_inputs() should not have .cuda().

Wait, in the problem's given code's get_inputs() is:

def get_inputs():
    return [torch.rand(batch_size, in_features)]

So the inputs are on CPU. But in the model's forward(), when using .cuda(), the tensors would need to be moved. However, in the problem's code, the original model is not necessarily on CUDA. But to get speedups with CUDA kernels, the data must be on GPU. Therefore, perhaps the user's intention is that the inputs are moved to CUDA in the forward pass, or the model is on GPU. Hmm, this is a point to clarify. The problem's original code's forward function is using the Linear layer, which by default is on CPU. So perhaps the user expects that the model and data are on GPU.

Wait, in the example given by the user, their original code's get_inputs() returns CUDA tensors. The problem's given code's get_inputs() returns CPU tensors. So there's inconsistency. But according to the problem statement, the user says to make sure that the new code's get_inputs() is the same as the original. So in the problem's given code, the inputs are on CPU. But the custom CUDA kernels require the data to be on CUDA. So perhaps the model's parameters and inputs are moved to CUDA. Therefore, in the new code's forward function, the tensors need to be on GPU. Therefore, in the get_inputs(), we should return tensors on CUDA, but according to the problem's instruction, we have to keep get_inputs() the same as the original, which returns CPU tensors. That's conflicting.

Wait, perhaps there's a misunderstanding. The user's example in the problem's given code (the one with the Model class) has get_inputs() returning CPU tensors. So when they say "the new code's get_inputs() must be the same as the original", that means we can't change it. Therefore, in the new code, the inputs are on CPU. But the custom CUDA kernels need to process data on the GPU. Therefore, the model must be placed on the GPU, and the input tensors should be moved to GPU when passed to the model. However, the get_inputs() function returns CPU tensors, but when the model is called, the tensors would be moved to the model's device. Therefore, the model should be initialized on CUDA. 

Therefore, in the new code, when creating the model instance, it's placed on CUDA. The get_inputs() returns CPU tensors, which are then moved to CUDA when passed to the model. 

Therefore, the get_inputs() remains unchanged. The code in the problem's given code for the original has batch_size, in_features, out_features as global variables, so in the new code, we need to keep those definitions the same. 

Now, in the new code's ModelNew class, the Linear layer is initialized on CPU, but when the model is moved to CUDA, it will be placed on the GPU. So in the code, when creating the model, it's placed on CUDA. But in the problem's code, the user probably wants the model to be on CUDA, but the get_inputs() returns CPU tensors. So the code is okay as long as the model is on CUDA and the input tensors are moved.

However, in the code provided by the user in their example, they had to move the tensors to CUDA in the get_inputs(). But in the problem's given code, the get_inputs() returns CPU tensors, so the new code must keep that.

Therefore, in the code, the model is placed on CUDA, and the input tensors are moved automatically when passed to the model. 

Therefore, the code for get_inputs() remains as the original.

Now, putting it all together.

Another thing to check: the CUDA kernel code uses 'const int block_size = 256', which is okay. The 'size' is input.numel(), which is correct.

Another possible optimization: the expf in the sigmoid can be expensive. Maybe using a lookup table or approximation could help, but that might reduce accuracy. Since the user requires that the kernel is faster than PyTorch's default, and PyTorch's sigmoid is already optimized, perhaps this is unavoidable. However, fusing all steps into a single kernel may still be faster overall due to reduced overhead.

Another point: in PyTorch, the sigmoid, tanh, and other activations are implemented in optimized CUDA kernels. For example, torch.sigmoid uses a specialized kernel. However, by fusing all the steps into a single kernel, we can reduce the number of kernel launches and memory copies between the steps, which can lead to a net gain in performance.

Testing this code in mind, it should work. Now, let's write the code properly in the answer.

Also, in the original code, the model has a parameter 'bias', so the new model must also handle that correctly.

Now, putting all together into the code block:

The complete Python code would be:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused activation
elementwise_fused_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_activation_kernel(const float* input, float* output, int64_t size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        
        // Compute Swish: x * sigmoid(x)
        float sigmoid_x = 1.0f / (1.0f + expf(-x));
        float swish = x * sigmoid_x;
        
        // Divide by 2
        float after_div = swish / 2.0f;
        
        // Clamp between -1 and 1
        float clamped1 = fminf(fmaxf(after_div, -1.0f), 1.0f);
        
        // Compute tanh
        float tanh_out = tanhf(clamped1);
        
        // Final clamp (as per original code)
        output[idx] = fminf(fmaxf(tanh_out, -1.0f), 1.0f);
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int64_t size = input.numel();
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    fused_activation_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), output.data_ptr<float>(), size
    );
    
    return output;
}
"""

elementwise_fused_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor input);"
)

# Compile the fused activation CUDA code
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=elementwise_fused_cpp_source,
    cuda_sources=elementwise_fused_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_cuda_cflags=["-std=c++14"],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.fused_activation = fused_activation  # The loaded module

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_activation.fused_activation_cuda(x)
        return x

# Keep the same get_inputs and get_init_inputs as the original
batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
```

Wait, but the get_inputs() in the original code returns tensors on CPU. However, the model's parameters and the custom kernel expect the data to be on CUDA. To ensure the model is on CUDA, the user would need to move the model to CUDA. The code as written will work if the model is placed on CUDA, and the input tensors are moved when passed to the model.

But in the code above, the ModelNew is not explicitly moved to CUDA. So the user needs to do that when creating the model instance. However, the problem's instructions don't require us to change the get_inputs function, so the code as written is correct.

Another possible optimization: The gemm (linear layer) can be replaced with a custom kernel. Let's see: the Linear layer does a matrix multiply plus a bias. The matrix multiply is O(N^3), which is time-consuming. However, PyTorch's implementation of Linear is already using optimized CUDA kernels (cuBLAS), so replacing it with a custom kernel may not be faster unless we can fuse it with subsequent steps.

Fusing the gemm with the Swish would require a kernel that takes input, weight, bias, and then computes the gemm plus bias, followed by Swish and the rest. But this is a more complex kernel and may not be worth it unless the benefits outweigh the effort. Since the original problem allows replacing some operators, and the user's example replaced a simple add, perhaps focusing on the element-wise steps is better.

Therefore, the code I wrote earlier should be correct and should provide a speedup by fusing the activation steps into a single kernel, reducing kernel launch overhead.

Another thing to check: in the CUDA kernel, the 'size' variable is passed as an int64_t, which is correct since numel() can be larger than 2^31.

Also, the 'block_size' is 256, which is a standard value.

This code should compile and work, and the fused kernel should be faster than the original sequence of PyTorch operations.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

elementwise_fused_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_activation_kernel(const float* input, float* output, int64_t size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        
        // Compute Swish: x * sigmoid(x)
        float sigmoid_x = 1.0f / (1.0f + expf(-x));
        float swish = x * sigmoid_x;
        
        // Divide by 2
        float after_div = swish / 2.0f;
        
        // Clamp between -1 and 1
        float clamped1 = fminf(fmaxf(after_div, -1.0f), 1.0f);
        
        // Compute tanh
        float tanh_out = tanhf(clamped1);
        
        // Final clamp (as per original code)
        output[idx] = fminf(fmaxf(tanh_out, -1.0f), 1.0f);
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int64_t size = input.numel();
    
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    
    fused_activation_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), output.data_ptr<float>(), size
    );
    
    return output;
}
"""

elementwise_fused_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor input);"
)

# Compile the fused activation CUDA code
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=elementwise_fused_cpp_source,
    cuda_sources=elementwise_fused_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_cuda_cflags=["-std=c++14"],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.fused_activation = fused_activation  # The loaded module

    def forward(self, x):
        x = self.gemm(x)
        x = self.fused_activation.fused_activation_cuda(x)
        return x

# Keep the same get_inputs and get_init_inputs as the original
batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
```