The problem is to replace the operators in the original model with custom CUDA implementations. The original model is doing a 3D transposed convolution followed by element-wise operations (scaling, bias addition, another scaling). 

So here's the plan for optimization:

1. **Operator Replacement**: The main computational bottlenecks are likely the ConvTranspose3d and the subsequent element-wise operations. Replacing the 3D transposed convolution with a custom kernel could offer a speedup. Additionally, the element-wise operations (scaling, addition, another scaling) can be combined into a single kernel to reduce memory transfers and kernel launch overhead.

2. **Operator Fusion**: Instead of having separate operations for scaling, average pooling, bias addition, and final scaling, we can fuse them into one or two kernels. However, average pooling is a more complex operation, so perhaps the element-wise operations after pooling can be fused into a single kernel.

3. **CUDA Kernel Implementation**:
   - A fused kernel for the post-convolution operations (scale1, average pooling, bias addition, scale2). Wait, but average pooling requires a reduction over local regions. So perhaps the element-wise operations after pooling can be fused.

Wait, the sequence is:

conv_transpose → * scale1 → avg_pool → + bias → * scale2

The conv_transpose is the first step. So maybe the first step is to replace the conv_transpose with a custom CUDA kernel. But writing a 3D transposed convolution kernel is quite involved. However, maybe there's a way to optimize it or find that the existing PyTorch implementation is already optimized and not worth replacing. Alternatively, perhaps the element-wise operations after can be fused into a single kernel.

Alternatively, perhaps the element-wise scaling and addition can be fused into a single kernel. Let's see:

The element-wise operations are:

After convolution: multiply by scale1 (element-wise scaling).

Then average pool.

Then add bias (element-wise addition).

Then multiply by scale2 (element-wise scaling).

The average pooling is a spatial operation, so it can't be easily combined with the element-wise steps. However, the steps before the pool (scale1) can be applied before the pooling, but that's already part of the existing code.

The post-pooling steps (add bias, scale2) can be fused into a single kernel. So the idea is to combine the "x = x * scale1" step with the pooling? Not sure. Wait, the scale1 is applied before the pooling. So the pooling is on (x * scale1). 

Alternatively, perhaps the average pooling can be optimized, but that's also a complex operator. Maybe it's better to focus on fusing the element-wise operations after the pooling.

So after the average pooling, the steps are:

x = x + bias (element-wise addition with a bias that's broadcastable)

x = x * scale2 (element-wise scaling)

These can be combined into a single kernel.

Additionally, perhaps the scaling before the pooling (x * scale1) can be fused into the convolution itself. But that would require modifying the convolution kernel to include the scaling, but maybe the existing convolution can be followed by scaling in a kernel.

Alternatively, the scaling before pooling is just an element-wise multiplication, which is straightforward to implement in a kernel, but maybe doing it in a separate kernel isn't worth it. However, fusing the scaling into the convolution might save some steps.

Alternatively, the main optimization is to replace the convolution with a custom implementation. But writing a custom 3D transposed convolution is quite involved and might not be trivial. So perhaps the user should focus on the element-wise operations, which are easier to fuse.

Alternatively, perhaps the average pooling can be optimized by writing a custom kernel, but that's also non-trivial.

Alternatively, the main speedup can come from fusing the three element-wise operations (scale1, bias addition, scale2). Wait, but the scale1 is before the average pooling. So the sequence is:

conv → scale1 → avg_pool → (add bias and scale2)

Thus, the post-pooling operations (add bias and scale2) can be fused.

Alternatively, the scale1 can be part of the convolution output, but perhaps it's better to do it in a separate kernel.

Perhaps the most straightforward optimization is to fuse the post-pooling operations (add bias and multiply by scale2). These are element-wise operations, so they can be done in a single kernel.

Additionally, maybe the average pooling can be optimized by using a custom kernel, but that requires more work.

Alternatively, let's consider replacing the convolution with a custom kernel. Let me think about the steps:

The ConvTranspose3d is the first operator, which is a 3D transposed convolution. Implementing that in a custom kernel would require handling the 3D input and output, the kernel, the stride, padding, etc. It's quite involved. Maybe it's better to look for other optimizations first.

Let me think of the steps again:

Original forward pass steps:

1. Conv3DTranspose: This is the bulk of the computation, especially for 3D tensors. Maybe the existing PyTorch implementation is optimized, but perhaps there's room for improvement.

2. Scaling by scale1 (element-wise multiply)

3. Average pooling over a 2x2 window (kernel_size=2)

4. Add bias (element-wise addition with a tensor of shape (out_channels,1,1,1))

5. Scaling by scale2 (element-wise multiply)

The element-wise operations after the pooling (steps 4 and 5) can be fused into a single kernel. The scaling before the pooling (step 2) is also an element-wise multiply, which could be done in a separate kernel, but maybe it's better to combine it with the convolution.

Alternatively, the scaling before pooling (step 2) can be done inline with the convolution's output, but that would require modifying the convolution kernel to include the scaling. However, this might not be straightforward. Alternatively, doing a separate element-wise multiply kernel for step 2.

So, for element-wise operations, it's better to combine the steps that are consecutive and can be done in a single pass.

Let me outline possible kernels:

Kernel 1: Custom ConvTranspose3d (complex, but potentially faster if implemented well)

Kernel 2: Fused element-wise after pooling (add bias and multiply by scale2)

Alternatively, let's start with the element-wise operations after pooling first.

The steps after pooling are:

x = x + bias

x = x * scale2

The bias is added to each channel, so it's a tensor of shape (out_channels, 1, 1, 1). So when adding, it's broadcasted to the spatial dimensions.

Thus, for each element in x, the operation is x[i] += bias[channel], then multiplied by scale2.

This can be done in a single kernel:

for each element:

out[i] = (x[i] + bias[channel]) * scale2

Thus, this is a fused operation. To implement this, we can write a kernel that takes the input tensor, the bias tensor (which has the same number of channels), and the scale2, and applies the operation in a single step.

This would reduce the memory accesses and kernel launches.

Similarly, the scaling by scale1 is an element-wise multiplication, which can be done in a separate kernel, but perhaps it's better to leave it as a separate kernel since it comes before the pooling.

Alternatively, the scaling before pooling can be done in a separate kernel.

So the plan is:

1. Keep the ConvTranspose3d as is (since implementing it in a custom kernel is complex and may not be worth the effort unless it's a bottleneck)

2. Replace the scaling by scale1 with a custom kernel. However, element-wise multiplication is straightforward, but maybe PyTorch's implementation is already optimized. However, if we have to write it, it's easy.

Alternatively, perhaps the scaling can be done inline with the convolution's output, but that requires modifying the convolution code.

Alternatively, proceed step by step.

First, let's tackle the post-pooling element-wise operations (add bias and scale2).

Let me structure the new code.

The ModelNew will replace the element-wise operations with custom kernels.

The ModelNew class would have the same ConvTranspose3d, then the element-wise scaling, but the post-pooling steps are replaced by a fused kernel.

Alternatively, here's the outline of the new forward function:

class ModelNew(nn.Module):

    def __init__(self, ...):

    def forward(self, x):

        x = self.conv_transpose(x)

        x = self.scale1_kernel(x)  # scale by scale1

        x = self.avg_pool(x)  # maybe keep this as is?

        x = fused_add_scale(x, self.bias, self.scale2)  # fused kernel for adding bias and scaling by scale2

Alternatively, maybe the average pooling can be replaced with a custom kernel for better performance. However, average pooling is a reduction operation over a local window, which can be complex to implement, especially in 3D.

Alternatively, if the average pooling is already optimized in PyTorch, perhaps leave it as is.

Thus, the main custom kernels would be:

- scale1 (element-wise multiplication)

- fused add bias and scale2 (element-wise operations)

But the scale1 is an element-wise multiplication, which can be implemented as a simple kernel. Let's see:

Element-wise multiplication by a scalar (scale1 is a scalar, since it's a single value: scale1 = 0.5).

Wait, in the original model, scale1 and scale2 are parameters, but they are initialized with a single value (scale1=0.5, scale2=1.0). Thus, they are scalar parameters. Wait, looking at the original code:

The scale1 is a nn.Parameter(torch.tensor(scale1)), so it's a tensor of shape (1,).

Similarly for scale2. Therefore, scaling by scale1 is an element-wise multiplication by a scalar.

Thus, the element-wise multiplication for scale1 can be done via a simple kernel, but even more simply, just using the *= operator in PyTorch might be efficient. However, for the sake of optimization, perhaps fusing with other operations would be better.

Wait, but after scaling by scale1, it goes through the average pooling. The average pooling is a spatial operation, so perhaps the element-wise scaling before pooling can be done in a custom kernel, but it's trivial. However, combining it with the convolution might not be feasible.

Alternatively, perhaps the convolution and the scale can be fused into a single kernel. Let me think:

ConvTranspose3d's output is computed as per the transposed convolution formula, then each element is multiplied by scale1. If the convolution kernel can be modified to include this scaling, that could save a separate kernel. However, this requires modifying the convolution's implementation, which is complex.

Alternatively, the scaling is just x = x * scale1, which can be done with a simple kernel:

def scale1_kernel(x, scale):

    out = x * scale

But in CUDA, this is straightforward. However, if the scaling is done with PyTorch's operator, it might already be optimized. However, in a custom kernel, it can be done in parallel.

But for the purposes of this exercise, let's proceed to implement the fused kernel for the post-pooling steps.

So let's first create a kernel for the fused add and scale:

The fused operation after avg_pool is:

out = (x + bias) * scale2

The bias is of shape (out_channels, 1, 1, 1), so when added to x (which has shape (batch, out_channels, depth, height, width)), it's broadcasted over the spatial dimensions.

The scale2 is a scalar.

Thus, the kernel would:

- For each element, compute the channel index.

- Add the bias value for that channel.

- Multiply by scale2.

The CUDA kernel would need to:

- Iterate over all elements of the input tensor.

- For each element at position (n, c, d, h, w), the bias is self.bias[c].

Thus, the kernel can be written as follows:

First, the input is after the average pooling, so let's assume the input is a 5D tensor.

The kernel code:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_kernel(
    const float* x_data,
    const float* bias_data,
    const float scale,
    float* out_data,
    int batch_size,
    int channels,
    int depth,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * depth * height * width) return;

    int c = (idx / (depth * height * width)) % channels;
    // compute the bias value
    float bias_val = bias_data[c];
    float x_val = x_data[idx];
    out_data[idx] = (x_val + bias_val) * scale;
}

Then, the wrapper function would take the input tensor, the bias tensor, and the scale, and compute this.

But in PyTorch, the input x is a tensor, and the bias is also a tensor of shape (channels, 1, 1, 1). The kernel needs to access the bias for each channel.

The parameters for the kernel launch would be the sizes of the input tensor (so we can compute the total number of elements).

This is manageable.

Another thing to note is that in PyTorch, the tensors are contiguous, so we can assume that.

Thus, the fused kernel is feasible.

Now, for the scaling before the average pooling (scale1), which is a scalar multiplication. The code for that kernel is simpler:

__global__ void scale_kernel(const float* in, float scale, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = in[idx] * scale;
    }
}

But again, this might be redundant if PyTorch's implementation is already optimized.

However, for the purpose of this problem, we'll proceed to write these kernels.

Now, let's also consider the average pooling. The average pooling is an nn.AvgPool3d(kernel_size=2). The kernel size is 2, so the average is over a 2x2x2 region? Or is it 2 in each spatial dimension? The AvgPool3d with kernel_size=2 would have kernel_size=(2,2,2) if unspecified. Wait, the original code defines the AvgPool3d as:

self.avg_pool = nn.AvgPool3d(kernel_size=2)

In PyTorch, the kernel_size is a single integer, which is interpreted as the same in all spatial dimensions. So, for 3D pooling, the kernel is 2x2x2? Or is it 2 in depth, height, and width?

Wait, the input to the AvgPool3d is a 5D tensor (batch, channels, depth, height, width). The kernel_size=2 would apply to depth, height, and width. So the kernel is 2x2x2. The stride is also 2 by default, I think? Or it's equal to the kernel size. Let me check:

The default stride for AvgPool3d is equal to the kernel_size. So with kernel_size=2, the stride is 2.

Thus, the average pooling reduces each spatial dimension by a factor of 2.

Implementing this in a custom kernel is possible but more complex. Let's see:

The output dimensions would be:

out_depth = (depth + 2*padding - kernel_size) / stride + 1

But in the original model, there is no padding mentioned for the AvgPool3d. The default padding is 0. So:

Assuming input depth is D, after kernel_size 2 and stride 2, output depth is (D - 2)/2 +1 = (D)/2 if D is even.

But implementing this requires for each output position, taking the average of a 2x2x2 window. This requires more complex indexing and handling of the input and output tensors.

Given the time constraints and complexity, maybe it's better to leave the average pooling as the PyTorch implementation, unless it's a major bottleneck.

Thus, the main custom kernels are the fused post-pooling add and scale, and perhaps the scale1.

Thus, the ModelNew will have:

- The ConvTranspose3d as in the original (no change)

- The scale1 is replaced by a custom kernel (though it's a simple operation)

- The avg_pool remains as the PyTorch module

- The fused add and scale kernel for the final steps.

Thus, the code would look like this:

First, define the CUDA kernels for the scale and fused operations.

First, the scale1 kernel (element-wise multiplication by a scalar):

scale1_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_kernel(const float* in, float scale, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = in[idx] * scale;
    }
}

torch::Tensor scale_cuda(torch::Tensor in, float scale) {
    auto size = in.numel();
    auto out = torch::empty_like(in);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    scale_kernel<<<num_blocks, block_size>>>(
        in.data_ptr<float>(),
        scale,
        out.data_ptr<float>(),
        size
    );

    return out;
}
"""

scale1_cpp = "torch::Tensor scale_cuda(torch::Tensor in, float scale);"

Then, the fused add and scale kernel:

fused_add_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_kernel(
    const float* x,
    const float* bias,
    float scale,
    float* out,
    int batch_size,
    int channels,
    int depth,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * depth * height * width) return;

    int c = (idx / (depth * height * width)) % channels;
    float bias_val = bias[c];
    out[idx] = (x[idx] + bias_val) * scale;
}

torch::Tensor fused_add_scale_cuda(
    torch::Tensor x,
    torch::Tensor bias,
    float scale
) {
    auto batch_size = x.size(0);
    auto channels = x.size(1);
    auto depth = x.size(2);
    auto height = x.size(3);
    auto width = x.size(4);

    auto out = torch::empty_like(x);

    const int block_size = 256;
    const int num_elements = batch_size * channels * depth * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_add_scale_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale,
        out.data_ptr<float>(),
        batch_size,
        channels,
        depth,
        height,
        width
    );

    return out;
}
"""

fused_add_scale_cpp = "torch::Tensor fused_add_scale_cuda(torch::Tensor x, torch::Tensor bias, float scale);"

Then, we need to compile these kernels.

In the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.scale1 = nn.Parameter(torch.tensor(scale1))
        self.avg_pool = nn.AvgPool3d(kernel_size=2)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale2 = nn.Parameter(torch.tensor(scale2))

        # Load the custom CUDA kernels
        self.scale1_kernel = load_inline(
            name="scale1",
            cuda_sources=scale1_source,
            cpp_sources=scale1_cpp,
            functions=["scale_cuda"],
            verbose=True,
        )

        self.fused_add_scale = load_inline(
            name="fused_add_scale",
            cuda_sources=fused_add_scale_source,
            cpp_sources=fused_add_scale_cpp,
            functions=["fused_add_scale_cuda"],
            verbose=True,
        )

    def forward(self, x):
        x = self.conv_transpose(x)
        # Apply scale1 using custom kernel
        x = self.scale1_kernel.scale_cuda(x, self.scale1.item())
        x = self.avg_pool(x)
        # Apply fused add bias and scale2
        x = self.fused_add_scale.fused_add_scale_cuda(x, self.bias, self.scale2.item())
        return x

Wait, but in the original model, the scale1 and scale2 are parameters. Since they are scalars, in the custom kernels, we pass their .item() values as float.

This should work.

However, there's a potential issue: in the fused kernel, the bias is a tensor, so it can be passed directly. The kernel accesses the bias via bias.data_ptr<float>(), which is correct.

Testing the kernel:

The fused_add_scale_cuda function takes the bias as a tensor, which is correct, because in PyTorch, when you pass a tensor to a CUDA kernel, it's handled properly.

Now, check the sizes:

The fused kernel's parameters include batch_size, channels, depth, height, width. These are obtained from the input tensor x's size. So that should be correct.

Another thing to note is that the input x after the avg_pool has the correct dimensions. Let's assume that the AvgPool3d is applied correctly.

Now, check if the code is correct:

Yes, the forward function applies the conv, scales by scale1 via the custom kernel, then applies the avg_pool, then uses the fused kernel to add the bias and scale by scale2.

This replaces the original steps:

Original:

x = x * self.scale1

x = self.avg_pool(x)

x = x + self.bias

x = x * self.scale2

With:

x = scale_cuda(x, scale1)

...

x = fused_add_scale_cuda(x, bias, scale2)

Thus, this should work.

Now, the next step is to ensure that the code compiles and runs. The custom kernels should be properly loaded.

Potential issues:

1. The scale1 kernel's scale parameter is a float, which is passed via self.scale1.item().

2. The fused kernel's bias is a tensor with shape (out_channels, 1, 1, 1). However, in the kernel, the code is using bias[c], where c is the channel index. Since the bias is stored in a tensor with shape (C, 1, 1, 1), the data is contiguous in memory, so accessing bias[c] is correct, since the first dimension is the channel. The other dimensions are 1, so bias[c] is equivalent to bias[c, 0, 0, 0].

Yes, that's correct.

Another thing to check is whether the input tensors are contiguous. Since PyTorch's functions usually return contiguous tensors, but when using custom kernels, it's better to ensure that the input tensors are contiguous. The code uses torch.empty_like(x) which should create a contiguous tensor if x is contiguous.

Thus, the code should work.

Now, putting this all together in the correct syntax with the required imports and model structure.

The complete code would be:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for scaling
scale1_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_kernel(const float* in, float scale, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = in[idx] * scale;
    }
}

torch::Tensor scale_cuda(torch::Tensor in, float scale) {
    auto size = in.numel();
    auto out = torch::empty_like(in);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    scale_kernel<<<num_blocks, block_size>>>(
        in.data_ptr<float>(),
        scale,
        out.data_ptr<float>(),
        size
    );

    return out;
}
"""

scale1_cpp = "torch::Tensor scale_cuda(torch::Tensor in, float scale);"

# Define the fused CUDA kernel for adding bias and scaling
fused_add_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_kernel(
    const float* x,
    const float* bias,
    float scale,
    float* out,
    int batch_size,
    int channels,
    int depth,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * depth * height * width) return;

    int c = (idx / (depth * height * width)) % channels;
    float bias_val = bias[c];
    out[idx] = (x[idx] + bias_val) * scale;
}

torch::Tensor fused_add_scale_cuda(
    torch::Tensor x,
    torch::Tensor bias,
    float scale
) {
    auto batch_size = x.size(0);
    auto channels = x.size(1);
    auto depth = x.size(2);
    auto height = x.size(3);
    auto width = x.size(4);

    auto out = torch::empty_like(x);

    const int block_size = 256;
    const int num_elements = batch_size * channels * depth * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_add_scale_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale,
        out.data_ptr<float>(),
        batch_size,
        channels,
        depth,
        height,
        width
    );

    return out;
}
"""

fused_add_scale_cpp = "torch::Tensor fused_add_scale_cuda(torch::Tensor x, torch::Tensor bias, float scale);"

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.scale1 = nn.Parameter(torch.tensor(scale1))
        self.avg_pool = nn.AvgPool3d(kernel_size=2)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale2 = nn.Parameter(torch.tensor(scale2))

        # Load the custom CUDA kernels
        self.scale1_kernel = load_inline(
            name="scale1",
            cuda_sources=scale1_source,
            cpp_sources=scale1_cpp,
            functions=["scale_cuda"],
            verbose=True,
        )

        self.fused_add_scale = load_inline(
            name="fused_add_scale",
            cuda_sources=fused_add_scale_source,
            cpp_sources=fused_add_scale_cpp,
            functions=["fused_add_scale_cuda"],
            verbose=True,
        )

    def forward(self, x):
        x = self.conv_transpose(x)
        # Apply scale1 using custom kernel
        x = self.scale1_kernel.scale_cuda(x, self.scale1.item())
        x = self.avg_pool(x)
        # Apply fused add bias and scale2
        x = self.fused_add_scale.fused_add_scale_cuda(x, self.bias, self.scale2.item())
        return x
```

Wait, but in the original code, the scale1 and scale2 are parameters. The original model's scale1 is initialized with scale1=0.5, which is a float. The code here uses self.scale1.item() to get the float value for the kernel. That's correct.

Also, the bias_shape is (out_channels,1,1,1), so the bias tensor has the correct shape.

Another check: the fused_add_scale_cuda function requires the bias tensor to be passed. The code passes self.bias, which is a parameter of the model, so that's correct.

Thus, this should work.

Potential further optimizations:

- Fusing the scale1 into the ConvTranspose3d kernel. But that would require modifying the convolution kernel, which is complex.

- Replacing the average pooling with a custom kernel. But that's more involved.

- The scale1 kernel could be replaced with PyTorch's own multiplication, but for the sake of the problem, we've implemented the kernel.

Therefore, this code should meet the requirements.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for element-wise scaling by a scalar
scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_kernel(const float* in, float scale, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = in[idx] * scale;
    }
}

torch::Tensor scale_cuda(torch::Tensor in, float scale) {
    auto size = in.numel();
    auto out = torch::empty_like(in);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    scale_kernel<<<num_blocks, block_size>>>(
        in.data_ptr<float>(),
        scale,
        out.data_ptr<float>(),
        size
    );

    return out;
}
"""

scale_cpp = "torch::Tensor scale_cuda(torch::Tensor in, float scale);"

# Custom fused CUDA kernel for adding bias and scaling by a scalar
fused_add_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_add_scale_kernel(
    const float* x,
    const float* bias,
    float scale,
    float* out,
    int batch_size,
    int channels,
    int depth,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * depth * height * width) return;

    int c = (idx / (depth * height * width)) % channels;
    float bias_val = bias[c];
    out[idx] = (x[idx] + bias_val) * scale;
}

torch::Tensor fused_add_scale_cuda(
    torch::Tensor x,
    torch::Tensor bias,
    float scale
) {
    auto batch_size = x.size(0);
    auto channels = x.size(1);
    auto depth = x.size(2);
    auto height = x.size(3);
    auto width = x.size(4);

    auto out = torch::empty_like(x);

    const int block_size = 256;
    const int num_elements = batch_size * channels * depth * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_add_scale_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale,
        out.data_ptr<float>(),
        batch_size,
        channels,
        depth,
        height,
        width
    );

    return out;
}
"""

fused_add_scale_cpp = "torch::Tensor fused_add_scale_cuda(torch::Tensor x, torch::Tensor bias, float scale);"

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.scale1 = nn.Parameter(torch.tensor(scale1))
        self.avg_pool = nn.AvgPool3d(kernel_size=2)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale2 = nn.Parameter(torch.tensor(scale2))

        # Load custom CUDA kernels
        self.scale1_kernel = load_inline(
            name="scale",
            cuda_sources=scale_source,
            cpp_sources=scale_cpp,
            functions=["scale_cuda"],
            verbose=True,
        )

        self.fused_add_scale = load_inline(
            name="fused_add_scale",
            cuda_sources=fused_add_scale_source,
            cpp_sources=fused_add_scale_cpp,
            functions=["fused_add_scale_cuda"],
            verbose=True,
        )

    def forward(self, x):
        x = self.conv_transpose(x)
        # Apply scale1 using custom kernel
        x = self.scale1_kernel.scale_cuda(x, self.scale1.item())
        x = self.avg_pool(x)
        # Apply fused add bias and scale2
        x = self.fused_add_scale.fused_add_scale_cuda(x, self.bias, self.scale2.item())
        return x
```