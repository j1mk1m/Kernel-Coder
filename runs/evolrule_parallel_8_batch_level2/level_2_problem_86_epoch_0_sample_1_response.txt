    When writing kernels, consider the following tips:
    - Prefer operator fusion. Try to combine multiple operators into a single kernel to reduce memory copies and kernel launch overhead.
    - You can choose to use the torch.utils.cpp_extension.load_inline function to embed the CUDA code directly in the Python script.
    - If you need to create multiple kernels, group them in a single code block with appropriate names and make sure they are properly separated.
    - Make sure all dependencies are properly included (e.g., torch/extension.h, cuda_runtime.h).
    - Ensure that the code is correct and can be compiled with NVCC and PyTorch's extension loader.
    - Avoid unnecessary memory allocations. For example, if an output can be written in-place, do so, but note that PyTorch's autograd may require specific behaviors.
    - Pay attention to the data types (e.g., float vs. half) and ensure consistency between the kernel and the input tensors.
    - If you decide to write a fused kernel for matmul + divide + gelu, you must also implement the backward pass for the fused kernel if you need gradients. However, since the problem statement doesn't mention needing gradients, maybe the forward pass is sufficient. Wait, but in PyTorch, all modules are expected to handle backward automatically via autograd. If you replace operators with a custom kernel that does not have a backward, then autograd will fail. So, to make it work, either:
        1. Only replace operators that are differentiable and implement the backward in the CUDA kernel (by creating a custom autograd.Function), or
        2. Replace non-differentiable parts. 
    However, in the given architecture, the entire forward path is differentiable (linear is differentiable, division by a scalar is differentiable, GELU is differentiable). Therefore, if we replace the entire forward path with a custom kernel, we need to provide a backward implementation. Alternatively, we can implement a fused forward kernel that combines matmul, division, and GELU, and implement its gradient in a backward kernel. However, writing a backward kernel for GELU might be complex. Alternatively, perhaps we can find a way to fuse the operations without losing differentiability, but that might require custom autograd functions.

    Alternatively, since the problem says "you have complete freedom to choose the set of operators you want to replace", perhaps replacing only some operators which are not differentiable, but in this case, all are differentiable. Therefore, perhaps the best approach is to implement a fused forward kernel and also the backward kernel for the fused operation, using a custom autograd.Function.

    Alternatively, perhaps the division by a scalar can be incorporated into the linear layer's weights, but since the divisor is a parameter (fixed value), perhaps we can combine the linear's weight matrix multiplication with the division. For example, instead of (x @ W) / divisor, we can do x @ (W / divisor). But the divisor is a scalar, so that could be done in the linear layer's initialization. However, that would eliminate the division operator, but the rest remains. However, the problem says to replace pytorch operators, so perhaps this is allowed. However, perhaps the question expects writing a custom CUDA kernel.

    Given the instructions, the user wants us to write a custom CUDA kernel. So let's proceed.

    Let's think: The forward path is Linear (matmul + bias?), divide by scalar, then GELU. Wait, the linear layer includes a bias term. In PyTorch, nn.Linear is y = x @ W^T + b. Therefore, the forward is:

    x = x @ W^T + b (from linear)
    x = x / divisor
    x = gelu(x)

    So the operators here are matmul (with bias), division, and GELU.

    To fuse these into a single kernel, we need to combine these steps. The problem is that the matmul is a large operation, and then element-wise operations.

    The main computational cost is the matrix multiplication. The division and GELU are element-wise. So fusing them into one kernel can save some memory copies.

    So the plan is to write a custom kernel that takes x, W, b, divisor, and computes:

    out = gelu( (x @ W^T + b) / divisor )

    So the fused kernel would compute the matmul, add the bias, divide by the scalar, apply GELU, all in one kernel.

    To do that, the kernel would need to perform:

    For each element in the output, compute the matmul step, add bias, divide, then apply GELU.

    However, the matmul is between (batch_size, input_size) and (output_size, input_size) (since Linear's weight is (output_size, input_size)), resulting in (batch_size, output_size). Then adding bias (output_size), then dividing by scalar, then GELU.

    So the fused kernel would need to handle the matrix multiplication, then the element-wise operations.

    The kernel will need to process each element of the output. For the matrix multiplication part, each element out[i,j] = sum_{k} x[i,k] * W[j,k] + b[j]

    Then divide by divisor, then apply GELU.

    To compute this in a single kernel, we can structure the kernel to compute each output element's value through the steps.

    However, the matrix multiplication is a dense operation and may require a more optimized approach. However, given the problem's constraints, perhaps we can proceed with a straightforward implementation.

    Alternatively, since the matrix multiplication is the most compute-intensive part, perhaps we should use cuBLAS for that, and then do the element-wise operations in the same kernel.

    However, integrating cuBLAS into a custom kernel may be possible but may require careful handling.

    Alternatively, perhaps the best approach is to implement the entire computation in a CUDA kernel. But implementing a general matrix multiplication in CUDA is non-trivial and may not be efficient compared to cuBLAS. Therefore, perhaps it's better to use cuBLAS for the matrix multiplication part, then perform the rest in the same kernel.

    Let me think: The fused operation is:

    out = gelu( (linear(x) ) / divisor )

    So the steps are:

    1. Compute linear(x): x @ W^T + b
    2. Divide each element by divisor
    3. Apply GELU

    To implement this in a custom kernel, we can use cuBLAS for the matmul part, then handle the rest.

    However, using cuBLAS requires launching it as a separate kernel or using the cuBLAS API within the kernel. Since cuBLAS is a library, it's typically called from the host code, not the device. Therefore, in a CUDA kernel, we can't directly call cuBLAS functions. Therefore, perhaps the only way is to compute the matmul manually in the kernel, which may not be efficient.

    Alternatively, perhaps we can structure the kernel to first compute the matrix multiplication, then proceed with the element-wise operations. But for large matrices, this might be slow.

    Alternatively, perhaps the problem expects us to replace the division and GELU with a custom kernel, leaving the linear layer as is. Since the linear layer's matmul is already optimized in PyTorch, maybe the best optimization is to fuse division and GELU into a single kernel. Let's consider that.

    The division by the scalar is an element-wise operation, as is GELU. Therefore, fusing these two into a single kernel would save memory copies.

    So the plan is: the forward path is linear (using PyTorch's optimized implementation) followed by a fused division and GELU.

    Therefore, the custom kernel would take the output of the linear layer, divide by divisor, then apply GELU.

    That's a simpler approach. Let's proceed with this.

    So the code for ModelNew would have a linear layer, then call the fused division-GELU kernel.

    Therefore, the custom kernel would take the input (after linear), the divisor, and compute (input / divisor).gelu().

    So the kernel would be:

    for each element: out[i] = gelu( input[i] / divisor )

    Since GELU can be implemented with a formula.

    The GELU function is 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))

    Alternatively, using the approximation or exact computation.

    Let's code the GELU function in CUDA.

    Therefore, the fused kernel would take the input tensor, the divisor, and compute element-wise (input / divisor), then apply GELU.

    This is straightforward to implement.

    Therefore, the steps are:

    1. In ModelNew, keep the Linear layer as in PyTorch.

    2. After the linear layer, call the custom fused kernel for division and GELU.

    This approach allows us to avoid touching the matrix multiplication, which is already optimized, and focus on the element-wise steps.

    Therefore, this is a viable approach.

    Let's proceed with that.

    The code for the kernel would be:

    - A CUDA kernel that takes input, divisor, and outputs the result of (input / divisor).gelu()

    So the kernel function would be something like:

    __global__ void fused_div_gelu_kernel(float* out, const float* in, float divisor, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            float x = in[idx] / divisor;
            // compute GELU
            float y = gelu(x);
            out[idx] = y;
        }
    }

    Then, in Python, we can call this kernel after the linear layer.

    However, in PyTorch, the GELU is a differentiable function, so if we implement this kernel, we need to provide a backward implementation, otherwise autograd will fail.

    Wait, the problem didn't specify whether the model needs to be trained or just run the forward pass. Since the original code uses torch.nn.functional.gelu, which is differentiable, the user probably expects the optimized version to also be differentiable. Therefore, the custom kernel must have a backward function.

    Therefore, to make this work, we need to create a custom autograd.Function that implements both forward and backward passes for the fused division and GELU.

    Therefore, the steps are:

    1. Implement a forward kernel for division and GELU.

    2. Implement a backward kernel for the gradient of the fused function.

    3. Create a custom autograd.Function to wrap these.

    Let's proceed.

    First, the forward kernel.

    The forward computation is:

    out = gelu( input / divisor )

    So for each element:

    x = in[idx] / divisor

    out[idx] = gelu(x)

    The backward computation for this function is:

    d_input = d_out * ( derivative of gelu w.r. to x ) * (1/divisor)

    Where x = in / divisor.

    Therefore, the gradient with respect to the input is (d_out * dgelu/dx) * (1/divisor)

    The gradient with respect to the divisor is sum over all elements of (d_out * dgelu/dx * (-in[idx]/divisor²) )

    But since the divisor is a scalar (a parameter of the model), we need to compute its gradient as well.

    However, in the original model, divisor is a parameter (self.divisor). Wait, in the original Model class:

    class Model(nn.Module):
        def __init__(self, input_size, output_size, divisor):
            super(Model, self).__init__()
            self.linear = nn.Linear(input_size, output_size)
            self.divisor = divisor

    Wait, but self.divisor is a scalar stored as a parameter? Or is it a buffer?

    In PyTorch, parameters are typically nn.Parameters. However, in the original code, self.divisor is just a scalar stored as an attribute. Therefore, it is not a learnable parameter, so its gradient is not tracked. Therefore, the gradient with respect to the divisor is not needed. Therefore, in the backward pass, we can ignore the derivative with respect to the divisor.

    Therefore, in the custom function's backward, we only need to compute the gradient w.r. to the input, and not the divisor.

    Therefore, the backward kernel will compute:

    grad_input = grad_output * (dgelu/dx) * (1/divisor)

    where x = input / divisor

    So, the backward kernel would need to compute this.

    Therefore, to implement this, we can write the forward and backward kernels.

    Let's proceed step by step.

    First, the forward:

    Implement a CUDA kernel that takes input and divisor, applies division and GELU, stores in output.

    The backward kernel takes grad_output, input, divisor, and computes grad_input.

    So the code would look like this:

    Define a custom CUDA function with forward and backward.

    Let's write the CUDA code.

    The GELU function can be implemented as follows:

    float gelu(float x) {
        return 0.5f * x * (1.0f + tanhf(sqrtf(2.0f / M_PI) * (x + 0.044715f * x * x * x)));
    }

    The derivative of GELU is:

    dgelu_dx = 0.5 * (1 + tanh(a*x^3 + b*x) ) + 0.5x*( ... )

    Wait, perhaps it's better to compute the derivative numerically or look up the formula.

    The derivative of GELU is:

    dgelu/dx = gelu(x) + 0.5x * (1 - tanh^2(g(x))) * (derivative of g(x))

    where g(x) = sqrt(2/pi)*(x + 0.044715*x^3)

    Alternatively, perhaps it's easier to compute the derivative numerically or find an expression.

    Alternatively, since GELU's derivative can be expressed in terms of the original function and its components.

    Let me think: The standard GELU is:

    GELU(x) = 0.5 * x * (1 + tanh( sqrt(2/π) (x + 0.044715 x^3) ) )

    Let me denote g(x) = sqrt(2/pi)*(x + 0.044715 x^3)

    So GELU(x) = 0.5 x (1 + tanh(g(x)) )

    Then the derivative is:

    dGELU/dx = 0.5 [ (1 + tanh(g)) + x * [ (1 - tanh(g)^2) * g’(x) * sqrt(2/pi) ) ]

    Wait, let's compute it step by step:

    Let me denote:

    Let’s compute the derivative of GELU(x):

    Let’s let’s define f(x) = 1 + tanh(g(x)), so GELU(x) = 0.5 x f(x)

    Then,

    dGELU/dx = 0.5 [ f(x) + x f’(x) ]

    f’(x) = d/dx [1 + tanh(g(x))] = sech^2(g(x)) * g’(x)

    Since derivative of tanh(u) is sech^2(u) * du/dx.

    Therefore,

    f’(x) = sech²(g(x)) * g’(x)

    So putting together:

    dGELU/dx = 0.5 [ f(x) + x sech²(g(x)) g’(x) ]

    Alternatively, in code, it might be more efficient to compute it numerically.

    Alternatively, perhaps there is an approximation or existing implementation.

    For simplicity, perhaps we can write a helper function for the derivative:

    To compute dgelu_dx(x):

    float g = sqrt(2.0f / M_PI) * (x + 0.044715f * x * x * x);
    float tanh_g = tanhf(g);
    float sech_sq = 1 - tanh_g * tanh_g;
    float dg_dx = sqrt(2.0f / M_PI) * (1 + 0.134145f * x * x); // derivative of (x + 0.044715 x^3) is 1 + 0.134145 x² (since 3*0.044715=0.134145)
    float term1 = 0.5f * (1 + tanh_g);
    float term2 = 0.5f * x * sech_sq * dg_dx;
    return term1 + term2;

    Alternatively, perhaps there is a more optimized way.

    Let me see the derivative formula again.

    Alternatively, perhaps we can find an expression for dgelu_dx in terms of GELU(x) and x:

    Note that GELU(x) = 0.5x(1 + tanh(g(x)))

    Then:

    dgelu/dx = 0.5 [1 + tanh(g)] + 0.5x * derivative of tanh(g) * derivative of g w.r. to x

    = 0.5 (1 + tanh(g)) + 0.5x * sech^2(g) * g'

    So to compute this, we need:

    - Compute g(x)

    - Compute tanh(g)

    - Compute sech^2(g) = 1 - tanh(g)^2

    - Compute g’(x) = sqrt(2/pi)*(1 + 0.134145 x^2)

    Then combine all terms.

    This requires multiple intermediate computations, but it's manageable.

    Therefore, in code, inside the backward kernel, for each element, we can compute x (input / divisor), then compute g(x), tanh_g, sech_sq, g_deriv, then compute the derivative terms.

    Now, let's proceed to write the CUDA kernels.

    First, the forward kernel:

    __global__ void fused_div_gelu_forward(
        const float* input, float divisor, float* output, int size)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx >= size) return;
        float x = input[idx] / divisor;
        // compute GELU(x)
        float g = sqrtf(2.0f / M_PI) * (x + 0.044715f * x * x * x);
        float tanh_g = tanhf(g);
        output[idx] = 0.5f * x * (1.0f + tanh_g);
    }

    The backward kernel:

    __global__ void fused_div_gelu_backward(
        const float* grad_output, const float* input, float divisor,
        float* grad_input, int size)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx >= size) return;
        float x = input[idx] / divisor;
        float g = sqrtf(2.0f / M_PI) * (x + 0.044715f * x * x * x);
        float tanh_g = tanhf(g);
        float sech_sq = 1.0f - tanh_g * tanh_g;
        float g_deriv = sqrtf(2.0f / M_PI) * (1.0f + 0.134145f * x * x);
        float term1 = 0.5f * (1.0f + tanh_g);
        float term2 = 0.5f * x * sech_sq * g_deriv;
        float dgelu_dx = term1 + term2;
        grad_input[idx] = grad_output[idx] * dgelu_dx / divisor;
    }

    Then, in Python, we can write a custom autograd.Function that uses these kernels.

    The Python code would be something like:

    class FusedDivGeluFunction(torch.autograd.Function):
        @staticmethod
        def forward(ctx, input, divisor):
            ctx.save_for_backward(input)
            ctx.divisor = divisor
            # Allocate output
            output = torch.empty_like(input)
            # Launch kernel
            block_size = 256
            grid_size = (input.numel() + block_size - 1) // block_size
            fused_div_gelu_forward[grid_size, block_size](
                input.data_ptr(), divisor, output.data_ptr(), input.numel()
            )
            return output

        @staticmethod
        def backward(ctx, grad_output):
            input, = ctx.saved_tensors
            divisor = ctx.divisor
            grad_input = torch.empty_like(input)
            # Launch backward kernel
            block_size = 256
            grid_size = (input.numel() + block_size - 1) // block_size
            fused_div_gelu_backward[grid_size, block_size](
                grad_output.data_ptr(),
                input.data_ptr(),
                divisor,
                grad_input.data_ptr(),
                input.numel()
            )
            # Since divisor is a scalar and not a parameter (as per original model), no gradient for it.
            return grad_input, None

    Then, in the ModelNew class, we can use this function after the linear layer.

    Therefore, the ModelNew would look like:

    class ModelNew(nn.Module):
        def __init__(self, input_size, output_size, divisor):
            super().__init__()
            self.linear = nn.Linear(input_size, output_size)
            self.divisor = divisor

        def forward(self, x):
            x = self.linear(x)
            return FusedDivGeluFunction.apply(x, self.divisor)

    However, in the original Model, the divisor is a parameter, but in the code it's just stored as an attribute, not a nn.Parameter. Therefore, in the new model, it's still just an attribute.

    Additionally, we need to compile the CUDA kernels.

    To embed them using load_inline, we need to write the CUDA code as strings.

    Let's structure the code as follows.

    First, write the CUDA source code for the forward and backward kernels.

    The CUDA source would be:

    fused_div_gelu_source = """
    #include <torch/extension.h>
    #include <cuda_runtime.h>
    #include <math.h>

    __global__ void fused_div_gelu_forward(
        const float* input, float divisor, float* output, int size)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx >= size) return;
        float x = input[idx] / divisor;
        float g = sqrtf(2.0f / M_PI) * (x + 0.044715f * x * x * x);
        float tanh_g = tanhf(g);
        output[idx] = 0.5f * x * (1.0f + tanh_g);
    }

    __global__ void fused_div_gelu_backward(
        const float* grad_output, const float* input, float divisor,
        float* grad_input, int size)
    {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx >= size) return;
        float x = input[idx] / divisor;
        float g = sqrtf(2.0f / M_PI) * (x + 0.044715f * x * x * x);
        float tanh_g = tanhf(g);
        float sech_sq = 1.0f - tanh_g * tanh_g;
        float g_deriv = sqrtf(2.0f / M_PI) * (1.0f + 0.134145f * x * x);
        float term1 = 0.5f * (1.0f + tanh_g);
        float term2 = 0.5f * x * sech_sq * g_deriv;
        float dgelu_dx = term1 + term2;
        grad_input[idx] = grad_output[idx] * dgelu_dx / divisor;
    }
    """

    Then, the header for the CPP:

    fused_div_gelu_header = """
    void fused_div_gelu_forward(
        const at::Tensor& input, float divisor, at::Tensor& output);
    void fused_div_gelu_backward(
        const at::Tensor& grad_output, const at::Tensor& input, float divisor,
        at::Tensor& grad_input);
    """

    Then, the wrapper functions in C++:

    fused_div_gelu_cpp = """
    void fused_div_gelu_forward(
        const at::Tensor& input, float divisor, at::Tensor& output) {
        const int size = input.numel();
        const int block_size = 256;
        const int grid_size = (size + block_size - 1) / block_size;
        fused_div_gelu_forward<<<grid_size, block_size>>>(
            input.data_ptr<float>(), divisor, output.data_ptr<float>(), size);
    }

    void fused_div_gelu_backward(
        const at::Tensor& grad_output, const at::Tensor& input, float divisor,
        at::Tensor& grad_input) {
        const int size = input.numel();
        const int block_size = 256;
        const int grid_size = (size + block_size - 1) / block_size;
        fused_div_gelu_backward<<<grid_size, block_size>>>(
            grad_output.data_ptr<float>(),
            input.data_ptr<float>(),
            divisor,
            grad_input.data_ptr<float>(),
            size);
    }
    """

    Wait, but in the load_inline function, the functions need to be declared in the cpp_sources. Alternatively, perhaps the functions should be written in the cpp_sources as external functions.

    Alternatively, perhaps the code can be structured as follows.

    The fused_div_gelu_source includes both kernels, and the cpp_sources provides the wrapper functions that call them.

    However, when using load_inline, the functions listed in the functions parameter must be the ones declared in the cpp_sources.

    Let me see the example given:

    In the example, the cpp_sources is "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);"

    and the functions parameter is ["elementwise_add_cuda"]

    So the cpp_sources declares the function signatures, and the CUDA sources implement them.

    Therefore, in our case, we need to have in the cpp_sources declarations for the forward and backward functions.

    Wait, but in our case, the forward and backward are encapsulated in the autograd function, which calls the CUDA kernels via the wrapper functions.

    Alternatively, perhaps the functions should be declared in the cpp_sources as the entry points.

    Let me think:

    To use load_inline, the functions parameter lists the names of the functions to be exposed to Python. Therefore, in our case, we need to have two functions: one for forward and one for backward.

    However, in the autograd.Function, we need to call the forward and backward kernels. Therefore, perhaps the CUDA code should have the kernels and the wrapper functions in C++ that launch them, and then these wrapper functions are exposed to Python via load_inline.

    So the steps are:

    - Write the CUDA kernels (forward and backward) in the CUDA source.

    - Write C++ wrapper functions that launch these kernels, taking tensors as inputs and outputs.

    - Declare these wrapper functions in the cpp_sources.

    - The load_inline will compile everything and expose the wrapper functions to Python.

    Therefore, the code would be structured as follows:

    fused_div_gelu_cuda_source = """
    #include <torch/extension.h>
    #include <cuda_runtime.h>
    #include <math.h>

    __global__ void fused_div_gelu_forward_cuda(
        const float* input, float divisor, float* output, int size)
    {
        // same as before
    }

    __global__ void fused_div_gelu_backward_cuda(
        const float* grad_output, const float* input, float divisor,
        float* grad_input, int size)
    {
        // same as before
    }

    torch::Tensor fused_div_gelu_forward(
        torch::Tensor input, float divisor) {
        auto output = torch::empty_like(input);
        int size = input.numel();
        const int block_size = 256;
        const int grid_size = (size + block_size - 1) / block_size;
        fused_div_gelu_forward_cuda<<<grid_size, block_size>>>(
            input.data_ptr<float>(), divisor, output.data_ptr<float>(), size);
        return output;
    }

    torch::Tensor fused_div_gelu_backward(
        torch::Tensor grad_output, torch::Tensor input, float divisor) {
        auto grad_input = torch::empty_like(input);
        int size = input.numel();
        const int block_size = 256;
        const int grid_size = (size + block_size - 1) / block_size;
        fused_div_gelu_backward_cuda<<<grid_size, block_size>>>(
            grad_output.data_ptr<float>(),
            input.data_ptr<float>(),
            divisor,
            grad_input.data_ptr<float>(),
            size);
        return grad_input;
    }
    """

    Then the cpp sources would declare the function signatures:

    fused_div_gelu_cpp_header = """
    torch::Tensor fused_div_gelu_forward(torch::Tensor input, float divisor);
    torch::Tensor fused_div_gelu_backward(
        torch::Tensor grad_output, torch::Tensor input, float divisor);
    """

    Then, in the Python code:

    fused_div_gelu = load_inline(
        name="fused_div_gelu",
        cuda_sources=fused_div_gelu_cuda_source,
        cpp_sources=fused_div_gelu_cpp_header,
        functions=["fused_div_gelu_forward", "fused_div_gelu_backward"],
        verbose=True
    )

    Then, the custom autograd.Function can call these functions:

    class FusedDivGeluFunction(torch.autograd.Function):
        @staticmethod
        def forward(ctx, input, divisor):
            ctx.save_for_backward(input)
            ctx.divisor = divisor
            return fused_div_gelu.fused_div_gelu_forward(input, divisor)

        @staticmethod
        def backward(ctx, grad_output):
            input, = ctx.saved_tensors
            divisor = ctx.divisor
            grad_input = fused_div_gelu.fused_div_gelu_backward(grad_output, input, divisor)
            return grad_input, None

    This way, the CUDA kernels are properly wrapped and exposed via the load_inline.

    Now, putting all together in code:

    The final ModelNew will have the linear layer, then apply the fused_div_gelu function.

    Also, the get_init_inputs function in the original code passes input_size, output_size, divisor to the model's __init__. Therefore, in the new model, we need to ensure that the __init__ has the same parameters.

    Now, let's write all the code step by step.

    First, the CUDA sources.

    Then, the autograd function.

    Then, the ModelNew class.

    Now, let's write the full code in the required format.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernels for forward and backward passes
fused_div_gelu_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_div_gelu_forward_cuda(
    const float* input, float divisor, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= size) return;
    float x = input[idx] / divisor;
    float g = sqrtf(2.0f / M_PI) * (x + 0.044715f * x * x * x);
    float tanh_g = tanhf(g);
    output[idx] = 0.5f * x * (1.0f + tanh_g);
}

__global__ void fused_div_gelu_backward_cuda(
    const float* grad_output, const float* input, float divisor,
    float* grad_input, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= size) return;
    float x = input[idx] / divisor;
    float g = sqrtf(2.0f / M_PI) * (x + 0.044715f * x * x * x);
    float tanh_g = tanhf(g);
    float sech_sq = 1.0f - tanh_g * tanh_g;
    float g_deriv = sqrtf(2.0f / M_PI) * (1.0f + 0.134145f * x * x);
    float term1 = 0.5f * (1.0f + tanh_g);
    float term2 = 0.5f * x * sech_sq * g_deriv;
    float dgelu_dx = term1 + term2;
    grad_input[idx] = grad_output[idx] * dgelu_dx / divisor;
}

torch::Tensor fused_div_gelu_forward(
    torch::Tensor input, float divisor) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    const int grid_size = (size + block_size - 1) / block_size;
    fused_div_gelu_forward_cuda<<<grid_size, block_size>>>(
        input.data_ptr<float>(), divisor, output.data_ptr<float>(), size);
    return output;
}

torch::Tensor fused_div_gelu_backward(
    torch::Tensor grad_output, torch::Tensor input, float divisor) {
    auto grad_input = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    const int grid_size = (size + block_size - 1) / block_size;
    fused_div_gelu_backward_cuda<<<grid_size, block_size>>>(
        grad_output.data_ptr<float>(),
        input.data_ptr<float>(),
        divisor,
        grad_input.data_ptr<float>(),
        size);
    return grad_input;
}
"""

# Define the C++ function declarations for the CUDA kernels
fused_div_gelu_cpp_header = """
torch::Tensor fused_div_gelu_forward(torch::Tensor input, float divisor);
torch::Tensor fused_div_gelu_backward(
    torch::Tensor grad_output, torch::Tensor input, float divisor);
"""

# Load the CUDA extension
fused_div_gelu = load_inline(
    name="fused_div_gelu",
    cuda_sources=fused_div_gelu_cuda_source,
    cpp_sources=fused_div_gelu_cpp_header,
    functions=["fused_div_gelu_forward", "fused_div_gelu_backward"],
    verbose=True
)

# Custom autograd function to wrap the fused operations
class FusedDivGeluFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, divisor):
        ctx.save_for_backward(input)
        ctx.divisor = divisor
        return fused_div_gelu.fused_div_gelu_forward(input, divisor)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        divisor = ctx.divisor
        grad_input = fused_div_gelu.fused_div_gelu_backward(grad_output, input, divisor)
        return grad_input, None

# Define the optimized model
class ModelNew(nn.Module):
    def __init__(self, input_size, output_size, divisor):
        super().__init__()
        self.linear = nn.Linear(input_size, output_size)
        self.divisor = divisor

    def forward(self, x):
        x = self.linear(x)
        return FusedDivGeluFunction.apply(x, self.divisor)

# The input generation functions remain unchanged
batch_size = 1024
input_size = 8192
output_size = 8192
divisor = 10.0

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, output_size, divisor]
```

```python
# This is the CUDA kernel code for fused_div_gelu_forward_cuda and fused_div_gelu_backward_cuda
# (already included in the previous code block)
```