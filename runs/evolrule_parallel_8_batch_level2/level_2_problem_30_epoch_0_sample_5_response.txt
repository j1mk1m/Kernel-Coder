    You can use the same syntax as the example given to embed custom CUDA kernels in torch. Use torch's extension API for this. You may replace any operators in the forward pass with custom CUDA kernels. You may also choose to combine multiple operators into a single kernel. For example, you could combine matmul + group norm + hardtanh into one fused kernel. Or you could split them into separate kernels. The choice is yours. 

    You are free to write any helper functions or classes. The main requirement is to generate a ModelNew class that is a drop-in replacement for Model. The new model must have the same interface and take the same __init__ parameters. The get_inputs() and get_init_inputs() functions must also be compatible. 

    The following code is a valid starting point, but you can choose to replace any parts of the forward pass with your own code:

    from torch.utils.cpp_extension import load_inline

    class ModelNew(nn.Module):
        def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):
            super().__init__()
            self.gemm = nn.Linear(in_features, out_features)
            self.group_norm = nn.GroupNorm(num_groups, out_features)
            self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)

        def forward(self, x):
            x = self.gemm(x)
            x = self.group_norm(x)
            x = self.hardtanh(x)
            return x

    def get_inputs():
        return [torch.rand(batch_size, in_features).cuda()]

    def get_init_inputs():
        return [in_features, out_features, num_groups, hardtanh_min, hardtanh_max]

    Now, let me see your code!
I need to optimize the given PyTorch model by replacing some or all of its operations with custom CUDA kernels to improve performance. The original model consists of a linear layer (GEMM), followed by group normalization, and then a HardTanh activation. 

First, I'll analyze each component:

1. **Linear Layer (GEMM):** This involves a matrix multiplication between the input tensor and the weight matrix, plus a bias addition. Implementing this with a custom CUDA kernel might offer some speedup, especially if we can fuse it with subsequent operations.
   
2. **Group Normalization:** This operation normalizes the input across groups of channels, which involves calculating means and variances for each group. Doing this in a custom kernel could potentially reduce overhead compared to PyTorch's implementation, especially if fused with other steps.

3. **HardTanh:** A simple element-wise activation function that clamps values between min and max. Implementing this in a kernel is straightforward and might be faster when fused with other operations.

Fusing these operations into a single kernel could minimize memory transfers between the CPU and GPU and between different GPU operations. Since all three operations are element-wise or can be vectorized, combining them into a single kernel might yield better performance.

### Steps to Create a Fused Kernel:
- **Input and Output Tensors:** The input is a tensor of shape (batch_size, in_features). The linear layer transforms this to (batch_size, out_features). The group norm and HardTanh operate on this output.

- **Fusion Plan:** Combine the linear layer's computation (including bias), group normalization, and HardTanh into a single CUDA kernel. However, the linear layer involves a matrix multiplication, which is not element-wise, so this complicates fusion. Alternatively, we can implement each operation in separate kernels and see if that's beneficial, or try to fuse parts of them.

Wait, the linear layer (GEMM) is a matrix multiplication followed by a bias addition. The group norm is a more complex operation involving per-group mean and variance. Fusing all three might be too complex, but perhaps fusing the bias addition and group norm with the HardTanh is feasible.

Alternatively, perhaps we can implement the linear layer (matmul + bias) in a kernel, then group norm, then HardTanh, each as separate kernels but optimized individually. Let me consider each step.

### Option 1: Implement GEMM with bias as a custom kernel
The standard linear layer uses a GEMM (matrix multiplication) plus adding a bias. PyTorch's implementation is highly optimized, but perhaps in some cases (like specific tensor sizes), a custom kernel could be faster. However, for large matrices like 8192x8192, it's unlikely, so maybe better to keep the PyTorch Linear layer.

### Option 2: Implement GroupNorm in a custom kernel
GroupNorm involves splitting the channels into groups, computing mean and variance for each group, normalizing, and applying scale and shift. This might have some overhead in PyTorch's implementation. Writing a custom kernel could optimize the per-group calculations and reduce overhead.

### Option 3: Implement HardTanh as a custom kernel
HardTanh is a simple element-wise operation. PyTorch's implementation is already efficient, but a custom kernel might have lower overhead, especially if fused with other element-wise operations.

### Option 4: Fusion of GroupNorm and HardTanh
Since both are element-wise (after the per-group computations), perhaps combining them into a single kernel could reduce memory access and improve performance.

### Plan:
Since the linear layer's GEMM is likely already well-optimized, we might focus on GroupNorm and HardTanh. Let's see if we can combine these into a single kernel.

However, the GroupNorm requires intermediate steps (computing means and variances), so it's not purely element-wise. Therefore, perhaps the best approach is to implement GroupNorm and HardTanh in separate optimized kernels.

Alternatively, maybe we can combine the bias addition from the linear layer with the group norm and HardTanh into a single kernel. Let's think about that.

The linear layer computes:

output_linear = input @ weight.T + bias

Then group norm:

group norm applies to the output_linear tensor.

Then HardTanh.

So the operations after the linear layer are element-wise (group norm involves per-channel or per-group computations, but still can be parallelized).

Wait, group norm calculation:

For each group of channels:

Compute mean and variance over the group dimensions (batch and spatial dimensions, but in this case, since the input is 2D (batch_size, features), the group norm is along the batch dimension? Wait, group norm groups the features into groups. Let me recall:

GroupNorm divides the channels into groups and computes within each group the mean and variance across the batch and spatial dimensions. But in this case, the input is 2D (batch, features), so for each group, the mean and variance are computed over the batch dimension?

Wait, no. The group norm is applied over the feature dimension. Let me check:

The input shape is (N, C), where C is the number of channels (out_features). The group norm splits the C channels into G groups. For each group, the mean and variance are computed over the N samples (batch) and the spatial dimensions (but here, there are none). So for each group, the mean and variance are computed over the batch dimension (N), and the group's channels.

Wait, actually, according to the PyTorch documentation, GroupNorm computes the mean and variance for each channel separately? Or per group?

No, group norm computes the mean and variance across the entire group of channels for each sample. Wait, let me think again:

The GroupNorm formula: For each group g, compute the mean and variance over all elements in the group for each sample. Then normalize each element in the group using these statistics.

The input shape is (N, C, ...), but in our case, it's (N, C). So for each group of channels (divided into G groups), the mean and variance are computed over the channels in the group and across the batch? Wait, no: the mean and variance are computed for each group across the batch and the spatial dimensions (if any). Since in our case, there's no spatial dimension, so for each group, the mean and variance are computed over the batch dimension.

Wait, actually, according to the PyTorch documentation for GroupNorm:

The mean and variance for a given channel are computed based on the values of all the channels in its group.

Wait, here's the exact description from PyTorch's GroupNorm documentation:

Group Normalization divides the channels into groups and computes within each group the mean and variance for normalization. Formula:

y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

The expectation and variance are calculated per group:

E[x] = \frac{1}{C / G * H * W} \sum_{i=1}^{C / G} \sum_{j=1}^{H} \sum_{k=1}^{W} x_{i,j,k}

Var[x] = \frac{1}{C / G * H * W} \sum_{i=1}^{C / G} \sum_{j=1}^{H} \sum_{k=1}^{W} (x_{i,j,k} - E[x])^2

In our case, since the input is (N, C), there are no spatial dimensions (H and W are 1). So for each group (dividing C into G groups), the mean and variance are computed across the batch dimension (N) and the channels in the group. Wait, actually, the dimensions over which the mean and variance are computed are the batch dimension and the spatial dimensions. Since there are no spatial dimensions here, it's just the batch dimension and the group's channels.

Wait, no: the mean and variance are computed over the batch and the group's channels. So for each group, you have (N, C/G) channels. The mean and variance are computed across all elements in that group for each sample? Or across all samples?

Wait the formula says the expectation is over the group's channels and the spatial dimensions. Since there are no spatial dimensions here, it's over the group's channels and the batch dimension. Wait, no, the batch dimension is N, so the group's channels are part of the C dimension. The expectation and variance are calculated per group, over the channels in the group and across the batch.

Wait, actually, the expectation is over the feature channels within the group and across the batch? Or is it over the batch and the spatial dimensions?

Looking at the PyTorch documentation example, for a 4D input (N, C, H, W), the group norm is applied by grouping the C channels into G groups, then for each group, the mean and variance are computed over the H*W*N elements (i.e., across the batch and spatial dimensions, but not the channels within the group). Wait, no:

Wait, according to the formula, the expectation is over the dimensions except the channel dimension. Wait, the input is (N, C, H, W). The group splits C into G groups. For each group, you compute the mean over the N, H, W, and the channels in the group? Wait, no:

Wait, in the formula, the expectation is over the elements within each group. The group has C/G channels. For each group, the expectation is taken over the N, H, W dimensions for each of those C/G channels. Wait, actually, the expectation is over the entire group's elements across all spatial and batch dimensions.

Wait, the formula shows that for each channel in the group, you compute the mean over all elements in that channel, but no, it's actually over all elements in the group across all channels in the group and all spatial and batch dimensions.

Wait, perhaps an example: For a group of channels 0-3, and a batch of N=2 samples, each of size (C=4, H=1, W=1), then for each sample, the group's channels are 0-3. The mean would be (x[0,0] + x[0,1] + x[0,2] + x[0,3]) /4 for sample 0? Or over all samples?

Wait the formula says the expectation is computed over the entire group, which includes all samples. Because the formula uses all N samples.

Wait the formula uses the sum over i (channels in group?), but perhaps the actual implementation is:

The expectation and variance are computed over the entire group's elements across the batch and spatial dimensions. So for a group of channels, the mean is computed as the mean of all elements in those channels across all samples and spatial positions (if any).

In our case, since the input is (N, C), and the group is divided into G groups (each of C/G channels), the mean and variance for each group are computed over all N samples and the C/G channels. Wait, no: the group is divided into groups along the channel dimension. For each group of channels, the mean and variance are computed across the batch dimension (N) and the channels in the group.

Wait, let me think numerically:

Suppose input is shape (N=1024, C=8192), and G=16 groups. Each group has C/G = 512 channels.

For each group (group 0 has channels 0-511, group 1 has 512-1023, etc.), the mean and variance are computed over all the elements in those 512 channels across all 1024 samples. So for each group:

mean = (sum over all samples (n=0 to 1023) and all channels (c=0 to 511) of x[n][c]) / (1024 * 512)

Similarly for variance.

Wait, that's how the formula seems to indicate. So the mean and variance are computed across the batch and the group's channels. Then, each element in the group is normalized using this mean and variance.

Therefore, the normalization is done per group, with statistics computed across the batch and the group's channels.

This computation involves:

1. For each group:

   a. Compute the sum over all elements in the group (summing over batch and group's channels).

   b. Compute the mean (sum / (N * (C/G)))

   c. Compute the sum of squared deviations from the mean to get variance.

   d. Variance is sum_sq / (N * (C/G))

   e. Then normalize each element in the group using these statistics.

This requires:

- Reduction operations (sum) over the batch and channels in the group.

Implementing this in a custom CUDA kernel could be beneficial if PyTorch's implementation has overhead, but given that PyTorch's GroupNorm is optimized, it's unclear. However, fusing with subsequent operations might be better.

Alternatively, perhaps fusing the group norm and HardTanh into a single kernel could save time.

Let me consider the steps after the linear layer:

After the linear layer's output x (shape N x out_features), we do group norm, then HardTanh.

The group norm requires:

For each element in the output:

x_normalized = (x - mean) / sqrt(var + eps) * gamma + beta

Then HardTanh clamps x_normalized between min and max.

But the group norm's gamma and beta are learnable parameters (affine=True). The default in PyTorch's GroupNorm is affine=True.

Wait, the original model uses nn.GroupNorm, which by default has affine parameters. So in the model's __init__, the group norm is initialized with affine parameters (scale and shift).

Therefore, the group norm has parameters: weight (gamma) and bias (beta).

Thus, the group norm is:

x = (x - mean) / sqrt(var + eps) * gamma + beta

Then HardTanh(x).

So combining group norm and HardTanh into a single kernel is possible, but requires the mean, var, gamma, and beta.

However, the mean and variance depend on the entire group, so they must be computed first, then applied element-wise.

The problem is that the computation of mean and variance requires a reduction across the group's elements. Thus, the kernel would need to first compute the mean and variance for each group, then apply the normalization and HardTanh.

This can be done in a single kernel but requires synchronization between threads to compute the sums for each group. Alternatively, we can split it into two steps: first compute the stats (mean and var) in a kernel, then apply normalization and HardTanh in another kernel.

But this might involve multiple kernel launches, which can have overhead.

Alternatively, we can implement the group norm and HardTanh in a single kernel with two passes:

- First pass: compute the sum and sum of squares for each group.

- Second pass: compute mean/var, normalize, and apply HardTanh.

However, in CUDA, this would require atomic operations or shared memory to accumulate sums, which can be tricky for large groups.

Alternatively, we can use a two-step approach:

1. Compute the mean and variance for each group.

2. Apply normalization and HardTanh.

This can be done in two separate kernels.

Alternatively, use PyTorch's implementation for group norm, which is already optimized, but then apply the HardTanh in a custom kernel.

Let me think of possible options:

Option A: Implement the HardTanh as a custom kernel. Since it's element-wise, this might save a little time over PyTorch's implementation.

Option B: Implement GroupNorm and HardTanh in a single custom kernel to reduce overhead.

Option C: Implement the entire forward pass (linear, group norm, hardtanh) in a single fused kernel. However, the linear layer involves a matrix multiply, which is not element-wise. So this would require implementing a GEMM with bias, group norm, and HardTanh in a single kernel, which is quite complex.

Given the time constraints and complexity, perhaps starting with fusing the group norm and HardTanh is more feasible.

Let me proceed with Option B.

### Step-by-Step Plan:

1. **Implement a fused GroupNorm + HardTanh kernel:**

   The kernel will:

   a. For each group:

      i. Compute the mean and variance over the group's elements across the batch.

      ii. Normalize using the computed stats.

      iii. Apply HardTanh.

   b. The kernel must handle the group divisions.

   However, the problem is that calculating mean and variance requires a reduction over the group's elements, which are spread across the batch and channels. To compute this in parallel, we can use a kernel that first computes partial sums for each group, then combines them.

   Alternatively, we can use a kernel that first computes the mean and variance in a separate step, then applies the normalization and HardTanh in another step.

   Let's break it down into two steps:

   Step 1: Compute the mean and variance for each group.

   Step 2: Normalize and apply HardTanh.

   The first step can be done with a kernel that accumulates sums and squares.

   Let's see:

   For each element in the input tensor:

   - Determine which group it belongs to.

   - Accumulate its value and squared value into the group's sum and sum_sq.

   Since this is a reduction over the group's elements, we can use a kernel that uses atomic adds or uses thread blocks per group.

   Alternatively, we can use a kernel where each thread processes a single element, and for each group, we use a separate thread block to accumulate the sums.

   Let me think of the approach:

   Suppose we have G groups. For each group:

   - The group has C/G channels. The total elements per group per sample is (C/G). Since the input is (N, C), the total elements per group across all samples is N * (C/G).

   To compute the mean and variance for each group:

   For each group g:

   sum = sum_{n=0}^{N-1} sum_{c in group g} x[n][c]

   sum_sq = sum_{n=0}^{N-1} sum_{c in group g} x[n][c]^2

   Then mean = sum / (N * (C/G))

   var = (sum_sq - sum^2 / (N*(C/G))) / (N*(C/G))

   (This uses the variance formula: E[x^2] - (E[x])^2 )

   So to compute these sums efficiently, perhaps:

   We can launch a kernel where each thread processes one element of the input tensor. For each element (n, c), determine the group g that c belongs to. Then, each thread adds its x value to the group's sum and x squared to sum_sq.

   To do this, we can have an array of sum and sum_sq per group, stored in global memory, and use atomicAdd operations to accumulate. However, atomicAdd can be slow for large N and G.

   Alternatively, use a reduction approach with shared memory:

   For each group, assign a block of threads to process all elements in that group. Each block for group g will process all N * (C/G) elements in that group, and use shared memory to accumulate partial sums, then write the total to global memory.

   This might be more efficient.

   Let's outline this:

   The kernel for step 1 (compute sums):

   - Each thread block is responsible for a single group.

   - The number of blocks is equal to the number of groups (G).

   - Each block processes all elements in its group.

   - The elements in a group are N samples, each with (C/G) channels.

   The block will have a grid of threads. For example, if the group has M elements (M = N * (C/G)), the block can have M threads, each processing one element.

   However, for large M, this might exceed the maximum number of threads per block (1024 or 2048). So we need to handle this by having multiple threads per element?

   Alternatively, use a 1D grid where each block handles a group, and threads in the block process chunks of the group's elements.

   Let me think through the code structure:

   Letâ€™s suppose that for each group g:

   - The group has C_per_group = out_features // num_groups channels.

   The total elements in the group across all samples is N * C_per_group.

   To compute the sum and sum_sq for the group:

   The kernel for this step would look something like:

   __global__ void compute_group_stats(
       const float* x,
       float* sums,
       float* sum_squares,
       int batch_size,
       int out_features,
       int num_groups) {

       int g = blockIdx.x;
       int c_start = g * (out_features / num_groups);
       int c_end = (g+1) * (out_features / num_groups);

       // Each thread in the block handles a sample and a channel in the group?
       // Or process all elements in the group.

       // Need to process all elements in the group (all N samples, all channels from c_start to c_end-1)

       // Each thread can process a chunk of elements.

       int total_elements = batch_size * (c_end - c_start);

       int tid = threadIdx.x + blockIdx.x * blockDim.x;
       // Not sure, need better approach.

       // Alternatively, use a loop over all samples and channels in the group.

       // But this may be inefficient.

       // Maybe better to have each thread process a single element.

       // Since this is a kernel per group, with multiple threads per block.

       // The block processes all elements in the group.

       // For simplicity, let's have each thread process one element:

       // total_elements = N * (C_per_group)

       // Each thread in the block can process one element.

       // The number of threads per block must be >= total_elements.

       // But that might be too many threads. So perhaps use a grid of threads per block.

       // Alternatively, use a grid where each thread handles an index in the group.

       // For each thread in the block:

       int index = threadIdx.x + blockIdx.x * blockDim.x;

       if (index < total_elements) {

           // Compute the corresponding n and c in the group.

           int c_in_group = index % (c_end - c_start);

           int n = index / (c_end - c_start);

           int c = c_start + c_in_group;

           float val = x[n * out_features + c];

           atomicAdd(&sums[g], val);

           atomicAdd(&sum_squares[g], val * val);

       }

   }

   This uses atomicAdd, which could be slow for large N and G. However, since the group sizes are large (8192/16=512 channels, batch 1024), the total elements per group is 1024 * 512 = 524,288 elements. For 16 groups, this is 8,388,608 elements. Using atomicAdd for each could be slow. So this approach may not be efficient.

   An alternative approach is to use a reduction with shared memory.

   Let me think of another approach:

   Each block is assigned to a group. The block will process all elements of that group.

   The block will have a number of threads, say 256. The elements in the group can be divided among the threads.

   Each thread loads a chunk of elements from the group, computes their partial sum and sum_squares, then writes to shared memory, and then does a reduction in shared memory.

   Here's a sketch:

   __global__ void compute_group_stats(
       const float* x,
       float* sums,
       float* sum_squares,
       int batch_size,
       int out_features,
       int num_groups) {

       extern __shared__ float shared[];

       int g = blockIdx.x;

       int c_start = g * (out_features / num_groups);
       int c_end = (g+1) * (out_features / num_groups);
       int C_per_group = c_end - c_start;

       int total_elements = batch_size * C_per_group;

       // Each thread in the block processes some elements.

       int tid = threadIdx.x;

       // Allocate shared memory for partial sums.
       int smem_offset = threadIdx.x * 2;
       float* s_sum = &shared[smem_offset];
       float* s_sumsq = &shared[smem_offset + 1];

       // Initialize to zero.
       s_sum[0] = 0.0f;
       s_sumsq[0] = 0.0f;

       __syncthreads();

       // Each thread processes a chunk of elements.
       for (int idx = tid; idx < total_elements; idx += blockDim.x) {

           int n = idx / C_per_group;
           int c_in_group = idx % C_per_group;
           int c = c_start + c_in_group;

           float val = x[n * out_features + c];

           s_sum[0] += val;
           s_sumsq[0] += val * val;
       }

       __syncthreads();

       // Perform reduction in shared memory.

       // Use a parallel reduction for the sums.

       for (int s=blockDim.x/2; s>0; s>>=1) {
           if (tid < s) {
               s_sum[0] += s_sum[tid + s];
               s_sumsq[0] += s_sumsq[tid + s];
           }
           __syncthreads();
       }

       if (tid == 0) {
           sums[g] = s_sum[0];
           sum_squares[g] = s_sumsq[0];
       }
   }

   The shared memory allocation would need to be 2 * blockDim.x floats. So for a block of 256 threads, 512 floats (2048 bytes).

   The total_elements is 1024 * 512 = 524,288 per group, so each thread in the block would process 524,288 / 256 = ~2048 elements each. That might be feasible, but the loop over all elements could be slow.

   Alternatively, we can process in chunks with coalesced memory accesses.

   The main point is that this approach avoids atomic operations and uses shared memory reduction, which is more efficient.

   Once the sums and sum_squares are computed, the mean and var can be calculated per group.

   Then, in a second kernel, we can apply the normalization and HardTanh.

   Let's structure the code:

   First, we need to compute the means and variances for each group:

   - Allocate tensors for sums, sum_squares, means, vars.

   Then, run the compute_group_stats kernel to fill sums and sum_squares.

   Then compute the mean and var for each group using those sums.

   The mean for group g is sums[g] / (batch_size * C_per_group).

   The variance is (sum_squares[g] - (sums[g]^2)/(batch_size*C_per_group)) / (batch_size*C_per_group).

   Once we have the means and vars, we can proceed to normalize each element and apply the HardTanh.

   The normalization and HardTanh can be done in a separate kernel.

   Here's the plan for the second kernel:

   For each element (n, c):

   - Determine its group g.

   - Compute the group's mean, variance.

   - Compute normalized_val = (x[n][c] - mean) / sqrt(var + eps).

   - Apply the learned gamma and beta (from GroupNorm parameters).

   - Apply HardTanh.

   However, the gamma and beta parameters are per channel, not per group. Wait, GroupNorm's affine parameters are per channel, right?

   Wait, no. In PyTorch's GroupNorm, the affine parameters (gamma and beta) are of size C (same as the number of channels). Each channel has its own gamma and beta, but grouped into groups for the normalization.

   Therefore, for each channel c in group g:

   normalized_val = (x[n][c] - mean_g) / sqrt(var_g + eps) * gamma[c] + beta[c]

   So, the gamma and beta are per channel, not per group.

   Therefore, in the normalization step, each element needs access to the gamma and beta of its channel.

   This adds more complexity, as the gamma and beta are parameters stored in the model.

   To implement this in a custom kernel, we need to pass the gamma and beta tensors as inputs.

   Thus, the steps for the fused kernel:

   1. Compute sums and sum_squares for each group (first kernel).

   2. Compute mean and var for each group on the CPU (since it's a simple calculation using the sums).

   3. Launch the normalization and HardTanh kernel, which uses these computed stats, along with gamma and beta.

   Wait, but moving the mean and var to the CPU and back could be time-consuming. Alternatively, compute the mean and var on the device.

   Let's see:

   After computing sums and sum_squares on the device, we can compute the means and vars in a separate kernel or in the normalization kernel.

   Alternatively, in the second kernel (normalization), for each group g:

   mean_g = sums[g] / (N * C_per_group)

   var_g = (sum_squares[g] - mean_g * sums[g]) / (N * C_per_group)

   Wait, variance is (E[x^2] - (E[x])^2). Since E[x^2] = sum_squares[g]/(N*C_per_group)

   So variance = (sum_squares[g]/(N*C_per_group)) - (mean_g)^2

   So, the variance can be computed per group on the fly in the kernel.

   Therefore, the normalization kernel can:

   For each element (n, c):

   a. Find its group g.

   b. Compute mean_g and var_g using sums and sum_squares.

   c. Compute normalized_val = (x[n][c] - mean_g) / sqrt(var_g + eps)

   d. Multiply by gamma[c], add beta[c].

   e. Apply HardTanh.

   But this requires that for each element, we can compute the mean and var for its group.

   To do this efficiently, we can precompute the means and variances in shared memory or global memory.

   Let me think of the normalization kernel:

   The kernel would need to:

   For each element:

       g = (c) / (C_per_group)

       C_per_group = out_features / num_groups

       mean_g = sums[g] / (batch_size * C_per_group)

       var_g = (sum_squares[g] - mean_g * batch_size * C_per_group * mean_g) / (batch_size * C_per_group)

       (since sum_squares[g] = sum x^2, so E[x^2] = sum_squares[g]/(N*C_per_group)

       variance = E[x^2] - (E[x])^2 = (sum_squares[g]/(N*C_per_group)) - (mean_g)^2

       So var_g = (sum_squares[g] - (sums[g]^2)/(N*C_per_group)) / (N*C_per_group)

       Wait, maybe it's better to compute variance as:

       variance = (sum_squares[g] - (sums[g] * sums[g])/(N*C_per_group)) / (N*C_per_group)

       So yes.

       So in code:

       mean_g = sums[g] / (batch_size * C_per_group)

       var_g = (sum_squares[g] - mean_g * sums[g]) / (batch_size * C_per_group)

       (since sums[g] = mean_g * (batch_size * C_per_group))

       Wait:

       mean_g = sums[g]/(N*Cg), so sums[g] = mean_g * N*Cg.

       So var_g = (sum_squares[g] - mean_g^2 * N*Cg ) / (N*Cg)

       Yes.

       So in code:

       float mean_g = sums[g] / denom;

       float var_g = (sum_squares[g] - mean_g * sums[g]) / denom;

       where denom = batch_size * C_per_group.

       Then compute the normalized value.

   Thus, the normalization kernel can compute the mean and var for the group on the fly.

   Now, the parameters gamma and beta are per channel. So for each channel c:

       gamma_val = gamma[c]

       beta_val = beta[c]

       So the kernel needs access to the gamma and beta tensors.

   Putting it all together, the kernel would look like:

   __global__ void group_norm_hardtanh(
       const float* x,
       float* y,
       const float* sums,
       const float* sum_squares,
       const float* gamma,
       const float* beta,
       float min_val,
       float max_val,
       int batch_size,
       int out_features,
       int num_groups,
       float eps) {

       int idx = blockIdx.x * blockDim.x + threadIdx.x;

       if (idx >= batch_size * out_features) return;

       int n = idx / out_features;

       int c = idx % out_features;

       int C_per_group = out_features / num_groups;

       int g = c / C_per_group;

       int denom = batch_size * C_per_group;

       float mean_g = sums[g] / denom;

       float var_g = (sum_squares[g] - mean_g * sums[g]) / denom;

       float std = rsqrtf(var_g + eps);

       float x_val = x[idx];

       float normalized = (x_val - mean_g) * std;

       normalized = normalized * gamma[c] + beta[c];

       y[idx] = __fmaxf(__fminf(normalized, max_val), min_val);

   }

   This kernel can process each element independently, so it's parallelizable.

   The steps for the fused kernel approach would be:

   1. Compute the sums and sum_squares for each group using the first kernel.

   2. Launch the normalization and HardTanh kernel.

   The parameters gamma and beta are stored in the GroupNorm module's parameters.

   Therefore, in the ModelNew class, we need to access these parameters.

   Now, putting this into code.

   Also, note that the original model's group norm has parameters (gamma and beta). Thus, in the ModelNew, we must retain those parameters.

   So the ModelNew class will still have a self.group_norm, but instead of using it, it will use the parameters (gamma and beta) in the custom kernel.

   Alternatively, we can replace the group norm with parameters directly.

   Let's think:

   The original model has a group norm layer with parameters:

       self.group_norm = nn.GroupNorm(num_groups, out_features)

   So the parameters are self.group_norm.weight and self.group_norm.bias.

   Therefore, in the new model, we can keep the group norm layer, but not use it, instead using its parameters in the custom kernels.

   Alternatively, we can store the gamma and beta as separate parameters.

   To avoid changes to the interface, the ModelNew must accept the same __init__ parameters as Model.

   Therefore, in the ModelNew's __init__:

   class ModelNew(nn.Module):
       def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):
           super().__init__()
           self.gemm = nn.Linear(in_features, out_features)
           self.group_norm = nn.GroupNorm(num_groups, out_features)
           self.hardtanh_min = hardtanh_min
           self.hardtanh_max = hardtanh_max

   So the parameters are still present in the group_norm layer.

   Now, in the forward pass, instead of calling self.group_norm(x), we will compute the group norm using our custom kernels.

   The steps for the forward pass in ModelNew would be:

   1. Compute the linear layer's output: x_linear = self.gemm(x)

   2. Compute group norm and HardTanh via custom kernels.

   The group norm and HardTanh steps would involve:

   a. Compute sums and sum_squares for each group using the first kernel.

   b. Run the normalization and HardTanh kernel