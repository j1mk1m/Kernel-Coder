The code should follow the same API as the original code (i.e. the constructor should take the same arguments, and forward function should have the same inputs and outputs). 

Additionally, you can create helper functions or submodules but they should be defined within the code submission. 

The only way to improve performance is by replacing operators with custom CUDA kernels. You may also consider operator fusion opportunities (combining multiple operators into a single kernel). 

The final code must be self-contained and not rely on external files. 

You may assume that the input dimensions are fixed as per the parameters given. 

You may use PyTorch's extension APIs (e.g., torch.utils.cpp_extension) to inline CUDA code in the Python file.




Okay, so I need to optimize the given Model class which uses a 3D transposed convolution followed by Softmax and Sigmoid activations. The goal is to replace some operators with custom CUDA kernels for speedups. Let me think through how to approach this.

First, I should look at the operators in the forward pass: ConvTranspose3d, Softmax, and Sigmoid. The main candidate for optimization here is probably the convolution layer since it's computationally heavy. However, writing a custom 3D transposed convolution kernel from scratch might be quite involved. Alternatively, maybe fusing the activation functions with the convolution could save some computation steps and memory transfers.

Wait, the problem mentions operator fusion opportunities, like combining matmul + ReLU. Here, maybe I can fuse the ConvTranspose3d with Softmax and Sigmoid? But Softmax and Sigmoid are element-wise operations. Let me see the sequence: after the convolution, the output goes through softmax and then sigmoid. Since both are element-wise, perhaps they can be combined into a single kernel.

Alternatively, maybe fusing the convolution with the softmax? Hmm, but the convolution is a much more complex operation. The element-wise functions might be easier to fuse. Let me think about the steps:

Original steps:
1. ConvTranspose3d: This involves a lot of computations, including padding, im2col, matrix multiplication, etc.
2. Softmax: Applied over dimension 1 (the channels)
3. Sigmoid: Applied element-wise.

The softmax and sigmoid are both element-wise, so maybe they can be combined into a single kernel. Let me think about the math:

Softmax(x)_i = exp(x_i) / sum(exp(x_j)) over the channel dimension.

Then Sigmoid(softmax_output) would be 1/(1 + exp(-softmax_output)). But combining these might not lead to any simplification. Alternatively, perhaps we can compute the combined function more efficiently.

Wait, perhaps the Sigmoid after Softmax can be replaced with a different activation? No, the problem states that the output must follow the same API, so the order must stay the same. So the outputs must first go through Softmax then Sigmoid.

Alternatively, maybe the convolution can be optimized by using a custom kernel that's tuned for the given parameters. Since the problem allows replacing operators, perhaps replacing the ConvTranspose3d with a custom CUDA kernel could give a speedup. But writing a 3D transposed convolution from scratch is quite complex. Maybe there's a better way?

Alternatively, since the input dimensions are fixed (as per the problem statement), maybe we can exploit the specific parameters (e.g., stride=2, kernel_size=3, padding=1, output_padding=1) to optimize the convolution. But even so, writing a custom kernel for that might be time-consuming.

Another thought: the Softmax and Sigmoid are both element-wise, so combining them into a single kernel could reduce the number of memory accesses and kernel launches, which is a common optimization. Let me think about that.

The Softmax is applied along the channel dimension. So for each spatial position (D, H, W), across the channels (out_channels), compute the softmax. Then, apply the sigmoid to each element.

Wait, the Sigmoid is applied element-wise, so after softmax, each element is passed through the sigmoid. So the combined function would be:

y = sigmoid(softmax(x))

But the softmax is along channels, so each element in the channel dimension is part of the softmax computation. However, the sigmoid is applied to each element individually. Therefore, these can't be combined into a single mathematical expression, but they can be computed in a single kernel by first computing the softmax for each channel, then applying the sigmoid to each element.

Therefore, the plan could be:

- Keep the convolution as it is (since replacing it might be too complex), but fuse the Softmax and Sigmoid into a single kernel. This would reduce the number of kernel launches and memory copies between the two operations.

Alternatively, perhaps even the convolution can be fused with the softmax and sigmoid? Unlikely, since the convolution is a spatial operation. But maybe not, since the convolution is the main compute-heavy part. Let me check the computational cost:

The 3D transposed convolution (deconvolution) has a high computational cost, especially with the given parameters. The Softmax and Sigmoid are relatively lightweight. So replacing the convolution with a custom kernel might have a bigger impact. But writing a custom 3D transposed convolution is non-trivial. Let me see if there's a way to do it.

Alternatively, maybe use a different algorithm for the convolution, but the problem mentions algorithmic changes are allowed. However, the user might not want to change the algorithm unless it's equivalent. So perhaps a better approach is to focus on fusing the two activations.

Let me outline steps for fusing Softmax and Sigmoid:

First, the forward pass would be:

x = conv_transpose(x)
x = softmax(x)
x = sigmoid(x)

By fusing the last two steps, the fused kernel would compute the softmax followed by sigmoid in one pass.

So, the fused kernel would take the output of the convolution, compute the softmax along the channel dimension, then apply the sigmoid to each element, and return the result. This would save the intermediate softmax result storage and the time to launch two kernels.

Now, let's think about how to implement that.

The Softmax requires, for each position (d, h, w), computing the exponential of each channel, sum them, then divide each exponential by the sum. Then apply the sigmoid. The sigmoid is 1/(1 + exp(-x)).

Alternatively, perhaps we can combine the two steps:

Let me denote the intermediate variable after convolution as z. Then,

softmax_z = exp(z) / sum(exp(z), dim=1)

Then, the result is 1 / (1 + exp(-softmax_z)).

But that's two element-wise operations. However, maybe we can compute this in a way that combines the two steps. Let's see:

Let me compute for each element:

result = 1 / (1 + exp( - (exp(z_i) / sum(exp(z)))) )

Hmm, not sure if that's helpful. Alternatively, perhaps the order can be swapped? But the problem requires applying softmax first then sigmoid.

Alternatively, perhaps there's a way to compute the combination more efficiently. Let me think numerically:

Let me denote s = softmax(z) = exp(z_i) / sum(exp(z_j)), then sigmoid(s) = 1/(1 + exp(-s)).

So, the combined function is:

y_i = 1 / (1 + exp( - (exp(z_i) / sum(exp(z_j))) ) )

But this doesn't simplify much. So perhaps the best way is to compute the softmax first, then apply the sigmoid. But in a single kernel.

Therefore, the fused kernel would:

1. For each spatial position (d, h, w):

   a. Compute the sum of exp(z_i) over all channels (i=0..out_channels-1)

   b. For each channel i:

      i. Compute exp(z_i)

      ii. Divide by the sum to get softmax_z_i

      iii. Compute sigmoid(softmax_z_i) = 1/(1 + exp(-softmax_z_i))

So, the kernel would need to process each spatial position and channel, compute the exponential, accumulate the sum for the channel dimension, then compute the result.

The challenge here is that the sum over channels must be computed for each spatial position. To do this efficiently in CUDA, we can structure the threads so that each thread block handles a spatial position (d, h, w), and the threads in the block handle the channels. This way, the sum can be computed with a reduction within the block.

Alternatively, each thread can process a single element, but then the sum over channels would require synchronization across threads handling different channels for the same spatial position.

Hmm. Let me think of the kernel structure.

Suppose that for each spatial position (d, h, w), the number of channels is out_channels (64 in the given parameters). Let's say each thread block is responsible for a spatial position. The block size would be at least the number of channels, but that might be too big (64 threads). Alternatively, use multiple threads per channel and manage with shared memory.

Wait, perhaps a better way is:

Each thread processes a single element (i.e., a channel and spatial position). But for the sum over channels, we need to compute for each spatial position the sum of all channels. To do this, each thread can compute exp(z_i) for its channel, and then use a block reduction (since all threads in a block are handling the same spatial position) to compute the sum. Then, each thread can compute the softmax and apply the sigmoid.

Let me outline the steps for the fused kernel:

- Each thread block is assigned to a spatial position (d, h, w). The block size could be 64 threads (since there are 64 channels). Each thread in the block handles one channel.

- For each spatial position:

   1. Each thread (representing a channel) computes exp(z_i).

   2. Compute the sum of all exp(z_i) across channels in the block (using a reduction). This sum is stored in shared memory.

   3. Each thread then divides its exp(z_i) by the sum to get the softmax value.

   4. Apply the sigmoid to the softmax value to get the final result.

This approach would require shared memory for the reduction. The steps would look like:

Kernel code outline:

__global__ void fused_softmax_sigmoid_kernel(float* input, float* output, int batch_size, int channels, int D, int H, int W) {
    // Determine the spatial position (d, h, w) and channel index
    int batch = blockIdx.x / (D*H*W); // assuming blockIdx.x indexes over batch and spatial positions
    int pos = blockIdx.x % (D*H*W);
    int d = pos / (H*W);
    int hw = pos % (H*W);
    int h = hw / W;
    int w = hw % W;
    int channel = threadIdx.x;

    // Load input data for this channel and spatial position
    float z = input[get_index(batch, channel, d, h, w)];

    float exp_z = expf(z);

    // Shared memory for partial sums
    __shared__ float partial_sums[64]; // or some other size, but maybe 32?

    // Compute sum of exp_z across all channels in the block
    // Each thread contributes exp_z
    // Use a reduction within the block

    // Let's assume channels are 64, so block size 64 threads.
    // We can do a binary reduction here.
    // First, each thread writes its exp_z to shared memory.
    if (threadIdx.x < channels) {
        partial_sums[threadIdx.x] = exp_z;
    } else {
        partial_sums[threadIdx.x] = 0.0f;
    }
    __syncthreads();

    // Perform reduction in shared memory
    for (int stride = channels/2; stride > 0; stride >>=1) {
        if (threadIdx.x < stride) {
            partial_sums[threadIdx.x] += partial_sums[threadIdx.x + stride];
        }
        __syncthreads();
    }

    float sum_exp = partial_sums[0]; // the total sum is in thread 0's slot

    __syncthreads();

    // Compute softmax and sigmoid
    float softmax_val = exp_z / sum_exp;
    float sigmoid_val = 1.0f / (1.0f + expf(-softmax_val));

    // Write the result to output
    output[get_index(batch, channel, d, h, w)] = sigmoid_val;
}

Wait, but how to handle the indices correctly? The input is of shape (batch, channels, D, H, W). The output is the same shape. So the index calculation needs to be correct.

Alternatively, the blockIdx.x can index over batch, D, H, W, and threadIdx.x over channels. But the exact indexing would require some calculation. For example, the total number of spatial positions per batch is D*H*W. So for the kernel, the grid size would be batch_size * D*H*W, and each block handles a spatial position in a batch. The block size would be channels (64) threads.

Wait, but the number of threads per block can't exceed the maximum (like 1024), which is okay since 64 is small. However, if channels were larger, this might not be feasible. But in the given problem, out_channels is 64, so that's manageable.

The get_index function would need to compute the linear index given batch, channel, d, h, w. For example, for a 5D tensor, the linear index can be computed as:

index = batch * (C*D*H*W) + channel * (D*H*W) + d*(H*W) + h*W + w

But to avoid computing this in the kernel, perhaps it's better to compute the offset based on the current thread's position.

Alternatively, the input and output can be treated as contiguous arrays, and the kernel can compute the correct offset based on the thread and block indices.

Another consideration: the exp function can be computationally expensive, but it's necessary for softmax. However, since both softmax and sigmoid require exponentials, maybe there's a way to combine them, but I don't see an obvious simplification.

So, the fused kernel approach for Softmax and Sigmoid seems feasible. The main challenge is implementing the reduction for the sum across channels efficiently.

Now, let's think about replacing the ConvTranspose3d with a custom kernel. That would be more complex. The transposed convolution (deconvolution) involves:

- The input is upsampled, then convolved with the kernel. The output size is determined by the stride, padding, and output_padding.

The standard approach is to compute the output by upsampling the input (via transposed convolution) and applying the kernel. However, implementing this in CUDA requires handling the spatial dimensions and the kernel weights.

Given that the parameters are fixed (kernel_size=3, stride=2, padding=1, output_padding=1), perhaps we can optimize the kernel for these specific parameters.

Alternatively, maybe using a precomputed kernel and exploiting the specific parameters to simplify calculations. But this would require a detailed understanding of the transposed convolution algorithm.

The problem allows replacing operators with custom kernels, so maybe the best approach is to replace the entire ConvTranspose3d with a custom kernel, but that's quite involved.

Alternatively, perhaps the combination of convolution followed by the fused softmax and sigmoid can be done in a single kernel. But that would be even more complex.

Given time constraints, maybe focusing on fusing the two activations first would be better. Let me proceed with that.

So, the plan is:

- Keep the ConvTranspose3d as it is (since writing a custom one is time-consuming and error-prone for the purpose of this exercise).

- Replace the softmax followed by sigmoid with a fused kernel.

The fused kernel will be implemented in CUDA, and the new model will use the custom kernel after the convolution.

Now, let's structure the code accordingly.

First, in the ModelNew class:

- The convolution layer remains as a standard nn.ConvTranspose3d.

- The forward pass will first apply the convolution, then apply the fused_softmax_sigmoid kernel.

The fused kernel needs to be written as a CUDA function.

Now, coding steps:

1. Write the CUDA kernel for fused_softmax_sigmoid.

2. Compile it inline using torch.utils.cpp_extension.load_inline.

3. In the forward function, after the convolution, call the fused kernel instead of the individual softmax and sigmoid.

Wait, but the softmax is a nn.Softmax module. So in the original code, the model has self.softmax and self.sigmoid as modules, but in the new code, we can remove them and just use the kernel.

Now, let's draft the code.

First, the CUDA source for the fused kernel:

The input and output are both tensors of shape (batch, out_channels, D, H, W).

The kernel needs to process each spatial position (d, h, w) across all channels and batches.

The kernel's code would be something like this:

#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_softmax_sigmoid_kernel(
    const float* input,
    float* output,
    int batch_size,
    int channels,
    int D,
    int H,
    int W) {

    // Calculate the spatial position and batch based on blockIdx and threadIdx
    int batch = blockIdx.x / (D * H * W);
    int spatial_pos = blockIdx.x % (D * H * W);
    int d = spatial_pos / (H * W);
    int hw = spatial_pos % (H * W);
    int h = hw / W;
    int w = hw % W;

    int channel = threadIdx.x;

    if (channel >= channels) return;

    // Compute the index for the input tensor
    int input_offset = batch * channels * D * H * W
                      + channel * D * H * W
                      + d * H * W + h * W + w;

    float z = input[input_offset];

    float exp_z = expf(z);

    // Use shared memory for reduction
    __shared__ float shared_data[64]; // assuming channels is 64

    // Write exp_z to shared memory
    shared_data[threadIdx.x] = exp_z;
    __syncthreads();

    // Compute the sum of exp_z across all channels
    float sum_exp = 0;
    for (int i = 0; i < channels; i++) {
        sum_exp += shared_data[i];
    }

    // Wait until all threads have written
    __syncthreads();

    // Each thread computes the same sum, but only need it once
    // Alternatively, perform a reduction with threads.
    // Wait, the above loop is O(channels) which is 64 steps. That's acceptable for small channels.
    // But maybe better to use a reduction.

    // Alternative approach with reduction:

    // First, load into shared memory
    if (threadIdx.x < channels) {
        shared_data[threadIdx.x] = exp_z;
    } else {
        shared_data[threadIdx.x] = 0.0f;
    }
    __syncthreads();

    // Then perform a block reduction
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_data[threadIdx.x] += shared_data[threadIdx.x + stride];
        }
        __syncthreads();
    }

    float sum_exp = shared_data[0]; // the sum is in thread 0's position

    // Compute softmax value
    float softmax_val = exp_z / sum_exp;
    // Compute sigmoid of softmax_val
    float sigmoid_val = 1.0f / (1.0f + expf(-softmax_val));

    // Write the result to output
    output[input_offset] = sigmoid_val;
}

Wait, but in the block reduction approach, the shared_data array must be large enough to hold all threads. Since the block size is equal to the number of channels (64), then the shared memory array of size 64 is okay. The code above uses blockDim.x as the number of threads, which is equal to channels (64). So the reduction can be done properly.

Wait, in the kernel, the blockDim.x is set to channels (64). Each thread in the block is responsible for a different channel. So the first part, each thread loads their exp_z into shared_data[threadIdx.x]. Then, the reduction loop reduces the sum.

After the reduction, all threads have the sum (since it's stored in shared_data[0], which is accessible by all threads).

Therefore, the code should work.

But the grid dimension is set as follows: each block corresponds to a batch and spatial position. The total number of blocks is batch_size * D * H * W.

The number of threads per block is channels (64).

Now, in the Python code, when calling the kernel, we need to:

- Compute the grid and block dimensions.

- Ensure the input and output are contiguous (or at least have the correct strides).

The kernel function in Python would be something like:

def fused_softmax_sigmoid_cuda(input):
    batch_size, channels, D, H, W = input.shape
    output = torch.empty_like(input)
    # Determine block and grid sizes
    block_size = channels  # 64
    grid_size = batch_size * D * H * W
    fused_softmax_sigmoid_kernel<<<grid_size, block_size>>>(
        input.contiguous().data_ptr(), 
        output.data_ptr(), 
        batch_size, channels, D, H, W)
    return output

But need to check for CUDA errors, etc. Also, in the CUDA code, need to handle the case where channels != 64 (but in the problem's case it's 64). However, since the problem states that the input dimensions are fixed, we can hardcode channels=64 in the kernel if needed, but it's better to make it a parameter.

Wait, in the problem's parameters, the out_channels is 64. But in the kernel code, the shared array size is fixed to 64. That's a problem if the model is used with different out_channels. Since the problem says to assume fixed input dimensions, we can hardcode it here, but in the code, better to have it as a parameter. Alternatively, since the kernel is written for the specific parameters, perhaps it's okay.

Alternatively, to make the kernel more general, we can have the shared array size as blockDim.x, but that requires dynamic shared memory. Hmm, but the problem says to make it work for the given parameters, so perhaps it's okay to hardcode 64 here.

Alternatively, in the CUDA code, use a template or a #define, but in this case, given the time constraints, let's proceed with the assumption that channels are 64. The code can be adjusted later.

Now, compiling this kernel inline with load_inline.

Putting this all together:

The Python code would have:

from torch.utils.cpp_extension import load_inline

# Define the fused kernel CUDA code
fused_kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_softmax_sigmoid_kernel(
    const float* input,
    float* output,
    int batch_size,
    int channels,
    int D,
    int H,
    int W) {

    int batch = blockIdx.x / (D * H * W);
    int spatial_pos = blockIdx.x % (D * H * W);
    int d = spatial_pos / (H * W);
    int hw = spatial_pos % (H * W);
    int h = hw / W;
    int w = hw % W;

    int channel = threadIdx.x;

    if (channel >= channels) return;

    // Compute input offset
    int input_offset = batch * channels * D * H * W +
                      channel * D * H * W +
                      d * H * W + h * W + w;

    float z = input[input_offset];
    float exp_z = expf(z);

    // Use shared memory for reduction
    __shared__ float shared_data[64]; // channels is 64
    shared_data[threadIdx.x] = exp_z;
    __syncthreads();

    // Reduction step
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_data[threadIdx.x] += shared_data[threadIdx.x + stride];
        }
        __syncthreads();
    }

    float sum_exp = shared_data[0];

    // Compute softmax and sigmoid
    float softmax_val = exp_z / sum_exp;
    float sigmoid_val = 1.0f / (1.0f + expf(-softmax_val));

    output[input_offset] = sigmoid_val;
}

torch::Tensor fused_softmax_sigmoid(torch::Tensor input) {
    auto batch_size = input.size(0);
    auto channels = input.size(1);
    auto D = input.size(2);
    auto H = input.size(3);
    auto W = input.size(4);

    auto output = torch::empty_like(input);

    dim3 block_size(channels);
    dim3 grid_size(batch_size * D * H * W);

    fused_softmax_sigmoid_kernel<<<grid_size, block_size>>>(
        input.contiguous().data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, D, H, W
    );

    return output;
}
"""

cpp_header = """
torch::Tensor fused_softmax_sigmoid(torch::Tensor input);
"""

Then, load the kernel:

fused_mod = load_inline(
    name="fused_softmax_sigmoid",
    cpp_sources=cpp_header,
    cuda_sources=fused_kernel_source,
    functions=["fused_softmax_sigmoid"],
    verbose=True
)

Then, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, stride=stride,
            padding=padding, output_padding=output_padding, bias=bias
        )
        self.fused_softmax_sigmoid = fused_mod.fused_softmax_sigmoid

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.fused_softmax_sigmoid(x)
        return x

Wait, but in the forward pass, the function is called as a Python function. The fused_mod.fused_softmax_sigmoid is a function that takes a tensor and returns the fused result.

This should work, but need to make sure that the input is contiguous, as required by the kernel.

Now, potential issues to consider:

1. The shared memory size: in the kernel, we have __shared__ float shared_data[64];. If the number of channels (out_channels) is not 64, this would be a problem. But in the problem's parameters, it's 64, so that's okay. However, in the code, if the model is used with different parameters, it would fail. Since the problem states to assume fixed input dimensions, this is acceptable.

2. The grid and block dimensions must be correctly calculated. The grid size is batch_size * D * H * W. The block size is channels (64).

3. The kernel must be launched with the correct dimensions. The code above uses the input's dimensions to calculate batch_size, channels, D, H, W each time. Wait, in the fused_softmax_sigmoid function, the input's size is queried each time, so it can handle different input sizes as long as the channels are 64. But the problem says to assume fixed input dimensions, so that's okay.

4. The expf function and divisions must be handled properly. Since CUDA has float precision, it should be okay.

Now, compiling this code. The user has to have CUDA installed and the necessary PyTorch extensions.

Another thing to note is that the original code uses nn.Softmax(dim=1), which applies the softmax over the channel dimension (dim=1). The kernel correctly computes the sum over all channels for each spatial position, so that's correct.

Now, does this kernel actually work?

Let me check with a small example. Suppose the input is a single batch (batch=1), channels=2, D=1, H=1, W=1. Then, the input is a tensor of shape (1,2,1,1,1). Suppose the values are [0, 0]. Then, the exp would be 1 each, sum is 2. Softmax is [0.5, 0.5], then sigmoid of each is 1/(1+exp(-0.5)) â‰ˆ 0.6225. The output should have both elements as ~0.6225.

Testing this in the kernel would require setting up the input tensor correctly and running the kernel. But since I can't run it now, I have to proceed with the code.

Another possible optimization in the kernel: the reduction step. The current code uses a loop for reduction. The loop does a binary reduction, which is more efficient than the initial approach of looping through all elements. The reduction loop is correct.

Another consideration: the input must be contiguous. The kernel code calls input.contiguous() to ensure that. So that's handled.

Now, the other operators (conv_transpose) are left as-is. The main optimization is fusing the two activations.

Alternatively, perhaps the convolution can be replaced with a custom kernel for further speedup, but that's more involved. Let me think if there's a way to combine the convolution with the fused activation.

However, that would require handling the convolution computation along with the activation functions in a single kernel, which is more complex. For the sake of time and complexity, perhaps the fused activation is sufficient as an optimization.

Therefore, the final code would look like the above.

Wait, in the CUDA code, the function signature of fused_softmax_sigmoid requires the input tensor and returns the output. The kernel is launched with the correct parameters.

Now, in the ModelNew class, the __init__ must take the same arguments as the original Model class. The original Model's __init__ takes in_channels, out_channels, kernel_size, stride, padding, output_padding, bias.

The ModelNew's __init__ should have the same parameters, and pass them correctly to the ConvTranspose3d layer. That's done correctly in the code.

The forward function takes x, applies the conv, then the fused kernel, and returns.

Therefore, the code should be correct.

Now, putting it all together in code blocks as per the user's instructions.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, stride=stride,
            padding=padding, output_padding=output_padding, bias=bias
        )
        
        # Define fused softmax-sigmoid CUDA kernel
        fused_kernel_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>
        #include <math.h>

        __global__ void fused_softmax_sigmoid_kernel(
            const float* input,
            float* output,
            int batch_size,
            int channels,
            int D,
            int H,
            int W) {
            int batch = blockIdx.x / (D * H * W);
            int spatial_pos = blockIdx.x % (D * H * W);
            int d = spatial_pos / (H * W);
            int hw = spatial_pos % (H * W);
            int h = hw / W;
            int w = hw % W;

            int channel = threadIdx.x;

            if (channel >= channels) return;

            int input_offset = batch * channels * D * H * W +
                              channel * D * H * W +
                              d * H * W + h * W + w;

            float z = input[input_offset];
            float exp_z = expf(z);

            __shared__ float shared_data[64]; // channels=64 as per problem parameters
            shared_data[threadIdx.x] = exp_z;
            __syncthreads();

            for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
                if (threadIdx.x < stride) {
                    shared_data[threadIdx.x] += shared_data[threadIdx.x + stride];
                }
                __syncthreads();
            }

            float sum_exp = shared_data[0];
            float softmax_val = exp_z / sum_exp;
            float sigmoid_val = 1.0f / (1.0f + expf(-softmax_val));

            output[input_offset] = sigmoid_val;
        }

        torch::Tensor fused_softmax_sigmoid(torch::Tensor input) {
            auto batch_size = input.size(0);
            auto channels = input.size(1);
            auto D = input.size(2);
            auto H = input.size(3);
            auto W = input.size(4);

            auto output = torch::empty_like(input);

            dim3 block_size(channels);
            dim3 grid_size(batch_size * D * H * W);

            fused_softmax_sigmoid_kernel<<<grid_size, block_size>>>(
                input.contiguous().data_ptr<float>(),
                output.data_ptr<float>(),
                batch_size, channels, D, H, W
            );

            return output;
        }
        """
        
        cpp_header = """
        torch::Tensor fused_softmax_sigmoid(torch::Tensor input);
        """
        
        # Compile the fused kernel
        self.fused_mod = load_inline(
            name="fused_softmax_sigmoid",
            cpp_sources=cpp_header,
            cuda_sources=fused_kernel_source,
            functions=["fused_softmax_sigmoid"],
            verbose=True
        )
        self.fused_activation = self.fused_mod.fused_softmax_sigmoid

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.fused_activation(x)
        return x

# The get_inputs and get_init_inputs remain unchanged as per original code
batch_size = 16
in_channels = 32
out_channels = 64
D, H, W = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
output_padding = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, D, H, W).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding]
```