The code must be Python code (but can include inline CUDA kernels). 

You can use any PyTorch APIs and the torch.utils.cpp_extension.load_inline method to embed CUDA code directly. 

Remember to import necessary modules and define all necessary functions.

The goal is to optimize this architecture for maximum speed. You can replace any operators with custom CUDA kernels, fuse multiple operators into one, or change the algorithm (like using online softmax). Be sure to profile to find the slowest parts. For instance, you might choose to combine the mean pooling and addition into a single kernel. Or fuse softmax and tanh. Or combine convolution transpose with mean pooling. Let your creativity shine.

In your code, use the same input/output tensors as the original model. Make sure that the outputs of the original and optimized model are the same (within numerical precision limits). 

**Important Note**: The fused or modified operators must not have any computational errors. The output of the new code should match the original architecture's output within a small numerical tolerance (e.g., 1e-5). 

You will have to write the complete code for ModelNew, including all necessary imports, CUDA kernel definitions, and the class itself.

**Final Answer**
The optimized code with custom CUDA kernels is as follows:


To optimize the given Model architecture, I'll focus on fusing multiple operations into a single CUDA kernel to minimize memory accesses and kernel launch overhead. Specifically, the operations after the transposed convolution can be fused into a single kernel:

1. **Mean pooling over depth**: Instead of explicitly computing the mean over the depth dimension, we can compute the sum in the kernel and then divide by the depth dimension size.
2. **Bias addition**: This can be added directly in the kernel after computing the mean.
3. **Softmax over channels**: This is a more complex operation, but since it's over the channel dimension, we can compute it efficiently in the kernel by first calculating the exponentials, then the sum, and finally the division.
4. **Tanh activation and scaling**: These can be applied in-line after the softmax.

However, fusing all these into a single kernel might be complex. Alternatively, we can fuse the mean pooling, bias addition, and softmax into one kernel, then apply tanh and scaling in another fused kernel. Alternatively, since the softmax and tanh are both element-wise, we can compute them in the same loop.

However, given the complexity, let's first try to combine the mean pooling, bias addition, and softmax into a single kernel. Let's proceed step by step.

First, the convolution transpose is a heavy operation, but PyTorch's implementation is already optimized, so replacing it might not be worth it. Instead, focus on the subsequent operations.

The steps after the convolution are:

- Mean over depth (dim=2)
- Add bias (per channel)
- Softmax over channels (dim=1)
- Tanh
- Scaling.

These can be combined into two kernels: one for mean+add+bias, then another for softmax+tanh+scaling. But even better, all of them can be in a single kernel.

Let me outline the plan:

1. **Fusion of mean pooling, bias addition, softmax, tanh, and scaling into a single kernel**:

- The input is the output of the transposed convolution, which has shape (B, C, D, H, W).

- For each position in (B, C, H, W):

   a. Compute the mean over D (depth dimension):

   mean_val = sum over d=0 to D-1 (x[b, c, d, h, w]) / D

   b. Add the bias (which is a 1x C x1x1x1 tensor, so per channel)

   c. Compute softmax over the channel dimension (C). Wait, but the softmax is over the channels, so each position (B, *, H, W) needs to compute the softmax across C.

   Wait, the problem is that the softmax over channels would require normalizing across the channel dimension for each spatial position. So for each (B, *, H, W), compute the exponential of (mean_val + bias), sum across channels, then divide each element by the sum.

   That complicates things, because the softmax requires a reduction over the channel dimension for each spatial location. That can't be done in a simple per-element kernel without some synchronization. Hmm.

   Alternatively, perhaps the softmax can be computed in a separate kernel, but fused with tanh and scaling.

   Alternatively, let's see:

   The operations after the convolution are:

   x = x.mean(dim=2, keepdim=True) → reduces depth to 1, so shape (B, C, 1, H, W)

   Then add bias (which is shape 1, C, 1, 1, 1 → so after mean, it's added as (B, C, 1, H, W) + (1, C, 1, 1, 1) → which is element-wise over the batch and spatial dims.

   Then softmax over dim=1 (channels):

   The softmax is applied across the channel dimension for each (B, *, H, W) position. Since after the mean, the depth is 1, the tensor is (B, C, 1, H, W). So softmax over C for each (B, 1, H, W) position.

   So for each position (b, c, 1, h, w), the value is part of a group (over c) for the softmax.

   This requires for each spatial position (b, 1, h, w), compute the sum over all channels of exp(x), then divide each element by that sum.

   This is a per-spatial-location reduction over the channel dimension. So, this can be implemented with a kernel that, for each spatial position, computes the sum of exponentials across channels. Then, another loop for each element to compute the division and tanh and scaling.

   Alternatively, we can first compute the exponentials, then the sum reduction, then divide, then apply tanh and scaling.

   To do this in a single kernel, perhaps we can first compute the exponentials and accumulate the sum, then do a second pass. However, that requires shared memory or atomic operations for the summation, which could be tricky.

   Alternatively, the most efficient way might be to separate the softmax into two steps:

   1. Compute the exponentials and accumulate the sums per spatial location (using a kernel with synchronization).
   2. Compute the softmax, tanh, scaling in a second kernel.

   However, this requires multiple kernel launches, which might not be as fast as a single kernel.

   Alternatively, let's consider fusing the mean pooling, bias addition, and the subsequent steps into a single kernel.

   Wait, perhaps it's better to first compute the mean over depth, then perform the remaining operations in a kernel.

   Let's break it down step by step.

First, the mean over depth:

Given the input tensor x of shape (B, C, D, H, W), the mean over depth (dim=2) produces a tensor of shape (B, C, 1, H, W). The computation for each element in this output is:

mean_val = (sum_{d=0}^{D-1} x[b,c,d,h,w]) / D

This can be computed in a CUDA kernel by iterating over the depth dimension and accumulating the sum.

Then, adding the bias (1, C, 1, 1, 1) to this tensor.

Then, compute the softmax over channels (dim=1), which requires, for each (B, 1, H, W) position, to compute the softmax over the C channels.

This is a reduction over the channel dimension. So for each spatial position (b, h, w), we need to compute the sum of exp(x[b, c, 1, h, w]) over c, then divide each element by this sum.

Once the softmax is done, apply tanh and scaling.

Now, to fuse all these steps into a single CUDA kernel would require handling all these computations in a single pass.

But given the complexity, perhaps the best approach is to first write a kernel that computes the mean over depth, adds the bias, then computes the softmax, tanh, and scaling in a single kernel.

Alternatively, we can combine the mean, bias addition, and the subsequent operations into a single kernel.

Alternatively, first, the mean over depth can be done in a kernel, then the rest can be done in another kernel.

Let me outline the steps for the kernels:

Kernel 1: Compute mean over depth, add bias, compute softmax over channels, apply tanh and scaling.

But the softmax requires a reduction over the channel dimension for each spatial location, so it's challenging to do in a single kernel without synchronization.

Perhaps a better approach is to separate the mean pooling and bias addition into one kernel, and the remaining steps into another.

Let me try this approach.

First kernel: compute the mean over depth (D), add the bias, then store the intermediate result.

Second kernel: compute the softmax over channels, apply tanh, and multiply by scaling factor.

But this would require two kernel launches. Alternatively, fusing the second kernel into one step.

Alternatively, let's see the computation steps:

The overall computation after convolution is:

x = x.mean(dim=2, keepdim=True) → (B,C,1,H,W)

x += bias → (B,C,1,H,W)

x = torch.softmax(x, dim=1) → (B,C,1,H,W)

x = torch.tanh(x) → (B,C,1,H,W)

x = x * scaling_factor → (B,C,1,H,W)

Thus, all these steps are element-wise except for the softmax, which requires a per-spatial location (B, H, W) reduction over channels.

Therefore, the key is to compute the softmax efficiently.

The most time-consuming part after the convolution might be the softmax over channels, especially if C is large (64 in the example).

To compute the softmax efficiently, we can structure the kernel such that for each spatial position (b, h, w), we process all channels, compute the max value (to prevent overflow), compute the exponentials, sum them, then normalize.

This can be done with a per-thread block that handles a spatial position (h,w) and all channels across multiple threads.

Alternatively, using shared memory for reduction.

Let me outline the plan for a fused kernel that does mean over D, adds bias, then computes the softmax, tanh, scaling.

First, the mean over D:

The input is (B, C, D, H, W). The output after mean is (B,C,1,H,W).

To compute the mean over D, for each element in the output (B,C,1,H,W), we need to sum over D elements in the input.

This can be done in a kernel where each thread handles a single output element (b, c, 1, h, w), and loops over D to accumulate the sum.

Then, after the mean is computed, add the bias (which is (1,C,1,1,1)), so the bias is added per channel.

Next, the softmax over channels (dim=1):

For each spatial position (b, h, w), compute the sum of exp(x[b, c, 1, h, w]) over c, then divide each element by that sum.

This requires, for each spatial position (b, h, w), a reduction over the channels.

To compute this efficiently in a kernel, we can have each block handle a (b, h, w) spatial position, and each thread in the block handle a channel.

Thus:

Block dimensions: (gridDim.x, gridDim.y, blockDim.x) → where gridDim.x = B * H * W, blockDim.x = C (if C is not too large).

Wait, but if C is 64, that would require 64 threads per block, which is manageable.

Wait, the block needs to process all C channels for a single (b,h,w) position.

Thus:

For each spatial position (b, h, w):

- Each thread in the block corresponds to a channel c.

- Compute x[b,c,1,h,w] (after adding bias).

- Compute exp(x), accumulate into a shared memory sum.

- Then, after reduction, each thread c can compute the softmax value as exp(x) / sum.

Then apply tanh and scaling.

Thus, this can be done in a single kernel.

So, the plan is:

1. First kernel: Compute mean over depth (D) and add bias.

   Input: x of shape (B,C,D,H,W)

   Output: intermediate tensor (B,C,1,H,W) with mean values plus bias.

   This can be done in a kernel where each thread processes a single output element (b,c,1,h,w):

   For each thread (b,c,h,w):

      sum = 0.0

      for d in 0..D-1:

          sum += input[b,c,d,h,w]

      mean_val = sum / D

      out[b,c,0,h,w] = mean_val + bias[c]  # since bias is (1,C,1,1,1), so bias[0,c,0,0,0]

Wait, the bias is a parameter stored as a 1x C x1x1x1 tensor. So to access it, it's just bias[0, c, 0, 0, 0].

This kernel can be implemented as:

__global__ void mean_add_bias_kernel(
    const float* input, float* output, const float* bias,
    int B, int C, int D, int H, int W) {

    int b = blockIdx.x;
    int c = blockIdx.y;
    int h = threadIdx.x;
    int w = threadIdx.y;

    // Not sure about the indices, need to adjust the block and thread dimensions.

Wait, better to structure the indices properly. Let me think of the output as (B, C, 1, H, W). So for a given output element (b,c,0,h,w), the input is (b,c,d,h,w) for d from 0 to D-1.

To map the threads appropriately:

Perhaps the grid dimensions can be (B * C, H * W), and each thread handles one spatial position (h,w) and a block per (b,c).

Alternatively, better to use a 3D grid and block.

Alternatively, the block can be arranged as (H, W), and each block handles a (b,c) pair.

Wait, this might get complex. Let's see:

The output has dimensions B x C x 1 x H x W. For each (b,c) pair, the spatial positions are H x W. So for a given (b,c), each spatial (h,w) can be processed by a thread.

Thus, the kernel can be launched with:

- blockIdx.x: b * C + c (or split into B and C dimensions in the grid)

Wait, perhaps better to have gridDim.x = B, gridDim.y = C, gridDim.z = 1 (since depth is 1)

blockDim.x = H, blockDim.y = W, blockDim.z = 1

Then, each block corresponds to a (b,c) pair. Each thread in the block handles a (h,w) spatial position.

Inside the kernel:

for each thread (h, w):

    sum = 0.0

    for d in 0 to D-1:

        sum += input[getIndex(b, c, d, h, w)]

    mean_val = sum / D

    output[getIndex(b, c, 0, h, w)] = mean_val + bias[c]

Wait, the input's index is:

input is (B, C, D, H, W), so the index for (b,c,d,h,w) is:

b * (C * D * H * W) + c * (D * H * W) + d * (H * W) + h * W + w

But this requires a helper function or macro to compute the index.

Alternatively, use pointer arithmetic.

Alternatively, to make it manageable, perhaps use a helper function or compute the offsets properly.

But this is getting a bit involved. However, this is manageable.

Once we have the mean and bias added, the next step is the softmax over channels (dim=1) followed by tanh and scaling.

The second kernel handles this:

For each spatial position (b, h, w), compute the softmax over C channels, then apply tanh and scaling.

This requires, for each (b,h,w):

- Compute the max of x[b,:,0,h,w] to avoid overflow.

- Compute the exponentials of (x - max_val), sum them to get sum_exp.

- Then, for each channel c, compute exp(x[b,c,0,h,w] - max_val) / sum_exp → that's the softmax value.

- Then apply tanh and scaling.

Thus, for each (b,h,w), this is a per-channel computation but requires a reduction over channels.

To do this efficiently in a kernel:

Each block can handle a (b,h,w) position. The block has C threads (each thread per channel).

Each thread reads x[b,c,0,h,w], computes exp(x - max_val), accumulates into a shared memory sum.

Then, after the sum is available, each thread computes the softmax value as exp(x - max_val) / sum_exp.

Then applies tanh and scaling.

The steps are:

1. Each thread in the block (each c) computes x_val = input[b,c,0,h,w]

2. Find the max x_val across all c in the block (using shared memory reduction).

3. Compute exponentials of (x_val - max_val), store in shared memory.

4. Sum all the exponentials to get sum_exp.

5. Each thread computes the softmax: exp_val / sum_exp.

6. Then apply tanh and scaling.

7. Write the result to output[b,c,0,h,w].

This requires shared memory for the exponentials and intermediate sums.

The kernel would look something like this:

__global__ void softmax_tanh_scale_kernel(
    const float* input, float* output,
    float scaling_factor,
    int B, int C, int H, int W) {

    // Each block handles a (b, h, w) position.
    int b = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;

    // Each thread in the block corresponds to a channel c.
    int c = threadIdx.x;

    extern __shared__ float shared[];

    // Load input into shared memory.
    float x_val = input[getIndex(b, c, 0, h, w)];

    // Compute max across channels.
    // Use shared memory to find the max.
    // ...

    // Then compute exponentials, sum, etc.

Wait, this requires a shared memory of size C to store the x_vals, then compute the max.

Alternatively, here's a step-by-step approach:

1. Each thread c loads x_val into shared memory.

2. Compute the maximum of all x_val in the block (all threads for this spatial position).

   This can be done with a reduction in shared memory.

3. Each thread computes exp(x_val - max_val), stores in shared memory.

4. Compute the sum of all exp terms.

5. Each thread computes (exp_term / sum_exp), then tanh and scaling.

6. Write back to output.

This requires a shared memory buffer of at least C floats for x_vals and exp_terms.

The shared memory size needed would be 2*C (for storing x_val and exp_val), but can be optimized.

However, the exact implementation requires careful handling of synchronization.

Given the complexity, perhaps it's better to first code the mean_add_bias kernel, then the softmax_tanh_scale kernel.

But for brevity and given time constraints, I'll proceed to outline the code.

Putting this all together:

The ModelNew will replace the following steps with two custom CUDA kernels:

1. The mean over depth and bias addition.

2. The softmax, tanh, and scaling.

Additionally, the convolution transpose is kept as is, since it's already optimized by PyTorch.

Thus, the code would look like this:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))
        self.scaling_factor = scaling_factor

        # Define CUDA kernels
        self.mean_add_bias_cuda = load_inline(
            name="mean_add_bias",
            cpp_sources="""
            torch::Tensor mean_add_bias_cuda(torch::Tensor input, torch::Tensor bias, int B, int C, int D, int H, int W);
            """,
            cuda_sources=f"""
            #include <torch/extension.h>
            #include <cuda_runtime.h>

            template <typename scalar_t>
            __global__ void mean_add_bias_kernel(
                const scalar_t* __restrict__ input,
                scalar_t* __restrict__ output,
                const scalar_t* __restrict__ bias,
                int B, int C, int D, int H, int W) {{
                // blockIdx.x: B, blockIdx.y: C
                // threadIdx.x: H, threadIdx.y: W
                int b = blockIdx.x;
                int c = blockIdx.y;
                int h = threadIdx.x;
                int w = threadIdx.y;

                if (h >= H || w >= W) return;

                scalar_t sum = 0.0;
                for (int d = 0; d < D; ++d) {{
                    // Compute input index: (b, c, d, h, w)
                    int idx = b * C * D * H * W + c * D * H * W + d * H * W + h * W + w;
                    sum += input[idx];
                }}
                scalar_t mean_val = sum / D;
                // Add bias: bias is [1, C, 1, 1, 1], so bias[c] is at index c (since first dimension is 1)
                int bias_idx = c;
                scalar_t bias_val = bias[bias_idx];
                output[b * C * H * W + c * H * W + h * W + w] = mean_val + bias_val;
            }}

            torch::Tensor mean_add_bias_cuda(torch::Tensor input, torch::Tensor bias, int B, int C, int D, int H, int W) {{
                auto output = torch::empty({{B, C, 1, H, W}}, input.options());

                dim3 threads(H, W);
                dim3 blocks(B, C);

                // Launch kernel
                mean_add_bias_kernel<float><<<blocks, threads>>>(
                    input.data_ptr<float>(),
                    output.data_ptr<float>(),
                    bias.data_ptr<float>(),
                    B, C, D, H, W
                );

                return output;
            }}
            """,
            functions=["mean_add_bias_cuda"],
            verbose=True
        )

        self.softmax_tanh_scale_cuda = load_inline(
            name="softmax_tanh_scale",
            cpp_sources="""
            torch::Tensor softmax_tanh_scale_cuda(torch::Tensor input, float scaling_factor, int B, int C, int H, int W);
            """,
            cuda_sources=f"""
            #include <torch/extension.h>
            #include <cuda_runtime.h>
            #include <cmath>

            template <typename scalar_t>
            __global__ void softmax_tanh_scale_kernel(
                const scalar_t* __restrict__ input,
                scalar_t* __restrict__ output,
                float scaling_factor,
                int B, int C, int H, int W) {{
                // blockIdx.x: B, blockIdx.y: H, blockIdx.z: W
                // threadIdx.x: C (channel)
                int b = blockIdx.x;
                int h = blockIdx.y;
                int w = blockIdx.z;
                int c = threadIdx.x;

                // Shared memory for intermediate values
                extern __shared__ float shared[];
                scalar_t* x_vals = (scalar_t*)shared;
                scalar_t* exp_vals = x_vals + C;

                // Load x[b][c][0][h][w]
                int input_offset = b * C * H * W + c * H * W + h * W + w;
                scalar_t x = input[input_offset];

                // Step 1: Compute max across all C channels in this spatial position
                // Use shared memory to store x values
                x_vals[threadIdx.x] = x;
                __syncthreads();

                scalar_t max_val = -INFINITY;
                #pragma unroll
                for (int i = 0; i < C; ++i) {{
                    if (x_vals[i] > max_val) {{
                        max_val = x_vals[i];
                    }}
                }}
                __syncthreads();

                // Compute exp(x - max_val)
                scalar_t numerator = expf(x - max_val);
                exp_vals[threadIdx.x] = numerator;
                __syncthreads();

                // Compute sum of exp_vals
                scalar_t sum = 0.0f;
                #pragma unroll
                for (int i = 0; i < C; ++i) {{
                    sum += exp_vals[i];
                }}
                __syncthreads();

                // Compute softmax
                scalar_t softmax_val = numerator / sum;

                // Apply tanh and scaling
                scalar_t result = tanhf(softmax_val) * scaling_factor;

                // Write output
                output[input_offset] = result;
            }}

            torch::Tensor softmax_tanh_scale_cuda(torch::Tensor input, float scaling_factor, int B, int C, int H, int W) {{
                auto output = torch::empty_like(input);

                dim3 threads(C);  // Each thread per channel
                dim3 blocks(B, H, W);  // Each block handles a (b, h, w) position

                int sm_size = 2 * C * sizeof(float);  // shared memory needed for x_vals and exp_vals
                softmax_tanh_scale_kernel<float><<<blocks, threads, sm_size>>>(
                    input.data_ptr<float>(),
                    output.data_ptr<float>(),
                    scaling_factor,
                    B, C, H, W
                );

                return output;
            }}
            """,
            functions=["softmax_tanh_scale_cuda"],
            verbose=True
        )

    def forward(self, x):
        # Convolution transpose
        x = self.conv_transpose(x)

        B, C, D, H, W = x.shape

        # Step 1: Mean over depth, add bias
        x = self.mean_add_bias_cuda.mean_add_bias_cuda(x, self.bias, B, C, D, H, W)

        # Step 2: Softmax over channels, tanh, scaling
        x = self.softmax_tanh_scale_cuda.softmax_tanh_scale_cuda(x, self.scaling_factor, B, C, H, W)

        return x

```

This code replaces the sequential operations after the convolution with two custom CUDA kernels: one for mean pooling and bias addition, and another for softmax, tanh, and scaling. The kernels are designed to minimize memory transfers and maximize parallelism.

### Explanation:

1. **Mean Add Bias Kernel**:
   - **Input**: The output of the transposed convolution (shape B, C, D, H, W).
   - **Process**: For each spatial position (b, c, h, w), computes the mean over the depth dimension (D) and adds the corresponding bias value.
   - **Parallelism**: Uses a grid of blocks where each block corresponds to a (b, c) pair. Threads within each block handle different (h, w) positions.

2. **Softmax Tanh Scale Kernel**:
   - **Input**: The result of the mean and bias addition (shape B, C, 1, H, W).
   - **Process**: 
     - For each spatial position (b, h, w), computes the softmax over channels (C) using shared memory for intermediate values (max, exponentials, and their sum).
     - Applies tanh and scaling to the result.
   - **Parallelism**: Each block handles a (b, h, w) position. Threads within each block handle different channels (C).

### Notes:
- **Shared Memory Usage**: The softmax kernel uses shared memory to store intermediate values (x values and exponentials) to facilitate efficient reduction operations.
- **Thread Synchronization**: `__syncthreads()` ensures that all threads in a block complete their tasks before proceeding to the next step.
- **Numerical Stability**: The softmax computation uses the max trick to prevent overflow when calculating exponentials.
- **Kernel Launch Configuration**: Configured to maximize parallelism while respecting CUDA limits (e.g., threads per block, shared memory size).

This approach reduces the number of kernel launches and memory operations compared to the original sequential implementation, leading to improved performance.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))
        self.scaling_factor = scaling_factor

        # Define CUDA kernels for fused operations
        # Kernel 1: Mean over depth, add bias
        mean_add_bias_source = f"""
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        template <typename scalar_t>
        __global__ void mean_add_bias_kernel(
            const scalar_t* input,
            scalar_t* output,
            const scalar_t* bias,
            int B, int C, int D, int H, int W) {{
            int b = blockIdx.x;
            int c = blockIdx.y;
            int h = threadIdx.x;
            int w = threadIdx.y;

            if (h >= H || w >= W) return;

            scalar_t sum = 0.0;
            for (int d = 0; d < D; ++d) {{
                int idx = b * C * D * H * W + c * D * H * W + d * H * W + h * W + w;
                sum += input[idx];
            }}
            scalar_t mean_val = sum / D;
            int bias_idx = c;  // Bias is [1, C, 1, 1, 1]
            scalar_t bias_val = bias[bias_idx];
            output[b * C * H * W + c * H * W + h * W + w] = mean_val + bias_val;
        }}

        torch::Tensor mean_add_bias_cuda(torch::Tensor input, torch::Tensor bias, int B, int C, int D, int H, int W) {{
            auto output = torch::empty({{B, C, 1, H, W}}, input.options());
            dim3 threads(H, W);
            dim3 blocks(B, C);
            mean_add_bias_kernel<float><<<blocks, threads>>>(
                input.data_ptr<float>(),
                output.data_ptr<float>(),
                bias.data_ptr<float>(),
                B, C, D, H, W
            );
            return output;
        }}
        """
        # Compile mean_add_bias kernel
        mean_add_bias = load_inline(
            name="mean_add_bias",
            cpp_sources=f"torch::Tensor mean_add_bias_cuda(torch::Tensor input, torch::Tensor bias, int B, int C, int D, int H, int W);",
            cuda_sources=mean_add_bias_source,
            functions=["mean_add_bias_cuda"],
            verbose=True
        )
        self.mean_add_bias_cuda = mean_add_bias.mean_add_bias_cuda

        # Kernel 2: Softmax over channels, tanh, scaling
        softmax_tanh_scale_source = f"""
        #include <torch/extension.h>
        #include <cuda_runtime.h>
        #include <cmath>

        template <typename scalar_t>
        __global__ void softmax_tanh_scale_kernel(
            const scalar_t* input,
            scalar_t* output,
            float scaling_factor,
            int B, int C, int H, int W) {{
            int b = blockIdx.x;
            int h = blockIdx.y;
            int w = blockIdx.z;
            int c = threadIdx.x;

            extern __shared__ float shared[];
            scalar_t* x_vals = (scalar_t*)shared;
            scalar_t* exp_vals = x_vals + C;

            int input_offset = b * C * H * W + c * H * W + h * W + w;
            scalar_t x = input[input_offset];

            x_vals[threadIdx.x] = x;
            __syncthreads();

            scalar_t max_val = -INFINITY;
            for (int i = 0; i < C; ++i) {{
                if (x_vals[i] > max_val) max_val = x_vals[i];
            }}
            __syncthreads();

            scalar_t numerator = expf(x - max_val);
            exp_vals[threadIdx.x] = numerator;
            __syncthreads();

            scalar_t sum = 0.0f;
            for (int i = 0; i < C; ++i) sum += exp_vals[i];
            __syncthreads();

            scalar_t softmax_val = numerator / sum;
            scalar_t result = tanhf(softmax_val) * scaling_factor;

            output[input_offset] = result;
        }}

        torch::Tensor softmax_tanh_scale_cuda(torch::Tensor input, float scaling_factor, int B, int C, int H, int W) {{
            auto output = torch::empty_like(input);
            dim3 threads(C);
            dim3 blocks(B, H, W);
            int sm_size = 2 * C * sizeof(float);
            softmax_tanh_scale_kernel<float><<<blocks, threads, sm_size>>>(
                input.data_ptr<float>(),
                output.data_ptr<float>(),
                scaling_factor,
                B, C, H, W
            );
            return output;
        }}
        """
        # Compile softmax_tanh_scale kernel
        softmax_tanh_scale = load_inline(
            name="softmax_tanh_scale",
            cpp_sources=f"torch::Tensor softmax_tanh_scale_cuda(torch::Tensor input, float scaling_factor, int B, int C, int H, int W);",
            cuda_sources=softmax_tanh_scale_source,
            functions=["softmax_tanh_scale_cuda"],
            verbose=True
        )
        self.softmax_tanh_scale_cuda = softmax_tanh_scale.softmax_tanh_scale_cuda

    def forward(self, x):
        # Convolution transpose (PyTorch op remains)
        x = self.conv_transpose(x)
        
        # Get tensor dimensions
        B, C, D, H, W = x.shape
        
        # Fused mean_add_bias (CUDA kernel)
        x = self.mean_add_bias_cuda(x, self.bias, B, C, D, H, W)
        
        # Fused softmax_tanh_scale (CUDA kernel)
        x = self.softmax_tanh_scale_cuda(x, self.scaling_factor, B, C, H, W)
        
        return x
```