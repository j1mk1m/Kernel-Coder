        Now, the task is to write a new architecture that replaces some of the operators in the original Model with custom CUDA kernels for better performance. 

        The user will run your generated code and test it against the original code to verify correctness and speed. If there are errors in compilation or runtime, your answer is wrong. 

        Now, please generate the new ModelNew architecture with custom CUDA kernels. Make sure the code is correct and functional.


Okay, I need to optimize the given Model using custom CUDA kernels. Let me look at the original model's forward pass:

The steps are:
1. Gemm (Linear layer: matrix multiplication with bias)
2. BatchNorm1d
3. Scaling by a parameter
4. Softmax

The goal is to replace some of these operations with custom CUDA kernels to speed things up. Let's consider which parts can be optimized.

First, the Linear layer's matrix multiplication with bias can be fused. The BatchNorm1d involves normalization and scaling, which can also be fused with the next scaling step. The Softmax might also be optimized with an online algorithm.

Let me think step by step:

1. **Fusing Gemm and BatchNorm**: 
   - The Linear layer computes x = Wx + b. 
   - Then BatchNorm computes (x - mean)/(std + eps) * gamma + beta. 
   - Maybe these can be combined into a single kernel, avoiding intermediate memory copies and computations.

2. **Fusing Scaling and Softmax**: 
   - After BatchNorm, the scaling by 'scale' is a simple element-wise multiplication. 
   - Softmax is exp(x_i)/sum(exp(x)), which can be optimized with online computation (computing sum while calculating exponentials to reduce memory usage).

Alternatively, perhaps fusing all steps from Gemm to Softmax into one big kernel would be better, but that might be too complex. Maybe breaking into two fused kernels: Gemm + BatchNorm + Scale, then Softmax.

Alternatively, since the scaling after BatchNorm is a simple multiplication, maybe it can be incorporated into the BatchNorm computation.

Wait, the BatchNorm already has gamma and beta parameters. The scale here is another parameter. So the scaling step is an element-wise multiplication with self.scale (which is a tensor of shape (1,)), so effectively scaling each row by that value. 

Let me think of the order:

After Gemm (Wx + b), then BatchNorm (normalized with gamma and beta), then multiply by scale (element-wise), then Softmax.

Maybe combining the Gemm, BatchNorm, and scaling into a single kernel would be beneficial. Let me see the computation steps:

For each element in the output after Wx + b (assuming W is the weight matrix and b is bias):

Then, BatchNorm computes:

y = (x - mean) / sqrt(var + eps) * gamma + beta

Then scaling: y_scaled = scale * y

Then Softmax: exp(y_scaled) / sum(exp(y_scaled))

Hmm, combining the first three steps (Gemm, BN, scaling) could be done in a single kernel. Let's see:

The Gemm step is a matrix multiplication with bias. The BatchNorm has mean, var, gamma, beta. Then scaling is an element-wise multiplication. 

So perhaps a fused kernel that does:

out = scale * [ ( (Wx + b) - mean ) / sqrt(var + eps) ) * gamma + beta ]

But the mean and var in BatchNorm are computed over the batch. Wait, in BatchNorm1d, the batch is along the batch dimension (since it's 1D here). So for a batch of (batch_size, out_features), the mean and variance are computed across the batch dimension for each feature.

Wait, no: BatchNorm1d computes the mean and variance over the batch and the features? Wait, no: for 1D, the BatchNorm1d computes the mean and variance over the batch dimension for each feature. So for each feature (each column in the output), the mean is computed across the batch samples, and the variance similarly.

But during training, the mean and variance are computed from the current batch, but during inference, they use the running stats. Since the problem's given code uses a Linear layer followed by BatchNorm, but the initialization includes bn_eps and bn_momentum, perhaps the model is supposed to be in training mode?

Wait, the problem's get_init_inputs() returns parameters for initializing the model, but the main code doesn't specify training or evaluation mode. Since the code is a model, perhaps the user expects it to work in both, but for the purposes of kernel optimization, maybe the code must handle both. However, when fusing operations, the kernel may need to handle training vs inference modes, which complicates things.

Alternatively, maybe the BatchNorm parameters (running_mean, running_var) are already updated, so the kernel can use the running stats when not in training mode. But this requires handling the mode in the kernel.

Hmm, that might be too complicated. Alternatively, perhaps it's better to split the operations into two fused kernels:

First, combine the Gemm (matrix multiply with bias) and the scaling (since scaling is a simple element-wise multiplication, perhaps it can be incorporated into the computation).

Wait, the Gemm is a dense matrix multiply. Let's see:

The Linear layer's computation is:

x = x @ W^T + b

Then BatchNorm: 

y = (x - mean) / sqrt(var + eps) * gamma + beta

Then scaling by self.scale (assuming self.scale is a scalar or a tensor of shape (1, out_features)), but in the given code, scale_shape is (1,), so it's a scalar, so each element is multiplied by scale.

Then Softmax.

Alternatively, maybe the scaling can be incorporated into the BatchNorm's gamma term. Let me see:

scale * (gamma * ... ) = (scale * gamma) * ... 

But gamma is already part of the BatchNorm parameters. So the scaling can be combined with gamma. However, since the scale is a learnable parameter (nn.Parameter), it's separate, so that approach might not be possible unless we adjust the parameters, which we can't do.

Hmm. Maybe the best approach is to fuse the Gemm + BatchNorm + scaling into one kernel. Let's see:

The Gemm is the main computational heavy part. Let's consider that matrix multiply with bias can be done in a kernel, then the subsequent steps can be incorporated.

Alternatively, the entire sequence (Gemm, BN, scaling, Softmax) could be done in a single kernel. But Softmax is a reduction operation (the sum in denominator), so it's a bit more challenging.

Alternatively, separate into two fused kernels:

1. Gemm + BatchNorm + scaling into a kernel that outputs the pre-softmax values.

2. A custom softmax kernel that is optimized (like using log-sum-exp trick for numerical stability and to avoid overflow).

Alternatively, let's consider that the Softmax can be optimized. The standard Softmax implementation in PyTorch might have overhead, so writing a custom Softmax kernel could help.

Alternatively, maybe the BatchNorm and scaling can be fused with the Gemm's computation, but that requires handling the normalization steps, which may need the mean and variance computed over the batch.

Hmm. Let's think of the computations step by step.

First, the Gemm: W is (out_features, in_features), so the matrix multiply is (batch_size, in_features) * (out_features, in_features)^T gives (batch_size, out_features). Then add the bias (out_features).

This is the first operation. The matrix multiply is O(batch_size * in_features * out_features) which is 1024*8192*8192. Wait, that's a huge number. Wait, actually the in_features and out_features are both 8192 here. Wait, the input is batch_size=1024, in_features=8192, so the matrix multiply is 1024 x 8192 * 8192 (since each row of input is 8192, and the weight is 8192 x 8192). Wait, actually, the matrix multiply for a linear layer is input (B x I) multiplied by weight (O x I)^T, so the result is B x O, where O is out_features.

So the computational cost is B*I*O. Here, B=1024, I=8192, O=8192. So the total FLOPs for Gemm are 1024 * 8192 * 8192. That's 6.87e+10 operations. That's a lot. So optimizing the Gemm would be very important.

But writing a custom matrix multiply kernel is challenging because PyTorch already uses optimized BLAS libraries (like cuBLAS) for matrix operations. So maybe the standard implementation is already efficient. However, combining the Gemm with subsequent steps might reduce overhead.

Alternatively, perhaps the BatchNorm can be fused with the Gemm's computation. Let me think.

The BatchNorm requires calculating the mean and variance over the batch for each feature. For each feature j in 0..O-1:

mean_j = (1/B) * sum_{i=0}^{B-1} (x_{i,j})

var_j = (1/B) * sum_{i=0}^{B-1} (x_{i,j} - mean_j)^2

Then normalized_x = (x - mean_j) / sqrt(var_j + eps)

Then scaled_normalized_x = normalized_x * gamma_j + beta_j

Then scaling by 'scale' (which is a scalar here, since scale_shape is (1,)):

scaled_normalized_x * scale

So the BatchNorm computation requires computing mean and variance for each feature, then applying the affine transformation. 

The mean and variance calculations are O(B*O) each. The scaling steps are O(B*O) as well. 

The Softmax is O(B*O) (for the exponentials) plus O(B) for the sum (per sample), but actually, the sum is over the O features for each sample, so it's O(B*O).

So, the total computations after Gemm are O(B*O) for mean/var, O(B*O) for normalization, O(B*O) for scaling, then O(B*O) for Softmax exponentials, and O(B*O) for the final division.

If the Gemm is the dominant term (since it's O(BIO)), but the other steps are O(BO), which are manageable. However, the constants matter, and fusing these steps could reduce memory traffic.

The problem is that the standard implementation may have to store intermediate tensors, which can cause memory copies. If we can fuse some steps into a single kernel, we can reduce memory usage and synchronization overhead.

Let's consider fusing the Gemm, BatchNorm, and scaling into a single kernel. 

The steps would be:

1. For each output element x_ij (sample i, feature j):

   a. Compute x_ij = (W_j Â· x_i) + b_j. Wait, no, the linear layer is x * W^T + b, so each element is sum_{k=0}^{I-1} (x_i,k * W_j,k) + b_j.

But doing this in a kernel would require a matrix multiply kernel. However, writing an optimized matrix multiply kernel would be difficult. Maybe better to leave the matrix multiply to cuBLAS, but combine the subsequent steps with it.

Alternatively, can we do the BatchNorm and scaling as part of the same kernel that processes the output of the Gemm?

Yes, but the problem is that the mean and variance are computed over the entire batch for each feature. To compute them, we need to first compute the entire matrix (the output of the Gemm), then compute the means and variances. So the mean and var calculations can't be done in the same kernel as the Gemm.

Hmm, so perhaps the BatchNorm requires two passes: one to compute the mean and variance, then another to normalize. That complicates things.

Alternatively, during training, the mean and variance are computed over the batch, so perhaps the kernel for the first step (Gemm) can be followed by a kernel that computes the mean and var, then another kernel for the normalization and scaling, but this might not save much time.

Alternatively, using a fused kernel for the BatchNorm and scaling. Since the scaling is just multiplying by a scalar, it can be incorporated into the computation.

Wait, the scaling step is just multiplying the entire tensor by 'scale', which is a scalar. So that can be done as part of the normalization step.

Let me outline the steps after the Gemm:

Compute the mean for each feature j:

mean_j = (1/B) * sum_{i} x_{i,j}

Then compute the variance var_j = (1/B) * sum_{i} (x_{i,j} - mean_j)^2

Then for each x_{i,j}:

normalized_x = (x_{i,j} - mean_j) / sqrt(var_j + eps)

scaled_normalized = normalized_x * gamma_j + beta_j

then scaled_scaled = scaled_normalized * scale

So the scaling by gamma and beta, then by the 'scale' parameter can be done in a single step.

So the computation can be done as:

scaled_scaled = [ (x_{i,j} - mean_j) / sqrt(var_j + eps) ] * (gamma_j * scale) + (beta_j * scale)

Wait, but the scaling is after the affine transformation. So:

scaled_scaled = ( (x - mean_j)/sqrt(...) * gamma_j + beta_j ) * scale

= (x - mean_j)/sqrt(...) * (gamma_j * scale) + beta_j * scale

Therefore, if we can precompute gamma_j * scale and beta_j * scale, then the computation can be done with those parameters. However, since gamma and beta are parameters of the BatchNorm layer, and scale is another parameter, perhaps we can combine them into new parameters for the kernel, but since they are all learnable, the kernel needs to accept all of them as inputs.

Alternatively, the kernel can take gamma, beta, scale as parameters and compute the combined terms on the fly.

So perhaps the normalization, affine transformation, and scaling can be done in a single kernel, provided the means and variances are precomputed.

The problem is that the means and variances are computed across the batch, so they must be calculated before the normalization step.

Thus, the steps would be:

1. Compute the Gemm output (x) using PyTorch's built-in Linear layer (since cuBLAS is already optimized).

2. Compute the mean and var for each feature in x.

3. Compute the normalized and scaled values in a custom kernel.

4. Compute the Softmax in a custom kernel.

Alternatively, steps 2 and 3 could be done in a single kernel if the kernel can compute the means and variances and then the normalized values in a single pass. But that would require a reduction step (summing over the batch dimension for each feature), which can be done with atomic operations, but that might be inefficient.

Alternatively, the kernel can first compute the mean and variance for each feature by aggregating across the batch. Let me think of a possible approach:

The kernel could process each feature in parallel. For each feature j, threads in a block can compute partial sums for the mean and variance, then combine them with atomic operations. But for 8192 features, each requiring a reduction over 1024 elements, this could be manageable.

Wait, but 8192 features each with 1024 elements would require 8192 independent reductions (each for a feature). That's a lot of work, but perhaps manageable on a GPU.

Alternatively, since the features are independent, each block can handle a single feature.

Here's an idea: Let's handle each feature j with a separate block. Each block processes the B elements (samples) for that feature.

For example, for each feature j (looped over in the kernel):

- Each thread in the block (say, 256 threads) loads a subset of the B elements (1024 samples), computes the partial sum for the mean and the sum of squares.

- Then, within the block, perform a reduction to compute the total sum and sum of squares for the feature.

- Then compute the mean_j and var_j.

But since we need to do this for all features, each block can handle one feature, and the number of blocks would be O(F) (F is out_features=8192). That's a lot of blocks, but the GPU can handle it.

However, this approach might not be efficient because of the high number of blocks. Alternatively, we can tile the computation, but it's getting complicated.

Alternatively, compute the means and variances using CUDA's reduction functions. For example, using thrust or cuBLAS's reduction functions, but that might not be allowed in a custom kernel.

Alternatively, the kernel for the normalization can first compute the mean and var, but that would require storing the intermediate results (mean and var arrays).

Hmm, this seems quite involved, but perhaps manageable.

Alternatively, let's consider that the BatchNorm's parameters are already in the model (gamma, beta), and the scaling parameter is also a tensor. So perhaps the fastest way is to first compute the mean and var (using PyTorch's built-in functions for BatchNorm, but that might not be faster than a custom kernel).

Alternatively, maybe the entire process can be done in a single kernel that fuses the Gemm, BatchNorm, scaling, and Softmax. But that would require handling the matrix multiply, which is a large computation.

Alternatively, let's consider that the main computational cost is the matrix multiply, and the subsequent steps are relatively small. So maybe the main optimization is to optimize the Softmax and the scaling steps.

Alternatively, the Softmax can be optimized with a custom kernel.

Let me look at the Softmax implementation. The standard Softmax is:

softmax(x_i) = exp(x_i) / sum(exp(x_j))

The problem is that the exp can cause overflow, so the implementation usually subtracts the max before exponentiating.

The custom kernel can handle this. Let's see:

The Softmax can be optimized by:

1. Find the maximum value in each row (since it's dim=1, the features).

2. Subtract the max from each element to prevent overflow.

3. Compute exp of each element.

4. Sum the exps along the row.

5. Divide each element by the sum.

This can be done in a kernel with per-row operations.

Alternatively, fusing this into a kernel that handles multiple steps.

Alternatively, combining the scaling and Softmax steps. Since the scaling is a scalar multiplication, it can be incorporated into the Softmax computation.

Wait, the scaling is applied before the Softmax. So the input to Softmax is (scale * (BatchNorm result)). So if we can compute the scaling during the Softmax computation, that might save a step.

So here's a possible approach for the Softmax kernel:

Given the input tensor x (after BatchNorm and scaling):

Compute the Softmax as follows:

For each sample i:

   max_val = max(x[i, :])

   exps = exp(x[i, j] - max_val) for all j

   sum_exps = sum(exps)

   output[i, j] = exps[j] / sum_exps

This can be done in a CUDA kernel with per-row processing. Let's see:

Each thread block can handle a single row (sample), and threads in the block handle the elements of that row.

The steps would be:

For each row (sample):

1. Find the maximum value in the row.

   This can be done with a reduction step within the block.

2. Compute the exponentials of (x - max_val).

3. Compute the sum of the exponentials.

4. Divide each exponential by the sum.

5. Store the result.

This can be done efficiently with a block of threads per row, and each thread handles a portion of the elements.

So writing a custom Softmax kernel may provide a speedup over the default implementation, especially for large feature sizes (8192 here).

Similarly, the BatchNorm could be optimized with a custom kernel, but given the complexity of computing the means and variances across the batch, perhaps it's better to leave the BatchNorm as is, but use a custom kernel for the Softmax.

Alternatively, let's see if the entire pipeline can be fused into a few kernels.

Alternative plan:

- Leave the Gemm and BatchNorm to PyTorch (since they are already optimized, but perhaps their combination can be faster with a custom kernel).

Wait, but the Gemm is the main computational cost, so maybe the best optimization is to replace the Softmax with a custom kernel.

Alternatively, perhaps the scaling can be incorporated into the Softmax kernel.

Let me proceed step by step.

First, let's consider writing a custom Softmax kernel.

The Softmax computation for a batch of size B, features F.

Each sample is a vector of F elements. The kernel will process each sample independently.

The steps for a sample:

1. Find the maximum value in the sample's feature vector.

2. Subtract the max from each element.

3. Compute exp of each element.

4. Compute the sum of all exps.

5. Divide each element by the sum.

This can be done efficiently in CUDA.

Let's write this as a CUDA kernel.

First, define the kernel function.

The kernel will process each sample (row) in parallel. Each block handles one row.

The number of blocks is B (1024), and each block has enough threads to handle the F elements (8192).

Wait, but 8192 elements per row would require a large number of threads. Let's think in terms of threads per block.

Suppose we have a block size of 256 threads. For F=8192, that would require 32 threads per block (but 8192 /256 = 32, so each thread would process 32 elements).

Alternatively, each thread in the block can process a portion of the elements.

Let me outline the kernel:

__global__ void custom_softmax(float *input, float *output, int B, int F) {

   int sample_idx = blockIdx.x;

   if (sample_idx >= B) return;

   // Each block (sample) processes its row.

   extern __shared__ float shared[];

   // Step 1: Find the max value in the row.

   // Each thread reads a value and finds the max.

   // Need to perform a reduction.

   // Maybe use shared memory for the reduction.

   // Let's proceed with a block-wide reduction.

   int tid = threadIdx.x;

   float max_val = -INFINITY;

   for (int i = tid; i < F; i += blockDim.x) {

       float val = input[sample_idx * F + i];

       if (val > max_val) {

           max_val = val;

       }

   }

   // Use a reduction to find the block's max.

   // Using a warp-based reduction?

   // Or use shared memory.

   // Alternatively, use atomic operations, but that might be slow.

   // Let's use a reduction in shared memory.

   __syncthreads();

   // Write to shared memory.

   shared[tid] = max_val;

   __syncthreads();

   // Then perform a reduction in shared memory.

   // For simplicity, let's use a loop.

   for (int s = blockDim.x / 2; s > 0; s >>=1) {

       if (tid < s) {

           shared[tid] = max( shared[tid], shared[tid + s] );

       }

       __syncthreads();

   }

   max_val = shared[0];

   // Broadcast max_val to all threads in the block.

   __syncthreads();

   max_val = shared[0];

   // Now compute the exponentials and accumulate the sum.

   // Each thread can process multiple elements.

   // Allocate a shared memory array for the exp terms.

   // But maybe better to compute in two passes.

   // First compute all the exp terms and store in shared memory?

   // Alternatively, each thread computes a portion of the elements.

   // Let's compute the exp terms first.

   // Each thread computes a portion of the exp terms.

   float *exp_terms = shared;

   int num_threads = blockDim.x;

   for (int i = tid; i < F; i += num_threads) {

       float val = input[sample_idx * F + i] - max_val;

       exp_terms[i] = expf(val); // Wait, but shared memory is limited.

       // Wait, this approach may not work because shared memory is limited.

       // For F=8192, each block needs to store 8192 floats, which is about 32KB per block.

       // But the maximum shared memory per block is 48KB or so, depending on the GPU.

       // So 8192 floats is 32KB (since 8192*4 bytes = 32KB). So maybe possible.

       // However, the initial shared array allocation would be:

       // extern __shared__ float shared[];

       // The size of shared must be allocated dynamically.

       // So the kernel call must specify the shared memory size.

       // Let's say each block needs 8192 floats (for exp_terms) plus some for the reduction steps.

       // Hmm, perhaps this is getting too complex.

       // Alternative approach: compute the exponentials and accumulate the sum in the first pass, then do the division.

       // Each thread can compute a partial sum of the exponentials.

       // Let me try this approach.

       // Step 1: Compute exp terms and partial sums.

       float exp_val = expf( input[sample_idx * F + i] - max_val );

       // Add to a partial sum.

       // Each thread has a private variable for partial_sum.

       float partial_sum = 0;

       partial_sum += exp_val;

       // Also, write the exp_val to a temporary array or directly use it later?

       // Maybe store the exp_val in a local array, but that's memory-heavy.

       // Alternatively, compute the exp terms and accumulate the sum in a reduction.

       // Let's try this:

       // First, compute exp terms and accumulate the sum.

       // Each thread processes F/thread_count elements.

       // Let's have each thread compute its exp_val, then accumulate into a shared partial sum.

       // Then the block reduces to get total_sum.

       // Then each thread can compute the exp_val / total_sum.

       // So:

       // First pass: compute exp_val and accumulate sum.

       // Initialize shared memory for partial sums.

       // Suppose we use blockDim.x threads per block.

       // Let's split the work into per-thread segments.

       // For each thread's elements:

       // Compute exp_val and add to partial_sum.

       // Then do a block reduction for the total_sum.

       // Let's try this approach.

       float exp_val = expf( input[sample_idx * F + i] - max_val );

       partial_sum += exp_val;

   }

   // Now, each thread has a partial_sum. Need to sum these into total_sum.

   // Use a reduction in shared memory.

   extern __shared__ float shared[];

   // Use shared memory for the partial sums.

   // Each thread writes its partial_sum to shared memory.

   shared[threadIdx.x] = partial_sum;

   __syncthreads();

   // Perform a reduction in shared.

   for (int s = blockDim.x / 2; s > 0; s >>=1) {

       if (threadIdx.x < s) {

           shared[threadIdx.x] += shared[threadIdx.x + s];

       }

       __syncthreads();

   }

   float total_sum = shared[0];

   __syncthreads();

   // Now compute the output values.

   // Each thread computes their portion of the output.

   for (int i = tid; i < F; i += blockDim.x) {

       float exp_val = expf( input[sample_idx * F + i] - max_val );

       output[sample_idx * F + i] = exp_val / total_sum;

   }

}

Wait, but in this approach, the exp_val is computed twice (once in the first loop for the partial sum, then again in the second loop for the output). To avoid this redundancy, perhaps we need to store the exp_vals somewhere.

Alternatively, store them in shared memory. But with 8192 elements, that requires 8192 floats per block, which is 32KB per block. If the block size is 256 threads, then each thread can handle 8192 / 256 = 32 elements.

Wait, 8192 elements divided by 256 threads per block gives each thread processing 32 elements. For each of these elements, the exp is computed twice. That's inefficient.

Hmm, this is getting complicated. Maybe it's better to process each element in a single pass.

Alternative approach:

Each block processes a single row (sample). The block has enough threads to process all F elements in parallel. For F=8192, we can have 8192 threads per block, but that's too many (the maximum is 1024 threads per block on most GPUs). So that's not feasible.

Alternatively, use a block size of 256 threads, and each thread handles F/256 elements. Let's see:

Each thread can handle 8192 / 256 = 32 elements.

The steps would be:

1. Each thread reads its portion of the elements from the input.

2. Find the max value in their portion, then do a block-wide reduction to find the global max.

3. Compute exp of (each element - global_max).

4. Accumulate the sum of all exp terms.

5. Each thread divides their exp terms by the total sum.

This way, the exp is computed once.

But the reduction for the max and the sum can be done efficiently with shared memory.

Here's a better outline:

Kernel:

__global__ void custom_softmax(float *input, float *output, int B, int F) {

   int sample_idx = blockIdx.x;

   if (sample_idx >= B) return;

   // Each block processes a sample (row).

   int tid = threadIdx.x;

   extern __shared__ float shared[];

   // Phase 1: Compute max value.

   // Each thread reads a portion of the elements.

   float local_max = -INFINITY;

   for (int i = tid; i < F; i += blockDim.x) {

       float val = input[sample_idx * F + i];

       if (val > local_max) {

           local_max = val;

       }

   }

   // Reduce to find block's max.

   __syncthreads();

   // Use shared memory to store partial maxes.

   shared[tid] = local_max;

   __syncthreads();

   // Reduction step for max:

   for (int s = blockDim.x / 2; s > 0; s >>= 1) {

       if (tid < s) {

           shared[tid] = max( shared[tid], shared[tid + s] );

       }

       __syncthreads();

   }

   float global_max = shared[0];

   __syncthreads();

   // Phase 2: Compute exp terms and accumulate sum.

   float local_sum = 0;

   for (int i = tid; i < F; i += blockDim.x) {

       float val = input[sample_idx * F + i] - global_max;

       float exp_val = expf(val);

       local_sum += exp_val;

       // Store exp_val temporarily?

       // Maybe store in shared memory?

       // Alternatively, we can compute this again later.

   }

   // Accumulate the local sums into a global sum.

   __syncthreads();

   // Store local_sum in shared memory.

   shared[tid] = local_sum;

   __syncthreads();

   // Reduce to get total_sum.

   for (int s = blockDim.x / 2; s > 0; s >>= 1) {

       if (tid < s) {

           shared[tid] += shared[tid + s];

       }

       __syncthreads();

   }

   float total_sum = shared[0];

   __syncthreads();

   // Phase 3: Compute output.

   for (int i = tid; i < F; i += blockDim.x) {

       float val = input[sample_idx * F + i] - global_max;

       float exp_val = expf(val);

       output[sample_idx * F + i] = exp_val / total_sum;

   }

}

This way, each thread computes their portion of exp_val twice: once in phase 2 and again in phase 3. To avoid this, we can store the exp_val in shared memory during phase 2, but that would require storing F values per block, which may exceed shared memory capacity (for F=8192, that's 32KB per block, which is acceptable if the GPU has enough shared memory per block).

Alternatively, increase the shared memory usage to store the exp terms.

Wait, let's adjust phase 2 to store the exp terms in shared memory:

Wait, but F=8192 elements per sample. The shared memory would need to store F floats (each exp_val) plus some for the reductions. But for a block, each thread can store their portion in shared memory.

But with a block size of 256 threads, each thread can store 8192/256 = 32 floats. So 256 * 32 = 8192 elements.

Thus, the shared memory needed is 8192 floats (32KB) plus some for the max and sum reductions. So the total shared memory per block would be 32KB + 256 floats (for the local sums in phase 2). So total around 33KB, which should be acceptable.

But this requires careful handling.

Alternative approach with storing exp_vals in shared:

Phase 1: Compute the max as before.

Phase 2:

Compute each exp_val and store in shared memory.

Each thread processes their elements and writes the exp_val to shared memory.

Then, compute the total_sum by summing all exp_vals stored in shared.

Phase 3: Read the exp_vals from shared and divide by total_sum.

Let me try this:

Kernel:

__global__ void custom_softmax(float *input, float *output, int B, int F) {

   int sample_idx = blockIdx.x;

   if (sample_idx >= B) return;

   int tid = threadIdx.x;

   extern __shared__ float shared[];

   // Shared memory is divided into two parts:

   // First part: for exp_terms (size F)

   // Second part: for reduction steps (max and sum)

   // But maybe better to compute step by step.

   // First compute the max.

   // Phase 1: Compute global max.

   // Each thread computes their local max.

   float local_max = -INFINITY;

   for (int i = tid; i < F; i += blockDim.x) {

       float val = input[sample_idx * F + i];

       if (val > local_max) {

           local_max = val;

       }

   }

   __syncthreads();

   // Reduce local_max to global_max.

   // Use shared memory for reduction.

   shared[tid] = local_max;

   __syncthreads();

   for (int s = blockDim.x / 2; s > 0; s >>=1) {

       if (tid < s) {

           shared[tid] = max( shared[tid], shared[tid + s] );

       }

       __syncthreads();

   }

   float global_max = shared[0];

   __syncthreads();

   // Phase 2: Compute exp_terms and store in shared.

   // Each thread computes their portion of exp_terms.

   for (int i = tid; i < F; i += blockDim.x) {

       float val = input[sample_idx * F + i] - global_max;

       shared[i] = expf(val);

   }

   __syncthreads();

   // Now compute total_sum by summing all elements in shared.

   float local_sum = 0;

   for (int i = tid; i < F; i += blockDim.x) {

       local_sum += shared[i];

   }

   __syncthreads();

   // Accumulate local_sum into a global_sum.

   shared[tid] = local_sum;

   __syncthreads();

   for (int s = blockDim.x / 2; s > 0; s >>=1) {

       if (tid < s) {

           shared[tid] += shared[tid + s];

       }

       __syncthreads();

   }

   float total_sum = shared[0];

   __syncthreads();

   // Phase 3: Compute output.

   for (int i = tid; i < F; i += blockDim.x) {

       float exp_val = shared[i];

       output[sample_idx * F + i] = exp_val / total_sum;

   }

}

This way, the exp terms are stored in shared memory once, and used in phase 3.

However, the first part of the shared memory is used for the exp_terms (size F), which requires 8192 floats (32KB) per block. With a block size of 256 threads, this should be manageable.

The total shared memory per block would be:

- 8192 floats for exp_terms: 8192 * 4 = 32KB

- The reduction steps (for max and sum) can be done in the same shared memory, but need to be careful not to overlap.

Wait, in phase 2, the exp_terms are stored starting at shared[0], and each thread writes to their respective indices. So for F=8192, the exp_terms take up the first 8192 elements of shared.

Then, in phase 3, when accumulating the local_sum, each thread reads their portion from the shared memory exp_terms.

But in phase 3's first loop for local_sum:

   for (int i = tid; i < F; i += blockDim.x) {

       local_sum += shared[i];

   }

Yes