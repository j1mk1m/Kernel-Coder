api_query_interval: 0.0
autorule_alignment_threshold: 0.7
autorule_num_alignment_samples: 20
autorule_num_samples_per_problem: 1
autorule_sample_best_and_worst: true
autorule_total_validation_limit: 200
base_traceset_path: /data/user_data/gyeongwk/flashinfer-trace
batch_size: 250
benchmark: KernelBench
build_cache_with_cpu: false
build_dir: /data/user_data/gyeongwk/Kernel-Coder/cache
dataset_name: ScalingIntelligence/KernelBench
dataset_src: local
eval: false
gpu_arch: Ampere
hardware: A6000_babel
language: CUDA
level: 2
max_tokens: 32768
measure_performance: true
memory: rules
memory_embedding_model_name: voyage/voyage-3-large
memory_model_name: hosted_vllm/Qwen/QwQ-32B
method: best-of-N
model_name: hosted_vllm/Qwen/QwQ-32B
num_correct_trials: 5
num_cpu_workers: 16
num_epochs: 1
num_eval_devices: 4
num_iterations: 1
num_parallel: 8
num_perf_trials: 100
num_workers: 1
run_name: evolrule_parallel_8_batch_level2
server_type: vllm
target_gpu: H100
temperature: 0.7
test: false
timeout: 300
verbose: false
vllm_host: babel-t9-32
vllm_port: 8082
