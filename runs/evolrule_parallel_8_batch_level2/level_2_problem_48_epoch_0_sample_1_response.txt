**Fusion Opportunities**: The code uses a series of operations: convolution, element-wise multiplication (scale), tanh, another element-wise multiplication (bias), and finally sigmoid. Instead of separate CUDA kernels for each of these steps, I should consider fusing these operations into a single fused kernel to minimize memory traffic and kernel launch overhead. This fused kernel would process all these steps in one go. The challenge would be to handle the convolution efficiently in CUDA (which is non-trivial) and combine it with the element-wise operations. However, writing a 3D convolution from scratch might be time-consuming and error-prone, especially handling padding and strides. Maybe instead, I can fuse the post-convolution element-wise operations (scale * tanh * bias * sigmoid) into a single kernel. The convolution itself might remain as the PyTorch operator, but the following element-wise operations can be fused.

**Element-wise Fusion**: After the convolution, the operations are:

x = x * scaling_factor 

x = torch.tanh(x)

x = x * self.bias 

x = torch.sigmoid(x)

These four steps can be combined into a single kernel. Let me see the sequence:

The scaling_factor is a parameter of shape (out_channels, 1, 1, 1). The bias is also of the same shape. Let's see the operations step by step for each element:

First, multiply by scaling_factor (element-wise multiplication, but since scaling_factor is a 4D tensor with singleton dimensions beyond channel, it's a channel-wise scaling). Then apply tanh, then multiply by the bias (another channel-wise scaling), then apply sigmoid.

Wait, but tanh and sigmoid are applied element-wise. So the combined computation would be:

result = sigmoid(tanh(x * scaling_factor) * bias)

Wait, but the order is:

After convolution:

x = conv_out

x = x * scaling_factor (element-wise, channel-wise scaling)

x = tanh(x) 

x = x * bias (another element-wise channel-wise scaling)

x = sigmoid(x)

Therefore, the final expression is:

sigmoid( (tanh( (conv_out * scaling_factor) ) * bias) )

Wait, but the last multiplication by bias is after tanh and before sigmoid.

Alternatively, perhaps the operations can be expressed as:

sigmoid( bias * tanh( scaling_factor * conv_out ) )

But the order is important here. Since multiplication is commutative, but scaling factors are channel-wise, the exact sequence may matter for the kernel.

But in any case, combining these into a single kernel would allow processing all these steps in parallel without intermediate memory copies. Let's think of the element-wise operations after the convolution.

The fused kernel can take the input tensor (convolution output), scaling_factor (a parameter), bias (a parameter), and perform all the element-wise operations in one pass.

Thus, the steps for the fused kernel would be:

for each element in the tensor:

value = input * scaling_factor[channel]

value = tanh(value)

value = value * bias[channel]

value = 1 / (1 + exp(-value))  # sigmoid

Wait, but the bias is a parameter with shape (out_channels, 1, 1, 1). So for each element in the tensor, the scaling factor and bias are determined by the channel index.

Therefore, in the kernel, for each element (n, c, d, h, w):

value = input[n, c, d, h, w]

value *= scaling_factor[c, 0, 0, 0]

value = tanh(value)

value *= bias[c, 0, 0, 0]

value = 1.0 / (1.0 + exp(-value))

The output is this value.

Therefore, this can be implemented in a CUDA kernel as follows:

- The input tensor is 5D (batch, channels, depth, height, width).

- The scaling_factor and bias are 4D tensors (since bias_shape is (out_channels, 1, 1, 1)). However, in the code, they are stored as parameters. The kernel can treat them as 1D arrays where the index corresponds to the channel.

Wait, but in PyTorch, the tensors are stored in contiguous memory. So for scaling_factor, which is (C, 1, 1, 1), the first element of the flattened array corresponds to channel 0, and so on. So for any element in the input tensor at position (c, ...), the scaling factor for that element is scaling_factor[c], since the other dimensions are 1. Therefore, in the kernel, for a given element, the channel index is known, so we can index into the scaling and bias parameters using the channel.

Thus, the fused kernel can be written to process all these steps in a single pass.

Therefore, the plan is:

- Keep the Conv3d as is (since writing a custom 3D convolution is complex and may not be worth the time, especially given that PyTorch's implementation is optimized).

- Fuse the subsequent element-wise operations (scale * tanh * bias * sigmoid) into a single custom CUDA kernel.

Thus, in the ModelNew class, after the convolution, we replace the sequence of element-wise operations with a call to this fused kernel.

Therefore, the steps are:

1. Write the fused CUDA kernel.

2. Compile it using load_inline.

3. Use it in the forward pass.

Additionally, we might also consider if the order of operations can be rearranged for efficiency. For instance, multiplying by scaling_factor and bias can be combined into a single scaling factor (scaling_factor * bias), but since they are parameters, their values can be combined only at runtime. Alternatively, in the kernel, since both are scalars per channel, we can multiply them first in the code, but perhaps the kernel is straightforward as written.

Now, let's proceed to write the fused kernel.

The input to the kernel will be:

- The output of the convolution (x).

- scaling_factor parameter (a tensor of shape (C, 1, 1, 1)).

- bias parameter (same shape as scaling_factor).

The kernel will process each element in x, applying the above sequence.

CUDA kernel considerations:

- The input tensors are 5D, but the kernel can treat them as 1D arrays for processing, with the channel being part of the index.

- To compute the channel from the linear index, we need to know the strides or the tensor's dimensions. Alternatively, we can compute the channel index based on the position.

Alternatively, since the scaling and bias are channel-wise, for a given element, the channel is the second dimension. So, for each element's position (n, c, d, h, w), the channel c can be extracted. Therefore, in the kernel, given a linear index, we can compute the channel by:

Assuming that the tensor is stored in C order (row-major), the layout is:

element index = n * C * D * H * W + c * D * H * W + d * H * W + h * W + w

Thus, the channel index can be obtained by:

c = (index // (D * H * W)) % C

But this may be computationally expensive in the kernel. Alternatively, we can precompute the number of elements per channel (D*H*W), and compute c = (index / elements_per_channel).

Alternatively, to simplify, perhaps it's better to compute the channel from the position. However, since the kernel is processing elements in a 1D fashion, perhaps it's more efficient to compute the channel as (index // (D*H*W)) % C.

Alternatively, in the kernel, the input tensors have dimensions that can be passed as arguments. For example, the kernel can be given the dimensions (batch_size, channels, depth, height, width), and then for a given linear index, compute the channel.

But passing these dimensions as parameters would require the kernel launch code to capture them, which is manageable.

Alternatively, since the scaling_factor and bias are of shape (C, 1, 1, 1), their data is contiguous and the c-th element can be accessed as scaling_factor[c], since the rest are singleton dimensions.

Thus, in the kernel, for each element, the channel can be computed, and then the scaling and bias factors are read from the respective tensors.

Let me outline the CUDA kernel code.

First, the kernel function:

__global__ void fused_operations_kernel(
    const float* input, const float* scaling, const float* bias, float* output,
    int batch_size, int channels, int depth, int height, int width) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int size = batch_size * channels * depth * height * width;

    if (index < size) {
        // Compute the channel index
        int elements_per_channel = depth * height * width;
        int c = (index / elements_per_channel) % channels;

        // Compute the value
        float val = input[index] * scaling[c];  // scaling factor
        val = tanhf(val);                       // tanh
        val *= bias[c];                         // multiply by bias
        val = 1.0f / (1.0f + expf(-val));       // sigmoid

        output[index] = val;
    }
}

Wait, but the scaling and bias tensors are 4D (C, 1, 1, 1). So when we access scaling[c], the actual stride may not be 1. Wait, PyTorch stores tensors in contiguous memory. So a tensor of shape (C, 1, 1, 1) with contiguous memory has a stride of (1, 0, 0, 0). Therefore, the elements are stored as C elements in a row. Therefore, the c-th element can be accessed as scaling[c], because the rest dimensions are 1, so the stride after the first dimension is 0. So yes, scaling[c] is correct.

Thus, this kernel should work.

Then, the wrapper function in the CUDA code would:

- Compute the total number of elements (size).

- Allocate the output tensor (same shape as input).

- Launch the kernel with appropriate grid and block dimensions.

Now, the kernel requires the dimensions of the input tensor (batch_size, channels, depth, height, width). But when the kernel is called, these parameters can be extracted from the input tensor's sizes.

Alternatively, the kernel can take the dimensions as arguments. In the wrapper function, we can get them from the input tensor.

Now, the wrapper function in C++:

torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor scaling, torch::Tensor bias) {
    CHECK_INPUT(input);
    CHECK_INPUT(scaling);
    CHECK_INPUT(bias);

    int batch_size = input.size(0);
    int channels = input.size(1);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    const int num_elements = batch_size * channels * depth * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_operations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        scaling.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, depth, height, width
    );

    return output;
}

Wait, but in the kernel's argument list, the parameters are:

input, scaling, bias, output, batch_size, channels, depth, height, width.

So the kernel function must be declared with those parameters.

Now, in the Python code, the kernel is defined via load_inline. The code must be written correctly.

Now, putting it all together in the code:

First, the CUDA source code as a string.

Then, the wrapper function in C++.

Additionally, need to include headers.

Now, in the example given, the user used load_inline with functions listed. The example had:

functions=["elementwise_add_cuda"]

So in this case, the wrapper function is called fused_operations_cuda.

Thus, the CUDA source code:

fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_operations_kernel(
    const float* input, const float* scaling, const float* bias, float* output,
    int batch_size, int channels, int depth, int height, int width) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total_size = batch_size * channels * depth * height * width;

    if (index < total_size) {
        // Compute channel
        int elements_per_channel = depth * height * width;
        int c = (index / elements_per_channel) % channels;

        float val = input[index] * scaling[c];
        val = tanhf(val);
        val *= bias[c];
        val = 1.0f / (1.0f + expf(-val));

        output[index] = val;
    }
}

torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor scaling, torch::Tensor bias) {
    // Check that the inputs are on the same device and are contiguous
    CHECK_CUDA(input);
    CHECK_CUDA(scaling);
    CHECK_CUDA(bias);
    CHECK_CONTIGUOUS(input);
    CHECK_CONTIGUOUS(scaling);
    CHECK_CONTIGUOUS(bias);

    int batch_size = input.size(0);
    int channels = input.size(1);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    int num_elements = batch_size * channels * depth * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_operations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), scaling.data_ptr<float>(),
        bias.data_ptr<float>(), output.data_ptr<float>(),
        batch_size, channels, depth, height, width
    );

    cudaDeviceSynchronize();  // Ensure kernel finishes

    return output;
}
"""

Wait, but in the code, need to include the CHECK_CUDA and CHECK_CONTIGUOUS macros. Wait, in PyTorch's extension, these are part of the AT_ASSERT macros. Alternatively, maybe they are not defined, so perhaps I need to use the standard checks.

Wait, in the example provided in the user's example, they didn't include such checks. Perhaps it's better to omit them for brevity, but in practice, they are important. Alternatively, use the standard code.

Alternatively, maybe the code can work without those checks, but to be safe, perhaps add the necessary includes.

Wait, the example code used:

#include <torch/extension.h>

Which includes the necessary headers.

The functions like CHECK_CUDA are not standard macros. So perhaps the user's example just used the code without those checks, so I can skip them for simplicity, but the code should work as long as the tensors are on the correct device and contiguous.

Alternatively, in the wrapper function, we can assert that the input tensors are on the same device, but since we're using PyTorch's tensor.data_ptr, they must be on CUDA.

Wait, in the PyTorch model, the parameters scaling and bias are parameters of the model, so they should be on the same device as the input. The user's get_inputs() returns tensors on CPU, but in the original code, the model is presumably moved to CUDA when needed.

Wait, the original get_inputs() in the given code:

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]

Wait, in the original code, the parameters scaling_factor and bias are nn.Parameters, so they are initialized with torch.randn and placed on the device (probably CUDA if the model is on CUDA). The inputs from get_inputs() are generated with torch.rand, which by default is on CPU. But in the original code's forward function, the inputs would need to be on the same device as the model's parameters.

However, in the ModelNew code, we can assume that all parameters and inputs are on the same device (CUDA), so the CUDA kernel can proceed.

Therefore, perhaps the code can proceed without those checks.

Now, the wrapper function is called fused_operations_cuda, which takes input (the convolution output), scaling (the scaling_factor parameter), and bias (the bias parameter).

Thus, in the Python code, the model's forward function would be:

def forward(self, x):
    x = self.conv(x)
    x = self.fused_operations.fused_operations_cuda(x, self.scaling_factor, self.bias)
    return x

Wait, but in the original model, the scaling_factor and bias are parameters of the model. Therefore, in the new model, the parameters are still part of the model, so they need to be stored as attributes.

Therefore, in the ModelNew class, the parameters (scaling_factor and bias) are kept as nn.Parameters, and the fused kernel is called with those parameters.

Therefore, the code structure would be:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.scaling_factor = nn.Parameter(torch.randn(bias_shape))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        # Load the fused CUDA kernel
        self.fused_operations = fused_operations  # The module returned by load_inline

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_operations.fused_operations_cuda(x, self.scaling_factor, self.bias)
        return x

Wait, but in the example given, the user's code had:

elementwise_add = load_inline(...)

then in the model:

self.elementwise_add = elementwise_add

and then called self.elementwise_add.elementwise_add_cuda(...).

Thus, the structure here is similar.

Therefore, in the code:

First, define the CUDA source code as a string.

Then, compile it using load_inline, with the function name.

Therefore, the code steps would be:

First, define the fused_ops_source as above.

Then:

fused_ops_cpp_source = "torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor scaling, torch::Tensor bias);"

Then:

fused_operations = load_inline(
    name="fused_operations",
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=["fused_operations_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Therefore, putting all together, the ModelNew class would have the parameters, the conv layer, and the fused_operations module.

Now, the input to the fused kernel is the output of the convolution, and the scaling and bias parameters.

Now, testing if this code would compile:

Potential issues:

- The kernel's computation of the channel index. Let's see:

elements_per_channel = depth * height * width

The index is the global linear index (since the input is contiguous). So for a given index:

The batch index is batch_id = index / (channels * depth * height * width)

Then, within the batch, the remaining index is:

remaining = index % (channels * ... )

But perhaps the way we compute 'c' is correct.

Alternatively, perhaps the channel can be computed as:

c = (index / (depth * height * width)) % channels

Wait, let's see:

Suppose the batch size is 1.

Then, for the first element (index 0):

elements_per_channel is D*H*W. So index / elements_per_channel is 0, so c = 0 % channels.

Which is correct.

For the next elements within the same channel, the division would still give 0 until the elements_per_channel is reached.

Wait, elements_per_channel is D*H*W. The channel is for each batch and channel. Wait, the total elements per channel per batch is D*H*W. So for a given batch, the channels are sequential.

Thus, for a given index:

total elements per batch: channels * D * H * W

Thus, within a batch, the channel is (index_in_batch) / (D*H*W). 

Therefore, the calculation of 'c' as (index / elements_per_channel) % channels is correct.

Because:

index = batch_id * (channels * D*H*W) + channel_id * D*H*W + rest.

Therefore, dividing by elements_per_channel (D*H*W) gives (batch_id * channels + channel_id) + rest/(D*H*W), but since rest < D*H*W, it's (batch_id * channels + channel_id). Then mod channels gives channel_id.

Yes, that works.

Another possible issue: the dimensions of scaling and bias must match the channels. The code assumes that scaling and bias have shape [channels, 1, 1, 1], so the size(0) must equal channels. Since in the model's __init__, scaling_factor and bias are initialized with shape bias_shape, which is (out_channels, 1, 1, 1), so that should be okay.

Another point: the sigmoid function in CUDA uses expf. The code uses 1.0f / (1.0f + expf(-val)). That is correct.

Now, the final code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_operations_kernel(
    const float* input, const float* scaling, const float* bias, float* output,
    int batch_size, int channels, int depth, int height, int width) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total_size = batch_size * channels * depth * height * width;

    if (index < total_size) {
        // Compute channel index
        int elements_per_channel = depth * height * width;
        int c = (index / elements_per_channel) % channels;

        float val = input[index] * scaling[c];
        val = tanhf(val);
        val *= bias[c];
        val = 1.0f / (1.0f + expf(-val));

        output[index] = val;
    }
}

torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor scaling, torch::Tensor bias) {
    int batch_size = input.size(0);
    int channels = input.size(1);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    int num_elements = batch_size * channels * depth * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_operations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), scaling.data_ptr<float>(),
        bias.data_ptr<float>(), output.data_ptr<float>(),
        batch_size, channels, depth, height, width
    );

    cudaDeviceSynchronize(); // Ensure kernel completion

    return output;
}
"""

fused_ops_cpp_source = (
    "torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor scaling, torch::Tensor bias);"
)

# Compile the fused operations CUDA kernel
fused_operations = load_inline(
    name="fused_operations",
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=["fused_operations_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.scaling_factor = nn.Parameter(torch.randn(bias_shape))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_operations = fused_operations  # The loaded CUDA module

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_operations.fused_operations_cuda(x, self.scaling_factor, self.bias)
        return x

def get_inputs():
    batch_size = 128
    in_channels = 3
    depth, height, width = 16, 64, 64
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    in_channels = 3
    out_channels = 16
    kernel_size = 3
    scaling_factor = 2
    bias_shape = (out_channels, 1, 1, 1)
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]
```

Wait, in the get_inputs function of the original code, the tensors are generated on CPU. But in the new code, the model is presumably on CUDA, so inputs should be on CUDA. So in get_inputs(), we need to move the tensors to CUDA. The original get_inputs() didn't have .cuda().

Looking back at the original problem's code:

Original get_inputs() returns [torch.rand(...)], which is on CPU. The original get_init_inputs() is for model initialization.

But in the user's example, the get_inputs() for the first example had .cuda().

In the given architecture for the current problem, the user's get_inputs() is:

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

But since the model is to be used on GPU, the inputs must be moved to CUDA. Therefore, the new get_inputs() should include .cuda():

Hence, in the new code's get_inputs() function, I added .cuda().

Also, the parameters in the ModelNew are initialized with torch.randn(bias_shape). Those should also be on CUDA. However, since the model is typically placed on the desired device (e.g., model.cuda()), the parameters will be moved there. Alternatively, to ensure they are initialized on CUDA, we can use .cuda() in their initialization. But in PyTorch, parameters are initialized on the current device, so if the model is created on the CPU, they would be on CPU. To avoid that, perhaps better to initialize them on the correct device.

However, the code as written may work if the model is moved to CUDA after initialization.

Alternatively, to make sure, in the __init__ of ModelNew:

self.scaling_factor = nn.Parameter(torch.randn(bias_shape).cuda())
self.bias = nn.Parameter(torch.randn(bias_shape).cuda())

But this assumes that CUDA is available. Alternatively, the user can handle device placement. Since the problem states to make the code fully functional, perhaps we should include .cuda() in the get_inputs() and in the parameters' initialization.

Therefore, adjusting the ModelNew's __init__ to:

self.scaling_factor = nn.Parameter(torch.randn(bias_shape).cuda())
self.bias = nn.Parameter(torch.randn(bias_shape).cuda())

But in the code above, the user may have the model on CPU, but in practice, the model is moved to GPU. Alternatively, perhaps it's better to have the parameters initialized on the same device as the input.

However, in the problem's original code, the parameters are initialized with torch.randn, which is on CPU. The example given by the user in their first code had parameters initialized on CPU, but then the inputs were on CUDA. Therefore, perhaps the user expects the parameters to be on the same device as the inputs. Therefore, in the new code, the parameters should be on CUDA.

Therefore, in the __init__ of ModelNew:

self.scaling_factor = nn.Parameter(torch.randn(bias_shape).cuda())
self.bias = nn.Parameter(torch.randn(bias_shape).cuda())

Also, the get_inputs() function now returns CUDA tensors.

Thus, in the code above, the get_inputs() has .cuda() added.

Wait, in the code I wrote earlier, the get_inputs() was written as:

def get_inputs():
    batch_size = 128
    in_channels = 3
    depth, height, width = 16, 64, 64
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

Wait, but in the original problem's code, those variables (batch_size, in_channels, etc.) are defined outside. But in the problem's given code, they are global variables. However, in the new code, those values may not be available. Therefore, perhaps better to hardcode them in the get_inputs() function as in the original problem's code.

Looking at the original code:

The original code had:

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 64, 64
kernel_size = 3
scaling_factor = 2
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]

Thus, in the new code's get_inputs(), the variables are in the global scope, so they can be used directly. However, when moving the code to ModelNew, perhaps the get_inputs() and get_init_inputs() are part of the problem's setup, so they can remain as they were, but in the new code, the get_inputs() should return CUDA tensors.

Wait, the original code's get_inputs() returns tensors on CPU. To make the new code functional, the tensors should be on CUDA. So modifying get_inputs() as:

def get_inputs():
    a = torch.rand(batch_size, in_channels, depth, height, width).cuda()
    return [a]

But since the problem states to output the new code, we can adjust the get_inputs() function accordingly.

Therefore, in the code block above, I have included the .cuda() in get_inputs().

Also, the parameters in ModelNew's __init__ should be initialized on CUDA.

Hence, the final code would have those adjustments.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

fused_ops_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_operations_kernel(
    const float* input, const float* scaling, const float* bias, float* output,
    int batch_size, int channels, int depth, int height, int width) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total_size = batch_size * channels * depth * height * width;

    if (index < total_size) {
        // Compute channel index
        int elements_per_channel = depth * height * width;
        int c = (index / elements_per_channel) % channels;

        float val = input[index] * scaling[c];
        val = tanhf(val);
        val *= bias[c];
        val = 1.0f / (1.0f + expf(-val));

        output[index] = val;
    }
}

torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor scaling, torch::Tensor bias) {
    int batch_size = input.size(0);
    int channels = input.size(1);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    int num_elements = batch_size * channels * depth * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_operations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), scaling.data_ptr<float>(),
        bias.data_ptr<float>(), output.data_ptr<float>(),
        batch_size, channels, depth, height, width
    );

    cudaDeviceSynchronize(); // Ensure kernel completion

    return output;
}
"""

fused_ops_cpp_source = (
    "torch::Tensor fused_operations_cuda(torch::Tensor input, torch::Tensor scaling, torch::Tensor bias);"
)

# Compile the fused operations CUDA kernel
fused_operations = load_inline(
    name="fused_operations",
    cpp_sources=fused_ops_cpp_source,
    cuda_sources=fused_ops_source,
    functions=["fused_operations_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.scaling_factor = nn.Parameter(torch.randn(bias_shape).cuda())
        self.bias = nn.Parameter(torch.randn(bias_shape).cuda())
        self.fused_operations = fused_operations  # The loaded CUDA module

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_operations.fused_operations_cuda(x, self.scaling_factor, self.bias)
        return x

def get_inputs():
    batch_size = 128
    in_channels = 3
    depth, height, width = 16, 64, 64
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    in_channels = 3
    out_channels = 16
    kernel_size = 3
    scaling_factor = 2
    bias_shape = (out_channels, 1, 1, 1)
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape]
```