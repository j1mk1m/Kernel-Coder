The user wants the code to be as fast as possible. 

The user also wants you to **only replace the operators with custom CUDA kernels and leave the model structure unchanged.** 

The user also specifies that you may choose to replace any operators in the forward method, but you cannot change the model structure. 

The user wants you to make the fastest possible version of this model.

The user also specifies that you can replace multiple operators with custom CUDA kernels, fuse operators, or use algorithmic changes. The user also wants you to not change the model structure. 

The user wants you to make sure the code compiles. 

The user also wants you to write the code in the same style as the example given. 

The user wants the code to be completely self contained. 

The user wants you to inline the CUDA code in the same file, using the torch.utils.cpp_extension.load_inline method. 

The user wants you to name the new model as ModelNew.

The user wants you to use PyTorch extension APIs where possible (i.e. use torch::Tensor in the CUDA code, not raw pointers unless necessary). 

The user wants you to consider operator fusion opportunities (like combining multiple operations into one kernel, e.g. the tanh(softplus(x)) * x part can be fused into a single kernel).

The user wants you to make sure that the output code works and is faster than the original. 

The user wants you to not change the model structure, so the new model should have the same layers (Conv2d, BatchNorm2d), and the same forward sequence (conv -> activation -> bn). 

The user wants you to output the code for the new ModelNew class, along with any required imports, the definition of the custom CUDA kernels, and the get_inputs and get_init_inputs functions. 

The user wants you to make sure the code is as fast as possible. 

The user wants you to make sure that the fused kernel for the activation part (tanh(softplus(x)) * x) is implemented in a single CUDA kernel for maximum speed. 

The user wants you to also consider whether fusing the convolution with the activation or with the batch norm would be beneficial, but the user wants the model structure to remain the same. 

The user wants you to not modify the parameters of the model (e.g., in_channels, out_channels, etc.) or the forward pass steps. 

The user wants the code to be fully self-contained and compilable. 

The user wants the code to use PyTorch's C++ extensions with inline CUDA code. 

The user wants you to write the code in Python with the CUDA kernels inlined as strings. 

The user also wants you to ensure that the custom kernels are loaded properly using load_inline. 

The user wants you to name the custom kernels appropriately, such as "fused_activation_cuda" for the activation part. 

The user also wants you to make sure that all the dependencies are properly included. 

The user also wants to minimize memory allocations and temporary tensors by fusing operations. 

The user also wants you to implement the fused activation (tanh(softplus(x)) * x) in a single kernel to avoid intermediate tensors. 

The user also wants you to consider whether the convolution can be fused with the activation or batch norm, but since the model structure must stay the same, perhaps only the activation can be fused. 

The user wants you to make sure that the code is correct and that the outputs are the same as the original model (within numerical precision). 

The user wants you to also think about optimizing the CUDA kernel for coalesced memory access, block/grid dimensions, etc. 

The user wants you to also think about using shared memory for the convolution, but the user may not have enough expertise, so better to stick to simpler optimizations first. 

The user wants you to also consider that the batch normalization might have some optimizations possible, but again, since the structure must remain, perhaps just replacing the activation with a fused kernel is the main way. 

The user wants you to proceed with the fused activation kernel as the main optimization, and possibly also replacing the convolution with a custom kernel if possible, but considering that PyTorch's Conv2d is already highly optimized, maybe not. 

The user wants you to make the decision based on what gives the best speedup without changing the structure. 

The user wants you to prioritize the fused activation kernel first. 

The user wants you to also make sure that all the CUDA kernels are correctly defined and that the Python code correctly calls them. 

The user also wants you to make sure that the get_inputs and get_init_inputs functions are included in the new code as well, but their implementations can stay the same unless required by the new model structure. 

The user also wants the new code to have the same interface as the original, so the forward method must take the same inputs and return the same outputs. 

The user also wants the code to be compatible with PyTorch versions >= 1.13. 

The user wants you to output the code as described. 

Okay, I need to optimize the given PyTorch model by replacing some operators with custom CUDA kernels to improve speed. The model has a forward pass that does convolution, then an activation function involving tanh(softplus(x)) * x, followed by batch normalization. 

First, I should look at the activation part: torch.multiply(torch.tanh(torch.nn.functional.softplus(x)), x). This involves multiple operations: softplus, tanh, and multiplication. Since these are all element-wise operations, they can be fused into a single CUDA kernel. This would eliminate intermediate tensors and reduce memory traffic, which should speed things up.

The convolution and batch norm are already part of PyTorch's optimized layers, so maybe they don't need replacement unless there's a way to fuse them with the activation. But the user specified not to change the model structure, so I can't combine layers. So I'll focus on fusing the activation steps.

Let me think about the fused kernel. The activation is element-wise, so each element can be computed independently. The operations are:

For each element x_i:

result_i = tanh(softplus(x_i)) * x_i

Softplus is log(1 + exp(x)), so tanh of that is tanh(log(1 + exp(x))). Let's see if this can be simplified algebraically. Let's compute:

Let s = softplus(x) = log(1 + exp(x))
tanh(s) = (exp(s) - exp(-s)) / (exp(s) + exp(-s)) 

But exp(s) = exp(log(1+exp(x))) = 1 + exp(x). So substituting:

tanh(s) = ( (1 + exp(x)) - 1/(1 + exp(x)) ) / ( (1 + exp(x)) + 1/(1 + exp(x)) )

Hmm, not sure if this simplifies. Maybe it's better to compute numerically as per the original expression.

In code terms, for each element:

temp = torch.tanh(torch.nn.functional.softplus(x))
result = temp * x

But in CUDA, I can compute this in a single step for each element without creating intermediate tensors. Let's write the kernel.

First, the softplus function: softplus(x) = log(1 + exp(x)). But numerically, this can be optimized. For large x, it's ~x; for small x, ~log(1+exp(x)). But in CUDA, I can compute this directly.

Wait, tanh(softplus(x)):

Let me see: 

softplus(x) = log(1 + exp(x))

Then tanh(softplus(x)) = tanh(log(1 + exp(x))).

Let me compute tanh(log(1 + exp(x))):

Let me let y = exp(x). Then:

log(1 + y) is the argument to tanh.

tanh(log(1+y)) = [ (1+y) - 1/(1+y) ] / [ (1+y) + 1/(1+y) ) ]

Multiply numerator and denominator by (1+y):

Numerator: (1+y)^2 -1 = (1 + 2y + y² -1) = 2y + y² = y(2 + y)

Denominator: (1+y)^2 +1 = 1 + 2y + y² +1 = 2 + 2y + y² = ?

Wait, maybe there's a simpler way. Let me compute tanh(log(1+y)) where y=exp(x):

Let’s let z = log(1 + y). Then tanh(z) = (e^z - e^{-z})/(e^z + e^{-z}) 

But e^z = 1+y, so:

(e^z - 1/(e^z)) / (e^z + 1/(e^z)) 

= [ ( (1+y) - 1/(1+y) ) / ( (1+y) + 1/(1+y) ) ]

Multiply numerator and denominator by (1+y):

Numerator: (1+y)^2 -1 = 1 + 2y + y² -1 = 2y + y² = y(2 + y)

Denominator: (1+y)^2 +1 = 1 + 2y + y² +1 = 2 + 2y + y² = (y² + 2y + 2) ?

Wait, maybe I made a mistake. Let me check:

Wait denominator after multiplying: (1+y)^2 +1?

Wait original denominator after multiplying by (1+y):

[(1+y)^2 +1] ?

Wait no, the denominator after multiplying is (1+y)^2 +1 ?

Wait the original denominator after multiplying is ( (1+y) + 1/(1+y) )*(1+y) = (1+y)^2 +1 ?

Yes, that's correct. So denominator is (1+y)^2 +1? Wait no, let me re-express:

Wait the denominator before multiplying was ( (1+y) + 1/(1+y) ), so multiplying by (1+y) gives (1+y)^2 +1. So denominator is ( (1+y)^2 +1 ) ?

Wait, let me re-express the entire thing:

tanh(z) where z = log(1+y):

= [ (1+y) - 1/(1+y) ] / [ (1+y) + 1/(1+y) ]

Let me write this as [ ( (1+y)^2 -1 ) / (1+y) ] divided by [ ( (1+y)^2 +1 ) / (1+y) ]

So numerator becomes ( (1+y)^2 -1 ) / (1+y)

Denominator becomes ( (1+y)^2 +1 ) / (1+y)

Thus, tanh(z) = [ (1+y)^2 -1 ] / [ (1+y)^2 +1 ]

Expand numerator: (1 + 2y + y² -1 ) = 2y + y²

Denominator: 1 + 2y + y² +1 = 2 + 2y + y² 

Wait, so:

tanh(z) = (2y + y²) / (2 + 2y + y² )

Factor numerator and denominator:

Numerator: y(2 + y)

Denominator: (y² + 2y + 2) ?

Wait, maybe there's no simplification here. Alternatively, perhaps we can express this in terms of y:

Let me see:

Let me denote y = exp(x). Then tanh(softplus(x)) = [ y(2 + y) ] / ( y² + 2y + 2 )

Hmm, but not sure if this helps computationally. Maybe better to compute as per the original expression in code.

Alternatively, compute tanh(softplus(x)) directly using the definitions.

But in CUDA, we can compute this in a single step for each element.

The plan is to create a CUDA kernel that takes an input tensor x, and computes the output tensor as tanh(softplus(x)) * x for each element. This avoids creating intermediate tensors for softplus and tanh, which saves memory and time.

So the kernel function would loop over each element and compute the value directly.

Next, the convolution and batch norm are already part of PyTorch's optimized ops, so maybe replacing them won't give much benefit. However, perhaps the batch norm can also be fused into the activation step? But the model structure requires that the batch norm comes after the activation. Since we can't change the order, we can't fuse them. So only the activation can be optimized.

Therefore, the main optimization is fusing the activation into a single kernel.

Now, let's think about implementing the fused activation kernel.

The CUDA kernel would look like:

__global__ void fused_activation_kernel(const float* x, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float xi = x[idx];
        float softplus = logf(1.0f + expf(xi)); // compute softplus
        float tanh_softplus = tanhf(softplus); // compute tanh(softplus)
        out[idx] = tanh_softplus * xi; // multiply by x
    }
}

Wait, but expf and logf may be computationally expensive. Wait, but the original code uses PyTorch's softplus, which might use an optimized implementation. Let me check PyTorch's implementation of softplus.

PyTorch's softplus is defined as log(1 + exp(x)), but for numerical stability, when x is large, it approximates to x, and when x is very negative, approximates to 0.

The implementation uses the formula:

softplus(x) = max(x, 0) + log(1 + exp(-abs(x)))

This is more numerically stable. So to replicate this, we can compute:

float softplus_val = (x > 20.0) ? x : -log1pf(-expf(x));

Wait, no, perhaps better to implement the stable version.

Wait the stable formula for softplus is:

if x >= 20: return x (since exp(x) would overflow, but for x=20, exp(20) is 4.8e8, which is manageable in float?)

Wait, actually in PyTorch's implementation, for numerical stability:

softplus(x) = x if x > threshold else log(1 + exp(x))

where threshold is like 20, but maybe 19?

Alternatively, the stable implementation is:

softplus(x) = max(x, 0) + log(1 + exp(-abs(x)))

This avoids overflow when x is large. Let me compute this.

So for each xi:

float abs_xi = fabs(xi);
float z = expf(-abs_xi);
float softplus = (xi > 0) ? xi + log1pf(z) : log1pf(expf(xi));

Wait, perhaps better to compute as:

if (xi > 20) {
    softplus = xi;
} else if (xi < -20) {
    softplus = exp(xi);
} else {
    softplus = log1p(exp(xi));
}

But need to think of the exact formula. Alternatively, using the stable version.

Alternatively, implement the formula using the stable method:

softplus_val = max(0.0f, xi) + log1pf(expf(-fabsf(xi)));

Wait, let's see:

Let me denote y = exp(-|x|)

Then log(1 + exp(x)) can be written as:

if x >=0: x + log(1 + exp(-x)) = x + log1p(exp(-x))

since exp(-x) = exp(-|x|)

if x <0: log(1 + exp(x)) = log(exp(x) +1) = log(1 + exp(x)) which is the same as log1p(exp(x)), but when x is negative, exp(x) is small, so log1p(exp(x)) is okay.

Wait, but exp(x) when x is negative is exp(-|x|). Let me think:

Wait, when x is negative: let x = -a where a>0

Then exp(x) = exp(-a), so log(1 + exp(x)) = log(1 + exp(-a)) = log1p(exp(-a)) which is okay.

So, the stable way to compute softplus is:

float softplus_val = (x > 0) ? (x + log1pf(expf(-x))) : log1pf(expf(x));

But in code, using max and fabs:

float softplus_val = fmaxf(xi, 0.0f) + log1pf(expf(-fabsf(xi)));

Wait, this way, for xi positive, it's xi + log(1 + exp(-xi)), which equals log(1 + exp(xi)).

Wait let me check:

Let xi be positive.

Then fmax(xi,0) = xi,

then exp(-xi) is exp(-xi).

log1p(exp(-xi)) = log(1 + exp(-xi))

so total is xi + log(1 + exp(-xi)) 

= log(exp(xi)) + log(1 + exp(-xi)) 

= log( exp(xi)*(1 + exp(-xi)) ) 

= log( exp(xi) + 1 )

Yes, correct.

Similarly for xi negative:

fmax(xi,0)=0, so log1p(exp(-|xi|)) 

since |xi| = -xi when xi <0.

Thus:

log1p(exp(-|xi|)) = log1p(exp(xi)) = log(1 + exp(xi)), which is correct.

Therefore, this formula is numerically stable and can be used in the CUDA kernel.

Thus, in code, the kernel can be written as:

float softplus_val = fmaxf(xi, 0.0f) + log1pf(expf(-fabsf(xi)));

Then compute tanh(softplus_val), then multiply by xi.

Wait, but in CUDA, I need to use the math functions available in device code. For example, log1pf is available?

In CUDA, the math functions like log1pf and expf are available in device code. So that's okay.

Thus, the kernel code would be:

__global__ void fused_activation_kernel(const float* x, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float xi = x[idx];
        float abs_xi = fabsf(xi);
        float exp_neg_abs_xi = expf(-abs_xi);
        float softplus = fmaxf(xi, 0.0f) + log1pf(exp_neg_abs_xi);
        float tanh_softplus = tanhf(softplus);
        out[idx] = tanh_softplus * xi;
    }
}

This should compute the fused activation correctly.

Now, in Python, I need to wrap this into a CUDA function using torch.utils.cpp_extension.load_inline.

Then, in the ModelNew class, replace the activation computation with a call to this fused kernel.

The original forward is:

x = self.conv(x)
x = torch.multiply(torch.tanh(torch.nn.functional.softplus(x)), x)
x = self.bn(x)

We need to replace the second line with the fused kernel.

Thus, in ModelNew:

def forward(self, x):
    x = self.conv(x)
    x = self.fused_activation(x)
    x = self.bn(x)
    return x

Where self.fused_activation is the function from the CUDA kernel.

Now, the custom kernel needs to be defined and compiled.

So, in code:

First, define the CUDA source strings.

Then, load the kernel using load_inline.

The code structure would be similar to the example provided.

So, putting it all together:

First, the CUDA source for the fused activation kernel.

Then, the CPP sources for the header.

Then, the ModelNew class.

Additionally, the get_inputs and get_init_inputs functions are unchanged, so they can be copied from the original.

Wait, but the user says "output the new code in codeblocks in markdown format, completely self contained."

So the new code will include:

- Imports (including the ones needed for CUDA inline)

- The CUDA kernel code as strings

- The compiled modules using load_inline

- The ModelNew class

- The get_inputs and get_init_inputs functions.

Now, let's write this step by step.

First, the CUDA kernel source:

fused_activation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activation_kernel(const float* x, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float xi = x[idx];
        float abs_xi = fabsf(xi);
        float exp_neg_abs_xi = expf(-abs_xi);
        float softplus = fmaxf(xi, 0.0f) + log1pf(exp_neg_abs_xi);
        float tanh_softplus = tanhf(softplus);
        out[idx] = tanh_softplus * xi;
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor x) {
    auto size = x.numel();
    auto out = torch::empty_like(x);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    fused_activation_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), out.data_ptr<float>(), size);

    return out;
}
"""

The CPP source is the function declaration:

fused_activation_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor x);"
)

Then, load the kernel:

fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
)

Now, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)
        # The fused activation is a function from the loaded module
        self.fused_activation = fused_activation

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_activation.fused_activation_cuda(x)
        x = self.bn(x)
        return x

Wait, but the original model's __init__ parameters are in_channels, out_channels, kernel_size, etc. So the new model should have the same __init__ parameters, and the layers are kept the same.

Wait, in the original Model class:

def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
    super().__init__()
    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
    self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)

Therefore, the ModelNew must have the same __init__ signature.

Thus, in the code above, the ModelNew's __init__ is correct.

Now, the get_inputs and get_init_inputs functions are the same as original, so include them as:

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]

Wait, but in the original code, those functions are outside the class. So in the new code, they should be included as well. However, the user's instruction says to output the code with the same structure. 

Wait, looking back at the example given by the user:

The original code had:

class Model(...):
    ...

def get_inputs():
    ...

def get_init_inputs():
    ...

So the new code should also have the same structure, so include those functions.

But in the new code, the batch_size, in_channels, etc. are defined as global variables. In the original code, they are:

batch_size = 64
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3

Therefore, in the new code, those variables must also be present.

Thus, the complete code should include:

- The global variables (batch_size etc.)

- The CUDA kernel definitions

- The ModelNew class

- The get_inputs and get_init_inputs functions.

Wait, but in the user's original code, those variables are defined after the Model class. So in the new code, they should be placed similarly.

Therefore, putting it all together:

The code would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused activation CUDA kernel
fused_activation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activation_kernel(const float* x, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float xi = x[idx];
        float abs_xi = fabsf(xi);
        float exp_neg_abs_xi = expf(-abs_xi);
        float softplus = fmaxf(xi, 0.0f) + log1pf(exp_neg_abs_xi);
        float tanh_softplus = tanhf(softplus);
        out[idx] = tanh_softplus * xi;
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor x) {
    auto size = x.numel();
    auto out = torch::empty_like(x);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    fused_activation_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), out.data_ptr<float>(), size);

    return out;
}
"""

fused_activation_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor x);"
)

# Compile the fused activation kernel
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)
        self.fused_activation = fused_activation  # The loaded kernel function

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_activation.fused_activation_cuda(x)
        x = self.bn(x)
        return x

# Global variables as in the original code
batch_size = 64
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]

Wait, but in the original get_inputs function, the tensors are generated on CPU (using torch.rand). But since the model is on CUDA, the inputs should be moved to CUDA. So in the new code's get_inputs, I should add .cuda().

Original get_inputs was:

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

Thus, in the new code, it's better to have them on GPU, so adding .cuda().

Also, in the example provided by the user, the inputs were moved to CUDA. So yes, this is necessary.

Additionally, the load_inline might require extra flags. The user's example had extra_cflags and extra_ldflags as empty lists, so I'll include those to be safe.

Wait, in the example code, the load_inline call had:

extra_cflags=[""],
extra_ldflags=[""],

So in our code, we should add those as well.

Thus, in the load_inline call for fused_activation:

fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

This ensures compatibility with the extension loading.

Now, check if all parts are there. The code includes:

- Imports for torch, nn, and load_inline.

- CUDA kernel code as strings.

- The compiled kernel via load_inline.

- The ModelNew class with the same structure.

- The global variables.

- The get_inputs and get_init_inputs functions.

This should be self-contained and compilable.

Potential issues to check:

- The CUDA kernel uses float precision. The model uses float32 tensors, which is correct.

- The kernel uses torch::empty_like to create the output tensor, which is correct.

- The grid and block sizes are set to 256 threads per block, which is a common choice and should work, but might need tuning for optimal performance. However, the user wants the fastest possible code, but without detailed tuning, this is a standard starting point.

- The math functions are correctly used (fabsf, expf, etc.) in device code.

- The function name and parameters match between the kernel and the Python wrapper.

- The ModelNew's __init__ correctly initializes the layers.

Now, testing the code for correctness:

If the fused kernel correctly computes tanh(softplus(x)) * x, then the model's output should match the original's. Since we've implemented the stable version of softplus, it should be numerically equivalent.

Potential optimizations:

- Using shared memory for temporary variables, but since it's element-wise, not needed.

- Adjusting block size for better occupancy, but 256 is standard.

- Using half-precision (float16), but the user didn't mention that.

- Unrolling loops, but for element-wise kernels, it's already efficient.

Thus, this should be a correct and faster implementation.

Final check for the code structure:

The user wants the code in markdown code blocks, so the entire code should be enclosed in ```python.

Wait, the user's instruction says:

"Output the new code in codeblocks in markdown format (i.e. ```python or ```cpp). Please generate real code, NOT pseudocode, make sure the code compiles and is fully functional. Do not output testing code."

So the final answer should be the code wrapped in triple backticks with the python language specified.

Therefore, the final answer is the code as written above, enclosed in ```python.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused activation CUDA kernel
fused_activation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_activation_kernel(const float* x, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float xi = x[idx];
        float abs_xi = fabsf(xi);
        float exp_neg_abs_xi = expf(-abs_xi);
        float softplus = fmaxf(xi, 0.0f) + log1pf(exp_neg_abs_xi);
        float tanh_softplus = tanhf(softplus);
        out[idx] = tanh_softplus * xi;
    }
}

torch::Tensor fused_activation_cuda(torch::Tensor x) {
    auto size = x.numel();
    auto out = torch::empty_like(x);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    fused_activation_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), out.data_ptr<float>(), size);

    return out;
}
"""

fused_activation_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor x);"
)

# Compile the fused activation kernel
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)
        self.fused_activation = fused_activation  # The loaded kernel function

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_activation.fused_activation_cuda(x)
        x = self.bn(x)
        return x

# Global variables as in the original code
batch_size = 64
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
```