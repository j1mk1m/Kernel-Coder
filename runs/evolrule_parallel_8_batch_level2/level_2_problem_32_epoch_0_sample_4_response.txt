The user has specified that the code must be optimized using custom CUDA kernels. The user wants you to replace some operators with CUDA kernels. You can choose which operators to replace. The user has mentioned operator fusion and algorithmic changes as options. The key here is to identify which operators in the given Model can be optimized by combining or replacing with custom CUDA kernels.

Looking at the Model's forward method:

1. Convolution: self.conv(x)
2. Scaling: x * self.scale_factor
3. Min along channels: torch.min(x, dim=1, keepdim=True)[0]

Possible optimizations:

- The scaling (element-wise multiplication by a scalar) is a simple operation that can be done in the same kernel as the convolution. Combining the conv2d and scaling into a single kernel would reduce memory access and kernel launch overhead.

- The min operation over channels: this is a reduction operation. It can be implemented with a custom CUDA kernel, especially since it's along the channel dimension. But combining all three steps (conv, scale, min) into a single kernel might be too complex, but perhaps combining conv + scale first, then min as a separate kernel?

Alternatively, maybe the min can be part of the same kernel as the conv and scaling. Let's think step by step.

First, the convolution is a 2D convolution. The standard Conv2d involves sliding a kernel over the input, computing dot products. The scaling is just multiplying each output by scale_factor. So the scaling can be incorporated into the convolution computation by multiplying the final result of each element by the scale before storing it. That would save an entire element-wise multiplication step.

Then, the min over the channel dimension. The output of the scaled conv is of shape (B, out_channels, H, W). The min over dim=1 (channels) would produce (B, 1, H, W). This reduction can be done in a separate kernel. The min reduction across channels can be optimized by a kernel that, for each spatial location (H, W), computes the min across the out_channels dimension.

So the plan is:

1. Replace the standard PyTorch Conv2d with a fused convolution that includes the scaling. The fused kernel would compute the convolution and immediately multiply by scale_factor. This avoids the need for a separate element-wise multiplication.

2. The min operation can be implemented as a custom CUDA kernel that takes the output tensor of the conv+scale and computes the min over the channel dimension.

Alternatively, maybe even combine all three steps (conv, scale, min) into a single kernel, but that might be more complex. Let's see.

First, let's see if it's feasible to combine Conv2d and scaling:

The standard convolution computes Y[i,j,k,l] = sum_m sum_n (X[i,m,k+di,l+dj] * kernel[m,j,di,dj]) for each output element. Then, scaling by a factor s would be Y[i,j,k,l] *= s. So the fused kernel can compute Y[i,j,k,l] = s * sum(...), which is straightforward.

Therefore, writing a fused convolution with scaling is possible. That would be the first kernel.

The second operation is min over channel dimension. The output after conv+scale is (B, out_channels, H, W). The min over dim=1 (channels) for each spatial position (H, W) would require, for each position (h, w), iterating over all channels (out_channels) and taking the minimum.

Implementing this as a separate kernel would be beneficial.

Now, the question is whether to implement both steps in separate kernels or combine them. Combining all three (conv+scale+min) into one kernel might reduce memory bandwidth, as the intermediate data is not stored, but it might complicate the kernel.

Let me think about the compute and memory:

- The fused conv+scale kernel will produce an intermediate tensor of shape (B, out_channels, H', W'), where H' and W' depend on padding/stride. However, in the original model, the input and output spatial dimensions are the same (since the kernel_size is 3, probably with padding=1 to keep the same dimensions). Assuming stride=1 and padding=1, then the output spatial dims remain 256x256.

Wait, in the given code, the parameters for the model's __init__ include kernel_size, but no padding or stride. The default for Conv2d is padding=0, stride=1. Therefore, the output spatial dimensions would be (H - kernel_size + 1, W - kernel_size + 1). But in the problem's get_inputs function, the input is (batch_size, in_channels, 256, 256). So the output would be (256 - 3 +1)=254. However, perhaps the user intended padding=1 to keep the spatial dimensions same. Since the problem does not specify, but when writing code, we need to handle as per the given parameters.

However, for the purposes of kernel fusion, perhaps we can proceed.

The min operation over channels requires, for each spatial position (h, w), to read all out_channels elements, find the minimum, and store it. This can be done with a reduction kernel.

So, the plan is:

1. Create a fused Conv2d + Scale kernel. Let's call this function 'conv2d_scale'.

2. Create a Min Reduction kernel over channels, which takes the output of the conv2d_scale and produces the final min result. Let's call this 'channel_min'.

By fusing the first two operations (conv and scaling), we can save some time. The min is a separate operation but can be implemented efficiently with a custom kernel.

Now, how to implement the custom CUDA kernels.

First, the fused conv2d + scaling.

The standard PyTorch Conv2d is optimized, but perhaps the overhead of an additional element-wise multiplication is non-trivial. However, if we can incorporate the scaling into the kernel, we might save some time. However, writing a custom conv2d is quite involved, since it's a complex operation. Implementing a naive convolution would likely be slower than PyTorch's optimized one. Therefore, this may not be the best approach.

Alternatively, perhaps the user expects us to fuse the element-wise scaling with the convolution. However, given that the convolution is already a PyTorch operator, maybe it's better to leave it as is and focus on other parts. Alternatively, maybe the scaling is so trivial that combining it with the convolution kernel is worth it, but perhaps not.

Wait, perhaps the user wants us to replace some operators, but not necessarily the convolution. Let me check the original problem again.

The user says "You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax)."

So, the user allows replacing some operators, and fusing operators.

The problem is, writing a custom convolution kernel is complex and may not be worth it. Therefore, perhaps the better approach is to focus on the scaling and the min operation.

The scaling is an element-wise multiplication by a scalar. This can be done in a custom kernel, but in PyTorch, element-wise operations are already optimized, so perhaps that's not worth it. The min operation is a reduction over a dimension, which may have overhead if implemented in a custom kernel. Let me think.

Alternatively, perhaps the min over channels can be implemented more efficiently with a custom kernel. Let's see.

The min over channels: for each position (b, h, w), we need to compute the minimum over the out_channels dimension. The input tensor is of size (B, C, H, W). The output is (B, 1, H, W).

The standard PyTorch torch.min() for dim=1 would do this, but perhaps the overhead of the kernel launch and memory access is worth replacing with a custom kernel.

Alternatively, maybe the scaling and min can be combined. For instance, the scaling is a scalar multiplication, which can be done in the same kernel as the min. Since the scaling is applied before the min, combining them might allow us to compute the scaled value and keep track of the minimum in a single pass.

Wait, here's an idea:

Instead of doing convolution -> scale -> min, we can do convolution -> min with scaling incorporated. However, the min is after the scaling, so scaling is applied before min. Therefore, the scaling can be incorporated into the min's computation.

Alternatively, perhaps we can combine the scale and min into a single step. However, the convolution itself is a separate step, but maybe the user expects to replace the entire sequence with a custom kernel.

Alternatively, perhaps the best approach here is to implement the min as a custom kernel, as it's a reduction and may have overhead.

Alternatively, the user's example replaced a simple addition with a custom kernel, so perhaps here, the min operation is a good candidate.

Let me outline possible steps:

Option 1: Replace the min operation with a custom CUDA kernel.

Option 2: Replace the scaling and min operations with a custom kernel (since scaling is a simple scalar multiply, can be done inline in the min kernel).

Option 3: Replace the entire sequence (conv + scaling + min) with a custom kernel. But since the convolution is a complex operator, this may not be feasible unless we use ATen or some helper functions. Alternatively, perhaps leave the convolution as PyTorch's, but fuse the scaling and min.

First, let's consider the min operation.

The standard PyTorch code for the min is:

x = torch.min(x, dim=1, keepdim=True)[0]

The min over the second dimension (since dim=1 for a 4D tensor (B,C,H,W)).

Implementing a custom kernel for this:

The input tensor is of size (B, C, H, W). The output is (B,1,H,W).

Each thread could be responsible for a spatial position (h,w) and compute the min over C channels.

Alternatively, a block could handle a batch element and a spatial position, with threads handling the channels.

Wait, the kernel would need to process each (b,h,w) and compute min over the C dimension.

Approach for the kernel:

- The output tensor has size (B,1,H,W). Each element at (b,0,h,w) is the min over x[b,:,h,w].

We can structure the kernel such that each thread block handles a batch element, and a spatial position (h,w). For example, each block is assigned to a (b, h, w) position, and then each thread in the block computes a channel's value, then perform a reduction within the block to find the minimum.

Alternatively, each thread can process a (b, h, w) position, and iterate over the channels, keeping track of the minimum. But with C being 128 (out_channels=128), this might be manageable.

Alternatively, use a 3D grid where the grid dimensions correspond to batch, height, width. Each thread in a block handles a channel, then reduce across the block.

Let's think in terms of dimensions:

Suppose the input is (B, C, H, W).

We can launch a kernel where each thread block is responsible for a (b, h, w) position. Each block has C threads, each processing a channel. Then, within the block, compute the minimum over all the channels' values at that position.

Wait, but if C is 128, then a block of 128 threads could do this. For example:

Grid dimensions: B * H * W.

Block dimensions: C threads per block.

But if B, H, W are large, this could lead to a very large grid size. However, since B=64, H=256, W=256, the total number of blocks would be 64 * 256 * 256 = 4194304. This is manageable, but the block size is 128.

Wait, but in CUDA, the maximum number of threads per block is 1024, so 128 is okay.

The steps would be:

For each block (b, h, w):

- Each thread i in the block (0..C-1) loads x[b][i][h][w]

- Then, perform a reduction within the block to find the minimum.

But with C=128, the reduction can be done using a parallel reduction in the block.

This seems feasible.

Alternatively, a simpler approach with more threads but less synchronization:

Each thread in the block can compute a partial min and then combine.

Alternatively, using a shared memory approach for the reduction.

Alternatively, perhaps use a grid where each block handles a spatial position (h,w) across all batches. But that might complicate indexing.

Alternatively, for simplicity, let's proceed with the first idea.

The kernel would be something like:

__global__ void channel_min_kernel(const float* input, float* output, int B, int C, int H, int W) {
    int idx = blockIdx.x; // Each block corresponds to a (b, h, w) position.
    int b = idx / (H * W);
    int rem = idx % (H * W);
    int h = rem / W;
    int w = rem % W;

    // Each thread in the block corresponds to a channel
    int c = threadIdx.x;
    if (c >= C) return;

    float val = input[b * C * H * W + c * H * W + h * W + w]; // Need to verify the memory layout.

    // Wait, the input is (B, C, H, W), so the memory is stored in row-major order, so the strides are important.

    // Alternatively, using the PyTorch's .data_ptr() and assume the data is contiguous.

    // The input is a contiguous tensor, so the index would be:

    // The first dimension (B) is outermost, so for batch b, the offset is b * C * H * W.

    // Then, within batch, the next dimension is C: so c * H * W.

    // Then h * W + w.

    // So the element is at position: b * C*H*W + c*H*W + h*W + w.

    // So the value is input[b][c][h][w].

    // Each thread c has its value.

    // Now, need to find the minimum among all c.

    // To perform this, each thread in the block contributes their value.

    // So we can use a block reduction.

    extern __shared__ float shared[];

    shared[threadIdx.x] = (threadIdx.x < C) ? val : FLT_MAX;

    __syncthreads();

    // Perform reduction in shared memory.

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared[threadIdx.x] = min(shared[threadIdx.x], shared[threadIdx.x + s]);
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        output[b * H * W + h * W + w] = shared[0]; // The output is (B,1,H,W), so stored as B*H*W + h*W + w.
    }
}

Wait, but the output is of shape (B,1,H,W), so the stride is (H*W*1, ...). The output's data is stored as B * (1 * H * W) + (0 * H * W) + h*W + w. So yes, the output's pointer can be treated as a 3D array (B, H, W).

The above kernel uses a block per (b, h, w), with C threads per block. Each thread holds a channel's value, then they perform a reduction in shared memory to find the minimum.

The shared memory size would need to be at least C floats. Since C is 128, that's 128 * 4 bytes = 512 bytes per block, which is acceptable.

The number of blocks is B * H * W = 64 * 256 * 256 = 4,194,304. That's a lot of blocks, but it's manageable since each block is small.

The kernel launch would be:

dim3 blocks(B * H * W);
dim3 threads(C); // since C is 128

channel_min_kernel<<<blocks, threads, C * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), B, C, H, W);

Wait, but in CUDA, the maximum number of threads per block is 1024, so 128 is okay. The number of blocks can be up to 2^31-1, so 4 million is okay.

This should work.

Alternatively, maybe there's a more efficient way, but this seems feasible.

So, the plan is to write a custom kernel for the min over channels, replacing the torch.min() call.

Now, the scaling: x = x * self.scale_factor.

This is an element-wise multiplication by a scalar. Since the input to the min is the scaled output of the conv, the scaling can be done in a separate kernel, but perhaps it's better to combine it with the min kernel to save memory.

Wait, if we can combine the scaling and the min into a single kernel, that would be better. Since the scaling is just multiplying each element by a scalar before taking the min, we can do the scaling and min in a single step.

Thus, the fused kernel would be:

Compute the scaled value (input[c][h][w] * scale) for each channel, then find the minimum of these scaled values across the channels.

This way, we save the memory access for the scaling step and combine it with the min computation.

Therefore, the fused kernel would take the output of the convolution (before scaling), and compute both scaling and min in one kernel. This would eliminate the intermediate storage and kernel launches.

Hence, the fused kernel would be better.

Let me adjust the kernel accordingly.

Modified kernel:

The kernel takes the convolution output (without scaling), applies the scale, and then computes the min.

Thus, the kernel would:

For each (b, h, w), compute the minimum over c of (x[b][c][h][w] * scale).

This can be done in the same kernel as before, but with the scaling applied during the computation.

So, modifying the kernel code:

__global__ void channel_min_scaled_kernel(const float* input, float* output, int B, int C, int H, int W, float scale) {
    int idx = blockIdx.x; // Each block corresponds to a (b, h, w) position.
    int b = idx / (H * W);
    int rem = idx % (H * W);
    int h = rem / W;
    int w = rem % W;

    int c = threadIdx.x;
    if (c >= C) return;

    float val = input[b * C * H * W + c * H * W + h * W + w] * scale;

    extern __shared__ float shared[];
    shared[threadIdx.x] = (threadIdx.x < C) ? val : FLT_MAX;

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared[threadIdx.x] = min(shared[threadIdx.x], shared[threadIdx.x + s]);
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        output[b * H * W + h * W + w] = shared[0];
    }
}

This way, the kernel handles both scaling and min in one pass.

Therefore, the steps in the model's forward would be:

1. Apply PyTorch's Conv2d (since writing a custom conv is too involved and may not be faster).

2. Apply the fused scaling and min via the custom kernel.

This reduces two operations (scale and min) into one kernel, which is better.

Hence, the optimized model would replace the scale and min operations with this custom kernel.

Therefore, the code would be structured as follows:

- The custom CUDA kernel for scaling and min.

- The model's forward uses PyTorch's conv, then the custom kernel.

Now, writing the code.

First, define the CUDA kernel as a string.

Then, compile it inline.

Then, in the ModelNew class, replace the forward steps with the kernel.

Now, considering the inputs:

The input to the kernel is the output of the convolution, which is a tensor of shape (B, C, H, W). The kernel requires the following parameters:

- input: the convolution output (Tensor)

- output: the result (Tensor of size (B,1,H,W))

- B, C, H, W: dimensions (which can be inferred from the input tensor)

- scale: self.scale_factor.

Wait, but in the model, the scale_factor is an attribute. So when defining the kernel function, we need to pass it as an argument.

Thus, the kernel function in Python would be something like:

def channel_min_scaled_cuda(input, scale):
    # compute output shape
    B, C, H, W = input.shape
    output = torch.empty(B, 1, H, W, device=input.device, dtype=input.dtype)
    # launch kernel
    ...

The CUDA kernel requires the scale factor as a parameter.

Now, putting this together.

The code for the kernel:

kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <float.h> // for FLT_MAX

__global__ void channel_min_scaled_kernel(const float* input, float* output, int B, int C, int H, int W, float scale) {
    int idx = blockIdx.x;
    int b = idx / (H * W);
    int rem = idx % (H * W);
    int h = rem / W;
    int w = rem % W;

    int c = threadIdx.x;
    if (c >= C) return;

    float val = input[b * C * H * W + c * H * W + h * W + w] * scale;

    extern __shared__ float shared[];
    shared[threadIdx.x] = (threadIdx.x < C) ? val : FLT_MAX;

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared[threadIdx.x] = min(shared[threadIdx.x], shared[threadIdx.x + s]);
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        output[b * H * W + h * W + w] = shared[0];
    }
}

torch::Tensor channel_min_scaled_cuda(torch::Tensor input, float scale) {
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty({B, 1, H, W}, torch::device(input.device()).dtype(input.dtype()));

    int num_blocks = B * H * W;
    int block_size = C; // Each block has C threads

    // Check if block_size exceeds max threads per block (1024)
    if (block_size > 1024) {
        // Need to handle this case, but out_channels is 128 in this case, so okay
    }

    channel_min_scaled_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W,
        scale
    );

    return output;
}
"""

The corresponding header for compilation:

kernel_cpp = "torch::Tensor channel_min_scaled_cuda(torch::Tensor input, float scale);"

Now, compiling this into a module.

Then, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor
        # Load the custom kernel
        self.channel_min_scaled = load_inline(...)

    def forward(self, x):
        x = self.conv(x)
        # Apply the custom kernel
        x = self.channel_min_scaled.channel_min_scaled_cuda(x, self.scale_factor)
        return x

Wait, but the load_inline function returns a module with functions. So in the example code, the elementwise_add variable is a module with the function 'elementwise_add_cuda'.

Hence, the code would need to:

First, define the kernel_source and kernel_cpp as above.

Then, load the kernel:

channel_min_scaled = load_inline(
    name="channel_min_scaled",
    cpp_sources=kernel_cpp,
    cuda_sources=kernel_source,
    functions=["channel_min_scaled_cuda"],
    verbose=True,
)

But in the ModelNew's __init__, we need to have this module available. So the code structure would be:

Outside the class:

kernel_source = ... (as above)
kernel_cpp = ... (as above)

channel_min_scaled = load_inline(...)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        x = self.conv(x)
        # Call the custom function
        x = channel_min_scaled.channel_min_scaled_cuda(x, self.scale_factor)
        return x

Wait, but the load_inline creates a module that is a function. Wait, in the example given in the problem statement, the elementwise_add is a module, and the function is accessed as elementwise_add.elementwise_add_cuda(a, b).

Therefore, in the code, after defining channel_min_scaled = load_inline(...), then in the forward:

x = channel_min_scaled.channel_min_scaled_cuda(x, self.scale_factor)

But in the __init__ of ModelNew, the parameters are in_channels, etc. So the __init__ of ModelNew needs to take those parameters, same as the original Model.

However, in the original code, the parameters for Model are in_channels, out_channels, kernel_size, scale_factor. Therefore, in the new model, we need to have the same __init__.

Therefore, the complete code would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for scaling and min
kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <float.h>

__global__ void channel_min_scaled_kernel(const float* input, float* output, int B, int C, int H, int W, float scale) {
    int idx = blockIdx.x;
    int b = idx / (H * W);
    int rem = idx % (H * W);
    int h = rem / W;
    int w = rem % W;

    int c = threadIdx.x;
    if (c >= C) return;

    float val = input[b * C * H * W + c * H * W + h * W + w] * scale;

    extern __shared__ float shared[];
    shared[threadIdx.x] = (threadIdx.x < C) ? val : FLT_MAX;

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared[threadIdx.x] = min(shared[threadIdx.x], shared[threadIdx.x + s]);
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        output[b * H * W + h * W + w] = shared[0];
    }
}

torch::Tensor channel_min_scaled_cuda(torch::Tensor input, float scale) {
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty({B, 1, H, W}, torch::device(input.device()).dtype(input.dtype()));

    int num_blocks = B * H * W;
    int block_size = C;

    channel_min_scaled_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W,
        scale
    );

    return output;
}
"""

kernel_cpp = (
    "torch::Tensor channel_min_scaled_cuda(torch::Tensor input, float scale);"
)

# Compile the inline CUDA code
channel_min_scaled = load_inline(
    name="channel_min_scaled",
    cpp_sources=kernel_cpp,
    cuda_sources=kernel_source,
    functions=["channel_min_scaled_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        x = self.conv(x)
        x = channel_min_scaled.channel_min_scaled_cuda(x, self.scale_factor)
        return x

# The get_inputs and get_init_inputs functions remain the same as in the original.

Wait, but in the original code, the get_init_inputs() returns [in_channels, out_channels, kernel_size, scale_factor], which are the parameters passed to the model's __init__.

Hence, this should be compatible.

Now, check the CUDA kernel code:

- The input tensor must be contiguous. If the input to the kernel is not contiguous, this might cause issues. However, in PyTorch, when we do x = self.conv(x), the output of the convolution is typically contiguous, so this should be okay.

- The kernel uses the scale_factor as a float passed as an argument, which is correct.

- The output tensor is created with torch.empty, which is fine.

Potential issues:

- The block size is set to C (the number of channels). For out_channels=128, this is acceptable since 128 < 1024.

- The shared memory allocation is block_size * sizeof(float), which is correct.

- The indexing in the kernel must match the input's strides. Since the input is expected to be contiguous, the calculation of the index is correct.

- The output is stored in (B, 1, H, W). The output tensor is created with size (B,1,H,W), which matches.

- The kernel is launched with num_blocks = B*H*W. The maximum number of blocks in a grid is 65535 per dimension, but in 3D grids, it can be up to 2^31-1. Since 64*256*256=4,194,304 which is less than 2^32, but CUDA allows up to 2^31-1 per dimension. Wait, CUDA grids are 3D, so the total number can be up to 2^31-1 in each dimension. The default is a 1D grid, so the maximum is 2^31-1. Since 4 million is less than 2^31 (~2e9), it should be okay.

Thus, this should work.

Therefore, this is the optimized code.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for scaling and min reduction
kernel_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <float.h>  // for FLT_MAX

__global__ void channel_min_scaled_kernel(const float* input, float* output, int B, int C, int H, int W, float scale) {
    int idx = blockIdx.x;
    int b = idx / (H * W);
    int rem = idx % (H * W);
    int h = rem / W;
    int w = rem % W;

    int c = threadIdx.x;
    if (c >= C) return;

    float val = input[b * C * H * W + c * H * W + h * W + w] * scale;

    extern __shared__ float shared[];
    shared[threadIdx.x] = (threadIdx.x < C) ? val : FLT_MAX;

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            shared[threadIdx.x] = min(shared[threadIdx.x], shared[threadIdx.x + s]);
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        output[b * H * W + h * W + w] = shared[0];
    }
}

torch::Tensor channel_min_scaled_cuda(torch::Tensor input, float scale) {
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty({B, 1, H, W}, torch::device(input.device()).dtype(input.dtype()));

    int num_blocks = B * H * W;
    int block_size = C;

    channel_min_scaled_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W,
        scale
    );

    return output;
}
"""

kernel_cpp = (
    "torch::Tensor channel_min_scaled_cuda(torch::Tensor input, float scale);"
)

# Compile the inline CUDA code
channel_min_scaled = load_inline(
    name="channel_min_scaled",
    cpp_sources=kernel_cpp,
    cuda_sources=kernel_source,
    functions=["channel_min_scaled_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor

    def forward(self, x):
        x = self.conv(x)
        x = channel_min_scaled.channel_min_scaled_cuda(x, self.scale_factor)
        return x

# The following functions remain unchanged from the original
batch_size = 64
in_channels = 64
out_channels = 128
height = width = 256
kernel_size = 3
scale_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scale_factor]
```