        Now, the task is to optimize the given architecture by replacing some operators with custom CUDA kernels. 

        First, analyze the given architecture to determine which operators are compute-intensive or have high memory bandwidth requirements. The key operators here are the convolution, scaling (multiplication by a scalar), and the channel-wise minimum operation. 

        Next, decide which of these can be fused or optimized with custom CUDA kernels. The scaling and min operation might be fused with the convolution or each other. Alternatively, creating a single kernel to handle the entire computation could reduce memory overhead and kernel launch overhead.

        For example, the existing code's forward pass can be rewritten as a single CUDA kernel that computes the convolution, scales the result, and then takes the minimum across channels. Alternatively, you can split into multiple kernels but try to minimize memory copies and maximize parallelism.

        When writing CUDA kernels, ensure that thread blocks and grids are appropriately sized for the given problem dimensions. Also, be cautious with memory access patterns, especially for the convolution's input and output.

        The goal is to create a ModelNew class that replaces the original operators with custom CUDA implementations, maintaining the same functionality but with better performance. You need to provide the complete code for ModelNew, including the necessary CUDA kernel definitions and their registration in Python using load_inline. 

        You might need to handle the convolution manually, which can be complex, but since it's a 2D convolution with standard parameters, you can look up the implementation pattern for 2D convolutions in CUDA. Also, the scaling and min operation can be integrated into the kernel.

        You can also consider using PyTorch's existing CUDA functions for some parts if they are not the bottleneck, but the problem requires replacing some operators, so focus on those that can be optimized significantly.

        To start, define a CUDA kernel that takes the input tensor, applies the convolution, multiplies by the scale factor, and then computes the channel-wise minimum. The challenge is to manage the data efficiently in shared memory or registers to reduce global memory access latency.

        Note that implementing a general convolution in CUDA can be involved, especially handling padding, strides, and dilation. However, since the original model uses default parameters (assuming stride=1, padding=0, dilation=1), you can hardcode these values into the kernel for simplicity, or better, pass them as parameters if needed.

        Alternatively, if the convolution is the main bottleneck, you might need to implement it with optimized memory access patterns, such as using im2col or blocked algorithms. However, for the sake of time and code brevity, perhaps a straightforward implementation is acceptable.

        The min operation is along the channel dimension. Since the output of the convolution has shape (batch, out_channels, H, W), taking the min over the channel dimension (dim=1) would result in a tensor of shape (batch, 1, H, W). This can be done in parallel for each spatial location across all batches.

        Combining all three operations (convolution, scaling, min) into a single kernel may be challenging due to the convolution's computational complexity, but it could lead to better performance by reducing intermediate memory storage and kernel launches.

        Alternatively, you can split into two kernels: one for convolution + scaling, and another for the min operation. Or even combine convolution and scaling into one kernel since scaling is a simple element-wise multiplication.

        Let's plan to write a custom kernel for the convolution and scaling first, then another kernel for the min operation. Alternatively, see if they can be combined.

        Let me outline the steps:

        1. Convolution: For each output channel, each spatial location, compute the dot product of the input patch with the kernel, then scale by scale_factor.

        2. Min over channels: For each spatial location (h, w) and batch, find the minimum value across all output channels at that location.

        Since the min is performed after scaling, we can either compute the scaled convolution result first, then compute the min, or compute the convolution without scaling, compute the min, then scale the min (but that would be different; the scaling is applied before the min, so cannot reorder).

        So the order is important: conv -> scale -> min.

        Therefore, the min is over the scaled conv output.

        Thus, the steps are:

        For the custom kernel approach:

        Option 1: One kernel for convolution + scaling, then another kernel for the min.

        Option 2: A single kernel that does convolution, scales, and then computes min.

        Let's think about the feasibility.

        The convolution is the most complex part. Implementing convolution in CUDA requires handling the input data, kernel weights, and output storage. The scaling is straightforward. The min is across channels for each spatial location.

        If you do it all in one kernel, each thread could be responsible for a spatial location and compute the min over all channels there, but how to get all the channel values for that location?

        Alternatively, first compute the convolution and scaling in the first part of the kernel, store intermediate results, then compute the min in the same kernel. But this would require either using shared memory to buffer the intermediate results or global memory.

        Alternatively, the min can be computed as part of the convolution process, but since the min is over the output channels, which are generated by the convolution, it's not straightforward.

        Perhaps it's simpler to split into two kernels:

        1. Convolution + scaling: Output is (batch, out_channels, H, W).

        2. Min over channels: Output is (batch, 1, H, W).

        Implementing the first kernel (convolution + scaling) would be the main challenge. The second kernel for the min is simpler.

        So let's proceed step by step.

        First, implement the convolution kernel with scaling. Then the min kernel.

        For the convolution part:

        The standard 2D convolution with stride 1, padding 0, dilation 1.

        The input is (N, C_in, H_in, W_in).

        The kernel is (C_out, C_in, K, K).

        The output is (N, C_out, H_out, W_out), where H_out = H_in - K + 1, same for W_out.

        Each output element is computed as the sum over C_in, K, K of input * kernel.

        Then scaled by scale_factor.

        To implement this in CUDA, we can use a naive approach where each thread computes one output element.

        The grid and block dimensions would be set to cover all output elements.

        However, for large tensors, this might be inefficient, but for the purpose of this problem, let's proceed.

        The steps for the convolution kernel:

        For each output element (n, c_out, h_out, w_out):

        output[n, c_out, h_out, w_out] = scale_factor * sum_{c_in, kh, kw} input[n, c_in, h_out + kh, w_out + kw] * kernel[c_out, c_in, kh, kw]

        Wait, actually, the kernel is applied over a patch of the input. The correct formula is:

        The kernel is of size (kernel_size, kernel_size). So for each output position (h_out, w_out), the kernel is centered at (h_out, w_out) and slides over the input.

        Wait, actually, the standard convolution is:

        For each output channel c_out, the kernel is of size (C_in, K, K). So for each output position (h_out, w_out), the dot product between the kernel and the corresponding patch of the input.

        So for each output element (n, c_out, h_out, w_out):

        val = 0.0

        for c_in in 0..C_in-1:

            for kh in 0..K-1:

                for kw in 0..K-1:

                    val += input[n][c_in][h_out + kh][w_out + kw] * kernel[c_out][c_in][kh][kw]

        output[n][c_out][h_out][w_out] = val * scale_factor

        So the problem is that for each output element, you have to iterate over all the input channels and kernel elements. This is O(C_in * K^2) per output element.

        The challenge in CUDA is to parallelize this computation. One way is to have each thread handle one output element (n, c_out, h_out, w_out), and perform the loops over c_in, kh, kw sequentially. However, this could be slow for large K or C_in, but given that kernel_size=3, it's manageable.

        Alternatively, you can tile the computation, but that's more complex.

        For the purpose of this problem, let's proceed with the straightforward approach.

        Now, considering that PyTorch's Conv2d is already highly optimized, replacing it with a custom kernel might not yield a speedup unless the kernel is heavily optimized. However, the problem requires us to proceed.

        So first, let's define a CUDA kernel for convolution + scaling.

        The inputs to this kernel would be:

        - Input tensor (N, C_in, H_in, W_in)

        - Kernel weights (C_out, C_in, K, K) (note that in PyTorch, the Conv2d weight is stored as (out_channels, in_channels, kernel_size, kernel_size))

        - Bias? The original model doesn't have a bias, so we can ignore.

        - scale_factor (float)

        - Output tensor (N, C_out, H_out, W_out)

        The kernel will need to loop over all these dimensions.

        The kernel code would need to compute the indices correctly.

        To make this manageable, we can flatten the indices. For example, each thread can compute a linear index, then map it to (n, c_out, h_out, w_out).

        Let's proceed step by step.

        Now, for the second kernel, the min over the channel dimension.

        The input to this kernel is the output of the first kernel, of shape (N, C_out, H, W). The output is (N, 1, H, W).

        For each (n, h, w), compute the min over c_out.

        This can be done by having each thread compute a (n, h, w) position, then iterate over all C_out channels to find the minimum.

        Alternatively, use reduction techniques. Since C_out can be large (e.g., 128), this might be better done with a block-wide reduction.

        However, given that in the example, out_channels is 128, and the kernel size is 3, which is manageable.

        So for the min kernel:

        Each thread block could handle a (n, h, w) position. Each thread in the block can process a chunk of the C_out channels, then combine their results.

        But for simplicity, perhaps a naive approach is acceptable for the code submission.

        So, the steps are:

        1. Implement a custom CUDA kernel for the convolution plus scaling.

        2. Implement a custom CUDA kernel for the channel-wise min.

        Now, integrating this into the ModelNew class.

        However, note that the weights of the convolution layer are parameters of the model, so they need to be accessible in the custom kernel.

        In the original Model class, the conv layer's weights are stored in self.conv.weight, and the scale_factor is an attribute.

        In the custom implementation, to use these parameters, we need to pass them into the kernel.

        So in the forward function of ModelNew, we will have to pass the input tensor, the convolution weights, and the scale factor to the custom kernel.

        Wait, but in PyTorch, when you define a model with parameters, those parameters are automatically handled. So in the original Model, the conv layer's weights are parameters of the model. To use them in the custom kernel, we need to access them as tensors and pass them into the kernel.

        So in the ModelNew class, we can keep the same structure, perhaps.

        Now, here's a plan:

        The ModelNew class will have a conv layer (same as before), but in the forward method, instead of using PyTorch's conv, we'll call our custom CUDA kernel, passing in the weights and scale factor.

        Then, after scaling, apply the min kernel.

        But how to handle the parameters? The custom kernels need to take the weight tensor as an argument.

        So, in the forward method:

        def forward(self, x):

            weight = self.conv.weight

            scale = self.scale_factor

            # Run custom convolution + scaling kernel

            conv_scaled = custom_convolution_and_scale(x, weight, scale)

            # Run min kernel over channel dimension

            min_result = custom_min(conv_scaled)

            return min_result

        Thus, the kernels need to be written to take these parameters.

        Now, the first kernel (convolution + scaling):

        The input is x (N, C_in, H_in, W_in)

        The weight is (C_out, C_in, K, K)

        The output is (N, C_out, H_out, W_out), where H_out = H_in - K + 1 (assuming padding=0)

        The kernel code:

        We can write a CUDA kernel where each thread computes one output element.

        First, the kernel function:

        __global__ void conv_scale_kernel(

            const float* input, const float* weight, float scale,

            float* output,

            int N, int C_in, int H_in, int W_in,

            int C_out, int K, int H_out, int W_out

        )

        {

            // Calculate the output element indices

            int idx = blockIdx.x * blockDim.x + threadIdx.x;

            if (idx >= N * C_out * H_out * W_out)

                return;

            int w_out = idx % W_out;

            int h_out = (idx / W_out) % H_out;

            int c_out = (idx / (W_out * H_out)) % C_out;

            int n = idx / (W_out * H_out * C_out);

            // Compute the value

            float val = 0.0f;

            for (int c_in = 0; c_in < C_in; ++c_in) {

                for (int kh = 0; kh < K; ++kh) {

                    for (int kw = 0; kw < K; ++kw) {

                        int h_in = h_out + kh;

                        int w_in = w_out + kw;

                        if (h_in < H_in && w_in < W_in) {

                            val += input[

                                n * C_in * H_in * W_in +

                                c_in * H_in * W_in +

                                h_in * W_in +

                                w_in

                            ] * weight[

                                c_out * C_in * K * K +

                                c_in * K * K +

                                kh * K +

                                kw

                            ];

                        }

                    }

                }

            }

            output[

                n * C_out * H_out * W_out +

                c_out * H_out * W_out +

                h_out * W_out +

                w_out

            ] = val * scale;

        }

        Wait, but the input and weight dimensions may need to be accessed correctly.

        The input is stored in (N, C_in, H_in, W_in). So the stride for n is C_in * H_in * W_in, then c_in is H_in*W_in, etc.

        The weight is (C_out, C_in, K, K). So for each output channel c_out, it's the first dimension.

        So the code above should be correct.

        The problem is that for each output element, it's O(C_in*K^2) operations, which is okay for small K.

        Then, the kernel launch parameters need to be set.

        The number of threads per block can be 256, and the number of blocks is ceil(N*C_out*H_out*W_out / 256).

        Now, the second kernel for the min:

        The input is the output of the first kernel, which is (N, C_out, H_out, W_out). The output is (N, 1, H_out, W_out).

        Each element in the output is the min over the C_out dimension at that spatial location.

        So for each (n, h, w), compute the min over c_out from 0 to C_out-1.

        The kernel can be structured as:

        __global__ void channel_min_kernel(

            const float* input, float* output,

            int N, int C_out, int H, int W

        ) {

            int idx = blockIdx.x * blockDim.x + threadIdx.x;

            if (idx >= N * H * W)

                return;

            int w = idx % W;

            int h = (idx / W) % H;

            int n = idx / (W * H);

            float min_val = FLT_MAX;

            for (int c = 0; c < C_out; ++c) {

                float val = input[

                    n * C_out * H * W +

                    c * H * W +

                    h * W +

                    w

                ];

                if (val < min_val)

                    min_val = val;

            }

            output[

                n * H * W +

                h * W +

                w

            ] = min_val;

        }

        Here, the output is stored as (N, 1, H, W), so we can ignore the channel dimension (since it's always 0).

        The kernel launch would again use N*H*W threads, with blocks of 256.

        Now, in PyTorch, we need to define these kernels as inline CUDA code and compile them.

        The challenge is to handle the dimensions correctly, and pass the parameters.

        Let's structure the code.

        First, define the CUDA code for the convolution and scaling:

        conv_scale_source = """

        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void conv_scale_kernel(
            const float* input, const float* weight, float scale,
            float* output,
            int N, int C_in, int H_in, int W_in,
            int C_out, int K, int H_out, int W_out
        ) {
            // ... as above
        }

        torch::Tensor conv_scale_cuda(
            torch::Tensor input, torch::Tensor weight, float scale,
            int K, int H_out, int W_out
        ) {
            // Get dimensions
            int N = input.size(0);
            int C_in = input.size(1);
            int H_in = input.size(2);
            int W_in = input.size(3);
            int C_out = weight.size(0);

            // Output tensor
            auto output = torch::zeros({N, C_out, H_out, W_out}, input.options());

            // Calculate grid and block dimensions
            int total_elements = N * C_out * H_out * W_out;
            int threads_per_block = 256;
            int blocks_per_grid = (total_elements + threads_per_block - 1) / threads_per_block;

            conv_scale_kernel<<<blocks_per_grid, threads_per_block>>>(
                input.data_ptr<float>(),
                weight.data_ptr<float>(),
                scale,
                output.data_ptr<float>(),
                N, C_in, H_in, W_in,
                C_out, K, H_out, W_out
            );

            return output;
        }
        """

        Wait, but in the kernel function, the parameters include H_in, W_in, etc. These can be calculated from the input tensors.

        However, in the function signature, some parameters might be redundant, but it's okay.

        Next, the min kernel:

        channel_min_source = """

        __global__ void channel_min_kernel(
            const float* input, float* output,
            int N, int C_out, int H, int W
        ) {
            // ... as above
        }

        torch::Tensor channel_min_cuda(
            torch::Tensor input
        ) {
            int N = input.size(0);
            int C_out = input.size(1);
            int H = input.size(2);
            int W = input.size(3);

            auto output = torch::zeros({N, 1, H, W}, input.options());

            int total_elements = N * H * W;
            int threads_per_block = 256;
            int blocks_per_grid = (total_elements + threads_per_block - 1) / threads_per_block;

            channel_min_kernel<<<blocks_per_grid, threads_per_block>>>(
                input.data_ptr<float>(),
                output.data_ptr<float>(),
                N, C_out, H, W
            );

            return output;
        }
        """

        Now, we need to combine these into the Python code.

        Note that in the ModelNew class, we need to have access to the convolution's weight and the scale factor. Since the original code uses nn.Conv2d, perhaps we can keep the same structure but replace the forward pass with the custom kernels.

        So the ModelNew class would look like this:

        class ModelNew(nn.Module):
            def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
                super().__init__()
                self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
                self.scale_factor = scale_factor
                # Load the CUDA kernels
                self.conv_scale = conv_scale  # assuming the loaded module
                self.channel_min = channel_min

            def forward(self, x):
                # Get the convolution weight
                weight = self.conv.weight
                # Compute H_out and W_out
                H_in = x.size(2)
                W_in = x.size(3)
                K = self.conv.kernel_size[0]  # since it's a square kernel
                H_out = H_in - K + 1
                W_out = W_in - K + 1

                # Run the custom convolution and scaling
                conv_scaled = self.conv_scale.conv_scale_cuda(
                    x, weight, self.scale_factor, K, H_out, W_out
                )

                # Run the channel-wise min
                result = self.channel_min.channel_min_cuda(conv_scaled)

                return result

        However, the CUDA kernels need to be compiled and loaded inline. So in the Python code, we need to define the CUDA sources and use load_inline for both kernels.

        Putting it all together, the code would be:

        First, define the CUDA sources for both kernels:

        Then, compile them with load_inline.

        Here's the complete code:

        ```python
        import torch
        import torch.nn as nn
        from torch.utils.cpp_extension import load_inline

        # Define the convolution + scaling CUDA kernel
        conv_scale_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void conv_scale_kernel(
            const float* input, const float* weight, float scale,
            float* output,
            int N, int C_in, int H_in, int W_in,
            int C_out, int K, int H_out, int W_out
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= N * C_out * H_out * W_out) return;

            int w_out = idx % W_out;
            int h_out = (idx / W_out) % H_out;
            int c_out = (idx / (W_out * H_out)) % C_out;
            int n = idx / (W_out * H_out * C_out);

            float val = 0.0f;
            for (int c_in = 0; c_in < C_in; ++c_in) {
                for (int kh = 0; kh < K; ++kh) {
                    for (int kw = 0; kw < K; ++kw) {
                        int h_in = h_out + kh;
                        int w_in = w_out + kw;
                        if (h_in < H_in && w_in < W_in) {
                            val += input[
                                n * C_in * H_in * W_in +
                                c_in * H_in * W_in +
                                h_in * W_in +
                                w_in
                            ] * weight[
                                c_out * C_in * K * K +
                                c_in * K * K +
                                kh * K +
                                kw
                            ];
                        }
                    }
                }
            }
            output[
                n * C_out * H_out * W_out +
                c_out * H_out * W_out +
                h_out * W_out +
                w_out
            ] = val * scale;
        }

        torch::Tensor conv_scale_cuda(
            torch::Tensor input, torch::Tensor weight, float scale,
            int K, int H_out, int W_out
        ) {
            int N = input.size(0);
            int C_in = input.size(1);
            int H_in = input.size(2);
            int W_in = input.size(3);
            int C_out = weight.size(0);

            auto output = torch::zeros({N, C_out, H_out, W_out}, input.options());
            int total = N * C_out * H_out * W_out;
            int threads_per_block = 256;
            int blocks_per_grid = (total + threads_per_block - 1) / threads_per_block;

            conv_scale_kernel<<<blocks_per_grid, threads_per_block>>>(
                input.data_ptr<float>(),
                weight.data_ptr<float>(),
                scale,
                output.data_ptr<float>(),
                N, C_in, H_in, W_in,
                C_out, K, H_out, W_out
            );

            return output;
        }
        """

        conv_scale_cpp_source = (
            "torch::Tensor conv_scale_cuda(torch::Tensor input, torch::Tensor weight, float scale, int K, int H_out, int W_out);"
        )

        # Compile the convolution and scaling kernel
        conv_scale = load_inline(
            name="conv_scale",
            cpp_sources=conv_scale_cpp_source,
            cuda_sources=conv_scale_source,
            functions=["conv_scale_cuda"],
            verbose=True
        )

        # Define the channel-wise minimum CUDA kernel
        channel_min_source = """
        #include <torch/extension.h>
        #include <cuda_runtime.h>

        __global__ void channel_min_kernel(
            const float* input, float* output,
            int N, int C_out, int H, int W
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= N * H * W) return;

            int w = idx % W;
            int h = (idx / W) % H;
            int n = idx / (W * H);

            float min_val = FLT_MAX;
            for (int c = 0; c < C_out; ++c) {
                float val = input[
                    n * C_out * H * W +
                    c * H * W +
                    h * W +
                    w
                ];
                if (val < min_val) min_val = val;
            }
            output[
                n * H * W + h * W + w
            ] = min_val;
        }

        torch::Tensor channel_min_cuda(torch::Tensor input) {
            int N = input.size(0);
            int C_out = input.size(1);
            int H = input.size(2);
            int W = input.size(3);

            auto output = torch::zeros({N, 1, H, W}, input.options());
            int total = N * H * W;
            int threads_per_block = 256;
            int blocks_per_grid = (total + threads_per_block - 1) / threads_per_block;

            channel_min_kernel<<<blocks_per_grid, threads_per_block>>>(
                input.data_ptr<float>(),
                output.data_ptr<float>(),
                N, C_out, H, W
            );

            return output;
        }
        """

        channel_min_cpp_source = (
            "torch::Tensor channel_min_cuda(torch::Tensor input);"
        )

        # Compile the channel-wise minimum kernel
        channel_min = load_inline(
            name="channel_min",
            cpp_sources=channel_min_cpp_source,
            cuda_sources=channel_min_source,
            functions=["channel_min_cuda"],
            verbose=True
        )

        class ModelNew(nn.Module):
            def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
                super().__init__()
                self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
                self.scale_factor = scale_factor
                self.conv_scale = conv_scale
                self.channel_min = channel_min

            def forward(self, x):
                weight = self.conv.weight
                K = self.conv.kernel_size[0]
                H_in, W_in = x.size(2), x.size(3)
                H_out = H_in - K + 1
                W_out = W_in - K + 1

                conv_scaled = self.conv_scale.conv_scale_cuda(
                    x, weight, self.scale_factor, K, H_out, W_out
                )
                result = self.channel_min.channel_min_cuda(conv_scaled)
                return result

        # The get_inputs and get_init_inputs functions remain the same as before, so they are not included here.
        ```

        Now, some considerations:

        1. The CUDA kernels assume that the input has no padding, stride=1, dilation=1. This matches the original model's Conv2d parameters, which also use default values.

        2. The output size calculation (H_out = H_in - K + 1) is correct for no padding and stride 1.

        3. The weight tensor is accessed via self.conv.weight, which is a parameter of the model. This should be correct as long as the model is properly initialized.

        4. The conv_scale_cuda function requires passing K, H_out, and W_out. These are computed in the forward method based on the input tensor's shape and the kernel size.

        5. The channel_min kernel initializes the output tensor with the correct shape.

        6. Error checking in CUDA is omitted for brevity (e.g., checking kernel launch success), but in practice, this should be added.

        7. The kernels use a naive implementation which may not be the most efficient, but fulfills the problem's requirement of replacing operators with custom CUDA.

        This code should compile and function correctly, providing the same output as the original model but using custom CUDA kernels for the convolution, scaling, and channel-wise minimum operations.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the convolution + scaling CUDA kernel
conv_scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void conv_scale_kernel(
    const float* input, const float* weight, float scale,
    float* output,
    int N, int C_in, int H_in, int W_in,
    int C_out, int K, int H_out, int W_out
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * C_out * H_out * W_out) return;

    int w_out = idx % W_out;
    int h_out = (idx / W_out) % H_out;
    int c_out = (idx / (W_out * H_out)) % C_out;
    int n = idx / (W_out * H_out * C_out);

    float val = 0.0f;
    for (int c_in = 0; c_in < C_in; ++c_in) {
        for (int kh = 0; kh < K; ++kh) {
            for (int kw = 0; kw < K; ++kw) {
                int h_in = h_out + kh;
                int w_in = w_out + kw;
                if (h_in < H_in && w_in < W_in) {
                    val += input[
                        n * C_in * H_in * W_in +
                        c_in * H_in * W_in +
                        h_in * W_in +
                        w_in
                    ] * weight[
                        c_out * C_in * K * K +
                        c_in * K * K +
                        kh * K +
                        kw
                    ];
                }
            }
        }
    }
    output[
        n * C_out * H_out * W_out +
        c_out * H_out * W_out +
        h_out * W_out +
        w_out
    ] = val * scale;
}

torch::Tensor conv_scale_cuda(
    torch::Tensor input, torch::Tensor weight, float scale,
    int K, int H_out, int W_out
) {
    int N = input.size(0);
    int C_in = input.size(1);
    int H_in = input.size(2);
    int W_in = input.size(3);
    int C_out = weight.size(0);

    auto output = torch::zeros({N, C_out, H_out, W_out}, input.options());
    int total = N * C_out * H_out * W_out;
    int threads_per_block = 256;
    int blocks_per_grid = (total + threads_per_block - 1) / threads_per_block;

    conv_scale_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        scale,
        output.data_ptr<float>(),
        N, C_in, H_in, W_in,
        C_out, K, H_out, W_out
    );

    return output;
}
"""

conv_scale_cpp_source = (
    "torch::Tensor conv_scale_cuda(torch::Tensor input, torch::Tensor weight, float scale, int K, int H_out, int W_out);"
)

# Compile the convolution and scaling kernel
conv_scale = load_inline(
    name="conv_scale",
    cpp_sources=conv_scale_cpp_source,
    cuda_sources=conv_scale_source,
    functions=["conv_scale_cuda"],
    verbose=True
)

# Define the channel-wise minimum CUDA kernel
channel_min_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void channel_min_kernel(
    const float* input, float* output,
    int N, int C_out, int H, int W
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * H * W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int n = idx / (W * H);

    float min_val = FLT_MAX;
    for (int c = 0; c < C_out; ++c) {
        float val = input[
            n * C_out * H * W +
            c * H * W +
            h * W +
            w
        ];
        if (val < min_val) min_val = val;
    }
    output[
        n * H * W + h * W + w
    ] = min_val;
}

torch::Tensor channel_min_cuda(torch::Tensor input) {
    int N = input.size(0);
    int C_out = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::zeros({N, 1, H, W}, input.options());
    int total = N * H * W;
    int threads_per_block = 256;
    int blocks_per_grid = (total + threads_per_block - 1) / threads_per_block;

    channel_min_kernel<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C_out, H, W
    );

    return output;
}
"""

channel_min_cpp_source = (
    "torch::Tensor channel_min_cuda(torch::Tensor input);"
)

# Compile the channel-wise minimum kernel
channel_min = load_inline(
    name="channel_min",
    cpp_sources=channel_min_cpp_source,
    cuda_sources=channel_min_source,
    functions=["channel_min_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = scale_factor
        self.conv_scale = conv_scale
        self.channel_min = channel_min

    def forward(self, x):
        weight = self.conv.weight
        K = self.conv.kernel_size[0]
        H_in, W_in = x.size(2), x.size(3)
        H_out = H_in - K + 1
        W_out = W_in - K + 1

        conv_scaled = self.conv_scale.conv_scale_cuda(
            x, weight, self.scale_factor, K, H_out, W_out
        )
        result = self.channel_min.channel_min_cuda(conv_scaled)
        return result
```