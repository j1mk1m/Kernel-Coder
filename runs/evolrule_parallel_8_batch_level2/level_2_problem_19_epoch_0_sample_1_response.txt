Now, the user has provided a Python script that defines a neural network architecture involving `ConvTranspose2d`, `GELU`, and `GroupNorm`. The task is to optimize this architecture using custom CUDA kernels to achieve speedups. The example provided earlier shows how to replace an element-wise addition operation with a CUDA kernel. The user expects the optimized version (`ModelNew`) to use custom CUDA implementations for some or all of the operators in the original model.

### Approach

First, I need to analyze the given model and identify which operations are the most time-consuming or have potential for optimization. The main operations here are:

1. **Convolution Transpose (Deconvolution):** This is a computationally intensive operation, especially for large input sizes (like 256x256 images here). Replacing this with a custom CUDA kernel could offer significant speedups, especially if optimized for specific parameters (e.g., kernel size, stride, padding).

2. **GELU Activation Function:** The GELU function is an element-wise operation but involves a non-trivial computation (polynomial approximation). Implementing this in a CUDA kernel could allow for vectorization and memory access optimizations.

3. **Group Normalization:** This operation involves computing means and variances over groups of channels, which can be parallelized. A custom kernel might improve performance by optimizing the reduction operations across groups and channels.

### Choosing Which Operators to Replace

- **ConvTranspose2D:** Implementing a custom convolution transpose kernel might be challenging but offers the most potential gain due to its computational complexity. However, PyTorch's implementation is already optimized, so custom kernels would need to be highly optimized (e.g., using shared memory, optimized memory access patterns) to outperform existing ones. Alternatively, perhaps fusing operations like ConvTranspose2D and GELU could reduce memory overhead.

- **GELU:** Relatively straightforward to implement in a CUDA kernel. Since it's element-wise, a kernel can be written to apply GELU efficiently, possibly with better memory coalescing or by combining with another operation.

- **GroupNorm:** Also a candidate for a custom kernel. The computation involves per-group mean and variance, which can be parallelized. However, PyTorch's implementation is already optimized, so the gains might be marginal unless the batch size or input dimensions are very large.

### Plan for Implementation

1. **ConvTranspose2D:** Write a custom CUDA kernel for the transposed convolution. This requires understanding the algorithm and optimizing memory access. However, this is complex and may not yield better performance than PyTorch's existing implementation. Perhaps better to focus on fusing GELU and GroupNorm with the convolution.

2. **GELU + GroupNorm Fusion:** If possible, combine GELU and GroupNorm into a single kernel to minimize memory transfers. However, GroupNorm requires reductions (mean/variance) which might complicate fusion.

3. **GELU Kernel:** Implement a custom element-wise GELU kernel. This is simpler and likely to provide measurable speedups, especially if the input tensor is large.

4. **GroupNorm Kernel:** Create a custom kernel for GroupNorm, optimizing the reduction steps and parallelism.

Given the time constraints and complexity, focusing on GELU and GroupNorm might be more feasible and less error-prone than implementing a convolution transpose kernel. Additionally, fusing GELU and GroupNorm into a single kernel could reduce overhead.

### Step-by-Step Implementation

1. **Implement GELU Kernel:**

   The GELU function can be approximated using the formula:
   ```
   gelu(x) = 0.5 * x * (1 + tanh(sqrt(2 / Ï€) * (x + 0.044715 * x^3)))
   ```
   A CUDA kernel can compute this element-wise efficiently.

2. **Implement GroupNorm Kernel:**

   The steps are:
   - For each group, compute the mean and variance of the activations.
   - Normalize each element using these statistics.
   - Apply the scale and bias (if present). The GroupNorm layer in PyTorch includes learnable parameters (weight and bias), so the kernel must handle these.

   Since the original model's GroupNorm does not have affine parameters (weight/bias?), Wait, the GroupNorm in the given code uses `num_groups=num_groups, num_channels=out_channels`. Wait, in PyTorch's `GroupNorm`, by default, it has learnable parameters. The user's code initializes `GroupNorm` without specifying `affine=False`, so weight and bias are present. Thus, the kernel must include these parameters.

   However, in the given code, the `group_norm` layer is initialized with `num_groups=num_groups, num_channels=out_channels`, but the parameters (weight and bias) are learned. So, the kernel must take these as inputs.

3. **Fusion Possibility:**

   If we can combine the GELU and GroupNorm steps into a single kernel, that might reduce memory copies. However, since GroupNorm requires per-group reductions (mean and variance), which are compute-heavy, it might not be straightforward to fuse with GELU. Alternatively, after the convolution, apply GELU in one kernel, then GroupNorm in another, but perhaps optimize the sequence.

### Potential Issues

- **ConvTranspose2D:** Implementing this kernel is non-trivial. The transposed convolution involves outputting a larger spatial dimension by applying the kernel in reverse. The kernel's implementation requires handling the input strides and padding correctly. This might be too time-consuming without prior experience.

- **GroupNorm:** The kernel must handle groups of channels. Each group's mean and variance must be computed efficiently. Using CUDA's reduction operations (e.g., with `atomicAdd`) might be necessary, but could introduce synchronization issues.

### Final Decision

Given the example and the user's request, I'll proceed to implement custom CUDA kernels for both GELU and GroupNorm, since their implementations are more manageable than the convolution transpose. The convolution transpose can be left as is (using PyTorch's optimized implementation), unless there's a specific reason to replace it. Alternatively, perhaps fusing GELU and GroupNorm could be beneficial.

### Steps for Custom Kernels

1. **GELU Kernel:**
   - Write a CUDA kernel that applies the GELU function element-wise.
   - Compile and integrate it into the model.

2. **GroupNorm Kernel:**
   - Compute mean and variance per group.
   - Normalize and apply scale/bias.
   - Ensure that the kernel's parameters (weight, bias) are correctly handled.

### Implementing the GELU Kernel

The GELU formula can be implemented with a simple kernel. Here's a sketch:

```cpp
__global__ void gelu_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        float inner = M_SQRT_2 / M_PI * (x + 0.044715 * x * x * x);
        float tanh_val = tanh(inner);
        output[idx] = 0.5 * x * (1.0 + tanh_val);
    }
}
```

### Implementing the GroupNorm Kernel

This is more complex. Here's a simplified outline:

1. For each group:
   - Compute mean of the group.
   - Compute variance.
   - Normalize each element.
   - Apply weight and bias.

Assuming the input is of shape (N, C, H, W), with groups G:

- Each group has C/G channels.
- For each group, loop over all spatial positions and compute the mean and variance.

However, parallelizing this efficiently requires launching threads to handle the reductions. A possible approach is:

- Use a thread block per group.
- Each thread in the block computes the sum of its portion of the group's elements.
- Use a reduction within the block to compute the total mean and variance.

Here's a sketch of the kernel:

```cpp
template <typename scalar_t>
__global__ void group_norm_kernel(
    const scalar_t* input,
    scalar_t* output,
    const scalar_t* weight,
    const scalar_t* bias,
    int N, int C, int H, int W,
    int G, float eps) {

    int n = blockIdx.x / (C / G); // batch index
    int g = blockIdx.x % (C / G); // group index

    // Each group has C/G channels
    int c_start = g * (C / G);
    int c_end = (g + 1) * (C / G);

    // Compute mean and variance for this group across all spatial dimensions and batch
    scalar_t sum = 0.0;
    for (int c = c_start; c < c_end; ++c) {
        for (int h = 0; h < H; ++h) {
            for (int w = 0; w < W; ++w) {
                int idx = n * C * H * W + c * H * W + h * W + w;
                sum += input[idx];
            }
        }
    }
    // Reduce sum across threads in the block to compute the total sum for the group
    // ... (reduction steps using shared memory)

    // Then compute variance similarly
    // Then normalize each element and apply weight and bias
}
```

However, implementing this requires careful handling of thread synchronization and shared memory for reductions. This is quite involved and may have performance issues if not optimized properly.

### Compiling and Integrating

The code will need to use `load_inline` from PyTorch's `torch.utils.cpp_extension` to compile the CUDA kernels.

### Final Code Structure

The `ModelNew` will replace the PyTorch's `GELU` and `GroupNorm` with custom CUDA kernels. The `ConvTranspose2d` remains as is since replacing it would require significant effort.

### Potential Errors and Fixes

- **GELU Kernel:** Ensure the mathematical formula is correctly implemented. Check for possible overflow or precision issues.
- **GroupNorm Kernel:** Ensure that the reduction steps correctly compute the mean and variance across the entire group. Handle the weight and bias correctly.
- **Memory Access:** Ensure that the CUDA kernels use coalesced memory access for best performance.

### Final Answer

After careful consideration and planning, I'll proceed to write the code for the GELU and GroupNorm custom kernels. The ConvTranspose2d will remain as a standard PyTorch layer since optimizing it would be too complex without more specific optimizations.

Here's the code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom GELU kernel
gelu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

#define M_SQRT_2_OVER_PI 0.7978845608

__global__ void gelu_forward(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        float inner = M_SQRT_2_OVER_PI * (x + 0.044715 * x * x * x);
        float tanh_val = tanh(inner);
        output[idx] = 0.5 * x * (1.0 + tanh_val);
    }
}

torch::Tensor gelu_cuda(torch::Tensor input) {
    auto size = input.numel();
    auto output = torch::empty_like(input);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    gelu_forward<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);

    return output;
}
"""

gelu_cpp_source = (
    "torch::Tensor gelu_cuda(torch::Tensor input);"
)

# Compile the GELU kernel
gelu = load_inline(
    name="gelu",
    cpp_sources=gelu_cpp_source,
    cuda_sources=gelu_source,
    functions=["gelu_cuda"],
    verbose=True,
)

# Define the custom GroupNorm kernel
group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void group_norm_forward(
    const scalar_t* input,
    scalar_t* output,
    const scalar_t* weight,
    const scalar_t* bias,
    int N, int C, int H, int W,
    int G, float eps) {

    const int total_size = N * C * H * W;
    const int channels_per_group = C / G;
    
    // Thread index
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // Each thread handles one element
    if (idx >= total_size) return;
    
    // Compute group, channel, spatial indices
    int n = idx / (C * H * W);
    int c = (idx / (H * W)) % C;
    int h = (idx / W) % H;
    int w = idx % W;
    
    int g = c / channels_per_group;
    int local_c = c % channels_per_group;
    
    // Compute mean and variance for the group
    float mean = 0.0;
    float var = 0.0;
    
    // Accumulate sum for the group across all spatial and batch dimensions
    for (int batch = 0; batch < N; ++batch) {
        for (int ch = 0; ch < channels_per_group; ++ch) {
            for (int hh = 0; hh < H; ++hh) {
                for (int ww = 0; ww < W; ++ww) {
                    int pos = batch * C * H * W + (g * channels_per_group + ch) * H * W + hh * W + ww;
                    mean += input[pos];
                }
            }
        }
    }
    mean /= (N * channels_per_group * H * W);
    
    // Compute variance
    for (int batch = 0; batch < N; ++batch) {
        for (int ch = 0; ch < channels_per_group; ++ch) {
            for (int hh = 0; hh < H; ++hh) {
                for (int ww = 0; ww < W; ++ww) {
                    int pos = batch * C * H * W + (g * channels_per_group + ch) * H * W + hh * W + ww;
                    float val = input[pos] - mean;
                    var += val * val;
                }
            }
        }
    }
    var = var / (N * channels_per_group * H * W) + eps;
    float inv_std = 1.0f / sqrtf(var);
    
    // Normalize and apply weight/bias
    int output_idx = n * C * H * W + c * H * W + h * W + w;
    float x = input[output_idx];
    output[output_idx] = (x - mean) * inv_std * weight[g] + bias[g];
}

torch::Tensor group_norm_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int G,
    float eps = 1e-5) {

    auto N = input.size(0);
    auto C = input.size(1);
    auto H = input.size(2);
    auto W = input.size(3);
    
    auto output = torch::empty_like(input);
    
    const int threads = 256;
    const int blocks = (N * C * H * W + threads - 1) / threads;
    
    group_norm_forward<float><<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        N, C, H, W,
        G, eps);
    
    return output;
}
"""

group_norm_cpp_source = (
    "torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int G, float eps = 1e-5);"
)

# Compile the GroupNorm kernel
group_norm = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)
        self.gelu = gelu
        self.group_norm = group_norm
        # Initialize GroupNorm parameters (weight and bias)
        self.weight = nn.Parameter(torch.ones(num_groups))
        self.bias = nn.Parameter(torch.zeros(num_groups))
        self.num_groups = num_groups

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.gelu.gelu_cuda(x)
        x = self.group_norm.group_norm_cuda(
            x, self.weight, self.bias, self.num_groups
        )
        return x
```

### Potential Issues in the Generated Code

- **GroupNorm Kernel:** The current implementation of the kernel loops over all elements in the group for mean and variance calculation, which is extremely inefficient (O(N*C*H*W)^2). This will lead to very slow performance. To fix this, the reduction must be done with parallel reduction techniques using shared memory and thread synchronization.

- **Kernel Launch Parameters:** The group_norm_forward kernel is launched with threads equal to the total number of elements, but each thread is processing a single element, leading to a very high number of threads, which is not feasible. The kernel needs to be restructured to compute the reductions efficiently.

- **Weight and Bias Handling:** The current kernel applies the same weight and bias to all channels in a group, which is correct for GroupNorm. However, the parameters need to be correctly shaped and passed.

Due to time constraints and complexity, the GroupNorm kernel's implementation is flawed and needs a more optimized approach. For brevity and to meet the user's request for functional code, I'll proceed with the GELU kernel and note that the GroupNorm kernel requires optimization for real-world use. However, since the user requested a fully functional code, I'll adjust the GroupNorm kernel to a more efficient version.

### Revised GroupNorm Kernel Implementation (Optimized)

To optimize the GroupNorm kernel, we can parallelize the reduction steps within each group using shared memory.

Here's a revised version of the GroupNorm kernel:

```cpp
template <typename scalar_t>
__global__ void group_norm_forward(
    const scalar_t* input,
    scalar_t* output,
    const scalar_t* weight,
    const scalar_t* bias,
    int N, int C, int H, int W,
    int G, float eps) {

    const int channels_per_group = C / G;
    const int group_size = N * channels_per_group * H * W;

    // Each block handles a group
    int g = blockIdx.x;
    int group_offset = g * channels_per_group;

    // Shared memory for partial sums
    extern __shared__ float shared[];

    float* sum = shared;
    float* sum_sq = shared + blockDim.x;

    // Initialize shared memory to zero
    sum[threadIdx.x] = 0.0f;
    sum_sq[threadIdx.x] = 0.0f;
    __syncthreads();

    // Each thread processes a portion of the group's data
    for (int idx = threadIdx.x; idx < group_size; idx += blockDim.x) {
        int n = idx / (channels_per_group * H * W);
        int c = (idx / (H * W)) % channels_per_group;
        int h = (idx / W) % H;
        int w = idx % W;

        int input_idx = n * C * H * W + (group_offset + c) * H * W + h * W + w;
        scalar_t val = input[input_idx];

        sum[threadIdx.x] += val;
        sum_sq[threadIdx.x] += val * val;
    }

    // Synchronize to ensure all partial sums are computed
    __syncthreads();

    // Reduce the partial sums
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sum[threadIdx.x] += sum[threadIdx.x + s];
            sum_sq[threadIdx.x] += sum_sq[threadIdx.x + s];
        }
        __syncthreads();
    }

    // Only thread 0 computes mean and variance
    if (threadIdx.x == 0) {
        float total_sum = sum[0];
        float total_sum_sq = sum_sq[0];
        float mean = total_sum / group_size;
        float var = total_sum_sq / group_size - mean * mean;
        var = 1.0f / sqrtf(var + eps);

        // Now, normalize and apply weight/bias
        for (int idx = 0; idx < group_size; ++idx) {
            int n = idx / (channels_per_group * H * W);
            int c = (idx / (H * W)) % channels_per_group;
            int h = (idx / W) % H;
            int w = idx % W;

            int input_idx = n * C * H * W + (group_offset + c) * H * W + h * W + w;
            float val = input[input_idx];
            output[input_idx] = (val - mean) * var * weight[g] + bias[g];
        }
    }
}

// ... rest of the code (setup and kernel launch)
```

This version uses a block per group, with threads in the block processing elements of the group. Shared memory is used for partial sums, and a reduction is performed within the block. Only thread 0 computes the final mean and variance and applies normalization. However, this might still have scalability issues for large groups, but it's a better approach.

### Final Code with Optimized GroupNorm Kernel

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom GELU kernel
gelu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

#define M_SQRT_2_OVER_PI 0.7978845608

__global__ void gelu_forward(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        float inner = M_SQRT_2_OVER_PI * (x + 0.044715 * x * x * x);
        float tanh_val = tanh(inner);
        output[idx] = 0.5 * x * (1.0 + tanh_val);
    }
}

torch::Tensor gelu_cuda(torch::Tensor input) {
    auto size = input.numel();
    auto output = torch::empty_like(input);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    gelu_forward<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);

    return output;
}
"""

gelu_cpp_source = (
    "torch::Tensor gelu_cuda(torch::Tensor input);"
)

gelu = load_inline(
    name="gelu",
    cpp_sources=gelu_cpp_source,
    cuda_sources=gelu_source,
    functions=["gelu_cuda"],
    verbose=True,
)

# Custom GroupNorm kernel
group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void group_norm_forward(
    const scalar_t* input,
    scalar_t* output,
    const scalar_t* weight,
    const scalar_t* bias,
    int N, int C, int H, int W,
    int G, float eps) {

    const int channels_per_group = C / G;
    const int group_size = N * channels_per_group * H * W;
    const int group_offset = blockIdx.x * channels_per_group;

    extern __shared__ float shared[];
    float* sum = shared;
    float* sum_sq = shared + blockDim.x;

    sum[threadIdx.x] = 0.0f;
    sum_sq[threadIdx.x] = 0.0f;
    __syncthreads();

    for (int idx = threadIdx.x; idx < group_size; idx += blockDim.x) {
        int n = idx / (channels_per_group * H * W);
        int c = (idx / (H * W)) % channels_per_group;
        int h = (idx / W) % H;
        int w = idx % W;

        int input_idx = n * C * H * W + (group_offset + c) * H * W + h * W + w;
        scalar_t val = input[input_idx];
        sum[threadIdx.x] += val;
        sum_sq[threadIdx.x] += val * val;
    }

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sum[threadIdx.x] += sum[threadIdx.x + s];
            sum_sq[threadIdx.x] += sum_sq[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float mean = sum[0] / group_size;
        float var = sum_sq[0] / group_size - mean * mean;
        var = 1.0f / sqrtf(var + eps);

        for (int idx = 0; idx < group_size; ++idx) {
            int n = idx / (channels_per_group * H * W);
            int c = (idx / (H * W)) % channels_per_group;
            int h = (idx / W) % H;
            int w = idx % W;

            int input_idx = n * C * H * W + (group_offset + c) * H * W + h * W + w;
            output[input_idx] = (input[input_idx] - mean) * var * weight[blockIdx.x] + bias[blockIdx.x];
        }
    }
}

torch::Tensor group_norm_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int G,
    float eps = 1e-5) {

    auto N = input.size(0);
    auto C = input.size(1);
    auto H = input.size(2);
    auto W = input.size(3);
    auto output = torch::empty_like(input);

    const int threads = 256;
    const int blocks = G;

    dim3 grid(blocks);
    dim3 block(threads);
    int shared_size = 2 * threads * sizeof(float);

    group_norm_forward<float><<<grid, block, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        N, C, H, W,
        G, eps
    );

    return output;
}
"""

group_norm_cpp_source = (
    "torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int G, float eps = 1e-5);"
)

group_norm = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)
        self.gelu = gelu
        self.group_norm = group_norm
        self.weight = nn.Parameter(torch.ones(num_groups))
        self.bias = nn.Parameter(torch.zeros(num_groups))
        self.num_groups = num_groups

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.gelu.gelu_cuda(x)
        x = self.group_norm.group_norm_cuda(
            x, self.weight, self.bias, self.num_groups
        )
        return x
```

### Final Considerations

- **ConvTranspose2d:** Left as a standard PyTorch layer.
- **GELU and GroupNorm:** Replaced with custom CUDA kernels. The GELU kernel should be faster due to element-wise parallelism. The GroupNorm kernel uses shared memory and parallel reduction, which should be more efficient than the initial flawed version.
- **Parameters:** The GroupNorm layer's weight and bias are learnable parameters, managed by the PyTorch `nn.Parameter`.
- **Kernel Launch:** For GroupNorm, each block handles a group, and threads within a block process elements of that group. The shared memory is used for partial sums, and a reduction is done within the block.

This implementation should be functional but may require tuning of block sizes and shared memory usage for optimal performance.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom GELU kernel
gelu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

#define M_SQRT_2_OVER_PI 0.7978845608

__global__ void gelu_forward(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        float inner = M_SQRT_2_OVER_PI * (x + 0.044715 * x * x * x);
        float tanh_val = tanh(inner);
        output[idx] = 0.5 * x * (1.0 + tanh_val);
    }
}

torch::Tensor gelu_cuda(torch::Tensor input) {
    auto size = input.numel();
    auto output = torch::empty_like(input);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    gelu_forward<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);

    return output;
}
"""

gelu_cpp_source = (
    "torch::Tensor gelu_cuda(torch::Tensor input);"
)

gelu = load_inline(
    name="gelu",
    cpp_sources=gelu_cpp_source,
    cuda_sources=gelu_source,
    functions=["gelu_cuda"],
    verbose=True,
)

# Custom GroupNorm kernel
group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void group_norm_forward(
    const scalar_t* input,
    scalar_t* output,
    const scalar_t* weight,
    const scalar_t* bias,
    int N, int C, int H, int W,
    int G, float eps) {

    const int channels_per_group = C / G;
    const int group_size = N * channels_per_group * H * W;
    const int group_offset = blockIdx.x * channels_per_group;

    extern __shared__ float shared[];
    float* sum = shared;
    float* sum_sq = shared + blockDim.x;

    sum[threadIdx.x] = 0.0f;
    sum_sq[threadIdx.x] = 0.0f;
    __syncthreads();

    for (int idx = threadIdx.x; idx < group_size; idx += blockDim.x) {
        int n = idx / (channels_per_group * H * W);
        int c = (idx / (H * W)) % channels_per_group;
        int h = (idx / W) % H;
        int w = idx % W;

        int input_idx = n * C * H * W + (group_offset + c) * H * W + h * W + w;
        scalar_t val = input[input_idx];
        sum[threadIdx.x] += val;
        sum_sq[threadIdx.x] += val * val;
    }

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (threadIdx.x < s) {
            sum[threadIdx.x] += sum[threadIdx.x + s];
            sum_sq[threadIdx.x] += sum_sq[threadIdx.x + s];
        }
        __syncthreads();
    }

    if (threadIdx.x == 0) {
        float mean = sum[0] / group_size;
        float var = sum_sq[0] / group_size - mean * mean;
        var = 1.0f / sqrtf(var + eps);

        for (int idx = 0; idx < group_size; ++idx) {
            int n = idx / (channels_per_group * H * W);
            int c = (idx / (H * W)) % channels_per_group;
            int h = (idx / W) % H;
            int w = idx % W;

            int input_idx = n * C * H * W + (group_offset + c) * H * W + h * W + w;
            output[input_idx] = (input[input_idx] - mean) * var * weight[blockIdx.x] + bias[blockIdx.x];
        }
    }
}

torch::Tensor group_norm_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    int G,
    float eps = 1e-5) {

    auto N = input.size(0);
    auto C = input.size(1);
    auto H = input.size(2);
    auto W = input.size(3);
    auto output = torch::empty_like(input);

    const int threads = 256;
    const int blocks = G;

    dim3 grid(blocks);
    dim3 block(threads);
    int shared_size = 2 * threads * sizeof(float);

    group_norm_forward<float><<<grid, block, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        N, C, H, W,
        G, eps
    );

    return output;
}
"""

group_norm_cpp_source = (
    "torch::Tensor group_norm_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int G, float eps = 1e-5);"
)

group_norm = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride)
        self.gelu = gelu
        self.group_norm = group_norm
        self.weight = nn.Parameter(torch.ones(num_groups))
        self.bias = nn.Parameter(torch.zeros(num_groups))
        self.num_groups = num_groups

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.gelu.gelu_cuda(x)
        x = self.group_norm.group_norm_cuda(
            x, self.weight, self.bias, self.num_groups
        )
        return x
```