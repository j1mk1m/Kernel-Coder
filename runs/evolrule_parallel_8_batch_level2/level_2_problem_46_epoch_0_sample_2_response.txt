I need you to write the entire optimized code, including all imports, the class definition, and any helper functions. The code must be correct and compilable. 

Please use the same inline CUDA kernel embedding method as shown in the example. 

Make sure you import torch and other necessary libraries. Also, ensure that the get_inputs() and get_init_inputs() functions are properly defined. 

Additionally, consider opportunities to fuse operators, such as combining the subtraction and tanh into a single kernel, or fusing the final subtraction and average pooling into another kernel, to minimize memory overhead and maximize performance. 

Also, note that the subtraction operations are scalar subtractions (subtracting a constant value), which might be optimized differently than element-wise operations. 

Please make sure that all the model parameters are correctly initialized and that the forward pass uses your custom kernels instead of the original PyTorch operators.



```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernel for convolution + first subtraction + tanh activation
conv_sub_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void conv_sub_tanh_kernel(
    const float* input,
    const float* weight,
    const float* bias,
    float subtract_val,
    float* output,
    int batch_size,
    int in_channels,
    int out_channels,
    int height,
    int width,
    int kernel_size,
    int out_height,
    int out_width) {
    
    int h_out = threadIdx.y;
    int w_out = threadIdx.x;
    int n = blockIdx.x;
    int c_out = blockIdx.y;

    int pad = (kernel_size - 1) / 2;
    int h_in = -pad + h_out;
    int w_in = -pad + w_out;

    float sum = bias[c_out];
    for (int k = 0; k < kernel_size; ++k) {
        for (int l = 0; l < kernel_size; ++l) {
            for (int c_in = 0; c_in < in_channels; ++c_in) {
                int h = h_in + k;
                int w = w_in + l;
                if (h >= 0 && h < height && w >= 0 && w < width) {
                    sum += input[n * in_channels * height * width + c_in * height * width + h * width + w] *
                           weight[c_out * in_channels * kernel_size * kernel_size + c_in * kernel_size * kernel_size + k * kernel_size + l];
                }
            }
        }
    }

    sum -= subtract_val;
    output[n * out_channels * out_height * out_width + c_out * out_height * out_width + h_out * out_width + w_out] = tanh(sum);
}

torch::Tensor conv_sub_tanh_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    float subtract_val,
    int kernel_size,
    int out_height,
    int out_width) {

    auto batch_size = input.size(0);
    auto in_channels = input.size(1);
    auto out_channels = weight.size(0);

    torch::Tensor output = torch::empty({batch_size, out_channels, out_height, out_width}, input.options());

    dim3 threads(kernel_size, kernel_size);
    dim3 blocks(batch_size, out_channels);

    conv_sub_tanh_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        subtract_val,
        output.data_ptr<float>(),
        batch_size,
        in_channels,
        out_channels,
        input.size(2),
        input.size(3),
        kernel_size,
        out_height,
        out_width);

    return output;
}
"""

conv_sub_tanh_cpp_source = (
    "torch::Tensor conv_sub_tanh_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, float subtract_val, int kernel_size, int out_height, int out_width);"
)

conv_sub_tanh = load_inline(
    name="conv_sub_tanh",
    cpp_sources=conv_sub_tanh_cpp_source,
    cuda_sources=conv_sub_tanh_source,
    functions=["conv_sub_tanh_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define custom CUDA kernel for second subtraction + average pooling
sub_avgpool_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void sub_avgpool_kernel(
    const float* input,
    float subtract_val,
    float* output,
    int batch_size,
    int channels,
    int in_height,
    int in_width,
    int kernel_size,
    int out_height,
    int out_width) {

    int n = blockIdx.x;
    int c = blockIdx.y;
    int h = threadIdx.y * kernel_size;
    int w = threadIdx.x * kernel_size;

    float sum = 0.0;
    for (int kh = 0; kh < kernel_size; ++kh) {
        for (int kw = 0; kw < kernel_size; ++kw) {
            int h_in = h + kh;
            int w_in = w + kw;
            if (h_in < in_height && w_in < in_width) {
                sum += input[n * channels * in_height * in_width + c * in_height * in_width + h_in * in_width + w_in] - subtract_val;
            }
        }
    }
    sum /= (kernel_size * kernel_size);

    int h_out = h / kernel_size;
    int w_out = w / kernel_size;
    output[n * channels * out_height * out_width + c * out_height * out_width + h_out * out_width + w_out] = sum;
}

torch::Tensor sub_avgpool_cuda(
    torch::Tensor input,
    float subtract_val,
    int kernel_size,
    int out_height,
    int out_width) {

    auto batch_size = input.size(0);
    auto channels = input.size(1);
    auto in_height = input.size(2);
    auto in_width = input.size(3);

    torch::Tensor output = torch::empty({batch_size, channels, out_height, out_width}, input.options());

    dim3 threads(kernel_size, kernel_size);
    dim3 blocks(batch_size, channels);

    sub_avgpool_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        subtract_val,
        output.data_ptr<float>(),
        batch_size,
        channels,
        in_height,
        in_width,
        kernel_size,
        out_height,
        out_width);

    return output;
}
"""

sub_avgpool_cpp_source = (
    "torch::Tensor sub_avgpool_cuda(torch::Tensor input, float subtract_val, int kernel_size, int out_height, int out_width);"
)

sub_avgpool = load_inline(
    name="sub_avgpool",
    cpp_sources=sub_avgpool_cpp_source,
    cuda_sources=sub_avgpool_source,
    functions=["sub_avgpool_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract1_value, subtract2_value, kernel_size_pool):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract1_value = subtract1_value
        self.subtract2_value = subtract2_value
        self.kernel_size_pool = kernel_size_pool
        self.conv_sub_tanh = conv_sub_tanh
        self.sub_avgpool = sub_avgpool

    def forward(self, x):
        # Get convolution parameters
        weight = self.conv.weight
        bias = self.conv.bias
        in_channels = self.conv.in_channels
        out_channels = self.conv.out_channels
        kernel_size = self.conv.kernel_size[0]
        padding = self.conv.padding[0]
        stride = self.conv.stride[0]

        # Compute output dimensions for convolution
        batch_size, _, height, width = x.size()
        out_height_conv = (height + 2 * padding - kernel_size) // stride + 1
        out_width_conv = (width + 2 * padding - kernel_size) // stride + 1

        # Execute fused convolution + subtract + tanh
        x = self.conv_sub_tanh.conv_sub_tanh_cuda(
            x,
            weight,
            bias,
            self.subtract1_value,
            kernel_size,
            out_height_conv,
            out_width_conv
        )

        # Compute output dimensions for average pooling
        kernel_size_pool = self.kernel_size_pool
        out_height_pool = (out_height_conv - kernel_size_pool) // 1 + 1
        out_width_pool = (out_width_conv - kernel_size_pool) // 1 + 1

        # Execute fused subtract + average pooling
        x = self.sub_avgpool.sub_avgpool_cuda(
            x,
            self.subtract2_value,
            kernel_size_pool,
            out_height_pool,
            out_width_pool
        )

        return x

def get_inputs():
    batch_size = 128
    in_channels = 64
    height, width = 128, 128
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [128, 64, 3, 0.5, 0.2, 2]
```
</think>
