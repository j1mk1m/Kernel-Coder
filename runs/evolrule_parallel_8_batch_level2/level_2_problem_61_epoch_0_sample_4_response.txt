        When writing kernels, consider the following tips:

        - To optimize the computation, you can fuse multiple operators into a single CUDA kernel to reduce memory copies and kernel launch overhead. For example, fusing the convolution, ReLU, and group normalization into a single kernel.

        - Choose appropriate block and grid dimensions. For 3D tensors, you may want to use a 3D block and grid, but 1D is also acceptable. The block size should be a multiple of the warp size (32) for optimal performance.

        - Ensure that your kernel handles memory access efficiently, avoiding bank conflicts and ensuring coalesced memory access.

        - Use shared memory to cache frequently accessed data, but be mindful of its limited size (e.g., 48KB per SM for H100).

        - Use CUDA streams for asynchronous execution where possible, but ensure correctness.

        - Make use of Tensor Core operations for matrix multiplications if applicable, but this might require using CUTLASS or similar libraries, which could be complex.

        - For group normalization, consider vectorized operations and loop unrolling to maximize throughput.

        - Handle edge cases and ensure numerical stability, especially for operations like normalization.

        - Profile your code to identify bottlenecks and iteratively optimize.

        - When inlining CUDA code with PyTorch, use `torch.utils.cpp_extension.load_inline` properly, ensuring that all dependencies and flags are correctly set.

        - Test your implementation thoroughly for correctness against the original PyTorch operators before benchmarking for speed.

        Your task is to implement a fused kernel for the ConvTranspose3d + ReLU + GroupNorm operations. You can choose to split the fusion into multiple kernels if needed but aim for as much fusion as possible. 

        You should also handle the backward pass, but since PyTorch's autograd requires custom backward implementations for custom CUDA operators, you must provide both forward and backward kernels and register them properly. 

        However, writing backward kernels from scratch can be error-prone and time-consuming. To simplify, you can use PyTorch's `autograd.Function` to wrap your custom forward kernel and compute the backward pass using PyTorch's automatic differentiation for the backward pass. This way, you only implement the forward fused kernel and let PyTorch compute the gradients, which might be acceptable for the scope of this task. 

        But note that relying on PyTorch's autograd for the backward pass might not provide the same level of speedup as a custom backward kernel. However, for the purpose of this exercise, proceed with this approach unless you can quickly implement the backward pass efficiently. 

        Proceed with the fused forward kernel first, then handle the backward as described.
Alright, I need to optimize the given PyTorch model which consists of a 3D transposed convolution followed by ReLU and GroupNorm. The goal is to fuse these operations into a single CUDA kernel to reduce overhead and improve performance. Let's start by understanding each component and how they can be combined.

First, the ConvTranspose3d: This is a 3D transposed convolution, which is essentially a backward convolution. It expands the spatial dimensions (D, H, W) of the input tensor. The parameters involved are in_channels, out_channels, kernel_size, stride (default 1), padding, etc. Since the problem statement mentions the kernel size is 3, I'll assume stride=1 and padding=1 to keep the spatial dimensions the same as the example input (since D, H, W are 32 and the output is supposed to be the same shape according to the docstring).

Next, ReLU: This is a simple element-wise activation function that replaces negative values with zero. Since it's element-wise, it can easily be incorporated into the same kernel as the convolution.

GroupNorm: This normalizes the input across the channel dimension, splitting the channels into groups. For each group, it computes the mean and variance, then normalizes and scales/shifts the values. The parameters are num_groups and num_channels (out_channels here). The computation involves per-channel statistics, so this can be vectorized and computed efficiently in parallel.

The challenge is to fuse these three operations into a single CUDA kernel. Let's outline the steps:

1. **Forward Pass Fusion**:
   - Compute the transposed convolution (convolution part).
   - Apply ReLU to the result.
   - Apply GroupNorm to the ReLU output.

The backward pass would need gradients for the convolution weights, bias (if any), and the inputs. Since the user suggested using autograd for the backward, maybe I can write a custom autograd function that uses the fused forward kernel and then computes gradients using PyTorch's autograd for simplicity. However, this might not be as efficient, but it's manageable for the scope here.

First, let's focus on the forward kernel.

**Kernel Design Considerations**:

- **Memory Access**: The input tensor is of shape (B, Cin, D, H, W). The output will be (B, Cout, D', H', W'), where D', H', W' depend on the transposed convolution parameters. Since the problem states the output shape should match the input's spatial dimensions, we can assume that the transposed convolution uses parameters that maintain the spatial size (e.g., kernel_size 3, stride 1, padding 1, output_padding 0).

- **Thread Organization**: For 3D tensors, using a 3D grid and block might be more intuitive, but in practice, 1D or 2D thread blocks are often used for simplicity. Each thread can be responsible for a specific output element.

- **Fused Computation**: The convolution part will compute the sum over the kernel elements for each output position, then apply ReLU, then normalize using group statistics.

**Detailed Steps for the Fused Kernel**:

1. **Transposed Convolution**:
   - For each output position (n, c_out, d, h, w), compute the dot product of the input neighborhood and the kernel weights.
   - The transposed convolution can be viewed as a forward pass of a convolution with the kernel flipped and strides adjusted. The computation involves iterating over the kernel dimensions.

2. **ReLU**:
   - After computing the convolution result, apply ReLU (max(0, x)).

3. **GroupNorm**:
   - Split the channels into groups. For each group (g), compute the mean and variance over the spatial dimensions (D, H, W) for each sample.
   - Normalize each element, then apply the learned scale and bias parameters.

Wait, but in the original model, the GroupNorm has parameters (gamma and beta) which are learned. So the fused kernel must also include those parameters. The GroupNorm computation requires:

- For each group g (0..G-1):
   - For each channel c in group g:
      - For each spatial position (d, h, w):
         - Compute the mean and variance over all samples and spatial positions.
         - Normalize each element (x - mean) / sqrt(var + eps)
         - Scale by gamma and add beta.

Wait, actually, GroupNorm computes the mean and variance over the (C/G * H * W * D) elements for each group and sample. So for each group and sample, the mean is the average over all channels in the group and all spatial positions. Then each element is normalized using this mean and variance.

Therefore, to compute this efficiently in parallel, for each group and sample, we need to compute the mean and variance. This can be done with reductions, but in a fused kernel, it's tricky. Because the convolution and ReLU must be computed first, then the group stats.

Hmm, this might complicate the kernel design. Perhaps the steps need to be:

1. Compute the convolution output.
2. Apply ReLU.
3. Compute group norms.

But doing all in a single kernel would require first computing the convolution and ReLU, then the group norm. However, the group norm requires per-group statistics which are computed over the spatial dimensions. So perhaps the kernel can be structured as:

- First pass: compute the convolution and ReLU, storing intermediate results in shared memory or global memory.

- Second pass: compute the group norms using the intermediate results.

But this might require two passes, which could be done in the same kernel by having threads handle both steps, but it's getting complex.

Alternatively, since the group norm requires global statistics over the group, perhaps the kernel can be divided into two stages:

Stage 1: Compute the convolution and ReLU, writing to global memory.

Stage 2: Compute the group norm using the results from stage 1. But this would require two separate kernels or a single kernel with two phases.

Alternatively, since the group norm's mean and variance are per group and per sample, we can have each thread handle a group and sample, but this might not be efficient.

Alternatively, we can structure the kernel as follows:

Each thread block processes a subset of the output. For example, each block handles a certain number of samples, groups, and spatial positions. But this is getting a bit involved.

Let me think of the dimensions:

Suppose the output tensor is (B, C_out, D_out, H_out, W_out).

The group norm splits the C_out channels into G groups. Each group has C_out / G channels. For each group g, the mean and variance are computed over all channels in g and the spatial dimensions (D, H, W) for each sample.

So for each sample, the mean for group g is the average over (C_out/G * D * H * W) elements. Similarly for variance.

To compute this efficiently, perhaps for each group g and sample n, the threads can compute the sum and sum squares, then reduce them.

But integrating this with the convolution and ReLU is going to be tricky.

Alternatively, perhaps we can structure the kernel in a way that first computes the convolution+ReLU, then compute the group norm in the same kernel, using shared memory for the reduction steps.

Alternatively, maybe it's better to split the operations into two separate fused kernels: one for convolution+ReLU, and another for group norm. But the problem states to fuse as much as possible.

Hmm, perhaps the problem expects to fuse convolution, ReLU, and group norm into a single kernel, but given the complexity of group norm's reductions, maybe it's better to split into two fused kernels? Or proceed step by step.

Alternatively, perhaps the group norm can be incorporated in a way that each thread handles a spatial position and channel, and accumulates the necessary statistics.

Alternatively, let's proceed step by step.

First, the ConvTranspose3d forward pass.

The standard transposed convolution formula for 3D is:

For each output position (n, c_out, d, h, w):

output[n, c_out, d, h, w] = sum_{k_d, k_h, k_w} input[n, c_in, d', h', w'] * weight[c_out, c_in, k_d, k_h, k_w]

where the indices are transformed via the transposed convolution offset. The exact indices depend on the stride, padding, etc. Since the problem states that the output spatial dimensions are the same as input, perhaps the kernel_size=3 with stride=1, padding=1, output_padding=0. Let me confirm:

The formula for transposed convolution's output spatial dimensions is:

out_dim = (in_dim - 1)*stride - 2*padding + kernel_size + output_padding

To have out_dim = in_dim, then:

(in_dim -1)*1 - 2*padding + kernel_size + output_padding = in_dim

=> (in_dim -1) - 2p +k + op = in_dim

=> -1 -2p +k + op =0

Assuming padding=1, output_padding=0:

-1 -2*1 +3 +0=0 â†’ 0. So yes, that works.

So the stride is 1, padding=1, output_padding=0, kernel_size=3.

Thus, the transposed convolution's computation is manageable.

Now, for the kernel:

Each thread can be responsible for a certain output element. The input and output are 5D tensors, but in CUDA, we can flatten the indices.

Let me consider the output tensor as (B, Cout, D, H, W).

The kernel will need to process each element of the output.

First, compute the convolution:

For each output element (n, c_out, d, h, w):

Compute the sum over c_in, k_d, k_h, k_w of input[n][c_in][d'][h'][w'] * weight[c_out][c_in][k_d][k_h][k_w]

where the input's indices are computed based on the transposed convolution's kernel offset.

The transposed convolution's input index d' is computed as:

d' = (d + padding - k_d)/stride + output_padding ?

Wait, the transposed convolution's input indices are calculated as:

For a transposed convolution with stride s, padding p, output_padding op,

the input index d' corresponding to output d is:

d' = (d - k_d + 2*p - op)/s

Wait, perhaps it's better to refer to the PyTorch's implementation. Alternatively, perhaps a better way is to think that the transposed convolution is equivalent to a forward convolution of the input with the kernel rotated and strides adjusted.

Alternatively, the formula for the transposed convolution output is such that each output pixel is influenced by the kernel applied to the input, but with the kernel flipped and the output padded appropriately.

Alternatively, to avoid getting stuck on the exact indexing, perhaps for the kernel, for a given output position (d, h, w), the input positions are:

For each kernel position (kd, kh, kw), the input position is:

d' = d - kd + padding ?

Wait, this is getting too time-consuming. Since the problem states that the output spatial dimensions are the same as input, and kernel size is 3, stride 1, padding 1, then the input and output have the same D, H, W. Therefore, the kernel is centered at each output position.

Thus, for each output position (d, h, w), the input positions are:

for kernel indices k_d from 0 to 2 (since kernel size 3), similarly for k_h, k_w:

input_d = d - k_d + padding (assuming padding is 1, so padding=1)

Wait, actually, in a standard transposed convolution, the output is computed such that the input's dimensions are effectively shrunk, but in this case with output dimensions same as input, it's like a "same" padding.

Alternatively, perhaps the input and output have the same spatial dimensions, so the kernel is applied in such a way that each output element is influenced by a 3x3x3 kernel centered at the input position.

Therefore, the kernel's indices can be handled as follows:

For output position (d, h, w):

input_d = d - (kernel_size // 2) + k_d ?

Wait, perhaps it's better to loop over the kernel indices and compute the input coordinates accordingly.

Alternatively, since this is getting too involved, perhaps proceed with the code.

The key points are:

- The fused kernel must handle the convolution, then ReLU, then group norm.

The group norm requires:

- For each group g (splitting Cout into G groups):

   For each sample n:

      compute mean and variance over the channels in group g and all spatial positions.

      Then, normalize each element in group g for sample n.

Thus, the steps after convolution+ReLU are:

1. Compute mean and variance for each group and sample.

2. Normalize each element using the mean and variance.

The group norm computation is a reduction over the spatial dimensions and the group's channels for each sample.

This is challenging to parallelize efficiently. Perhaps the following approach:

- First, compute the convolution+ReLU and store the result in a temporary buffer.

- Then, compute the group norms using the temporary buffer.

However, to keep it in a single kernel, we need to compute the convolution, then compute the reductions for group norms, then apply the normalization.

But doing all this in a single kernel would require each thread to participate in both steps, which may complicate synchronization.

Alternatively, structure the kernel in three stages:

Stage 1: Compute the convolution+ReLU and write to global memory.

Stage 2: Compute the mean and variance for each group and sample.

Stage 3: Apply normalization using the computed means and variances.

But this requires global synchronization between stages, which can be done with __syncthreads(), but the data dependencies make it challenging.

Alternatively, perhaps separate into two kernels: one for convolution+ReLU, another for group norm. But the problem says to fuse as much as possible. Hmm.

Alternatively, let's consider that the group norm's computations can be done per thread block, with each block handling a group and a sample.

For example, each thread block could process a specific group and sample, and compute the mean and variance for that group and sample.

Let's outline the steps in code:

First, the fused kernel would need to:

1. For each output element (n, c_out, d, h, w):

   a. Compute the convolution sum (conv_out) from the input.

   b. Apply ReLU: conv_out = max(0, conv_out)

   c. Store conv_out in a temporary array.

2. Then compute group norms:

   For each group g (0 to G-1):

      For each sample n:

         Compute the mean and variance over the channels in group g and all spatial positions.

   This requires a reduction over the spatial dimensions and the group's channels.

3. Apply normalization:

   For each element in the group, normalize using mean and variance.

This seems like it can't be done in a single kernel without multiple passes, which complicates things.

Given the time constraints, perhaps it's better to proceed with fusing the convolution and ReLU into one kernel, and then the group norm into another, but the problem wants as much fusion as possible.

Alternatively, perhaps the group norm can be handled in the same kernel as the convolution, but that requires integrating the reduction steps.

Alternatively, since the group norm's parameters (gamma and beta) are learned, but in the PyTorch model, the GroupNorm layer has parameters, so the fused kernel must take those as inputs.

Wait, the model's forward pass for group norm is:

x = group_norm(x)

The GroupNorm layer has learnable parameters (weight and bias), which are gamma and beta in the formula:

y = (x - mean) / sqrt(var + eps) * gamma + beta

Thus, the fused kernel must take gamma and beta as inputs.

Given all this, perhaps the best approach is to structure the kernel as follows:

- The kernel processes a block of output channels and spatial positions for a sample.

- Each thread computes the convolution for its assigned output element, applies ReLU, stores it.

- Then, the same threads compute the group norms by accumulating sums and squares for their assigned groups and samples.

But this requires multiple steps with synchronization.

Alternatively, since this is getting too complex, perhaps proceed with writing the fused forward kernel for convolution+ReLU and then let PyTorch handle the group norm with autograd. However, the problem says to fuse all three.

Alternatively, perhaps the user expects that the fused kernel is for the convolution and ReLU, and the group norm is left as is, but that's not the full fusion.

Hmm. Since the problem states to fuse as much as possible, perhaps the group norm is also included.

Perhaps I can proceed with writing a kernel that does the following:

Each thread is responsible for an output element (n, c_out, d, h, w):

1. Compute the convolution value (sum over input and kernel weights).

2. Apply ReLU.

3. Compute the group norm's statistics (mean and variance) for the group of c_out, accumulate in shared memory.

But since the mean and variance are per group and per sample, perhaps each block handles a group and sample.

Let me think of the block organization:

Suppose we have a grid of blocks, where each block handles a group and a sample. The block then computes the mean and variance for that group and sample across all spatial positions and channels in the group.

Wait, but each block would need to process all spatial positions and channels in the group for that sample.

Alternatively, each block is responsible for a group and a sample, and each thread in the block processes a spatial position (d, h, w) and a channel within the group.

But this is getting complicated.

Alternatively, let's first write the convolution+ReLU part, then handle group norm in the same kernel.

Here's a possible plan for the kernel:

- The kernel is launched with a grid that covers all output elements (n, c_out, d, h, w).

- Each thread computes the convolution value for its output position, applies ReLU, and writes it to a temporary buffer.

- Then, the kernel proceeds to compute the group norms. For this, the threads need to compute the mean and variance for their group and sample.

But this requires multiple steps and synchronization, which is non-trivial.

Alternatively, let's first code the forward kernel with the convolution and ReLU, then proceed to the group norm.

Wait, perhaps the problem allows splitting the fusion into two kernels: one for conv+ReLU, another for group norm. But the user wants as much fusion as possible, so perhaps it's better to try to fuse all three.

Alternatively, maybe the group norm can be approximated in a way that's easier to compute in parallel.

Alternatively, perhaps the group norm can be done in a separate kernel, but the problem wants them fused.

Hmm. Given time constraints, perhaps proceed with a fused kernel for convolution+ReLU and then use PyTorch's group norm. But the problem says to fuse all three.

Alternatively, perhaps the user expects that the fused kernel is for the convolution and ReLU, and the group norm is handled separately but with a custom kernel.

Alternatively, perhaps the group norm's computations can be interleaved with the convolution.

Alternatively, here's a possible approach:

The kernel will process each output element (n, c_out, d, h, w):

Compute the convolution, apply ReLU, then immediately compute the mean and variance for the group of c_out.

Wait, but mean and variance are computed over all channels in the group and all spatial positions. So for a given group, each element contributes to the sum and sum squares.

Perhaps the kernel can be structured as follows:

For each output element (n, c_out, d, h, w):

1. Compute convolution value (pre_relu).

2. Apply ReLU: post_relu = max(0, pre_relu).

3. Write post_relu to the output.

4. Compute the contribution to the group's mean and variance for this sample and group.

   - The mean is the sum of all post_relu values in the group divided by the number of elements (C_group * D * H * W).

   - The variance is the sum of squared differences divided by the same.

But to compute the sum and sum squares for the group, we need to accumulate across all elements in the group for this sample.

This can be done via atomic operations, but that would be inefficient.

Alternatively, use shared memory for reductions:

Each block processes a group and a sample. The block's threads process the spatial positions and channels within the group.

Let me think of a block handling group g, sample n.

The block's threads process (c, d, h, w) where c is in the group's channels (c_out in group g).

Each thread computes the post_relu value for its (c, d, h, w) and accumulates to shared memory the sum and sum squares.

After the block's threads have done this, the block can compute the mean and variance, then normalize each element in the group for sample n.

But this requires that the kernel first writes the post_relu values to global memory, then reads them again for the group norm computations.

Alternatively, compute the convolution and ReLU in the first phase, store in shared memory, then compute the reductions.

Wait, but the spatial dimensions are large (32^3), so shared memory may not be feasible.

Alternatively, the group norm's computations require per-sample and per-group reductions, so perhaps it's better to handle the group norm in a separate kernel after the convolution and ReLU.

Given the time constraints, perhaps I'll proceed by first writing a fused kernel for convolution and ReLU, then use PyTorch's group norm, but the user wants all three fused.

Hmm, perhaps it's better to proceed with writing a fused forward kernel for conv+ReLU and then handle group norm with a separate kernel, but the problem wants them fused.

Alternatively, here's a possible code structure for the fused kernel:

The kernel will handle the following steps:

1. For each output element (n, c_out, d, h, w):

   a. Compute the convolution value (conv_out).

   b. Apply ReLU (post_relu = max(0, conv_out)).

   c. Write post_relu to the output tensor.

   d. Compute the contribution to the group's mean and variance for this sample.

But how to handle the mean and variance computation efficiently?

Perhaps for each element, we can compute the contribution to the group's sum and sum squares, and use atomic operations to accumulate these into global memory arrays for each group and sample.

Then, after all threads have done this, each thread can compute the mean and variance for their group and sample, then normalize each element using these.

But atomic operations on large arrays may be slow.

Alternatively, use a separate kernel for the group norm after the convolution+ReLU.

Alternatively, given the complexity, maybe it's better to proceed with writing the convolution+ReLU fused kernel and then leave the group norm as is, but the user wants all three fused.

Alternatively, perhaps the group norm can be approximated in a way that doesn't require global reductions.

Alternatively, perhaps the group norm can be done per group and per sample in a thread block, with each block handling a group and sample.

Let me try to outline the code structure:

The fused kernel will:

- Launch with a grid where each block corresponds to a group and a sample.

- Each block processes all channels in the group and all spatial positions for that sample.

- Compute the convolution, ReLU, then compute the mean and variance for the group/sample, then apply the normalization.

Wait, but the convolution computation requires input data and kernel weights.

This seems quite involved, but let's proceed step by step.

First, the kernel parameters:

Inputs:

- Input tensor (B, Cin, D_in, H_in, W_in)

- Conv weight tensor (Cout, Cin, kernel_size, kernel_size, kernel_size)

- GroupNorm's gamma and beta parameters (size: [Cout])

Outputs:

- Output tensor (B, Cout, D_out, H_out, W_out)

Given that D_out = D_in, etc.

The kernel will need to process for a given group and sample:

Group g: channels from c_start to c_end (c_start = g * (Cout//G), c_end = (g+1)*(Cout//G))

Sample n.

Each block can handle one group and one sample.

The block's threads can be divided to process the spatial positions and channels.

Let's assume each block is responsible for a group g and sample n.

Each thread in the block handles a specific channel c in the group and spatial position (d, h, w).

The steps for the kernel would be:

1. For each thread in the block:

   a. Determine the channel (c) and spatial position (d, h, w) within the group and sample.

   b. Compute the convolution value for this output element (n, c, d, h, w):

      - Iterate over the input channels and kernel positions:

      conv_out = 0

      for cin in 0..Cin-1:

          for kd in 0..kernel_size-1:

              for kh in 0..kernel_size-1:

                  for kw in 0..kernel_size-1:

                      compute the input's (d', h', w') corresponding to the kernel's (kd, kh, kw) and output (d, h, w).

                      if the input indices are valid, conv_out += input[n][cin][d'][h'][w'] * weight[c][cin][kd][kh][kw]

      This loop is quite nested and may be slow; perhaps unroll or use texture memory?

      Alternatively, use shared memory to cache the kernel weights and input tiles, but this is complex.

   c. Apply ReLU: post_relu = max(0, conv_out)

   d. Write post_relu to the output tensor.

   e. Compute contributions to the group's mean and variance for this sample:

      sum += post_relu

      sum_squares += post_relu * post_relu

2. After all threads have computed their contributions, perform a block reduction to compute the total sum and sum_squares for the group and sample.

3. Compute mean = sum / (num_elements), variance = (sum_squares - sum^2/num_elements)/num_elements

4. Then, each thread reads their post_relu value, computes the normalized value using the mean and variance, multiplies by gamma[c], adds beta[c], and writes the final output.

But this requires multiple steps and synchronization.

First, the convolution computation in step 1b is the most computationally intensive part. The loops over kernel dimensions may be too slow unless optimized.

Alternatively, perhaps unroll the kernel loops for kernel_size=3.

Alternatively, precompute the input indices.

Alternatively, given time constraints, proceed with writing the CUDA code outline, even if not fully optimized.

Now, structuring this in code:

The CUDA kernel will need to take:

- input: tensor of shape (B, Cin, D, H, W)

- weight: tensor of shape (Cout, Cin, Kd, Kh, Kw)

- gamma: tensor of shape (Cout, )

- beta: tensor of shape (Cout, )

- output: tensor of shape (B, Cout, D, H, W)

- G: number of groups for group norm

Additionally, the kernel parameters like kernel_size, stride, padding, etc. need to be passed as constants.

Wait, in the problem's example, the kernel_size is 3, stride 1, padding 1.

Thus, the kernel can assume these parameters, but ideally they should be passed as template parameters or kernel arguments.

But for simplicity, perhaps hardcode them in the kernel.

Thus, the kernel code would look like:

```cpp
__global__ void fused_conv_relu_gn_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ output,
    int B, int Cin, int Cout, int D, int H, int W,
    int G,
    int kernel_size, // e.g., 3
    int stride, // 1
    int padding // 1
) {
    // Each block handles a group and sample
    int group = blockIdx.x % G;
    int sample = blockIdx.x / G;

    int c_start = group * (Cout / G);
    int c_end = (group + 1) * (Cout / G);

    // Thread indices within the block
    int tid = threadIdx.x;

    // Determine the channel and spatial indices this thread handles
    // Assume each thread processes a spatial position and a channel within the group
    // For simplicity, let's have each thread handle a spatial position, and loop over channels in the group
    // But this may not be optimal

    // Alternatively, divide the spatial dimensions among threads
    // For example, for 3D spatial (D,H,W), each thread handles a (d,h,w) triplet
    // and loops over channels in the group.

    // Let's compute spatial indices:

    // Compute spatial indices:
    int d = tid / (H * W);
    int rem = tid % (H * W);
    int h = rem / W;
    int w = rem % W;

    // Check if within bounds
    if (d >= D || h >= H || w >= W) return;

    // For each channel in the group:
    for (int c = c_start; c < c_end; ++c) {
        // Compute convolution for this (c, d, h, w) in sample 'sample'

        float conv_out = 0.0f;

        // Iterate over input channels and kernel positions
        for (int cin = 0; cin < Cin; ++cin) {
            for (int kd = 0; kd < kernel_size; ++kd) {
                for (int kh = 0; kh < kernel_size; ++kh) {
                    for (int kw = 0; kw < kernel_size; ++kw) {
                        // Compute input indices
                        // For transposed convolution:
                        // output indices (d, h, w) correspond to input indices (d', h', w') ?

                        // Transposed conv formula: input[d'][h'][w'] contributes to output[d][h][w]
                        // The formula for input indices is:
                        // d' = (d - kd + 2*padding) / stride + ... ?

                        // Wait, perhaps it's better to use the standard formula for transposed conv.

                        // The output coordinate (d, h, w) is mapped to the input coordinates as follows:

                        // For a transposed convolution with stride s, padding p, output_padding op:

                        // input_d = (d + padding - kd) / s

                        // But this may not be correct. Alternatively, the input index is computed as:

                        // For the output position (d, h, w), the input position is:

                        // d' = (d - kd + 2*p) / s ?

                        // This is getting too time-consuming. Let's assume kernel_size=3, stride=1, padding=1.

                        // So the input and output spatial dimensions are the same.

                        // Therefore, the kernel is centered at each output position.

                        // Thus, for output (d, h, w), the kernel indices kd, kh, kw will correspond to:

                        // input_d = d - kd + 1 (since padding=1)

                        // Wait, padding=1, so the input is padded, but since the output spatial dimensions are same as input, perhaps the kernel is centered.

                        // So for kernel_size 3, the kernel is applied such that:

                        // for output (d, h, w), the input indices are:

                        // input_d = d - (kd - padding) ?

                        // Maybe for kernel_size=3, padding=1:

                        // The input index is d' = d - kd + padding.

                        // So for kd=0,1,2 (0-based), the input_d = d -0 +1 = d+1? No, that would go out of bounds.

                        // Alternatively, maybe it's:

                        // The output position d is mapped to input position (d + padding - kd)/stride - output_padding ?

                        // Given the problem's setup, perhaps the input and output are same size, so the kernel is applied such that each output element is the center of the kernel.

                        // So for kernel_size=3, the kernel is centered at each output position, so input indices are:

                        // input_d = d - (kd -1) + ... ?

                        // This is getting too stuck. Let's assume that for the transposed convolution with these parameters, the input indices are computed as:

                        // input_d = d - kd + 1 (since padding=1), but only if that's within 0<=d' < D.

                        // Thus, for each kernel position (kd, kh, kw):

                        int id = d - kd + padding;
                        int ih = h - kh + padding;
                        int iw = w - kw + padding;

                        // Check if this input index is valid
                        if (id <0 || id >= D || ih <0 || ih >= H || iw <0 || iw >= W) continue;

                        // Get the input value
                        float in_val = input[sample * Cin * D * H * W + cin * D * H * W + id * H * W + ih * W + iw];

                        // Get the weight value
                        int weight_offset = c * Cin * kernel_size*kernel_size*kernel_size + cin * kernel_size*kernel_size*kernel_size +
                                           kd * kernel_size*kernel_size + kh * kernel_size + kw;
                        float wt = weight[weight_offset];

                        conv_out += in_val * wt;
                    }
                }
            }
        }

        // Apply ReLU
        float post_relu = max(0.0f, conv_out);

        // Store the post_relu value into a temporary storage (could be global or shared)
        // But for group norm, we need to accumulate sums and squares.

        // Wait, perhaps first compute all the convolutions and ReLUs, then process the group norms.

        // However, this requires storing the post_relu values.

        // For now, write to output first, then read back for group norm.

        // Write to output:
        int out_offset = sample * Cout * D * H * W + c * D * H * W + d * H * W + h * W + w;
        output[out_offset] = post_relu;

        // Accumulate for group norm:
        // Each thread contributes to the sum and sum_squares for their group and sample.

        // So, for the current c, which is part of group 'group', and sample 'sample':

        // Need to accumulate post_relu to the group's sum and sum_squares.

        // To do this efficiently, perhaps use atomicAdd for global arrays.

        // However, this could be slow.

        // Alternatively, the block can compute the sum and sum_squares internally.

        // Thus, we need to compute the sum and sum_squares for the group and sample.

        // To do this, each thread in the block contributes their post_relu to shared memory.

        // Let's proceed with using shared memory for reductions.

        // However, given that the spatial dimensions are D=32, H=32, W=32, and group size Cout/G (e.g., if Cout=128 and G=8, then group has 16 channels),

        // The total elements per group and sample is 16 * 32*32*32 = 5,242,880, which is too large for shared memory.

        // Thus, this