        Make sure you follow the format. The user will then copy your code verbatim, so any mistake in your code will result in a compilation error. Ensure that your code is correct and complete. 

        Do not use any external dependencies beyond the PyTorch library and standard Python modules. Your code will be evaluated for correctness and speed. 

        If you need to write multiple kernels, group them into one or more load_inline calls as appropriate. 

        When you write CUDA kernels, please follow the example syntax precisely, using load_inline with functions and kernel names that match the function signatures. 

        The output should have the new ModelNew class, and any custom CUDA operators must be defined before the class. 

        You are allowed to replace any subset of the operators, but to get good speedups, you should focus on the operators that are compute-intensive or have high memory access overhead. For example, in the given problem, the Conv3d, the division, max_pool, global_avg_pool, add, and sum operations may all be candidates for optimization. 

        You can also consider fusing operators together. For example, fusing the division with the convolution output, or combining max_pool and the subsequent operations. 

        The key is to identify which operations can benefit most from being in a single kernel to reduce memory copies and kernel launch overhead. 

        For instance, the division by a constant and the addition of a bias could be fused into the same kernel as the convolution output. However, you have to check whether that is feasible. 

        The sum over a specific dimension may also be done in a separate kernel or fused with the preceding operations. 

        You have to decide which operators to fuse and which to leave as separate, and then write the corresponding CUDA kernels. 

        Your code should be as efficient as possible.


<think>
Okay, I need to optimize the given PyTorch model using custom CUDA kernels. Let me start by understanding the model's operations. The forward pass includes a 3D convolution, division by a constant, max pooling, global average pooling, adding a bias, and summing along a specific dimension. 

First, I should identify which operations are compute-intensive or have high overhead. The convolution is definitely a big one, but writing a custom 3D convolution might be complex. Maybe I can fuse some of the following operations with it. The division by a constant and adding the bias are element-wise operations, so those could be fused into the convolution's output processing. The max pooling and global average pooling are also significant; perhaps fusing those could help reduce memory transfers.

Wait, but fusing convolution with division and bias might be manageable. Let's see: after convolution, instead of doing x / divisor and then x += bias, I can compute (conv_result / divisor) + bias in a single kernel. That would save a couple of steps. 

Then the max pooling and global average pooling: those are separate layers. Max pooling reduces the spatial dimensions, and global average pooling further reduces to 1x1x1. Maybe fusing max pooling with the global average pooling isn't straightforward, but perhaps combining the max pooling with the subsequent global average pooling could be done in a single step? Hmm, not sure. Alternatively, maybe the global average pooling can be optimized with a custom kernel that's more efficient than PyTorch's implementation.

The sum over a specific dimension (sum_dim=1) could be handled by a dedicated kernel that accumulates along that dimension. 

Alternatively, maybe the most impactful is to combine the convolution, division, and bias addition into a single kernel. Let me think about how that would work. The standard convolution outputs a tensor, then we divide by a scalar and add a bias term. Since division by a scalar is element-wise, and adding the bias (which has shape (out_channels, 1, 1, 1)), which is also element-wise over the spatial dimensions, this could be done in a post-processing step right after the convolution.

However, the convolution itself is the most compute-heavy part. Writing a custom 3D convolution might be difficult. Maybe instead of replacing the entire convolution with a custom kernel, I can use PyTorch's native conv3d but then fuse the subsequent division and bias addition. That might be easier. So first, run the convolution as is, then perform (x / divisor) + bias in a custom kernel. That's probably manageable.

Alternatively, maybe the division by a constant can be incorporated into the convolution's computation. Since the divisor is a scalar, during the convolution, each output element can be divided by it and then add the bias. That would avoid an extra element-wise step. So modifying the convolution's computation to include these steps. But writing a custom 3D convolution kernel would be necessary here, which is more involved. 

Hmm. Let's see. The standard PyTorch convolution is already quite optimized, so perhaps the gains from fusing these steps into the convolution might not be worth the complexity. Maybe it's better to first handle the division and bias in a single kernel after the convolution. Let me consider that approach first.

The next steps after convolution are division, max pool, global average pool, add bias (wait, no—the add bias is after the max pool and global average?), wait let me check the model code again:

The forward steps are:

x = self.conv(x)
x = x / self.divisor
x = self.max_pool(x)
x = self.global_avg_pool(x)
x = x + self.bias
x = torch.sum(x, dim=self.sum_dim)

Wait, the division is after the convolution. Then max pool, then global average pool, then adding the bias (which has shape (out_channels, 1, 1, 1)), but after the global average pool, the shape is (batch, out_channels, 1, 1, 1). So adding the bias (which is of shape (out_channels,1,1,1)) would be element-wise. Wait, but the global average pool reduces the spatial dimensions to 1, so the tensor after global average pool is (batch, out_channels, 1,1,1). The bias is (out_channels,1,1,1), so adding them is okay. Then the sum over dim=1 (the channel dimension?), resulting in (batch, 1, 1,1,1).

Hmm, maybe the max pooling and global average pooling can be fused into a single kernel? For example, after the convolution, instead of first doing max pooling and then global average pooling, perhaps a single kernel can compute some combined operation. Not sure. Alternatively, maybe the global average pool can be optimized by using a custom kernel that's faster than PyTorch's implementation.

Alternatively, the sum over the specified dimension (sum_dim=1, which is the channel dimension here after the global average pool) could be optimized with a custom kernel. 

Alternatively, the sequence of operations after the convolution can be fused into a single kernel. Let's see: after the convolution, the tensor is (batch, out_channels, depth, height, width). Then divided by divisor, then max_pooled, then global avg pooled, then add bias, then summed over dim=1. 

The max pooling reduces the spatial dimensions, then the global average pooling further reduces to 1 in each spatial dimension. Maybe instead of doing max pooling and then global average pooling, it's possible to combine them into a single step. For example, compute a global max-pool followed by averaging, but not sure. 

Alternatively, the entire sequence from convolution to the sum could be handled in a single fused kernel. But that might be too complex.

Let me think step by step:

1. Convolution is compute-heavy, but maybe not worth replacing unless I can fuse other steps. Maybe better to keep it as is, but then handle the division, pooling steps with custom kernels.

2. The division by divisor is a simple element-wise operation, which can be done in a kernel, but fusing it with the next operation (max pool) might not be straightforward.

3. The max pooling is a standard operation. Maybe a custom max pool kernel can be faster? Possibly, but perhaps PyTorch's implementation is already optimized.

4. The global average pooling is another step. If we can combine max pool and global average pool into a single kernel, that might save time. Let me think: the global average pooling after max pooling would be equivalent to averaging the max-pooled results over all spatial dimensions. But the max pooling reduces the spatial dimensions, and then the global average pooling averages over the remaining spatial dimensions. Maybe there's a way to compute this more efficiently, but perhaps it's already efficient enough.

5. The addition of the bias term is element-wise, so combining that with a previous step could help.

6. The sum over dim=1 is a reduction operation. That could be done in a custom kernel that accumulates along that dimension.

So perhaps the most impactful optimizations are:

- Fusing division by divisor and adding the bias into a single kernel after the convolution. 

Wait, but the division is before the max pool, and the bias addition is after the global average pool. So the order is important here. Let me see the order again:

After convolution: x is divided by divisor, then max pooled, then global avg pooled, then add bias, then summed.

So the division is before the pooling steps, so it can't be fused into the convolution's computation unless the convolution's output is processed immediately with those steps. 

Alternatively, perhaps the division and the max pooling can be combined into a single step. Since division is element-wise, it's easy to include in a kernel that also computes max pooling. For example, in the max pooling kernel, after dividing each element by divisor, compute the max over the pooling window. That might be feasible.

Similarly, the global average pooling could be done as part of a subsequent kernel.

Alternatively, the entire chain from convolution to sum could be handled in a series of fused operations. Let me outline possible fusion points:

Option 1:

Fusion 1: Conv3d + division. Not sure how to do that, since convolution is a different operation. Alternatively, after convolution, apply division in a kernel, then max pool, then global average pool, then add bias, then sum.

But the division after convolution is just element-wise, so a simple kernel can do that. But maybe the PyTorch's division is already optimized, so not much gain.

Alternatively, the max pooling and global average pooling can be fused into a single step. Let me think: Max pooling reduces the spatial dimensions by taking the maximum over a window, then global average pooling averages over the remaining dimensions. Perhaps combining them could be done by first taking max over the max pool window, then averaging over the entire remaining spatial dimensions. But the max pooling and global average pooling steps have different window sizes. The max pooling uses a fixed window (pool_size), and the global average pooling uses the entire remaining spatial dimensions. So unless the max pool window is such that after that the global average pooling is just taking the average over the remaining pixels, which might be possible. For example, if the max pooling reduces each spatial dimension to 1, then the global average would just be that value. But that depends on the input size and pool_size.

In the given example, the input depth=16, height and width=64. The max pool kernel is (2,2,2). So the output after max pool would be (16/2, 64/2, 64/2) → (8,32,32). Then global average pool reduces to (1,1,1). Alternatively, perhaps the max pool is applied with a kernel that's not reducing to 1, so the subsequent global average pool is needed. 

Hmm, perhaps fusing the max pool and global average pool is not straightforward. 

Another approach: The entire sequence from convolution to the final sum can be expressed as a series of operations that can be written in a custom kernel. Let me think of the data flow:

Conv3d → (element-wise division) → MaxPool3d → AdaptiveAvgPool3d → (element-wise add bias) → sum over dim=1.

Maybe the division, max pool, and the global average can be combined into a single kernel. Wait, the max pooling is a local operation, while the global average is a global one. Not sure.

Alternatively, the entire process can be broken into steps where the max pool and global average can be optimized with a custom kernel.

Alternatively, the sum over dim=1 is a reduction operation that can be done more efficiently with a custom kernel. Let me consider writing a custom kernel for the sum over a specific dimension.

Wait, let's consider each possible candidate for optimization:

1. Conv3d: As the most compute-heavy, replacing it with a custom kernel would give the biggest gain, but implementing a 3D convolution from scratch is quite involved. Maybe not worth it unless we can fuse other operations into it. Alternatively, maybe using PyTorch's native conv is okay.

2. Element-wise division by a constant: this is simple, so a custom kernel might not give much gain, but combining it with another step could be better.

3. MaxPool3d: Could be replaced with a custom kernel. Maybe.

4. Global Avg Pool (AdaptiveAvgPool3d(1,1,1)): This is a reduction over all spatial dimensions. A custom kernel could be faster.

5. Bias addition: Element-wise, so easy to include in a kernel with another step.

6. Sum over dim=1: A reduction operation; custom kernel could help.

Given the time constraints, perhaps the most impactful is fusing the division, max pool, and global average pool into a single kernel, then adding the bias and sum. Alternatively, the division can be part of the convolution's output processing.

Alternatively, let me think of the steps after convolution:

After convolution, x is divided by divisor. Let's make a kernel that does this division and then applies max pooling. So, first, the division is element-wise, then max pooling. Since max pooling is a local operation, perhaps a kernel can process each pooling window after dividing the elements. 

Then, after max pooling, the global average pooling is needed. The global average pooling can be implemented as a reduction over all spatial dimensions. So a kernel could compute the sum over all spatial elements for each channel and then divide by the number of elements, but perhaps that's more efficient as a custom kernel.

Alternatively, the global average pooling after max pooling can be done by summing all the elements in the spatial dimensions and dividing by the product of their sizes. That might be manageable in a kernel.

Then adding the bias and summing over the channel dimension. 

Alternatively, the entire sequence from division to the final sum can be done in a single kernel, but that would require handling all steps in one pass, which might be complex but efficient.

Let me outline a possible approach:

1. Keep the convolution as a PyTorch module (since replacing it is time-consuming).

2. Create a custom kernel that takes the output of the convolution, divides by the divisor, applies max pooling, then global average pooling, adds the bias, and finally sums over the specified dimension. 

This would require a single kernel to handle all those steps, which might be feasible but requires careful implementation.

Alternatively, break it into two steps: first, a kernel that does division, max pool, and global average pool, then another kernel to add the bias and sum. 

Alternatively, first, a kernel for division and max pool, then another for global average, bias addition, and sum.

Let me think of the first approach. Let me try to outline the kernel steps:

The input tensor after convolution is (batch, out_channels, D, H, W). 

First, divide each element by the divisor. 

Then apply max pooling with kernel size (2,2,2). So each pooling window is 2x2x2 in spatial dimensions. The output after max pool would be (batch, out_channels, D/2, H/2, W/2).

Then apply global average pooling over the remaining spatial dimensions (D/2, H/2, W/2). This reduces to (batch, out_channels, 1,1,1).

Then add the bias (shape (out_channels,1,1,1)), resulting in (batch, out_channels, 1,1,1).

Finally, sum over dimension 1 (the out_channels dimension), resulting in (batch, 1, 1,1,1).

All these steps can be done in a single kernel. The steps are:

- Iterate over each output element in the final result (after all steps).

But how to structure this in a CUDA kernel?

Alternatively, the kernel can process each batch element and each channel, compute the necessary steps.

Alternatively, process each spatial element in the original tensor, applying the steps in sequence.

This seems complex but possible.

Alternatively, perhaps breaking it into two kernels:

Kernel 1: combines division, max pool, and global average pool.

Kernel 2: adds bias and sums over dim.

Let me think of Kernel 1:

Input: conv_out (batch, C, D, H, W)

Output: after_div_max_avg (batch, C, 1,1,1)

Processing steps:

1. Divide each element of conv_out by divisor.

2. Apply max pooling with kernel_size (2,2,2). So for each spatial position in the output of max pool (which is (D/2, H/2, W/2)), we take the max over a 2x2x2 window in the divided tensor.

3. Then compute the average over all spatial dimensions (after max pool) for each channel. So for each (batch, c), the average over D/2 x H/2 x W/2 elements.

This can be done in a kernel as follows:

For each output element (batch, c, 1,1,1):

- The value is the average over all spatial elements in the max-pooled tensor for that channel and batch.

But how to compute the max pool and then the average in a single pass?

Alternatively, the max pooling can be computed first, then the global average. But even that might be faster in a custom kernel than using PyTorch's layers.

Alternatively, the global average after max pool can be computed by summing all elements in the spatial dimensions and dividing by the total number of elements.

So, the kernel can compute for each channel and batch:

sum over (d, h, w) of the max_pooled value at (d, h, w) for that channel and batch, then divide by (D/2 * H/2 * W/2).

So the steps:

1. Divide each element by divisor.

2. For each max pool window, compute max.

3. For each channel and batch, compute the sum of all max pooled spatial elements, then divide by the number of elements to get the average.

But doing this in a single kernel would require handling the max pooling and accumulation for the sum.

Alternatively, the kernel can compute the max pool and then accumulate the sum for each channel.

This might be manageable.

Then, adding the bias (which is per-channel) and summing over channels can be done in a second kernel.

Alternatively, all steps can be done in one kernel, but let's proceed step by step.

First, let's define the parameters given in the problem:

The model has:

- in_channels=8

- out_channels=16

- kernel_size=(3,3,3)

- divisor=2.0

- pool_size=(2,2,2)

- bias_shape=(out_channels,1,1,1) → (16,1,1,1)

- sum_dim=1 (the channel dimension)

The input tensor to the model is of size (batch_size, in_channels, depth, height, width) → (128, 8, 16,64,64).

After convolution, the output size would be (128, 16, D', H', W'), where D' is depth - kernel_size[0] + 1 (assuming no padding and stride 1). Wait, but the kernel size is 3, so D' = 16-3+1 =14, H and W similarly 64-3+1=62 each? But in the problem's code, it's possible that the convolution uses padding or different strides, but in the given code, the Conv3d is initialized with default parameters (stride 1, padding 0). So the output spatial dimensions after convolution would be (14, 62, 62). Wait, but the problem's code doesn't mention padding, so assuming no padding. Let me confirm:

Conv3d's output size calculation:

For a 3D convolution with kernel_size (kD, kH, kW), stride (sD, sH, sW), padding (pD, pH, pW):

Output depth: (D_in - kD + 2*pD) / sD + 1

Same for height and width.

Default stride is 1, padding 0. So:

depth_out = (16 -3 +0)/1 +1 = 14

height_out = (64-3)/1 +1=62

width same as height.

So after convolution, the tensor is (128,16,14,62,62).

Then divided by divisor (2.0).

Then max pool with kernel_size (2,2,2):

MaxPool3d(pool_size) with default stride equal to kernel size (assuming no dilation), so stride (2,2,2). 

So the output after max pool would be (128, 16, 14/2=7, 62/2=31, 62/2=31) → (7,31,31). Wait, 14 divided by 2 gives 7 exactly, but 62 divided by 2 is 31 with no remainder? 62/2=31, yes. So dimensions after max pool are (7,31,31).

Then global_avg_pool (AdaptiveAvgPool3d(1,1,1)) reduces this to (1,1,1), so the shape becomes (128,16,1,1,1).

Adding the bias (shape 16,1,1,1) is element-wise addition, resulting in same shape.

Then sum over dim=1 (the channel dimension) gives (128, 1, 1, 1, 1).

So the total steps after convolution are:

1. Division: simple element-wise.

2. Max pool with kernel (2,2,2), stride (2,2,2).

3. Global average pool → reduce all spatial dims to 1.

4. Add bias.

5. Sum over channel.

To optimize, let's see where the bottlenecks are. The max pool and global average might have high memory access overhead. Fusing them could help.

Let me consider writing a custom kernel that takes the convolution output, divides by the divisor, applies max pooling, computes the global average, adds the bias, and then sums the channels.

This would require a kernel that for each batch and channel, computes the average over all spatial elements after max pooling, adds the bias, then sums over all channels.

Alternatively, the kernel can process each spatial element in the convolution output, compute the max over the pooling windows, accumulate the sum for each channel, then divide by the spatial elements count, add the bias, and sum over channels.

Alternatively, here's a possible plan:

Kernel 1 (post_conv_processing):

Input: conv_output (batch, C, D, H, W)

Constants: divisor, pool_size (2,2,2), global_avg (which requires summing all spatial elements after max pool)

Output: intermediate tensor (batch, C, 1,1,1)

Steps:

- For each element in the conv_output, divide by divisor.

- Compute max over each 2x2x2 window in the spatial dimensions (D, H, W). 

- After max pooling, the spatial dimensions become D_out = ceil(D / 2), H_out = ceil(H/2), W_out = ceil(W/2). Wait, no, since the kernel size is 2 and stride 2, it's exactly D/2 etc.

- Then compute the average over all spatial elements (D_out * H_out * W_out) for each channel and batch.

This can be done by for each batch and channel, accumulate the sum of all max-pooled spatial elements, then divide by (D_out * H_out * W_out).

So, the kernel would need to handle these steps efficiently.

Another approach is to compute the max pooling and global average in a single step by processing each max pool window and accumulating the sum.

Here's a possible CUDA kernel outline:

For each output channel and batch:

- Iterate over all spatial positions in the original tensor.

- For each spatial position, determine which max pool window it belongs to.

- Keep track of the max value in each window.

- Then sum all the max values across all windows and divide by the total number of elements.

Alternatively, this might be more efficient as a two-step process, but let's try to structure the kernel.

Alternatively, use a grid of threads where each thread processes a certain region.

Alternatively, the kernel could be structured as follows:

Each thread is responsible for a spatial position in the max-pooled output. 

Wait, let me think in terms of the max pool step first.

The max pooling step requires taking each 2x2x2 window in the divided tensor and outputting the maximum value.

So for each position (d_out, h_out, w_out) in the max-pooled spatial dimensions, the input window is from d_start to d_start+1, h_start to h_start+1, w_start to w_start+1. 

Wait, the stride is 2, so the windows are non-overlapping.

So for each output spatial position (d_out, h_out, w_out), the input window starts at (d_out*2, h_out*2, w_out*2) and spans 2 in each dimension.

The maximum value in this window is stored in the max_pooled tensor at (d_out, h_out, w_out).

Then, the global average is the average of all these max values across all d_out, h_out, w_out.

Thus, the steps can be:

1. Compute the divided tensor (conv_output / divisor).

2. Compute the max pooled tensor (max over each 2x2x2 window).

3. Compute the sum over all spatial elements of the max_pooled tensor for each channel and batch.

4. Divide by the number of elements (total_spatial_elements = D_out * H_out * W_out).

5. Add the bias (per-channel).

6. Sum over the channel dimension.

This can be done in two kernels: one for the max pool and global sum, then another for the final steps.

Let me outline the kernels:

First, the max pool and global sum:

Kernel 1: compute_max_pooled_sums

Input: conv_div (batch, C, D, H, W)

Output: sum_per_channel (batch, C) → since we need to sum over all spatial elements after max pool.

Parameters: divisor, pool_size (2,2,2)

Steps:

- For each batch and channel:

   - Iterate over all spatial positions in the max-pooled output (D_out = D/2, H_out = H/2, W_out = W/2)

   - For each such (d_out, h_out, w_out):

      - Find the max in the 2x2x2 window in the divided tensor.

      - Add this max value to the sum for that channel and batch.

Thus, each thread can handle a (d_out, h_out, w_out) position for a given batch and channel.

But how to structure this in CUDA:

Perhaps launch a grid where each thread block handles a channel and batch, and threads within the block handle spatial positions. 

Alternatively, have a thread block per spatial position.

Alternatively, let's think of the kernel:

Each thread is responsible for a certain (batch, channel, d_out, h_out, w_out). 

But with batch=128, C=16, D_out=7, H_out=31, W_out=31, this might be a large number of threads.

Alternatively, use a grid where each thread block handles a batch and channel, and threads process spatial positions.

Alternatively, the kernel could be:

__global__ void compute_max_pooled_sums(const float* conv_div, float* sum_per_channel, int batch, int C, int D, int H, int W, int D_out, int H_out, int W_out) {

   int batch_idx = blockIdx.x;

   int c = blockIdx.y;

   if (batch_idx >= batch || c >= C) return;

   // Each thread in the block handles a spatial position (d_out, h_out, w_out)

   int tid = threadIdx.x;

   // Iterate over all d_out, h_out, w_out positions.

   // Since blockDim.x might be 1024, we can use a loop over all spatial indices.

   // Alternatively, parallelize over all spatial positions.

   for (int idx = tid; idx < D_out * H_out * W_out; idx += blockDim.x) {

      int d_out = idx / (H_out * W_out);

      int rem = idx % (H_out * W_out);

      int h_out = rem / W_out;

      int w_out = rem % W_out;

      // Compute the window in the divided tensor:

      float max_val = -INFINITY;

      for (int kd = 0; kd < 2; ++kd) {

         int d = d_out * 2 + kd;

         if (d >= D) continue;

         for (int kh = 0; kh < 2; ++kh) {

            int h = h_out * 2 + kh;

            if (h >= H) continue;

            for (int kw = 0; kw < 2; ++kw) {

               int w = w_out * 2 + kw;

               if (w >= W) continue;

               float val = conv_div[batch * C * D*H*W + c * D*H*W + d*H*W + h*W + w];

               if (val > max_val) {

                  max_val = val;

               }

            }

         }

      }

      // Add to sum_per_channel[batch][c]

      atomicAdd(&sum_per_channel[batch * C + c], max_val);

   }

}

Wait, but atomicAdd is slow. Maybe instead, each thread accumulates a partial sum and then writes to a per-block buffer, then reduces. Alternatively, this approach may be too slow due to the atomic operations.

Hmm, perhaps this kernel is not efficient. Maybe the PyTorch's max pooling is faster. Let me think of alternative approaches.

Alternatively, using PyTorch's MaxPool3d and then a custom kernel for the global average.

Let me try to consider which steps are easiest to implement.

Perhaps the division and the bias addition can be combined with the sum over channels.

Alternatively, let's think of the final sum over dim=1. The final step is summing over the channel dimension, which has size 16. Since 16 is small, maybe it's better to do this in a separate kernel.

Alternatively, after the global average pool and adding the bias, the tensor is (batch, 16, 1,1,1). To sum over dim=1 (channels), you can simply sum along the channel dimension.

A custom kernel for this could be:

__global__ void sum_channels_kernel(float* input, float* output, int batch, int C, int spatial_elements) {

   int batch_idx = blockIdx.x;

   if (batch_idx >= batch) return;

   float sum = 0;

   for (int c = 0; c < C; c++) {

      sum += input[batch_idx * C * spatial_elements + c * spatial_elements];

   }

   output[batch_idx] = sum;

}

This is straightforward. Since spatial_elements is 1 here (after global average), it's just summing the channels.

This could be efficient.

Now, the global average pooling can be implemented with a custom kernel that sums over spatial dimensions and divides by their product.

The input to global_avg_pool is the max_pooled tensor of shape (batch, C, D_out, H_out, W_out).

The global average would compute for each (batch, c):

sum over all d, h, w of max_pooled[batch][c][d][h][w] divided by (D_out * H_out * W_out).

This can be done with a kernel like:

__global__ void global_avg_pool_kernel(float* input, float* output, int batch, int C, int D, int H, int W) {

   int batch_idx = blockIdx.x;

   int c = blockIdx.y;

   if (batch_idx >= batch || c >= C) return;

   float sum = 0;

   for (int d = 0; d < D; ++d) {

      for (int h = 0; h < H; ++h) {

         for (int w = 0; w < W; ++w) {

            sum += input[batch_idx * C * D*H*W + c * D*H*W + d*H*W + h*W + w];

         }

      }

   }

   output[batch_idx * C + c] = sum / (D * H * W);

}

This kernel could be launched with a grid of (batch * C) blocks, each handling a (batch, c) pair. Each thread in a block would compute the sum for that channel and batch.

But again, with batch=128 and C=16, this gives 128*16=2048 blocks, which is manageable. Each block would have a single thread (since the loops are done inside the kernel). However, this might not be efficient because each thread is doing a lot of work (looping over D, H, W). 

Alternatively, parallelize the spatial dimensions. For example, each thread in a block handles a spatial position.

Let me restructure:

Each block handles a (batch, c) pair.

The block has multiple threads, each handling a portion of the spatial elements.

For example, the kernel:

__global__ void global_avg_pool_kernel(float* input, float* output, int batch, int C, int D, int H, int W) {

   int batch_idx = blockIdx.x;

   int c = blockIdx.y;

   if (batch_idx >= batch || c >= C) return;

   int spatial_size = D * H * W;

   int tid = threadIdx.x;

   float local_sum = 0;

   for (int idx = tid; idx < spatial_size; idx += blockDim.x) {

      int d = idx / (H * W);

      int rem = idx % (H * W);

      int h = rem / W;

      int w = rem % W;

      float val = input[batch_idx * C * D*H*W + c * D*H*W + d*H*W + h*W + w];

      local_sum += val;

   }

   __shared__ float shared_sum[THREADS_PER_BLOCK];

   shared_sum[tid] = local_sum;

   __syncthreads();

   // Perform reduction in shared memory

   // Assuming THREADS_PER_BLOCK is a power of 2.

   for (int s = blockDim.x/2; s >0; s >>=1) {

      if (tid < s) {

         shared_sum[tid] += shared_sum[tid + s];

      }

      __syncthreads();

   }

   if (tid ==0) {

      output[batch_idx * C + c] = shared_sum[0] / (D * H * W);

   }

}

This way, each block uses shared memory to reduce the sum, which is more efficient.

This would require choosing a suitable block size (e.g., 256 threads).

Now, putting this together, the steps would be:

1. Run the PyTorch convolution.

2. Element-wise division by divisor → this can be done with a simple kernel.

3. Max pooling → use PyTorch's MaxPool3d or a custom kernel.

   - Custom kernel might be better if we can combine it with the global average.

4. Global average pooling → use the custom kernel above.

5. Add bias → element-wise addition.

6. Sum over channels → use the sum_channels kernel.

Alternatively, combining steps 2 and 3 into a single kernel might be better.

Let me outline the code structure.

First, for the element-wise division and max pooling:

Perhaps create a kernel that does both. Let me see.

The division is straightforward: each element is divided by divisor. Then the max pooling is over 2x2x2 windows. So maybe a kernel can compute both steps, storing the max values in a temporary array.

But this requires storing the divided tensor. Alternatively, the kernel can first divide each element, then compute the max over the windows.

But memory-wise, it's better to process in place.

Alternatively, the kernel can compute the divided tensor and then the max pooling in a single step.

Alternatively, perhaps it's better to use PyTorch's division and max pooling, then use custom kernels for the remaining steps.

Let me proceed step by step.

First, the division after convolution is trivial. We can do this with a simple kernel:

elementwise_divide_source = """
...
"""

But maybe PyTorch's division is already optimized, so we can skip that.

Alternatively, using PyTorch's division is fine.

So the steps would be:

x = self.conv(x)

x = x / self.divisor → PyTorch's implementation.

x = self.max_pool(x) → PyTorch's MaxPool3d.

Then, for the global average, we can use the custom kernel above.

Then add bias and sum.

Alternatively, the max_pool could be replaced with a custom kernel to save time.

Alternatively, to save on memory copies, perhaps combining max_pool and global average in a single kernel.

Let me try writing the global_avg_pool kernel first.

Wait, let me think of the code structure:

We'll need to write multiple kernels, so let's outline them.

First, the global average pooling kernel:

global_avg_pool_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

template <int BlockDim>
__global__ void global_avg_pool_kernel(
   