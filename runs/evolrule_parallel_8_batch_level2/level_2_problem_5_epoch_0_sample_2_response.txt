Make sure to follow the syntax from the example above. Your code should be able to be run in a fresh environment with pytorch installed. 

You may want to replace some operators with custom CUDA kernels. For example, you can combine conv_transpose, bias subtraction, and tanh into a single kernel for better performance. But you are free to choose which operators to replace. 

Make sure that the new ModelNew has the same input and output dimensions and behavior as the original Model.

Make sure to also output the get_inputs() and get_init_inputs() function. 

First, I need to analyze the given Model's architecture to identify which operations can be optimized using custom CUDA kernels. The original Model performs a transposed convolution (ConvTranspose2d), subtracts a bias term, and applies a tanh activation. 

The goal is to replace some or all of these operations with custom CUDA kernels to achieve speedups. Since transposed convolution (deconvolution) is a computationally intensive operation, combining it with the subsequent operations (bias subtraction and tanh) into a single kernel might reduce memory overhead and kernel launch overhead. However, implementing a fused ConvTranspose2d with bias subtraction and tanh in CUDA could be quite complex. Alternatively, perhaps optimizing the bias subtraction and tanh into a single kernel could be simpler and still provide some benefit.

First, let me outline the steps in the forward pass:
1. **Convolution Transpose**: This is the main operation, and its implementation is already handled by PyTorch. Since it's a standard layer, replacing it with a custom CUDA kernel might not be straightforward unless there's a specific optimization (like avoiding memory copies or using algorithmic improvements). However, combining it with subsequent operations could be beneficial.
2. **Bias Subtraction**: Normally, bias addition is part of the convolution layer, but here it's a separate step where a bias term is subtracted. This is an element-wise operation.
3. **Tanh Activation**: Another element-wise operation.

Combining the element-wise operations (bias subtraction and tanh) into a single kernel would reduce the number of CUDA kernel launches, which can help. However, the main computational cost is likely in the conv_transpose. If possible, fusing the conv_transpose with the subsequent element-wise operations into a single kernel would be ideal but complex.

Alternatively, perhaps the conv_transpose can be optimized by using a custom kernel with better parallelism or memory access patterns, but that might be more involved.

Another point to consider: PyTorch's ConvTranspose2d might already be highly optimized, so replacing it might not yield significant gains unless there's a specific optimization. However, combining it with the bias and tanh could be beneficial. Let me think about the data flow:

The output of conv_transpose is a tensor, then subtract the bias (element-wise), then apply tanh. The bias is of shape (out_channels, 1, 1), so it's broadcasted over the spatial dimensions. 

Perhaps we can write a kernel that takes the output of the conv_transpose and applies both the bias subtraction and tanh in the same kernel. This would eliminate the need for intermediate tensors and reduce memory bandwidth.

However, to do that, the conv_transpose would still need to be computed first. Unless we can fuse the conv_transpose computation with the element-wise operations. But fusing a convolution with element-wise ops is non-trivial.

Alternatively, perhaps replacing the entire sequence (conv_transpose, subtract bias, tanh) with a single custom kernel. However, implementing a custom ConvTranspose2d is quite involved.

Alternatively, perhaps the subtraction and tanh can be combined into a single element-wise kernel. Let me see:

The current steps are:
x = conv_transpose(x)
x = x - bias
x = tanh(x)

The first step is the main computation, but the next two are element-wise. Let me see:

Suppose after the conv_transpose, we have an element-wise computation of (x - bias) followed by tanh. Since both are element-wise, perhaps they can be combined into a single kernel. That would reduce the number of CUDA launches from two (subtract and tanh) to one.

This is a feasible optimization and likely easier to implement than modifying the convolution.

Therefore, the plan is:

1. Keep the ConvTranspose2d as is, since it's already optimized by PyTorch. Alternatively, we might find that the PyTorch implementation is efficient enough, so replacing it isn't worth the effort unless we can combine it with other steps.

2. Combine the bias subtraction and tanh into a single CUDA kernel. This reduces two separate kernel launches into one.

Therefore, I will proceed to implement a custom CUDA kernel that performs (x - bias) followed by tanh in a single step.

The steps are:

- The input to the combined kernel is the output of the convolution, which is a tensor of shape (batch, out_channels, H, W). The bias is a tensor of shape (out_channels, 1, 1), so it is broadcastable over the spatial dimensions.

The kernel would process each element as follows:

out[i] = tanh( x[i] - bias[i_channel] )

where i_channel is the channel index of the element.

Implementing this in CUDA requires iterating over all elements, and for each element, compute the channel index to access the bias.

Now, to write this kernel:

First, the kernel function would take pointers to the input tensor (x), the bias tensor (bias), and the output tensor. The sizes would be needed as well.

The kernel would loop over each element. Since the input is a 4D tensor (batch, channels, height, width), the indices need to be calculated appropriately.

The CUDA kernel can be structured with a 1D grid, where each thread handles one element. The thread index can be mapped to the 4D coordinates via division.

Alternatively, we can use a 3D grid (blocks per batch, etc.), but a 1D approach is simpler for illustration.

The kernel code would look something like:

__global__ void fused_bias_tanh_kernel(
    const float* x_data,
    const float* bias_data,
    float* out_data,
    int batch_size,
    int channels,
    int height,
    int width) {

    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= batch_size * channels * height * width) return;

    // Calculate indices
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / (width * height)) % channels;
    int b = index / (width * height * channels);

    // Compute value
    float val = x_data[index] - bias_data[c];  // bias is (C,1,1), so index c
    out_data[index] = tanhf(val);  // or tanh if double, but assuming float
}

Wait, note that in PyTorch, tanh is an element-wise function. The CUDA math library has tanh as tanh (for double) or tanhf (for float). Since the tensors are float, we can use tanhf.

Wait, in CUDA, the standard math functions like tanh are in <math.h>, but for device code, we can use __device__ functions. Let me confirm: In CUDA, for device code, tanhf is available.

Yes, the standard C math functions are available in device code for float, so tanhf is okay.

Now, the kernel needs to be written with the correct parameters. The bias is a 3D tensor (out_channels, 1, 1), so the bias_data[c] is correct because the last two dimensions are 1, so accessing just c would give the correct value.

Now, in the Python code, we need to compile this kernel. Following the example, we can write the CUDA source code as a string and use torch.utils.cpp_extension.load_inline.

In the ModelNew class, the forward would first perform the conv_transpose as before, then apply the fused kernel.

Wait, but in the original Model, the bias is subtracted, not added. So the operation is x - self.bias. The bias is stored as a nn.Parameter, which is a tensor. So in the code, the kernel should subtract the bias (element-wise) then apply tanh.

Therefore, in the kernel, the value is x_data[index] - bias_data[c].

Now, the inputs to the fused kernel would be the output of the conv_transpose, the bias tensor, and the output tensor.

Thus, in the Python code, the fused function would be something like:

def fused_bias_tanh_cuda(x, bias):

    # Compute the output tensor
    output = torch.empty_like(x)

    # Get the shape parameters
    batch_size = x.size(0)
    channels = x.size(1)
    height = x.size(2)
    width = x.size(3)

    # Calculate the total number of elements
    num_elements = batch_size * channels * height * width

    # Block and grid sizes
    block_size = 256
    num_blocks = (num_elements + block_size - 1) // block_size

    # Launch the kernel
    fused_bias_tanh_kernel<<<num_blocks, block_size>>>(
        x.data_ptr(), bias.data_ptr(), output.data_ptr(),
        batch_size, channels, height, width
    )

    return output

But in the CUDA kernel code, the parameters must match. Let me structure this correctly.

Now, the kernel's parameters must be passed correctly. The CUDA kernel function is declared with the correct parameters, so the code in the string should match.

Putting this all together, the CUDA source code for the fused kernel would be part of the elementwise_add_source equivalent in the example.

Additionally, the ModelNew class would need to use the conv_transpose layer from PyTorch, then apply the fused kernel.

Wait, but in the original model, the bias is a Parameter (self.bias), so in the new model, we need to keep that Parameter.

Therefore, the steps for the new ModelNew class would be:

1. Keep the ConvTranspose2d layer as is.

2. Keep the bias Parameter.

3. In forward(), compute the conv_transpose, then apply the fused kernel combining bias subtraction and tanh.

Thus, the code outline would be:

class ModelNew(nn.Module):
    def __init__(self, ...):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(...)
        self.bias = nn.Parameter(...)
        self.fused_op = ...  # the compiled kernel function

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.fused_op(x, self.bias)
        return x

Now, compiling the CUDA kernel.

The CUDA source code for the fused kernel:

The kernel function is as above. Then, the wrapper function in the CUDA code.

Wait, the example in the problem statement uses a separate .cpp and .cu source. In the inline case, we can write the CUDA kernel in a string and the corresponding wrapper.

Let me structure this:

First, the CUDA source code string for the fused kernel:

fused_bias_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_bias_tanh_kernel(
    const float* x_data,
    const float* bias_data,
    float* out_data,
    int batch_size,
    int channels,
    int height,
    int width) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= batch_size * channels * height * width) return;

    // Calculate indices
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / (width * height)) % channels;
    int b = index / (width * height * channels);

    // Compute value
    float val = x_data[index] - bias_data[c];  // subtract bias
    out_data[index] = tanhf(val);
}

torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias) {
    // Ensure x and bias are on the same device
    auto output = torch::empty_like(x);

    int batch_size = x.size(0);
    int channels = x.size(1);
    int height = x.size(2);
    int width = x.size(3);

    int num_elements = batch_size * channels * height * width;

    int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_bias_tanh_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}
"""

Then, the corresponding C++ headers (if needed). Wait, in the example, they had a cpp_source and a cuda_source, but perhaps here we can write everything in the CUDA source string, as the function is defined there.

Wait, in the example, the elementwise_add_cuda function is defined in the CUDA source. So the wrapper function is in the CUDA code, and the kernel is also there. Thus, the fused_bias_tanh_source includes both the kernel and the wrapper function.

Therefore, the CUDA source code is self-contained.

Then, the corresponding C++ header declarations (the .h) would be needed for the function declarations. But in the inline case, the load_inline function requires the header as cpp_sources.

Wait, the example had:

elementwise_add_cpp_source = (
    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);"
)

This is the C++ header for the function. So for our fused function, the cpp_sources should declare:

"torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias);"

Thus, putting it all together:

fused_bias_tanh_cpp_source = (
    "torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias);"
)

Then, the load_inline would be:

fused_op = load_inline(
    name="fused_bias_tanh",
    cpp_sources=fused_bias_tanh_cpp_source,
    cuda_sources=fused_bias_tanh_source,
    functions=["fused_bias_tanh_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Wait, but in the example, the functions parameter was ["elementwise_add_cuda"], so the function name must match.

Therefore, in the code:

The fused kernel function is named fused_bias_tanh_cuda, so functions should include that.

Now, in the ModelNew class, we would have:

self.fused_op = fused_op  # the module returned by load_inline

and in forward:

x = self.fused_op.fused_bias_tanh_cuda(x, self.bias)

Thus, this setup should work.

Now, check if the inputs to the fused kernel are correct. The bias is of shape (out_channels, 1, 1), which is compatible with the channels dimension. The kernel uses bias_data[c], which is correct since for each channel c, the bias value is bias[c][0][0].

Now, the rest of the model parameters are passed as in the original Model's __init__.

Wait, the original Model's __init__ takes in_channels, out_channels, kernel_size, bias_shape, stride, padding, output_padding. So the new ModelNew should have the same parameters.

Thus, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 
        # Load the fused kernel
        fused_bias_tanh_source = ... # the source code as above
        fused_bias_tanh_cpp_source = ... # the header
        self.fused_op = load_inline(...) # as above

Wait, but the CUDA source code (fused_bias_tanh_source) and the cpp source (fused_bias_tanh_cpp_source) need to be defined before the class.

Therefore, in the code, the CUDA sources must be defined outside the class.

Putting all together, the full code would be structured as follows.

Also, note that the get_inputs() and get_init_inputs() functions are to be included as per the original. The get_init_inputs() in the original is returning [in_channels, out_channels, kernel_size, bias_shape]. So in the new code, these functions remain the same.

Now, compiling the CUDA code inline. The code must be in strings.

Another consideration: the CUDA code must be in a string, so the code needs to be properly escaped.

Now, let's write the full code step by step.

First, the CUDA kernel code as a string:

fused_bias_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_bias_tanh_kernel(
    const float* x_data,
    const float* bias_data,
    float* out_data,
    int batch_size,
    int channels,
    int height,
    int width) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= batch_size * channels * height * width) return;

    int w = index % width;
    int h = (index / width) % height;
    int c = (index / (width * height)) % channels;
    int b = index / (width * height * channels);

    float val = x_data[index] - bias_data[c];
    out_data[index] = tanhf(val);
}

torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias) {
    // Check that tensors are on the same device
    AT_ASSERTM(x.device().is_cuda(), "x must be a CUDA tensor");
    AT_ASSERTM(bias.device().is_cuda(), "bias must be a CUDA tensor");
    AT_ASSERTM(x.size(1) == bias.size(0), "Channels of x and bias must match");

    auto output = torch::empty_like(x);

    int batch_size = x.size(0);
    int channels = x.size(1);
    int height = x.size(2);
    int width = x.size(3);

    int num_elements = batch_size * channels * height * width;

    int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_bias_tanh_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}
"""

Wait, added assertions to ensure device and shape compatibility.

The corresponding cpp source (header declarations):

fused_bias_tanh_cpp_source = (
    "torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias);"
)

Then, the load_inline call:

from torch.utils.cpp_extension import load_inline

fused_op = load_inline(
    name="fused_bias_tanh",
    cpp_sources=fused_bias_tanh_cpp_source,
    cuda_sources=fused_bias_tanh_source,
    functions=["fused_bias_tanh_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Now, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        # Load the fused kernel
        # The fused_op is already loaded outside the class
        # Wait, but how to handle this?

Wait, the fused_op is defined outside the class, so in the class's __init__, we can assign it as an attribute.

Wait, actually, in the example, the elementwise_add was loaded before the class, then assigned to self.elementwise_add in the __init__.

So, in this case, we need to define the fused_op outside the class, then assign it to the class instance.

Alternatively, perhaps the fused_op can be a module that is loaded once outside the class.

Thus, the code structure would be:

First, import statements.

Then, define the fused kernel source strings.

Then, define the fused_op via load_inline.

Then, define the ModelNew class, which references the fused_op.

Wait, but in the example, the elementwise_add was loaded before the class, then assigned to self.elementwise_add.

Thus, in our case:

After defining the fused sources and loading the fused_op, the ModelNew can include:

    self.fused_op = fused_op

Thus, the full code would be:

import torch
import torch.nn as nn

# CUDA kernel code for fused bias subtraction and tanh activation
fused_bias_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_bias_tanh_kernel(
    const float* x_data,
    const float* bias_data,
    float* out_data,
    int batch_size,
    int channels,
    int height,
    int width) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= batch_size * channels * height * width) return;

    int w = index % width;
    int h = (index / width) % height;
    int c = (index / (width * height)) % channels;
    int b = index / (width * height * channels);

    float val = x_data[index] - bias_data[c];
    out_data[index] = tanhf(val);
}

torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias) {
    // Check device and shape
    AT_ASSERTM(x.device().is_cuda(), "x must be a CUDA tensor");
    AT_ASSERTM(bias.device().is_cuda(), "bias must be a CUDA tensor");
    AT_ASSERTM(x.size(1) == bias.size(0), "Channels of x and bias must match");

    auto output = torch::empty_like(x);

    int batch_size = x.size(0);
    int channels = x.size(1);
    int height = x.size(2);
    int width = x.size(3);

    int num_elements = batch_size * channels * height * width;

    int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_bias_tanh_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}
"""

fused_bias_tanh_cpp_source = (
    "torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias);"
)

# Load the fused CUDA kernel
fused_op = load_inline(
    name="fused_bias_tanh",
    cpp_sources=fused_bias_tanh_cpp_source,
    cuda_sources=fused_bias_tanh_source,
    functions=["fused_bias_tanh_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_op = fused_op  # Assign the loaded module

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.fused_op.fused_bias_tanh_cuda(x, self.bias)
        return x

# The get_inputs and get_init_inputs functions remain the same
batch_size = 32
in_channels  = 64  
out_channels = 64  
height = width = 256 
kernel_size = 4
bias_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]

Wait, but in the __init__ of ModelNew, the parameters must match the original. The original's __init__ has parameters (in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1). The ModelNew should have the same parameters.

Thus, the code is correct.

Now, check for possible errors:

- The bias_shape is (out_channels, 1, 1). The kernel accesses bias_data[c], where c is the channel index, so this should work.

- The CUDA kernel uses tanhf for float, which is correct.

- The output tensor is initialized with empty_like(x), which is correct.

- The kernel launch parameters are correct.

- The fused_op is loaded as a module, and in the forward, it's called as self.fused_op.fused_bias_tanh_cuda(...), which matches the example.

Now, the get_inputs and get_init_inputs functions are the same as the original.

Now, test if the output dimensions match.

Original Model:

After conv_transpose, the output shape would be computed as follows.

Given input shape (batch, in_channels, H, W), the ConvTranspose2d's output H and W are computed as:

out_height = (H - 1)*stride - 2*padding + kernel_size + output_padding

Similarly for out_width.

With original parameters:

input height/width = 256, stride=2, padding=1, output_padding=1, kernel_size=4.

Thus:

out_height = (256 -1)*2 - 2*1 +4 +1 = 255*2 = 510 -2 +4+1=510-2=508 +5=513? Wait, let me compute step by step:

Wait the formula for output size:

For a transposed convolution, the output spatial dimension can be calculated as:

out_dim = (input_dim - 1) * stride - 2 * padding + kernel_size + output_padding

So for input dim 256 (height and width):

out_height = (256 - 1)*2 - 2*1 +4 +1 = 255*2 = 510 -2 = 508 +4 = 512 +1 = 513?

Wait let me compute:

(256-1)*2 = 510

- 2*padding (2*1=2): 510 -2 = 508

+ kernel_size (4): 512

+ output_padding (1): 513.

So the output spatial dimensions would be 513x513.

Thus, the output tensor after conv_transpose would be (32, 64, 513, 513). Then, after subtracting the bias (64,1,1), which is broadcast, the final shape remains the same. The tanh doesn't change the shape.

In the new model, the fused kernel should produce the same shape.

Yes, because the kernel processes all elements, so the output is the same as the input.

Thus, the output dimensions are preserved.

Therefore, this code should work.

Another possible optimization: The original code uses x - self.bias. The bias is a Parameter. The kernel code subtracts the bias, which is correct.

Thus, this should be a valid optimization.

Now, compiling this code would require CUDA, but since the user said to assume it's in a fresh environment with PyTorch installed (which includes CUDA), it should compile.

Potential issues:

- The CUDA kernel may have a bug in indexing. For example, the calculation of indices may be incorrect. Let me recheck the index calculation.

The index is calculated as:

w = index % width

h = (index / width) % height

c = (index / (width * height)) % channels

b = index / (width * height * channels)

This is standard for a 4D tensor stored in row-major order (C order). The order of dimensions is (batch, channels, height, width), so the stride for batch is channels*height*width, then channels, etc.

Thus, the indices are correctly mapped.

Another possible issue is the use of tanhf. The function tanh in PyTorch uses the same as tanhf for float tensors. So this should be correct.

Thus, the code should work as intended.


```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# CUDA kernel code for fused bias subtraction and tanh activation
fused_bias_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_bias_tanh_kernel(
    const float* x_data,
    const float* bias_data,
    float* out_data,
    int batch_size,
    int channels,
    int height,
    int width) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= batch_size * channels * height * width) return;

    int w = index % width;
    int h = (index / width) % height;
    int c = (index / (width * height)) % channels;
    int b = index / (width * height * channels);

    float val = x_data[index] - bias_data[c];
    out_data[index] = tanhf(val);
}

torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias) {
    AT_ASSERTM(x.device().is_cuda(), "x must be a CUDA tensor");
    AT_ASSERTM(bias.device().is_cuda(), "bias must be a CUDA tensor");
    AT_ASSERTM(x.size(1) == bias.size(0), "Channels of x and bias must match");

    auto output = torch::empty_like(x);

    int batch_size = x.size(0);
    int channels = x.size(1);
    int height = x.size(2);
    int width = x.size(3);

    int num_elements = batch_size * channels * height * width;

    int block_size = 256;
    int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_bias_tanh_kernel<<<num_blocks, block_size>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels, height, width
    );

    return output;
}
"""

fused_bias_tanh_cpp_source = (
    "torch::Tensor fused_bias_tanh_cuda(torch::Tensor x, torch::Tensor bias);"
)

# Load the fused CUDA kernel
fused_op = load_inline(
    name="fused_bias_tanh",
    cpp_sources=fused_bias_tanh_cpp_source,
    cuda_sources=fused_bias_tanh_source,
    functions=["fused_bias_tanh_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape, stride=2, padding=1, output_padding=1):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_op = fused_op

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.fused_op.fused_bias_tanh_cuda(x, self.bias)
        return x

# Input and initialization functions
batch_size = 32
in_channels  = 64  
out_channels = 64  
height = width = 256 
kernel_size = 4
bias_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]
```