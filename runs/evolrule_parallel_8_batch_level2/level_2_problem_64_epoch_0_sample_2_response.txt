**Constraints:**
- The input and output tensors' shapes must remain the same as the original model. 
- The new model must be compatible with PyTorch and use the same initialization and forward() method.
- You can use PyTorch's built-in CUDA kernels except for the operators you replace.
- You must implement custom CUDA kernels for at least two operators. 
- You may replace multiple operators with custom implementations, consider operator fusion opportunities (combining multiple operators into a single kernel, for example, combining matmul+relu), or algorithmic changes (such as online softmax).
- Your code must be correct and functionally equivalent to the original model.

**Key points to consider:**
- The matrix multiplication (Gemm) is between a tensor of size (batch_size, in_features) and a weight matrix of size (out_features, in_features). 
- The LogSumExp is applied over dimension 1, keeping the dimensions.
- The two LeakyReLU layers have the same negative slope (0.01).
- The two GELU layers are consecutive and may be fused for optimization.

Your plan of action first before writing the code?:

First, I need to analyze which operators in the given model can be optimized with custom CUDA kernels to achieve speedups. Let's look at the sequence of operations:

1. **Linear Layer (Gemm):** This involves a matrix multiplication between the input tensor (batch_size, in_features) and the weight matrix (out_features, in_features), plus an optional bias addition. Since this is a fundamental operation, PyTorch already uses highly optimized CUDA kernels for this (e.g., cuBLAS). However, if we can fuse this with subsequent operations, it might still be beneficial.

2. **LogSumExp:** The LogSumExp operation is computed as log(sum(exp(x), dim)), which involves element-wise exponentiation, summation along a dimension, and a logarithm. This could be a candidate for a custom kernel to avoid separate PyTorch function calls and reduce memory overhead.

3. **Two LeakyReLU Layers:** Both have the same negative slope. Since they are applied consecutively, their effects can be combined. The LeakyReLU activation is defined as max(0, x) + negative_slope * min(0, x). Applying it twice with the same slope is equivalent to a single LeakyReLU, because the second application doesn't change the result once the first one has been applied. Wait, actually, no: applying LeakyReLU twice with the same slope would not change the result further. Let me verify:

Suppose x is negative. First LeakyReLU: 0.01x. Second LeakyReLU: 0.01*(0.01x) = 0.0001x. Wait, that's different! Wait, no. Wait, LeakyReLU is applied element-wise. Wait, no, if you have x = -1, first LeakyReLU gives -0.01. Second LeakyReLU: since -0.01 is negative, it becomes 0.01 * (-0.01) = -0.0001. But the original model has two LeakyReLU layers with the same slope. Wait, that would actually change the result. But that's part of the original model's design. However, in the problem statement, it says the new model must be functionally equivalent. So we can't merge them unless the mathematical equivalence holds. Let me confirm:

Suppose x is negative: first LeakyReLU gives 0.01x. The second LeakyReLU would take that result and apply the same slope again. So if x is negative, after two LeakyReLUs, it becomes 0.01*(0.01x) = 0.0001x. Whereas if we had a single LeakyReLU with slope 0.01, it would be 0.01x. So they are not equivalent. Therefore, the two LeakyReLU layers are not redundant and must be kept as two separate applications. Therefore, we can't simply merge them into one, but perhaps fuse their computation into a single kernel.

4. **Two GELU Layers:** Similarly, applying GELU twice consecutively. The GELU function is a non-linear activation function. Applying it twice would not be equivalent to a single GELU. Therefore, the two GELU layers must be kept. However, perhaps their computation can be fused into a single kernel for efficiency.

Considering which operators can be optimized:

- **Fusing Linear + LogSumExp:** The Linear layer is a large matrix multiplication. The LogSumExp is applied across dimension 1, which after the Linear layer would have shape (batch_size, out_features). Wait no: the Linear layer's output is (batch_size, out_features). Then the LogSumExp is applied over dimension 1 (the features), resulting in (batch_size, 1). Wait, the original code says:

Wait, the code for the model is:

def forward(self, x):
    x = self.linear(x)  # shape (batch, out_features)
    x = torch.logsumexp(x, dim=1, keepdim=True)  # reduces dim 1, resulting in (batch, 1)
    then applies LeakyReLU, etc.

Wait, but after LogSumExp, the tensor has shape (batch_size, 1). Then the next operations (LeakyReLU, GELU, etc.) are applied on that. Therefore, the GELU operations are applied on a vector of shape (batch, 1).

Given that, perhaps the LogSumExp can be fused with the Linear layer's computation. Since the LogSumExp reduces over dimension 1 (the features), perhaps after computing the matrix multiplication, we can immediately compute the sum over features, then the logsumexp. But that would require rethinking the computation.

Alternatively, perhaps the entire sequence of operations after the Linear layer can be fused into a single kernel, as they are all element-wise except for the LogSumExp.

Alternatively, since after the Linear layer, the tensor is (batch, out_features), then LogSumExp reduces to (batch, 1), and the subsequent activations are applied element-wise on that. Therefore, the main compute is in the Linear layer (matrix multiply), and the rest are small operations. However, for large matrices (8192x8192), the matrix multiplication is the most compute-intensive part. However, since PyTorch's Linear already uses cuBLAS, which is highly optimized, it might not be easy to get a speedup there unless we can fuse it with another operation.

Alternatively, perhaps the LogSumExp can be implemented as a custom kernel, but given its small size after reduction, maybe the overhead isn't worth it. Let me think: the LogSumExp involves exponentiating each element, summing them over a dimension, then taking log. For a tensor of size (batch_size, out_features), which is (1024, 8192), the LogSumExp over dim=1 reduces each row to a single value, so the output is (1024,1).

The LogSumExp computation steps:

1. Compute exp(x_i) for each element in dim 1 (so 8192 elements per sample)
2. Sum over all those exp(x_i) along dim 1 (per sample)
3. Take log of the sum, resulting in a scalar per sample (so per batch element).

Therefore, the computation for LogSumExp is O(batch_size * out_features) for the exponentials, O(batch_size * out_features) for the sum reduction (though the sum is O(N) per batch element), and O(batch_size) for the log.

Given that out_features is 8192, this is a substantial computation but not as large as the matrix multiplication. However, if we can combine it with the matrix multiplication's output, perhaps in a fused kernel, it might save some memory bandwidth.

Alternatively, the matrix multiplication is the main bottleneck. Since PyTorch's Linear already uses optimized kernels, maybe not. But perhaps the fusion of Linear with LogSumExp is possible. Let me think: the matrix multiplication produces an output tensor of shape (batch, out_features). Then, to compute LogSumExp over dim 1, we can directly compute the sum of exp of each row, then take log. But this requires keeping all the intermediate values (the matrix multiplication output) only to compute the sum. If we can compute the LogSumExp as part of the matrix multiplication, perhaps we can avoid storing the entire matrix.

Wait, but the matrix multiplication's output is required to compute the LogSumExp, which requires all the elements. Unless there's a way to compute the sum of exp of each row's elements during the matrix multiply, but that seems unlikely since the matrix multiply is a dot product. Alternatively, maybe not. The matrix multiply is (batch_size, in_features) @ (out_features, in_features)^T → resulting in (batch_size, out_features). Then, for each row (each batch element), we compute the log of the sum of exp of all the elements in that row.

Alternatively, the LogSumExp over dim=1 is equivalent to log(sum(exp(matrix))) for each row. So, perhaps we can compute the sum(exp(matrix)) as part of the matrix multiply's computation. But I can't see a straightforward way to do that, since the matrix multiply is a linear operation and the sum of exponentials is non-linear.

Therefore, perhaps the matrix multiplication can't be fused with the LogSumExp. So maybe we can implement a custom kernel for the LogSumExp. Alternatively, perhaps the LeakyReLU and GELU layers can be fused.

Looking at the sequence after the LogSumExp:

After the LogSumExp, the tensor is (batch_size, 1). Then we have two LeakyReLU layers with the same slope, followed by two GELU layers. Since all subsequent operations are element-wise, and the tensor is now only of size (batch,1), their compute is minimal. Therefore, perhaps the main candidates for optimization are the matrix multiplication (if possible) and LogSumExp.

Alternatively, perhaps the LogSumExp and the subsequent activations can be fused into a single kernel. Since after the matrix multiplication, the only operations are LogSumExp followed by four element-wise operations (two LeakyReLU and two GELU). Since the LogSumExp is the only non-element-wise operation (it requires reduction over a dimension), the rest are element-wise. Therefore, perhaps the LogSumExp can be implemented in a custom kernel that directly computes the LogSumExp and then applies the activations, but that would require a custom kernel for that.

Alternatively, perhaps the matrix multiplication is too big to optimize, so the best bet is to implement a custom kernel for LogSumExp, and another for combining the two LeakyReLU and/or the two GELU.

Alternatively, the two GELU layers could be fused into a single kernel, but applying GELU twice is not equivalent to a single GELU. However, since GELU is a non-linear function, applying it twice would have a different effect. Therefore, the two GELU layers must remain separate. But perhaps their computation can be combined into a single kernel, processing each element twice.

Alternatively, since the tensor is now (batch, 1), the element-wise operations (LeakyReLU and GELU) are trivial. For a batch size of 1024, each of these operations would involve 1024 elements. That's minimal computation, so perhaps not worth optimizing.

Therefore, the most promising candidates are the LogSumExp and the matrix multiplication. Since the matrix multiplication is already using optimized kernels, perhaps we can only optimize the LogSumExp. However, to fulfill the requirement of replacing at least two operators, perhaps we can also fuse the two LeakyReLU layers or the two GELU layers.

Wait, but the two LeakyReLU layers are not redundant. Applying LeakyReLU twice with the same slope is different from once. Let me check:

Suppose x is -1. First LeakyReLU gives -0.01. Second LeakyReLU: since that result is negative, it becomes 0.01 * (-0.01) = -0.0001. So the second application reduces the negative value further. Therefore, the two layers are indeed necessary. However, since they both have the same slope, we can combine their computation into a single kernel that applies LeakyReLU twice. That could save some computation, but since the tensor is (batch, 1), the computation is minimal, but perhaps the kernel call overhead is worth it.

Alternatively, perhaps combining all element-wise operations (two LeakyReLU and two GELU) into a single kernel would be beneficial. Since all are element-wise and the tensor is small, a single kernel could handle all four steps, reducing kernel launch overhead.

Therefore, the plan is:

1. Implement a custom kernel for LogSumExp, replacing the existing torch.logsumexp.

2. Implement a fused kernel for the two LeakyReLU and two GELU activations, combining them into a single element-wise kernel.

This would satisfy replacing at least two operators (LogSumExp and the four activations combined), and possibly get some speedup from reducing kernel launches and memory copies.

Alternatively, perhaps the matrix multiply can be fused with the LogSumExp. Let me think again. Suppose we can compute the matrix multiply and immediately compute the LogSumExp over dim=1 in a single kernel. For each row (each batch element), we need to compute the sum of exp of each element in that row, then log. The matrix multiply produces a row vector for each batch element. To compute the LogSumExp, we need the sum of exp of all elements in that row. But the matrix multiply is a dot product, so the elements of the output row are each the dot product of the input vector with the corresponding weight vector. However, the LogSumExp requires the sum of the exponentials of all elements in the row. There's no obvious way to compute this during the matrix multiply, so it's probably not possible to fuse them.

Therefore, proceed with the plan:

Implement a custom LogSumExp kernel and a fused element-wise activation kernel for the two LeakyReLU and two GELU.

Now, to implement the LogSumExp kernel.

The LogSumExp function for a vector x is log(sum(exp(x_i))).

To compute this for each row in a matrix (each batch element), we can do:

For each row in the matrix:

1. Compute exp(x_i) for each element in the row.

2. Sum all these exp(x_i) to get S.

3. Compute log(S).

This requires:

- A parallel kernel that can compute the exponentials in parallel, then perform a reduction to sum them, then compute the log.

However, the reduction is a bit tricky because it's per row. We can process each row independently.

Let me outline the steps for the LogSumExp kernel:

The input is a tensor of shape (batch_size, out_features). We need to output a tensor of shape (batch_size, 1).

The algorithm for each row:

1. Compute all exp(x_i) for the row's elements.

2. Sum them to get S.

3. Compute log(S).

To implement this in CUDA:

- Each block can handle a single row (since batch_size is 1024, which is manageable with 1024 blocks).

- Within a block, each thread can process a chunk of the elements.

Wait, but for a row with 8192 elements, a block of 256 threads would need to process 32 elements per thread (8192 / 256 = 32). The steps could be:

For each row (handled by a block):

- Each thread computes exp of their assigned elements, stores in shared memory.

- Then perform a parallel reduction within the block to sum the exponentials.

- The result is written to the output.

Alternatively, we can use CUDA's atomic operations for the summation, but that might be slower. A block-wise parallel reduction is better.

The kernel steps for LogSumExp:

Kernel function:

__global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int dim_size) {

    extern __shared__ float shared_mem[];

    // Each block processes one row (one batch element)
    int batch_id = blockIdx.x;

    // Each thread in the block processes a portion of the elements in the row
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    float sum = 0.0f;

    // Load data into shared memory or process directly
    // Iterate over the elements assigned to this thread
    for (int i = tid; i < dim_size; i += num_threads) {
        float val = input[batch_id * dim_size + i];
        sum += expf(val);
    }

    // Perform reduction in shared memory
    // Use a parallel reduction within the block
    // Implementation of reduction step

    // After reduction, the sum is in thread 0
    if (tid == 0) {
        float s = logf(sum);
        output[batch_id] = s;
    }
}

But to do the reduction efficiently, we need to implement a block reduction. Let's see.

Alternatively, using shared memory for the reduction:

Each block has a shared memory array to store partial sums from each thread.

Each thread first computes its partial sum (sum of exps for their assigned elements) and stores it in shared memory.

Then, perform a parallel reduction within the shared memory array.

The reduction can be done using a binary reduction approach.

The size of shared memory would need to be at least the block size (number of threads per block). Let's say the block size is 256. Then, the shared memory array size is 256 floats.

The steps:

- Each thread computes their partial sum (sum of exps for their chunk of elements).

- Write to shared memory.

- Sync threads.

- Then, perform a reduction in shared memory, halving the array each step.

- Finally, thread 0 writes the log(sum) to the output.

This approach requires shared memory and careful synchronization.

Alternatively, if the dimension size (dim_size = 8192) is a power of two, the reduction can be efficient. Since 8192 is 2^13, which is a power of two, the reduction steps can proceed smoothly.

Alternatively, the code for the kernel would need to handle that.

Implementing the LogSumExp kernel requires careful handling of the reduction.

Alternatively, perhaps use thrust's reduce function, but that might complicate things.

Alternatively, let's proceed with the kernel outline.

Now, for the element-wise activations: combining the two LeakyReLU and two GELU layers.

The two LeakyReLU layers have the same slope (0.01). The two GELU layers are applied consecutively.

Since all operations are element-wise, we can process each element through all four steps in a single kernel.

The computation steps for each element:

Given an input x (after LogSumExp):

1. Apply LeakyReLU with slope 0.01: out1 = max(x, 0.01*x)

2. Apply LeakyReLU again with slope 0.01: out2 = max(out1, 0.01*out1)

Wait, no: LeakyReLU is defined as:

LeakyReLU(x) = max(0, x) + negative_slope * min(0, x)

So, for negative x, it's slope*x. So applying LeakyReLU twice with slope 0.01:

Suppose x is -1:

First LeakyReLU: 0.01*(-1) = -0.01

Second LeakyReLU: 0.01*(-0.01) = -0.0001

So the result is slope squared times the original x if negative. Whereas if we had a single LeakyReLU with slope 0.0001, it would be equivalent. But since the original model applies two LeakyReLUs, we have to keep both.

Similarly, GELU is a non-linear function, so applying it twice is different from once. So the two GELU layers must be kept.

Therefore, the element-wise computation for each element is:

input_x → LeakyReLU1 → LeakyReLU2 → GELU1 → GELU2 → output

So, the fused kernel would process each element through all these steps in sequence.

The code for the kernel would be straightforward:

For each element x:

x = leaky_relu(leaky_relu(gelu(gelu(x))))

Wait no:

Wait, the order is:

After LogSumExp, the value is passed through LeakyReLU twice, then GELU twice. So:

First LeakyReLU: x1 = leaky_relu(x)

Second LeakyReLU: x2 = leaky_relu(x1)

First GELU: x3 = gelu(x2)

Second GELU: x4 = gelu(x3)

So the fused kernel would process each element through these four steps.

Implementing this in CUDA would involve:

__global__ void fused_activations_kernel(const float* input, float* output, int size) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < size) {

        float x = input[idx];

        // First LeakyReLU
        x = (x > 0) ? x : 0.01 * x;

        // Second LeakyReLU (same slope)
        x = (x > 0) ? x : 0.01 * x;

        // First GELU
        x = gelu(x); // Need to implement GELU function here

        // Second GELU
        x = gelu(x);

        output[idx] = x;
    }
}

However, implementing the GELU function in CUDA requires a mathematical expression. The GELU function can be approximated as 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))). Or the exact form can be used, but the approximation is commonly used for computational efficiency.

Alternatively, use the exact definition using the error function (erf), but that might be more computationally intensive.

The GELU implementation in PyTorch's torch.nn.functional.gelu uses the approximation for fast computation. Let me recall the formula:

GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x^3)))

So, in code, this can be written as:

float gelu(float x) {
    return 0.5f * x * (1.0f + tanhf(sqrt(2.0f / M_PI) * (x + 0.044715f * x * x * x)));
}

We need to include the necessary math functions (like tanh, sqrt, etc.) in the kernel.

Therefore, the fused kernel would need to implement this GELU function.

Now, considering the above, let's outline the steps to write the code:

1. Implement the LogSumExp kernel.

2. Implement the fused activations kernel.

3. Integrate these kernels into the ModelNew class.

First, for the LogSumExp kernel.

The kernel will take the output of the linear layer (a tensor of shape (batch_size, out_features)), and return a tensor of shape (batch_size, 1).

The input to the kernel is a 2D tensor. The kernel must process each row independently, compute the LogSumExp for that row, and write the result to the output tensor (which is 2D with shape (batch_size, 1)).

The CUDA kernel for LogSumExp:

We can structure it so that each block handles one row (batch element). Each thread in the block processes a portion of the elements in the row. The number of threads per block should be chosen so that the block can efficiently process the elements. Since the dimension size is 8192, which is 2^13, a block size of 256 threads can handle 32 elements per thread (8192 / 256 = 32).

The kernel function:

__global__ void logsumexp_kernel(const float* input, float* output, int batch_size, int dim_size) {
    // Each block handles one batch element
    int batch_id = blockIdx.x;
    if (batch_id >= batch_size) return;

    extern __shared__ float shared[];

    // Each thread in the block processes a chunk of the dim_size elements
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    float sum = 0.0f;

    // Compute partial sum for this thread's elements
    for (int i = tid; i < dim_size; i += num_threads) {
        float val = input[batch_id * dim_size + i];
        sum += expf(val);
    }

    // Perform reduction in shared memory
    // Write partial sum to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Now reduce the shared memory array
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    // The final sum is in shared[0]
    if (tid == 0) {
        output[batch_id] = logf(shared[0]);
    }
}

The shared memory size needed is blockDim.x * sizeof(float). Since we're using blockDim.x threads, we need that much space.

The kernel launch would need to:

- Set the block size (e.g., 256 threads per block).

- The grid size is batch_size (each block handles a batch element).

- The shared memory size is blockDim.x * sizeof(float).

Wait, the shared memory size is specified as an argument in the kernel launch. For example, when launching the kernel:

dim3 blocks(batch_size);
dim3 threads(block_size);

logsumexp_kernel<<<blocks, threads, block_size * sizeof(float)>>>(input, output, batch_size, dim_size);

This should work.

Now, the function wrapper for the kernel:

torch::Tensor logsumexp_cuda(torch::Tensor input) {
    auto batch_size = input.size(0);
    auto dim_size = input.size(1);
    auto output = torch::empty({batch_size, 1}, input.options());

    const int block_size = 256;
    const int shared_size = block_size * sizeof(float);

    logsumexp_kernel<<<batch_size, block_size, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        dim_size
    );

    return output;
}

Wait, but the input is a 2D tensor (batch, dim_size), and output is (batch, 1).

Now, the fused activations kernel.

Implementing the GELU function in the kernel:

First, include the math functions. Since CUDA uses the C math library, we can use tanhf, sqrtf, etc.

The GELU function implementation inside the kernel:

float gelu(float x) {
    const float sqrt_2_over_pi = sqrt(2.0f / M_PI);
    const float inner = x + 0.044715f * x * x * x;
    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * inner));
}

Wait, but in CUDA, M_PI is defined in math.h, so we need to include that.

However, in CUDA device code, some math functions are available via <math.h>, but for device functions, we need to use the appropriate functions.

Wait, in CUDA, the math functions like sqrt, tanh, etc., are available in device code, but we need to include <math.h>.

Therefore, in the CUDA code, we can write:

#include <math.h>

Then, the gelu function can be written as a __device__ function.

Wait, but in the kernel, since the computation is done per element, the gelu function can be inlined.

Alternatively, define a __device__ function.

So, in the fused kernel:

__device__ float gelu(float x) {
    const float sqrt_2_over_pi = sqrtf(2.0f / M_PI);
    float inner = x + 0.044715f * x * x * x;
    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * inner));
}

Then, the kernel:

__global__ void fused_activations_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];

        // First LeakyReLU
        x = fmaxf(x, 0.01f * x);

        // Second LeakyReLU
        x = fmaxf(x, 0.01f * x);

        // First GELU
        x = gelu(x);

        // Second GELU
        x = gelu(x);

        output[idx] = x;
    }
}

Wait, but fmaxf is the function to compute max. The LeakyReLU can be written as:

if (x > 0) x else 0.01*x. However, using fmaxf might not capture that exactly. Let me think:

The LeakyReLU formula is:

out = max(0, x) + negative_slope * min(0, x)

Which can be rewritten as:

out = x if x > 0 else negative_slope * x

Therefore, for negative x, it's slope*x, which is less than x (since slope <1). So:

The expression fmaxf(x, 0.01*x) would be correct?

Let me see:

Suppose x is positive:

0.01*x < x → fmaxf(x, 0.01*x) = x → correct.

If x is negative:

0.01*x > x (since x is negative, multiplying by 0.01 makes it closer to zero). So fmaxf(x, 0.01*x) = 0.01*x → correct.

Therefore yes, fmaxf(x, slope * x) gives the correct LeakyReLU.

Therefore, using fmaxf is the right approach.

Therefore, the fused kernel is okay.

Now, the wrapper for the fused activations:

torch::Tensor fused_activations_cuda(torch::Tensor input) {
    auto size = input.numel();
    auto output = torch::empty_like(input);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    fused_activations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );

    return output;
}

Putting this all together.

Now, the model structure:

In ModelNew, we will replace:

- The torch.logsumexp with the custom kernel.

- The two LeakyReLU and two GELU with the fused kernel.

So the forward method would be:

def forward(self, x):
    x = self.linear(x)  # Gemm (linear layer)
    x = self.logsumexp(x)  # replaced with custom kernel
    x = self.fused_activations(x)  # replaced with custom kernel
    return x

Therefore, the new model's forward would call the two custom kernels.

Now, in code:

First, define the custom kernels in the Python script.

The LogSumExp kernel:

logsumexp_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename T>
__global__ void logsumexp_kernel(const T* input, T* output, int batch_size, int dim_size) {
    int batch_id = blockIdx.x;
    if (batch_id >= batch_size) return;

    extern __shared__ T shared[];

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    T sum = 0.0;

    // Compute partial sum for this thread's elements
    for (int i = tid; i < dim_size; i += num_threads) {
        T val = input[batch_id * dim_size + i];
        sum += exp(val);
    }

    // Write partial sum to shared memory
    shared[tid] = sum;
    __syncthreads();

    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[batch_id] = log(shared[0]);
    }
}

at::Tensor logsumexp_cuda(at::Tensor input) {
    const int batch_size = input.size(0);
    const int dim_size = input.size(1);

    at::Tensor output = at::empty({batch_size, 1}, input.options());

    const int block_size = 256;
    const size_t shared_size = block_size * sizeof(float);

    dim3 blocks(batch_size);
    dim3 threads(block_size);

    AT_DISPATCH_FLOATING_TYPES(input.type(), "logsumexp_cuda", ([&] {
        logsumexp_kernel<scalar_t><<<blocks, threads, shared_size>>>(
            input.data<scalar_t>(),
            output.data<scalar_t>(),
            batch_size,
            dim_size
        );
    }));

    return output;
}
"""

Wait, but the above code uses templates for T, but in PyTorch, the tensors can be float or double, but the original model uses float (since get_inputs uses torch.rand, which is float by default). So we can hardcode for float if needed, but better to use AT_DISPATCH_FLOATING_TYPES for generality.

The fused activations kernel:

fused_activations_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

__device__ float gelu(float x) {
    const float sqrt_2_over_pi = sqrt(2.0f / M_PI);
    float inner = x + 0.044715f * x * x * x;
    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * inner));
}

__global__ void fused_activations_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];

        // First LeakyReLU (slope 0.01)
        x = fmaxf(x, 0.01f * x);

        // Second LeakyReLU (slope 0.01)
        x = fmaxf(x, 0.01f * x);

        // First GELU
        x = gelu(x);

        // Second GELU
        x = gelu(x);

        output[idx] = x;
    }
}

torch::Tensor fused_activations_cuda(torch::Tensor input) {
    int size = input.numel();
    auto output = torch::empty_like(input);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    fused_activations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );

    return output;
}
"""

Wait, but the fused kernel uses float types. If the input is double, this would fail. To make it general, we can use templates as well, but for simplicity, since the original model uses float, we can proceed with float.

Now, in the Python code, we need to compile these kernels.

However, the logsumexp kernel uses a template with AT_DISPATCH_FLOATING_TYPES, so the C++ code must include the appropriate headers.

The full code would look like this:

First, compile the two kernels.

First, the LogSumExp kernel:

logsumexp_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

template <typename T>
__global__ void logsumexp_kernel(const T* input, T* output, int batch_size, int dim_size) {
    int batch_id = blockIdx.x;
    if (batch_id >= batch_size) return;

    extern __shared__ T shared[];

    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    T sum = static_cast<T>(0.0);

    for (int i = tid; i < dim_size; i += num_threads) {
        T val = input[batch_id * dim_size + i];
        sum += exp(static_cast<double>(val)); // cast to double to avoid overflow?
    }

    shared[tid] = sum;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[batch_id] = log(static_cast<double>(shared[0]));
    }
}

at::Tensor logsumexp_cuda(at::Tensor input) {
    const int batch_size = input.size(0);
    const int dim_size = input.size(1);

    at::Tensor output = at::empty({batch_size, 1}, input.options());

    const int block_size = 256;
    const size_t shared_size = block_size * sizeof(float);

    dim3 blocks(batch_size);
    dim3 threads(block_size);

    AT_DISPATCH_FLOATING_TYPES(input.type(), "logsumexp_cuda", ([&] {
        logsumexp_kernel<scalar_t><<<blocks, threads, shared_size>>>(
            input.data<scalar_t>(),
            output.data<scalar_t>(),
            batch_size,
            dim_size
        );
    }));

    return output;
}
"""

Wait, but when using T, exp and log might require casting to double to prevent overflow. For example, if the elements are large, exp(x)