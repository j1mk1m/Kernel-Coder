The code must import the required packages and define all necessary functions. The code must follow the same interface as the original Model. The get_inputs() and get_init_inputs() functions must also be redefined in the new code. The code must not have any typos. The code must be complete and self-contained. 

The code must use torch.utils.cpp_extension.load_inline for any custom CUDA kernels. The ModelNew class must have an attribute with the same name as the original operators that are replaced. For example, in the example above, the original model has an add operator, and the new model has an attribute called elementwise_add, which is used to call the custom CUDA kernel. 

You may choose which operators to replace. For example, replacing the linear layers with fused gemm-sigmoid, or the logsumexp, etc. You may replace multiple operators. Operator fusion opportunities are encouraged. 

You are free to split the problem into any number of CUDA kernels, but you must include all the code here. You may choose to replace some operators and not others. The code must run correctly with the get_inputs() and get_init_inputs() functions provided. 

Make sure that all the CUDA kernels are correctly defined and compiled via load_inline. 

The code should be written in Python, using the same framework as the original model (PyTorch). 

The code should be as efficient as possible. The solution must be a self-contained Python script that can be run on a system with PyTorch installed and a CUDA-enabled GPU.

Okay, so I need to optimize this PyTorch model using custom CUDA kernels. The original model has two linear layers with a sigmoid activation in between and a logsumexp at the end. The goal is to replace some of these operations with custom CUDA code for better performance. Let's think about which parts can be optimized.

First, the linear layers involve matrix multiplications (GEMMs). The first linear layer is followed by a sigmoid activation. Maybe I can fuse the GEMM and the sigmoid into a single kernel to reduce memory copies and latency. The second linear layer is followed by the logsumexp operation. However, logsumexp is a reduction, so maybe that can also be fused into another kernel.

Alternatively, maybe the two linear layers can be fused into a single computation? Not sure. Let's see:

Original steps:
1. Linear1 (x @ W1 + b1)
2. Sigmoid
3. Linear2 (result @ W2 + b2)
4. LogSumExp over dim 1

Fusing linear1 + sigmoid into a single kernel might save some time because we can compute the matrix multiplication and apply the sigmoid in the same kernel, avoiding an intermediate copy. Similarly, after the second linear layer, maybe the logsumexp can be combined with the second GEMM? Hmm, but the logsumexp is a reduction over the features (dim=1), so that might complicate things. Alternatively, maybe the second linear's GEMM and the logsumexp can be combined into a single kernel as well?

Alternatively, maybe just focus on fusing linear1 + sigmoid first. Let me think about the steps involved in each operation:

The first linear layer (x @ W1 + b1) is a matrix multiplication followed by adding a bias. The sigmoid is an element-wise operation. So if I can do the matrix multiplication, add the bias, then apply sigmoid in the same kernel, that would save some steps. Similarly, the second linear layer is another GEMM plus bias, but then we have logsumexp. The logsumexp is a bit more involved because it's a reduction.

Alternatively, maybe the second linear layer and the logsumexp can be fused. Let's see:

The second linear is x = linear2(x) which is (x @ W2 + b2). Then, logsumexp over dim=1 would compute for each sample (row) the log(sum(exp(x))). So perhaps the second GEMM can be combined with the logsumexp in a way that avoids storing the full output of the second layer, but directly computes the logsumexp. That might be tricky, but could save memory and computation time.

Alternatively, maybe just focus on fusing the first linear and sigmoid, and leave the second linear and logsumexp as separate. Let's start with that.

First, I'll need to write a custom CUDA kernel for the fused linear1 + sigmoid. The linear1 is a dense matrix multiplication, so the kernel needs to handle the GEMM part, then add the bias, then apply the sigmoid.

Wait, the linear layer in PyTorch's nn.Linear is x @ W.T + b. So for the first linear layer, if the input is (batch_size, input_size), then the weight is (hidden_size, input_size), and the bias is (hidden_size,). So the output is (batch_size, hidden_size).

Similarly, the second linear layer is (hidden_size, output_size) weights and output_size bias.

So for the fused linear1 + sigmoid kernel:

Input: x (batch_size, input_size)
Weights: linear1.weight (hidden_size, input_size)
Bias: linear1.bias (hidden_size,)

Output: (batch_size, hidden_size) after applying sigmoid.

The plan is to write a CUDA kernel that performs the GEMM (matrix multiplication) of x and W1, adds the bias, applies the sigmoid, and stores the result.

But how to structure this kernel?

Alternatively, since matrix multiplication is a standard operation, perhaps using a tiled approach or leveraging existing CUDA libraries, but since we need to write inline kernels, maybe a simple approach is better.

Alternatively, using a kernel for GEMM with bias and activation. Let me think.

Alternatively, let's structure the kernel as follows:

The kernel will process elements in a way that each thread computes one element of the output. For the GEMM part, each element of the output matrix is the sum of x_row * W1_col. To compute this efficiently, we can have each thread compute a partial sum.

Wait, maybe the standard way is to use a tiled GEMM approach, but that might be complicated for a simple kernel. Alternatively, given that the input size is 2048 and hidden size is 4096, which are both large, but perhaps for a simple kernel, we can proceed with a straightforward approach.

Alternatively, let's think of the matrix multiplication as follows:

For each output element (i,j), it is the sum over k of x[i][k] * W1[j][k], then add the bias[j], then apply sigmoid.

Each thread can handle one (i,j) element, but that would require a lot of threads. Alternatively, use a block for each row or column.

Alternatively, perhaps a better approach is to split the computation into rows for the output matrix. For each row (i), each thread in a block can compute a column (j) element. So for the first layer:

The output matrix has dimensions (batch_size, hidden_size). Let's assume batch_size is 16384 and hidden_size 4096. So total elements are 16384 * 4096 = ~67 million. That's a lot. But CUDA can handle that.

Alternatively, we can structure the kernel so that each thread processes a single element (i,j). The number of threads would be batch_size * hidden_size, which is 16384 * 4096 = 67,108,864 threads. That's 67 million threads, which may be too much. So perhaps a better approach is to use a grid of blocks, each block handles a chunk of rows.

Alternatively, maybe a more optimized approach is needed, but for the purposes of this exercise, perhaps a straightforward implementation is better even if not the most efficient.

Alternatively, perhaps using the cuBLAS library for the matrix multiplication, but then combining with the bias and activation in a custom kernel. But since the problem requires using custom CUDA kernels, perhaps we need to implement the GEMM ourselves.

Alternatively, perhaps use a kernel where each thread computes one row of the output matrix. Let me think:

Each row of the output matrix is of length hidden_size. For each row i (sample), each element j in 0..hidden_size-1 is the dot product of x[i] with the j-th row of W1, plus bias[j], then sigmoid.

Wait, actually, the weight matrix for linear1 is stored as (input_size, hidden_size) or (hidden_size, input_size)? Wait, in PyTorch, nn.Linear(input_size, hidden_size) has weight of shape (hidden_size, input_size). So the matrix multiplication is x @ W1, which would be (batch_size, input_size) @ (hidden_size, input_size)^T → (batch_size, hidden_size). Wait, the matrix multiplication between a batch_size x input_size tensor and the weight matrix (hidden_size, input_size) would be:

The correct way is x @ W1.t() + b1.

So the matrix multiplication is of size (batch_size, input_size) multiplied by (input_size, hidden_size) → (batch_size, hidden_size).

So the kernel needs to compute for each element (i,j) in the output matrix: sum_{k=0 to input_size-1} x[i][k] * W1[j][k], then add bias[j], then apply sigmoid.

Wait, no. Wait, the weight matrix is (hidden_size, input_size), so when you transpose it, it's (input_size, hidden_size). So the multiplication x @ W1.t() gives (batch_size, hidden_size).

So each element (i,j) in the output is sum_{k=0 to input_size-1} x[i][k] * W1[j][k], because W1[j][k] is the j-th row (for hidden_size) and k-th column (input_size).

Hmm. So for the kernel, perhaps each thread can compute a single element (i,j). But for large matrices, this might not be efficient.

Alternatively, each block can handle a tile of the output matrix, and threads compute elements within the tile. But this requires more complex indexing.

Alternatively, let's proceed with a simple kernel that for each row i in the batch, and each column j in the hidden_size, compute the sum, then add bias, then apply sigmoid. But how to parallelize this.

Alternatively, let's have each thread handle a single row i, and within that row, compute all the j elements. But with 16384 rows, we can have 16384 threads, each handling a row. But each thread would need to process hidden_size elements (4096) for that row. So for each thread, it loops over j from 0 to hidden_size-1, and for each j, compute the sum over k of x[i][k] * W1[j][k], add bias[j], apply sigmoid, and store in the output.

But this requires each thread to process a lot of elements (4096 per thread). The loop over k (input_size=2048) would be 2048 iterations per j. That's a lot, but perhaps manageable in CUDA.

Alternatively, since input_size is 2048, which is a multiple of 32, maybe unrolling loops, but perhaps for simplicity, proceed with a straightforward approach.

Alternatively, this might be too slow, but since the problem requires writing a kernel, perhaps proceed.

Wait, but the problem allows us to choose which operators to replace. Maybe the linear layers can be replaced with fused GEMM + activation, but doing a fully custom GEMM might be too time-consuming. Perhaps there's a better way.

Alternatively, maybe use the existing cuBLAS GEMM for the matrix multiplication, then apply the bias and activation in a separate kernel. But the user wants custom CUDA kernels, so combining all steps in one kernel.

Alternatively, perhaps the first linear + sigmoid can be replaced with a custom kernel, and the second linear can be replaced with another kernel, perhaps also fused with the logsumexp. Let's think.

The second linear layer's output is (batch_size, output_size), and then logsumexp over dim 1 (i.e., for each sample, compute log(sum(exp(x_i))) over the output features. So the final output is (batch_size,).

Wait, logsumexp(x, dim=1) reduces the dimension, so the output is (batch_size,). So the second linear's output is (batch_size, output_size), and then we sum over output_size.

So for the second part, after the second linear, we can have a kernel that computes the logsumexp. But maybe we can combine the second linear's computation with the logsumexp, to avoid storing the intermediate matrix.

The logsumexp can be expressed as:

log( sum_{j} exp( x_j ) )

where x_j is the j-th element of the linear2 output for a sample.

So the linear2's output is (batch_size, output_size). The logsumexp for each sample is a scalar, so the final output is (batch_size,).

Therefore, for each sample, the computation is:

y = W2 * x_prev + b2 (where x_prev is the output of the first layer after sigmoid)

then compute log( sum(exp(y_i)) for i in 0..output_size-1 )

But to combine the linear2 and logsumexp into a single kernel:

Instead of computing the full y matrix (batch_size x output_size), compute directly for each sample the sum(exp(y_i)) and then take the log.

Wait, but the linear2's computation is y_i = sum_{k} x_prev[k] * W2[i][k] + b2[i]

Then, logsumexp over i is log( sum_{i} exp(y_i) )

Therefore, the total computation can be done by, for each sample:

sum_exp = 0

for each i in 0..output_size-1:

   y_i = sum_{k} x_prev[k] * W2[i][k] + b2[i]

   sum_exp += exp(y_i)

log_sum_exp = log( sum_exp )

Therefore, the second part can be optimized by combining the computation of y_i and the sum_exp, thus avoiding storing the entire y matrix. This could save memory and computation time, especially since the final result doesn't need the intermediate y matrix.

Therefore, fusing the second linear layer with the logsumexp would be beneficial.

So the plan is:

1. Replace the first linear layer and sigmoid with a fused kernel.

2. Replace the second linear layer and logsumexp with a fused kernel.

This would eliminate the intermediate tensors between the two fused operations, reducing memory usage and data movement.

Alright, let's tackle each fused kernel.

First, the first fused kernel: linear1 + sigmoid.

The inputs are x (batch_size, input_size), W1 (hidden_size, input_size), b1 (hidden_size).

Output is (batch_size, hidden_size), each element is sigmoid( x * W1^T + b1 )

Wait, the weights are stored as (hidden_size, input_size). So the matrix multiplication is:

x (batch_size x input_size) multiplied by W1's transpose (input_size x hidden_size) → batch_size x hidden_size.

Then add bias (hidden_size) → batch_size x hidden_size.

Then apply sigmoid.

So the kernel needs to compute for each element (i,j) of the output:

output[i][j] = sigmoid( sum_{k=0 to input_size-1} x[i][k] * W1[j][k] + b1[j] )

So how to structure this in CUDA:

Approach 1: Each thread handles one output element (i,j). The grid has batch_size * hidden_size threads. Each thread computes its (i,j) position.

But with batch_size=16384 and hidden_size=4096, that's 67,108,864 threads. That might be too many, but CUDA can handle it if we structure blocks properly. Let's see:

CUDA blocks can have up to 1024 threads per block, but the maximum grid size is limited (like 65535 per dimension). So perhaps this approach may not be feasible due to grid size limitations.

Alternatively, structure the computation in a way that each block handles a row (i) of the output, and each thread in the block handles a column (j).

For example:

- Each block corresponds to a sample (i), with blockDim.x = 256 or 512 threads. Since hidden_size is 4096, each block would need at least 4096 threads, but that's too much. Hmm.

Alternatively, each block handles a block of rows, but this is getting complicated.

Alternatively, use a tiled approach where each thread computes a small tile of the output matrix, but that's getting into more complex GEMM implementation.

Hmm, perhaps a better approach is to use a kernel where each thread is responsible for a row (i) and a column (j). Let's see:

Total threads: batch_size * hidden_size. Let's say batch_size is 16384, hidden_size 4096. Total threads would be 16384 * 4096 = ~68 million. Which is way over the maximum threads per grid (which is usually 2^31-1, but the maximum grid dimensions are 65535 in each dimension). So if we have a grid with blockDim.x of 256, the number of blocks would be (68 million / 256) ~ 266,000 blocks, which may exceed the grid dimension limits.

Alternatively, we need to find a better way to structure this.

Alternatively, process each row of the output (i) in a block. For each row (i), the output has hidden_size elements. So for a row, each thread can compute a subset of the hidden_size elements.

Suppose we have a block with 256 threads. Each thread can handle 4096 / 256 = 16 elements per thread. So per row, each block would have 256 threads, each doing 16 elements.

But how to compute each element's value?

Each element in row i, column j is the sum over k of x[i][k] * W1[j][k] + b1[j], then sigmoid.

So for a given row i and column j, the calculation is:

sum = 0.0

for k in 0..input_size-1:

    sum += x[i][k] * W1[j][k]

sum += b1[j]

sigmoid_val = 1 / (1 + exp(-sum))

So for each (i,j), we need to compute this sum over input_size elements (2048 terms). That's a lot of loops.

Hmm, perhaps this is computationally heavy. Maybe the kernel's performance won't be great, but perhaps it's manageable.

Alternatively, we can have each thread process a single row, and within that row, loop over columns. Wait, but how many threads?

Suppose each thread handles a row. So the number of threads would be batch_size (16384). Each thread would need to compute hidden_size (4096) elements. Each element requires a loop over 2048 terms.

So per thread, the work is:

for j in 0..hidden_size-1:

    sum = 0.0

    for k in 0..input_size-1:

        sum += x[i][k] * W1[j][k]

    sum += b1[j]

    out[i][j] = 1.0 / (1.0 + exp(-sum))

But the input_size is 2048, so each j requires 2048 multiplies and adds. For hidden_size=4096, each thread does 4096 * 2048 operations. That's 8 million operations per thread, and with 16k threads, that's a lot. This might be too slow.

Hmm, perhaps this approach is not feasible. Maybe we need to find a better way.

Alternatively, perhaps using shared memory to cache parts of the W1 matrix. Since W1 is (hidden_size, input_size), which is 4096x2048 elements. That's 8,388,608 elements, which is way too big to fit in shared memory. So that's not possible.

Hmm, maybe this approach is not the best. Perhaps the first fused layer isn't the best candidate for a custom kernel because the GEMM is too large. Maybe the second fused kernel (linear2 and logsumexp) is better.

Alternatively, perhaps the first linear layer can be replaced with a standard PyTorch GEMM, and focus on fusing the second linear with logsumexp.

Let me think about the second part: linear2 and logsumexp.

The second linear layer's output is (batch_size, output_size). The logsumexp is over dim=1, so the output is (batch_size,). The fused kernel can compute this directly, avoiding storing the intermediate matrix.

So for each sample (row) in the input (after sigmoid), we need to compute:

result[i] = log( sum_{j=0 to output_size-1} exp( y_j ) ), where y_j = (input[i] * W2[j] ) + b2[j]

Wait, the input to the second linear is (batch_size, hidden_size). The W2 is (output_size, hidden_size). The bias is (output_size,).

So y_j for sample i is the dot product of input[i] and the j-th row of W2 (since W2 is (output_size, hidden_size)), plus the j-th bias.

Therefore, the computation for each sample is:

sum_exp = 0.0

for j in 0..output_size-1:

    y_j = dot_product( input[i], W2[j] ) + b2[j]

    sum_exp += exp(y_j)

result[i] = log( sum_exp )

So the kernel needs to compute this for each sample (i), and can do so in parallel.

Each sample can be handled by a separate thread. The number of samples is 16384, so we can have a grid of 16384 threads, each processing one sample.

Each thread would loop over j from 0 to output_size-1 (1024 times). For each j, compute the dot product between the input vector (hidden_size elements) and the j-th row of W2 (hidden_size elements), add the bias, exponentiate, and accumulate.

The dot product for each j is over 4096 elements. So per thread, for each j:

sum = 0.0

for k in 0..hidden_size-1:

    sum += input[i][k] * W2[j][k]

sum += b2[j]

Then exp(sum), and add to sum_exp.

So per thread, for each of the 1024 j's, a loop over 4096 elements. That's 1024 * 4096 = ~4 million operations per thread, and 16384 threads. This is a lot of computation, but perhaps manageable with CUDA's parallelism.

Alternatively, maybe we can optimize this.

Wait, but perhaps the kernel can be written as follows:

For each sample i:

sum_exp = 0.0

for j in 0..output_size-1:

    temp = 0.0

    for k in 0..hidden_size-1:

        temp += input[i][k] * W2[j][k]

    temp += b2[j]

    sum_exp += exp(temp)

result[i] = log(sum_exp)

But this is O(batch_size * output_size * hidden_size), which with the given dimensions is 16384 * 1024 * 4096 = 68,719,476,736 operations. That's a lot, but maybe it's feasible on a GPU with enough parallelism.

Alternatively, perhaps this is too much, but let's see.

Alternatively, maybe we can transpose W2 to (hidden_size, output_size) and precompute the dot products for all j in parallel for each element. Not sure.

Alternatively, maybe the W2 is stored as (output_size, hidden_size), so each row is a vector of hidden_size elements. For each sample, the input is a vector of hidden_size elements. The dot product between input and W2[j] is the same as the dot product between W2[j] and input.

Hmm. To compute this efficiently, perhaps using shared memory to cache the input or parts of W2 for the threads. But given the sizes, perhaps not.

Alternatively, the problem requires that we write the kernel, so let's proceed.

Now, for the first fused kernel (linear1 + sigmoid), perhaps it's better to use the PyTorch linear layer and sigmoid, and focus on fusing the second part.

Alternatively, maybe the first layer can be fused with the sigmoid using a kernel that uses matrix multiplication with a tiled approach. Let's think again.

Alternatively, let's look for existing examples. The user provided an example where a simple element-wise add is replaced with a custom kernel. So for the first layer, perhaps using a GEMM with bias and activation.

The problem is that writing an efficient GEMM kernel is non-trivial. However, since the problem allows any approach, perhaps proceed with a simple kernel even if it's not the most optimized, just to fulfill the requirements.

Alternatively, use torch's mm or matmul for the first part, but fuse the sigmoid.

Wait, but the user wants to replace operators with custom CUDA kernels, so perhaps the first layer can be written as a custom kernel.

Alternatively, perhaps split the first layer into a GEMM, then bias addition, then sigmoid, each as separate kernels. But that's more work, and may not be better than using PyTorch's own optimized implementations.

Hmm, perhaps the best approach is to focus on the second fused kernel (linear2 + logsumexp) since that's a reduction and can save memory by not storing the intermediate matrix. Let's proceed with that.

So the plan is:

- Keep the first linear layer and sigmoid as PyTorch's own operations (since replacing them may not give significant benefit, or too complex).

- Replace the second linear layer and logsumexp with a fused kernel.

Alternatively, perhaps also fuse the second linear and logsumexp.

Let's proceed.

So the new ModelNew will have:

- linear1 as a standard nn.Linear (since we're not replacing it).

- Then sigmoid as normal.

- Then a custom fused kernel for linear2 and logsumexp.

Wait, but the logsumexp is after the second linear, so the second linear's output is not needed beyond computing the logsumexp. Therefore, combining them can save memory.

So the kernel will take the input (after first layer and sigmoid), apply the second linear's computation, then compute the logsumexp for each sample.

Thus, the output is a tensor of size (batch_size,).

The kernel will process each sample (i) in parallel. For each sample, loop over output_size elements to compute each y_j, accumulate the exponentials, then take the log.

So let's write the CUDA code for this kernel.

First, the kernel function:

__global__ void fused_linear_logsumexp(
    const float* input,  // (batch_size, hidden_size)
    const float* weight, // (output_size, hidden_size)
    const float* bias,   // (output_size,)
    float* output,       // (batch_size,)
    int batch_size,
    int hidden_size,
    int output_size
) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= batch_size) return;

    float sum_exp = 0.0f;
    for (int j = 0; j < output_size; j++) {
        // Compute y_j = input[i] * weight[j] + bias[j]
        float y = 0.0f;
        for (int k = 0; k < hidden_size; k++) {
            y += input[i * hidden_size + k] * weight[j * hidden_size + k];
        }
        y += bias[j];
        sum_exp += exp(y);
    }
    output[i] = logf(sum_exp);
}

Wait, but the input is stored as (batch_size, hidden_size), so the input pointer is a contiguous array. So for sample i, the starting index is i * hidden_size.

Similarly, weight is (output_size, hidden_size), so for row j, it's j * hidden_size.

But this requires that the input and weight are contiguous in memory. Assuming that the tensors are stored in row-major order, which is the default for PyTorch.

But in CUDA, the weight might be stored in column-major if using cuBLAS, but since we're using our own kernel, perhaps it's okay.

This kernel requires each thread to process one sample (i). The grid size would be batch_size (16384), and block size could be, say, 256. So 16384 / 256 = 64 blocks.

But the problem is the inner loops. For each j (up to 1024), and each k (up to 4096), the total operations per thread are 1024 * 4096 = ~4 million per thread, which is a lot.

This might be too slow. Perhaps we can optimize this by using shared memory or vectorization.

Alternatively, maybe transpose the weight matrix so that the inner loop is over the output_size first. Not sure.

Alternatively, perhaps the first layer can be left as PyTorch, and the second layer fused with the logsumexp is manageable.

Alternatively, maybe the problem requires replacing the first layer, so let's try.

Alternatively, perhaps the first fused kernel can be written as follows:

For linear1 + sigmoid:

Each thread processes one element of the output. The output is (batch_size, hidden_size).

The kernel would be:

__global__ void fused_linear_sigmoid(
    const float* input, // (batch_size, input_size)
    const float* weight, // (hidden_size, input_size)
    const float* bias, // (hidden_size,)
    float* output, // (batch_size, hidden_size)
    int batch_size,
    int input_size,
    int hidden_size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * hidden_size) return;

    int i = idx / hidden_size;
    int j = idx % hidden_size;

    float sum = 0.0f;
    for (int k = 0; k < input_size; k++) {
        sum += input[i * input_size + k] * weight[j * input_size + k];
    }
    sum += bias[j];
    output[idx] = 1.0f / (1.0f + exp(-sum));
}

This is similar to the earlier approach. The problem is the number of threads, which is 16384 * 4096 = ~68 million. Assuming a block size of 256 threads, the number of blocks would be ~68M / 256 ≈ 266,000 blocks. The maximum number of blocks in a grid is 65535 per dimension, so a 1D grid would need to have that. But 266,000 exceeds that. So this approach is not feasible.

Therefore, perhaps this approach is not viable. Maybe the first layer should not be replaced, and only the second layer and logsumexp are fused.

Alternatively, perhaps using a different kernel structure for the first layer:

Instead of each element being a thread, process rows. Each block processes a row (i), and threads in the block handle each hidden_size element.

For example:

Each block processes a row (i). The block has blockDim.x threads. Each thread handles a subset of the hidden_size elements.

Let’s say blockDim.x is 256. Then each block can handle up to 256 elements per thread. Since hidden_size is 4096, each thread would handle 16 elements (4096 / 256 = 16).

The kernel would be:

__global__ void fused_linear_sigmoid(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int input_size,
    int hidden_size
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    // Each thread in the block handles a chunk of hidden_size elements
    int tid = threadIdx.x;
    int num_threads = blockDim.x;

    for (int j_start = tid; j_start < hidden_size; j_start += num_threads) {
        // Process element (i, j_start) to (i, j_end)
        // Each thread handles a range of j indices
        // Let's say each thread handles 1 element (but 4096/256=16)
        for (int j = j_start; j < j_start + 16; j += 1) {
            // Check if j is within bounds
            if (j >= hidden_size) break;

            float sum = 0.0f;
            for (int k = 0; k < input_size; k++) {
                sum += input[i * input_size + k] * weight[j * input_size + k];
            }
            sum += bias[j];
            output[i * hidden_size + j] = 1.0f / (1.0f + exp(-sum));
        }
    }
}

Wait, but how to distribute the j indices evenly among threads. Let's see:

The block has 256 threads. For 4096 elements, each thread can handle 16 elements. So the loop over j_start would be in steps of 256 (number of threads), so each thread starts at j = tid, tid + 256, etc. Wait, perhaps:

Each thread in the block is responsible for a set of j indices. The total number of elements per block is hidden_size (4096). So for 256 threads, each thread can handle 16 elements (4096 /256 = 16).

Thus, in the loop, for each j_start = tid, the thread processes j_start, j_start + 256, j_start + 512, etc., but that might not be the case.

Alternatively, the j indices can be divided into chunks of 16 per thread. For example, each thread's starting j is tid * 16. But that would require:

for (int j = tid * 16; j < (tid+1)*16; j++) {

}

But this requires that hidden_size is a multiple of 16, which it is (4096 / 16 = 256). So that's okay.

Wait, if blockDim.x is 256, and each thread handles 16 elements, then total per block is 256 *16 = 4096, which matches.

Thus, the kernel would be:

__global__ void fused_linear_sigmoid(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int input_size,
    int hidden_size
) {
    int i = blockIdx.x;
    if (i >= batch_size) return;

    int tid = threadIdx.x;

    // Each thread handles 16 elements of j
    for (int j = tid * 16; j < (tid + 1)*16; j++) {
        if (j >= hidden_size) break;

        float sum = 0.0f;
        for (int k = 0; k < input_size; k++) {
            sum += input[i * input_size + k] * weight[j * input_size + k];
        }
        sum += bias[j];
        output[i * hidden_size + j] = 1.0f / (1.0f + exp(-sum));
    }
}

Wait, but the inner loop over k is input_size=2048, which is 2048 iterations per j. That's a lot. Each j requires 2048 multiplies and adds. For each thread, handling 16 j's, that's 16 * 2048 = 32,768 operations per thread. With 256 threads per block, that's manageable, but the total per block would be 256 * 32,768 = 8,388,608 operations per block.

But in CUDA, this could be feasible if the threads can execute in parallel. However, each thread's work is independent.

This approach requires a grid of batch_size blocks (16384), each with 256 threads. That's 16384 * 256 threads. The maximum number of blocks in a grid is 65535, so 16384 is okay (since 16384 < 65535). The maximum threads per block is 1024, so 256 is acceptable.

Therefore, this kernel structure is feasible.

Now, the second kernel for fused_linear2_logsumexp.

The kernel function would be as follows:

__global__ void fused_linear_logsumexp(
    const float* input,  // (batch_size, hidden_size)
    const float* weight, // (output_size, hidden_size)
    const float* bias,   // (output_size,)
    float* output,       // (batch_size,)
    int batch_size,
    int hidden_size,
    int output_size
) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= batch_size) return;

    float sum_exp = 0.0f;
    for (int j = 0; j < output_size; j