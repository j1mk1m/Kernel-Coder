**Do not use any third-party libraries. Only use pytorch and cuda.** 

**DO NOT FORGET TO UPDATE THE get_inputs() AND get_init_inputs() functions in your code to the new ModelNew class if needed!** 

**Note: Please make sure that the code you write is compatible with the current versions of PyTorch and CUDA.**



Okay, I need to optimize the given PyTorch model using custom CUDA kernels. Let me look at the architecture first. The model does a convolution, subtracts a value, applies HardSwish, MaxPool, then Mish. The goal is to replace some of these operations with custom CUDA kernels for speedup.

First, I'll consider which operators are worth replacing. The convolution is a big part, but PyTorch's nn.Conv2d is already optimized, so maybe not worth it. The subtraction is simple, but it's an element-wise op, maybe combining it with other steps would help. The activations like HardSwish and Mish could be good candidates because they involve element-wise operations that might be fused into a single kernel with other steps. The MaxPool is also an element-wise operation but depends on local regions, which might be more complex to implement.

Looking at the example provided, the element-wise addition was replaced with a CUDA kernel. Maybe similar approach for subtraction. But combining multiple steps into one kernel might save overhead. Let's see:

The sequence after convolution is subtract, hardswish, maxpool, mish. Wait, the order is:

x = self.conv(x) → subtract → hardswish → maxpool → mish.

Hmm, MaxPool is between HardSwish and Mish. So the element-wise operations are subtract, hardswish, and mish. Maybe I can combine subtract with hardswish into a single kernel, then MaxPool, then Mish? Or perhaps even more fusion.

Alternatively, since subtract is an element-wise operation, combining it with hardswish into a single kernel might reduce memory accesses. Let's think about the functions:

HardSwish formula is x * ReLU6(x + 3)/6. So after subtracting the value, you can apply that. So the subtract is x = x - subtract_value, then hardswish is applied. So maybe a fused kernel for (x - subtract_value) followed by hardswish. That would be a single kernel instead of two separate steps. That could save time.

Similarly, the Mish activation is x * tanh(softplus(x)), which is also element-wise. So maybe combine the MaxPool with Mish? Not sure. MaxPool is a different operation, since it's over a window. But perhaps the Mish can be done in a separate kernel, but maybe combining with previous steps.

Alternatively, let's see which steps are element-wise and can be fused. The subtract is element-wise, hardswish is element-wise. MaxPool is a local max over a kernel, so that might need a separate kernel. Then Mish is element-wise again.

So maybe:

1. Combine subtract and hardswish into a single kernel. That's two operations into one.

2. The MaxPool is already optimized, so maybe not worth replacing unless there's a chance to combine with another step. Alternatively, maybe not, since it's a standard operation.

3. The Mish activation could be implemented in a custom kernel to see if it's faster than the PyTorch version.

Alternatively, perhaps the MaxPool can be fused with another step, but I'm not sure. Let me think about each possibility.

First, let's tackle the subtract and hardswish. Let's see:

The subtract is x = x - subtract_value. Then hardswish(x) = x * relu6(x + 3)/6. So the combined function would be:

hardswish(x - subtract_value) = (x - v) * relu6((x - v)+3)/6. Wait, actually the subtract is done first, then hardswish. So the combined kernel can take the input tensor, subtract the value, compute the hardswish, and store the result. That would be a single kernel, which can save the intermediate step of storing the subtracted tensor, thus saving memory and computation steps.

So I can write a custom CUDA kernel for that combination.

Similarly, the Mish activation: x * tanh(softplus(x)). Since that's element-wise, a custom kernel might help. Alternatively, maybe PyTorch's implementation is already optimized, but perhaps writing a fused version could help. Alternatively, combining Mish with another step.

Alternatively, after MaxPool, we have Mish, which is element-wise, so perhaps that can be a separate kernel.

Now, the MaxPool is a standard 2x2 pooling. Since PyTorch's MaxPool2d is optimized, maybe replacing it isn't worth it, but if I can combine it with another operation, like the subtract and hardswish, perhaps that's better. But MaxPool involves looking at neighborhoods, which complicates things. So maybe leave it as is.

So the plan is:

- Replace subtract and hardswish with a fused CUDA kernel.

- Replace the Mish activation with a custom CUDA kernel.

That would cover two of the operations. Let's see.

Now, let's think about implementing the fused subtract and hardswish kernel.

The formula:

def subtract_and_hardswish(x, subtract_value):

    temp = x - subtract_value

    return temp * torch.clamp(temp + 3, 0, 6) / 6

So in CUDA, for each element, do:

temp = x[i] - subtract_value

temp_plus_3 = temp + 3

if temp_plus_3 <0 → 0, else if >6 →6, else temp_plus_3

then multiply by temp, divide by 6.

So the kernel can compute this per element.

Similarly for Mish:

def mish(x):

    return x * torch.tanh(torch.nn.functional.softplus(x))

Softplus is log(1 + exp(x)), so the kernel needs to compute that, then tanh of it, multiply by x.

Implementing this in CUDA could be done with per-element operations.

So I need to write these two kernels.

Additionally, perhaps the subtraction is trivial, but if we can fuse it with hardswish, it's better.

Now, let's structure the code.

First, define the fused kernel for subtract and hardswish:

I'll need to inline the CUDA code for this. Let's see:

The code structure would be similar to the example. So:

First, define the CUDA kernel for subtract and hardswish.

The parameters would be input tensor, subtract value, output tensor.

The kernel loop over each element.

Similarly for Mish.

Now, let's code this step by step.

First, the subtract and hardswish kernel.

The CUDA code:

#include <torch/extension.h>

__global__ void subtract_and_hardswish_kernel(
    const float* input,
    float subtract_value,
    float* output,
    int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float temp = input[idx] - subtract_value;
        float temp_plus_3 = temp + 3.0f;
        float clamped = fmaxf(0.0f, fminf(temp_plus_3, 6.0f));
        output[idx] = temp * clamped / 6.0f;
    }
}

Then the launcher function:

torch::Tensor subtract_and_hardswish_cuda(
    torch::Tensor input,
    float subtract_value) {
    // Get size
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    // Set up CUDA grid
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    subtract_and_hardswish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        subtract_value,
        output.data_ptr<float>(),
        num_elements);
    
    return output;
}

Similarly for Mish:

__global__ void mish_kernel(const float* input, float* output, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float x = input[idx];
        float exp_x = expf(x);
        float softplus = logf(1.0f + exp_x); // softplus
        float tanh_sp = tanhf(softplus);
        output[idx] = x * tanh_sp;
    }
}

Then the launcher:

torch::Tensor mish_cuda(torch::Tensor input) {
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_elements);
    
    return output;
}

Wait, but for the softplus, the formula is log(1 + exp(x)), but when x is very large, exp(x) can overflow. However, in practice, for x > 18 or so, exp(x) is already too big. So maybe better to use max(x, 0) + log(1 + exp(-abs(x))) to prevent overflow. But perhaps in CUDA, the expf function can handle it, but for numerical stability, maybe better to use the alternative formula. However, for simplicity, perhaps proceed with the straightforward code and see. Alternatively, use the approximation.

Alternatively, let's see:

The softplus function can be written as max(x, 0) + log(1 + exp(-abs(x))) which avoids overflow when x is large. So, let's implement that:

In the kernel:

float softplus;
if (x > 20) { // arbitrary threshold
    softplus = x;
} else {
    float z = expf(-fabsf(x));
    softplus = x + log1pf(z); // log(1 + exp(-|x|))
}

Wait, log1p is a function that computes log(1 + z) accurately for small z, but here z is exp(-|x|), so when x is large, z is near zero, so log1p(z) = log(1+z) ~ z. So maybe better to use that.

Alternatively, let's implement:

float softplus = std::max(x, 0.0f) + log1pf(expf(-fabsf(x)));

Wait, perhaps better to code it as:

if (x > 20) {
    softplus = x;
} else {
    softplus = logf(1.0f + expf(x));
}

But this might still have overflow for very large x. Hmm. Alternatively, use the numerically stable version.

Alternatively, in the CUDA kernel, using the formula:

float softplus = log(1 + exp(x));

But when x is large, exp(x) is too big. So for x > 20, exp(x) is already 4.8e8, which is within the range of float (max is ~3e38). So maybe okay.

Alternatively, perhaps the straightforward code is okay for now. Let's proceed with the initial code, but note that this might have precision issues for very large x, but perhaps in practice it's manageable.

Now, the next step is to replace the subtract and hardswish with the fused kernel, and the mish with the custom kernel.

The model's forward is:

def forward(self, x):

    x = self.conv(x)

    x = x - self.subtract_value

    x = torch.nn.functional.hardswish(x)

    x = self.pool(x)

    x = torch.nn.functional.mish(x)

    return x

So in the new model, after the convolution, we can call the fused subtract and hardswish kernel, then the pool, then the custom mish kernel.

Thus, the ModelNew class will use the custom kernels for the subtract/hardswish and the mish.

Also, need to note that the subtract_value is a parameter in the model, so in the new model, we need to have that value accessible. Since the original model has self.subtract_value, the new model can keep that as an attribute.

So in the new model:

class ModelNew(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):

        super().__init__()

        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

        self.subtract_value = subtract_value

        self.pool = nn.MaxPool2d(pool_kernel_size)

        # Load the custom kernels here, maybe as attributes?

        # The custom functions are loaded via the load_inline, so perhaps stored in a module.

        # Wait, in the example, the elementwise_add is a module loaded via load_inline.

        # So similar to the example, we need to have the CUDA functions loaded as part of the model.

        # So, in the code, before the ModelNew class, we need to define the CUDA sources for the two kernels (subtract/hardswish and mish), then load them inline, and have those functions available in the model.

        # Thus, the code structure would have:

        # first define the CUDA source strings for subtract_and_hardswish and mish.

        # Then, load them into functions, and then in the model, call them.

So, first, write the CUDA code for the two kernels as strings.

First, the subtract_and_hardswish source code.

subtract_and_hardswish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void subtract_and_hardswish_kernel(
    const float* input,
    float subtract_value,
    float* output,
    int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float temp = input[idx] - subtract_value;
        float temp_plus_3 = temp + 3.0f;
        float clamped = fmaxf(0.0f, fminf(temp_plus_3, 6.0f));
        output[idx] = temp * clamped / 6.0f;
    }
}

torch::Tensor subtract_and_hardswish_cuda(
    torch::Tensor input,
    float subtract_value) {
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    subtract_and_hardswish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        subtract_value,
        output.data_ptr<float>(),
        num_elements);
    
    return output;
}
"""

subtract_and_hardswish_cpp_header = "torch::Tensor subtract_and_hardswish_cuda(torch::Tensor input, float subtract_value);"

Then, the Mish kernel:

mish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void mish_kernel(const float* input, float* output, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float x = input[idx];
        float exp_x = expf(x);
        float softplus = logf(1.0f + exp_x);
        float tanh_sp = tanhf(softplus);
        output[idx] = x * tanh_sp;
    }
}

torch::Tensor mish_cuda(torch::Tensor input) {
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_elements);
    
    return output;
}
"""

mish_cpp_header = "torch::Tensor mish_cuda(torch::Tensor input);"

Now, we need to load these using load_inline.

So after defining the source strings, we can do:

subtract_and_hardswish = load_inline(
    name="subtract_and_hardswish",
    cpp_sources=subtract_and_hardswish_cpp_header,
    cuda_sources=subtract_and_hardswish_source,
    functions=["subtract_and_hardswish_cuda"],
    verbose=True
)

mish = load_inline(
    name="mish",
    cpp_sources=mish_cpp_header,
    cuda_sources=mish_source,
    functions=["mish_cuda"],
    verbose=True
)

Then, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value = subtract_value
        self.pool = nn.MaxPool2d(pool_kernel_size)
        # Assign the loaded functions as attributes
        self.subtract_and_hardswish = subtract_and_hardswish
        self.mish = mish

    def forward(self, x):
        x = self.conv(x)
        # Call the fused kernel for subtract and hardswish
        x = self.subtract_and_hardswish.subtract_and_hardswish_cuda(x, self.subtract_value)
        x = self.pool(x)
        # Call the custom mish
        x = self.mish.mish_cuda(x)
        return x

Wait, but the subtract_and_hardswish_cuda requires the subtract_value, which is an attribute of the model. So in the forward function, we can pass it as an argument.

But in the original model, subtract_value is a parameter stored in the instance, so the new model should also have that as an attribute, which it does.

Now, checking the get_inputs and get_init_inputs functions. The original get_inputs returns a list with a single tensor, since the model takes a single input. The get_init_inputs returns the parameters needed to initialize the model, which are the in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size. So in the new model, the __init__ requires those parameters, so the get_init_inputs should remain the same. Thus, no changes needed to those functions unless the new model requires different parameters. Since the new model's __init__ is the same as the original, the get_init_inputs and get_inputs functions can stay as they are.

Wait, but the new model's __init__ has the same parameters as the original, so the get_init_inputs is okay.

So the final code would be:

The full code:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

subtract_and_hardswish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void subtract_and_hardswish_kernel(
    const float* input,
    float subtract_value,
    float* output,
    int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float temp = input[idx] - subtract_value;
        float temp_plus_3 = temp + 3.0f;
        float clamped = fmaxf(0.0f, fminf(temp_plus_3, 6.0f));
        output[idx] = temp * clamped / 6.0f;
    }
}

torch::Tensor subtract_and_hardswish_cuda(
    torch::Tensor input,
    float subtract_value) {
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    subtract_and_hardswish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        subtract_value,
        output.data_ptr<float>(),
        num_elements);
    
    return output;
}
"""

subtract_and_hardswish_cpp_header = "torch::Tensor subtract_and_hardswish_cuda(torch::Tensor input, float subtract_value);"

subtract_and_hardswish = load_inline(
    name="subtract_and_hardswish",
    cpp_sources=subtract_and_hardswish_cpp_header,
    cuda_sources=subtract_and_hardswish_source,
    functions=["subtract_and_hardswish_cuda"],
    verbose=True
)

mish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void mish_kernel(const float* input, float* output, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float x = input[idx];
        float exp_x = expf(x);
        float softplus = logf(1.0f + exp_x);
        float tanh_sp = tanhf(softplus);
        output[idx] = x * tanh_sp;
    }
}

torch::Tensor mish_cuda(torch::Tensor input) {
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_elements);
    
    return output;
}
"""

mish_cpp_header = "torch::Tensor mish_cuda(torch::Tensor input);"

mish = load_inline(
    name="mish",
    cpp_sources=mish_cpp_header,
    cuda_sources=mish_source,
    functions=["mish_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value = subtract_value
        self.pool = nn.MaxPool2d(pool_kernel_size)
        self.subtract_and_hardswish = subtract_and_hardswish
        self.mish = mish

    def forward(self, x):
        x = self.conv(x)
        x = self.subtract_and_hardswish.subtract_and_hardswish_cuda(x, self.subtract_value)
        x = self.pool(x)
        x = self.mish.mish_cuda(x)
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
subtract_value = 0.5
pool_kernel_size = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]

Wait, but in the original get_inputs, the tensors were on CPU? Or does the model expect them to be on GPU? The original code's get_inputs didn't have .cuda(), but the model is designed for CUDA? Wait in the original code's get_inputs, the example had a.cuda(), but in the given model, perhaps the user expects the inputs to be on the correct device.

The problem says to make sure the code is compatible with current versions. Also, in the original code's get_inputs, it was generating tensors with .cuda(). The original code's get_inputs function in the problem says:

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

But in the example, they used .cuda(). To ensure the inputs are on GPU, perhaps we should modify get_inputs to return tensors on CUDA.

Looking back at the problem's original code:

The original code's get_inputs returns [torch.rand(...)], but the example had .cuda(). The problem's get_inputs may have to be adjusted to return CUDA tensors, as the model is likely designed to run on GPU.

In the problem's given code for the original model, the get_inputs function returns tensors without .cuda(). However, since the model's forward is using CUDA (because the example had a.cuda()), maybe we should adjust get_inputs to return CUDA tensors.

The user instruction says: "Do not forget to update the get_inputs() and get_init_inputs() functions in your code to the new ModelNew class if needed!" But since ModelNew's __init__ has the same parameters as Model, the get_init_inputs doesn't need changes. However, get_inputs may need to return tensors on CUDA since the model will be on CUDA.

So, in the original problem's code, the get_inputs was:

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

But in the example, they used .cuda(). To make the inputs compatible with the model (assuming it's on GPU), the get_inputs should have .cuda(). So I'll modify get_inputs to return CUDA tensors.

Hence, the updated get_inputs is:

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

The get_init_inputs remains the same, as it's just parameters.

Now, checking for possible errors in the code:

- The CUDA kernels must handle the data type correctly (float, which is the default for PyTorch tensors here).

- The subtract_and_hardswish_cuda function takes a float subtract_value, which in the model is stored as a float, so passing self.subtract_value is correct.

- The Mish kernel: expf(x) could have overflow issues for very large x, but perhaps it's okay. Alternatively, using the stable version would be better, but given time constraints, proceed with the current code.

- The MaxPool is left as the PyTorch implementation, which is okay.

Another thing: the Mish kernel's output is stored in an empty_like tensor, which is correct.

Now, I need to ensure that the code is correctly structured as per the problem's instructions, using markdown codeblocks and correct syntax.

Putting it all together in the required format.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

subtract_and_hardswish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void subtract_and_hardswish_kernel(
    const float* input,
    float subtract_value,
    float* output,
    int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float temp = input[idx] - subtract_value;
        float temp_plus_3 = temp + 3.0f;
        float clamped = fmaxf(0.0f, fminf(temp_plus_3, 6.0f));
        output[idx] = temp * clamped / 6.0f;
    }
}

torch::Tensor subtract_and_hardswish_cuda(
    torch::Tensor input,
    float subtract_value) {
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    subtract_and_hardswish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        subtract_value,
        output.data_ptr<float>(),
        num_elements);
    
    return output;
}
"""

subtract_and_hardswish_cpp_header = "torch::Tensor subtract_and_hardswish_cuda(torch::Tensor input, float subtract_value);"

subtract_and_hardswish = load_inline(
    name="subtract_and_hardswish",
    cpp_sources=subtract_and_hardswish_cpp_header,
    cuda_sources=subtract_and_hardswish_source,
    functions=["subtract_and_hardswish_cuda"],
    verbose=True
)

mish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void mish_kernel(const float* input, float* output, int num_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_elements) {
        float x = input[idx];
        float exp_x = expf(x);
        float softplus = logf(1.0f + exp_x);
        float tanh_sp = tanhf(softplus);
        output[idx] = x * tanh_sp;
    }
}

torch::Tensor mish_cuda(torch::Tensor input) {
    int64_t num_elements = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;
    
    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_elements);
    
    return output;
}
"""

mish_cpp_header = "torch::Tensor mish_cuda(torch::Tensor input);"

mish = load_inline(
    name="mish",
    cpp_sources=mish_cpp_header,
    cuda_sources=mish_source,
    functions=["mish_cuda"],
    verbose=True
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value = subtract_value
        self.pool = nn.MaxPool2d(pool_kernel_size)
        self.subtract_and_hardswish = subtract_and_hardswish
        self.mish = mish

    def forward(self, x):
        x = self.conv(x)
        x = self.subtract_and_hardswish.subtract_and_hardswish_cuda(x, self.subtract_value)
        x = self.pool(x)
        x = self.mish.mish_cuda(x)
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
subtract_value = 0.5
pool_kernel_size = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]
```