Also, write a report in markdown format in Chinese that summarizes your approach. The report should have a section for each replaced operator, describing the original operator (name, function, input/output shapes, compute), the design of your custom CUDA kernel (what's implemented, algorithm, optimization techniques), and why you chose to replace it (e.g., operator is a bottleneck, parallelism opportunities). The report must not have any markdown formatting except for the headers (##, ###, etc.) and bullet points (using *). The rest of the report should be plain text. The report should be concise (max 500 words total). 

Also, you may optionally write an English explanation after the report section explaining your thought process and how you arrived at the solution. The English explanation should be concise (max 200 words). 

Make sure your code uses the same signature and structure as the original Model class. The code must be self-contained and include all necessary imports and dependencies. 

Now, please output the optimized architecture code in codeblocks and the report and English explanation as specified.

好的，我现在需要优化给定的Model类，使用自定义的CUDA内核来替换其中的PyTorch操作，以获得性能提升。首先，我得仔细分析原模型的结构和各个操作，找出可以优化的点。

原模型的forward过程包括三个主要步骤：线性层（Gemm）、缩放（scale乘法）和批归一化（BatchNorm）。我需要逐一检查这三个操作，看看是否有优化的潜力。

首先，线性层（nn.Linear）的实现通常是矩阵乘法，这在CUDA上已经高度优化了，可能不需要替换。不过，如果可以将矩阵乘法与后续的缩放操作融合到一个内核中，可能会减少内存访问和线程启动的开销。比如，Gemm的结果直接乘以scale参数，这样可以避免中间结果的存储和加载。

接下来是缩放操作，即x乘以scale参数。这个操作是一个元素级的乘法，可以和Gemm的结果直接融合，避免单独的内核调用。

然后是BatchNorm操作，它涉及均值、方差的计算，以及归一化和缩放。虽然PyTorch的BatchNorm已经优化了，但在某些情况下，特别是当batch size很大时，可能有一些优化空间。比如，可以将BatchNorm的计算与前面的缩放操作融合，减少内存访问次数。

因此，我决定将线性层（Gemm）、缩放（scale乘法）和BatchNorm三个步骤融合到一个自定义的CUDA内核中。这样可以减少多次内存读写，提升并行效率。

接下来，我需要设计这个融合的内核。首先，线性层的矩阵乘法计算：输入x（形状batch_size×in_features）乘以权重矩阵（in_features×out_features），得到中间结果。然后，乘以scale参数（形状out_features），接着计算BatchNorm的均值和方差。注意，BatchNorm在训练模式下需要计算均值和方差，这可能需要额外的原子操作来累积总和。不过，由于BatchNorm的统计是跨batch进行的，可能无法完全融合到每个元素的计算中，因此可能需要分阶段处理。

不过，考虑到训练时BatchNorm的前向传播包括计算均值和方差，这可能需要在内核中同时处理，这可能比较复杂。因此，可能需要将线性层和缩放操作融合，而BatchNorm保持原样，或者寻找其他优化方式。

或者，可能将线性层和缩放融合，因为它们都是线性操作，可以合并计算，减少内存访问。例如，在矩阵乘法后的每个元素乘以scale，这可以作为一个内核中的步骤。

此外，BatchNorm的计算可能涉及均值和方差，这可能需要全局的累加操作，这在CUDA中可以用原子操作或使用分块累加的方法处理。

不过，考虑到复杂度，可能先尝试将线性层（矩阵乘法）和scale的乘法融合到一个内核中。因为矩阵乘法本身可能已经足够高效，但融合scale可以减少一次内存读取。

例如，线性层的输出是xW + b，然后乘以scale。但线性层的bias可能已经被包含在矩阵乘法中，所以如果不需要bias的话，可以合并到矩阵乘法中。但原模型中的nn.Linear默认有bias，所以需要考虑这一点。

可能需要重新设计矩阵乘法加上scale乘法的内核。例如：

对于每个输出元素，计算sum_{i} x_i * W_{i,j}，然后加上bias_j，然后乘以 scale_j。

这样，可以将这三个步骤合并到一个内核中。这可能减少中间存储和内存带宽的使用。

此外，BatchNorm的计算可能需要均值和方差，这可能需要额外的步骤，但或许可以与其他计算并行处理。

不过，BatchNorm的计算可能涉及到所有样本的统计，这在CUDA中可能需要全局的reduction操作，这可能比较耗时，但可能无法通过单个内核优化太多，所以可能暂时不替换，而专注于前两个操作的融合。

因此，首先将线性层和scale的乘法合并到一个内核，然后考虑BatchNorm是否有可能优化。

现在，编写这个内核。需要处理矩阵乘法，加上bias，然后乘以scale。

矩阵乘法的计算：对于输出中的每个元素out_j，计算sum_{i} x_i * W_{i,j} + bias_j，然后乘以 scale_j。

所以，每个线程处理一个输出元素，即每个线程负责一个j，然后计算其累加和，加上bias，再乘以scale。

需要注意的是，输入x的维度是batch_size×in_features，权重矩阵是in_features×out_features，所以每个输出元素j需要遍历in_features个项。

这可能在CUDA中实现，但需要注意内存访问的局部性和并行性。例如，将线程块组织为处理输出维度的每个元素，或者按块处理。

但这样的计算方式可能效率不高，因为每个线程需要遍历in_features次，当in_features很大时（如4096），这可能导致每个线程的计算量较大，而内存访问可能不连续。

因此，可能需要更高效的矩阵乘法实现，比如使用共享内存来缓存权重或输入数据，但这样会增加复杂度。或者，可以利用CUDA的cuBLAS库函数，因为线性层的矩阵乘法已经由cuBLAS优化，但可能无法直接融合scale乘法。因此，可能需要在矩阵乘法后立即进行scale乘法，这可能更高效。

另一种方法是，将scale参数合并到权重矩阵中。例如，权重矩阵乘以scale向量，但这样只能在权重不变化的情况下预处理，而scale是参数，需要动态变化，所以不可行。

因此，可能需要在矩阵乘法之后立即进行元素级的乘法。

所以，可能将线性层的计算和scale的乘法合并到一个自定义的CUDA内核中，以减少内存拷贝。

接下来，我需要编写这个内核。例如：

在CUDA内核中，线程处理每个输出元素的计算。对于每个输入样本（batch中的每个元素），计算其对应的输出元素。

不过，对于批量处理，可能需要每个线程处理一个输出元素，或者采用分块的方式。

不过，具体实现可能需要更详细的思考。例如，假设输入x是形状（batch_size, in_features），权重是（in_features, out_features），那么输出是（batch_size, out_features）。每个输出元素的计算为：

out[b][j] = (x[b] * W[:,j]).sum() + bias[j]，然后乘以 scale[j]

所以，每个输出元素j的计算，对于每个batch样本，需要计算其对应的结果。

因此，可以设计内核，使得每个线程处理一个输出元素（j）和一个批次样本（b）。或者，组织线程块来处理每个批次样本，每个线程处理一个输出元素。

例如，线程块维度可以是（out_features, batch_size），每个线程处理一个（b,j）对，但可能线程数量过大。

另一种方式是，每个线程块处理一个批次样本，每个线程处理一个输出元素j。这样，线程块数量为batch_size，每个线程块有out_features线程。

这可能更可行，因为对于batch_size=16384，这会导致大量的线程块，可能超过设备的最大线程块数量限制？或者，可能需要调整。

不过，这样的设计可能更有效，因为对于每个批次样本，其数据在内存中是连续的。

具体来说：

线程块的大小为 blockDim.x = out_features，每个线程处理一个输出元素j。

每个线程块处理一个批次样本b，线程索引为j，计算该样本的输出元素j。

然后，将线程块数量设置为 batch_size。

这样，对于每个批次样本b：

out[b][j] = (sum_{i} x[b][i] * weight[i][j]) + bias[j]) * scale[j]

这样，在内核中，每个线程可以读取该批次样本的x数据，以及权重的列j的数据。

但是，这样的内存访问模式可能不是最优的，因为权重的存储是按行的，而j是列索引，所以每个线程需要跳跃访问权重矩阵的列，这可能导致不连续的内存访问，降低缓存效率。

因此，这可能需要优化内存布局，比如转置权重矩阵，或者使用共享内存来缓存权重的一部分。

这可能比较复杂，所以可能需要寻找更高效的实现方式。

或者，可以将矩阵乘法和scale的乘法分开，但使用CUDA的cuBLAS进行矩阵乘法，然后使用一个元素级的内核进行scale乘法，这可能更高效。

例如：

使用cuBLAS进行矩阵乘法（Gemm），然后进行scale乘法：

x = torch.addmm(bias, x, weight.t())  # 或者使用cuBLAS的gemm函数

然后，x = x * scale

其中，scale是形状（out_features），所以广播乘法。

这可能比纯CUDA内核更高效，因为cuBLAS已经高度优化。但scale的乘法是一个简单的元素级操作，可以写成一个CUDA内核，可能比PyTorch的默认实现更快，特别是当批量很大时。

因此，可能将scale乘法替换为自定义内核。

同样，BatchNorm的计算也可能有机会优化，但可能比较复杂。例如，在训练时，BatchNorm的前向传播需要计算均值和方差，这需要全局的reduction操作，这可能需要在CUDA中实现，但可能复杂。

因此，我可能选择替换scale乘法和BatchNorm，或者将线性层的bias和scale融合到一个内核中。

现在，我需要决定哪些操作值得替换：

1. 线性层（矩阵乘法）：可能已经由cuBLAS优化，所以可能不替换。

2. scale乘法：元素级乘法，可能用自定义内核加速。

3. BatchNorm：可能需要替换，但复杂度高。

首先尝试替换scale乘法。例如，将x乘以scale参数。这个操作的输入是（batch_size, out_features），scale是（out_features），所以每个元素x[b][j]乘以 scale[j]。

这可以通过一个简单的CUDA内核实现：

每个线程处理一个元素，或者使用并行块。

例如：

__global__ void scale_mul_kernel(float* x, const float* scale, int batch_size, int out_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int j = idx % out_features; // 对应scale的索引
        x[idx] *= scale[j];
    }
}

这可能比PyTorch的默认实现更高效，因为可以避免Python的overhead，直接在CUDA中执行。

此外，可以考虑将scale乘法与BatchNorm的归一化步骤融合，例如在计算均值和方差时同时进行scale的乘法，但可能需要更复杂的处理。

接下来，考虑BatchNorm的实现。BatchNorm的计算步骤包括：

计算均值：mean = mean(x, dim=0)

计算方差：var = var(x, dim=0)

标准化：x_hat = (x - mean) / sqrt(var + eps)

缩放和平移：y = gamma * x_hat + beta

其中，gamma和beta是参数，但在这个模型中，BatchNorm的参数是学习的，而模型中的scale可能已经是一个参数，但需要确认。

在原模型中，BatchNorm的gamma和beta是存在的，所以可能无法直接融合scale参数。

因此，可能需要单独处理BatchNorm的计算。但其计算可能涉及全局reduction，这在CUDA中需要使用原子操作或分块累加。

例如，计算均值需要所有样本的x在每个特征维度上的总和，除以batch_size。

这可以通过一个内核来实现，使用分块的sum-reduction方法。

例如，首先计算每个block的局部均值，然后将局部结果合并。

不过，这可能比较复杂，所以可能需要使用现有的优化方法。

综上，我决定替换scale乘法和BatchNorm的计算为自定义内核，同时可能将线性层的计算保持原样，但尝试将scale乘法与线性层的bias加法合并。

现在，编写代码：

首先，替换scale乘法：

编写一个CUDA内核，执行x乘以scale参数。

然后，替换BatchNorm的前向传播。

接下来，我需要编写这些内核，并将它们集成到ModelNew类中。

对于BatchNorm的自定义内核，需要实现：

前向传播的步骤包括：

1. 计算均值和方差。

2. 标准化输入。

3. 应用缩放和平移（即gamma和beta）。

在CUDA中，这可能需要多个步骤：

首先，计算均值和方差：

均值是每个特征（维度）的平均值，即对batch_size维度求平均。

方差是每个特征的方差，即平均（(x - mean)^2）。

因此，需要：

- 对每个特征j，计算总和x的j列，然后除以 batch_size 得到均值。

- 计算方差时，同样遍历每个元素，计算（x[b][j] - mean[j]）的平方，求总和，再除以 batch_size。

这可能需要两个pass：一个计算均值，第二个计算方差。

在CUDA中，可以使用分块的reduction方法。

例如，计算均值：

每个线程块处理一个特征j，计算该特征在所有batch样本中的总和。

然后，所有块的总和被收集，除以 batch_size 得到均值。

同样，方差的计算类似。

这可能比较高效，但需要多个内核调用，或者在一个内核中完成。

然后，标准化和应用gamma和beta可以在另一个内核中执行。

因此，自定义的BatchNorm内核可能分为几个步骤，但为了效率，可能需要将它们合并到尽可能少的内核中。

不过，这可能比较复杂，因此，我可能选择只替换scale乘法，因为这更容易实现。

现在，先实现scale乘法的内核：

编写一个内核，接收输入x，scale参数，然后执行元素级乘法。

然后，在ModelNew中，替换scale乘法为这个内核。

此外，可能可以将scale乘法与线性层的bias相加合并到一个内核中，以减少内存访问。

例如，线性层的计算结果是W*x + bias，然后乘以scale，所以可以合并为一个内核：

out = (W*x + bias) * scale

这可能比分开的计算更高效。

因此，我将尝试编写一个自定义的线性层+scale乘法的内核。

现在，具体编写内核代码：

假设线性层的权重是W（in_features × out_features），输入x是(batch_size × in_features)，bias是(out_features)，scale是(out_features)。

输出是batch_size × out_features。

每个输出元素out[b][j] = (x[b] · W[:,j] + bias[j]) * scale[j]

因此，每个元素的计算需要：

1. 计算点积sum_{i} x[b][i] * W[i][j]

2. 加上bias[j]

3. 乘以scale[j]

为了高效实现，可以设计内核使得每个线程处理一个输出元素（b,j）。

线程块组织可以是：

每个线程块处理一个输出j，然后并行处理所有batch样本。

或者，每个线程处理一个 (b,j) 对。

这可能线程数过多，因为当 batch_size是16384，out_features是4096时，总共有16384×4096≈65M个元素，每个线程处理一个元素，这需要很大的线程数。

因此，这可能不可行。

因此，需要更高效的线程组织方式。

另一种方法是，每个线程块处理一个批次样本b，然后每个线程处理一个输出j。

例如，线程块的数量是batch_size，每个线程块的线程数是out_features。

这样，每个线程块处理一个b，每个线程处理j的索引。

每个线程在计算out[b][j]时，需要计算点积sum_{i} x[b][i] * W[i][j]。

这可以通过遍历i的索引：

float sum = 0.0;

for (int i = 0; i < in_features; ++i) {

    sum += x[b][i] * W[i][j];

}

然后，加上bias[j]，乘以 scale[j]。

这需要每个线程遍历in_features次，当in_features=4096时，这可能需要较多的循环次数，但每个线程的计算是独立的。

此外，内存访问模式可能不是最优，因为W的存储是行优先，而访问的是列j，即每个线程访问的是W的列，这可能导致非连续的内存访问。

为了缓解这个问题，可以考虑将权重矩阵转置为in_features × out_features，这样每个线程的i循环可以连续访问内存。

或者，可以将权重存储为列主序，但可能需要额外的转置步骤。

这可能复杂，因此，或许可以接受一定的内存访问开销，因为计算量较大。

此外，可以考虑将循环展开或使用共享内存来缓存x[b][i]，但需要更多的优化。

现在，编写这个内核：

```cpp
__global__ void fused_linear_scale_kernel(
    const float* x_data,
    const float* weight_data,
    const float* bias_data,
    const float* scale_data,
    float* out_data,
    int batch_size,
    int in_features,
    int out_features) {

    int b = blockIdx.x;
    int j = threadIdx.x;

    if (j >= out_features) return;

    float sum = 0.0f;

    for (int i = 0; i < in_features; ++i) {
        sum += x_data[b * in_features + i] * weight_data[i * out_features + j];
    }

    sum += bias_data[j];
    sum *= scale_data[j];

    out_data[b * out_features + j] = sum;
}
```

这里，线程块数量是batch_size，每个线程块的线程数是out_features。

每个线程处理一个j，计算该批次样本b的输出元素j。

这可能有效，但需要确保线程块的数量不超过设备的最大线程块数限制。例如，当batch_size=16384，线程块数量是16384，这可能在某些设备上是可行的，但需要确认。

此外，线程块的线程数是out_features=4096，这可能超过最大线程数限制（通常每个块最多1024或2048线程）。例如，4096超过这个限制，因此这个内核可能无法运行。

因此，这需要重新设计线程组织。

可能的解决方案是，将线程块的线程数设为较小的数，例如256，并使用多个线程处理多个j。

例如，每个线程处理多个j，或者使用二维线程块。

例如，线程块的维度为 blockDim.x = 256，每个线程块处理一个批次样本b，然后每个线程处理多个j。

例如，每个线程处理j = threadIdx.x + blockDim.x * blockIdx.y（如果使用二维grid）。

但这样可能需要更复杂的索引计算。

另一种方式是将线程块的线程数设为256，每个线程负责处理多个j。例如，每个线程处理 (out_features / 256) j's per thread.

但可能需要更复杂的代码。

或者，可以将线程块的尺寸设为 blockDim.x = 256，每个线程块处理一个批次样本b，并将out_features划分为多个块。

例如：

每个线程块处理一个批次样本b，每个线程处理一个或多个j。

例如，每个线程处理j = threadIdx.x + blockDim.x * blockIdx.y，其中 blockIdx.y 是块的y维度，这样可以扩展到更多的j。

这可能需要将线程块的尺寸设为二维：

gridDim = (batch_size, (out_features + blockDim.y - 1)/blockDim.y )

blockDim = (blockDim.x, blockDim.y)

但可能比较复杂。

这可能变得过于复杂，因此，或许需要重新考虑是否值得将线性层和scale融合。

或许，更好的方法是使用cuBLAS进行矩阵乘法，然后进行scale乘法的内核。

这样，线性层的计算由cuBLAS处理，而scale乘法用自定义内核。

例如：

在forward中：

x = self.gemm(x)  # 这会调用cuBLAS的gemm

然后，调用自定义的scale乘法内核：

x = self.scale_mul(x, self.scale)

然后，调用BatchNorm。

这样，可能更容易实现。

因此，现在先实现scale乘法的内核。

编写scale乘法的CUDA内核：

```cpp
__global__ void scale_mul_kernel(
    const float* x,
    const float* scale,
    float* out,
    int batch_size,
    int out_features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int j = idx % out_features;
        out[idx] = x[idx] * scale[j];
    }
}
```

然后，在ModelNew中替换scale乘法：

在forward中：

x = self.gemm(x)

x = self.scale_mul(x, self.scale)

其中，scale_mul是自定义的内核函数。

接下来，考虑BatchNorm的优化。

BatchNorm的计算可能是一个瓶颈，特别是在大batch_size时，因为需要计算均值和方差，这涉及到全局的reduction。

PyTorch的实现可能已经优化，但或许可以替换为自定义内核。

编写一个自定义的BatchNorm内核：

需要实现均值和方差的计算，然后标准化并应用gamma和beta。

假设在训练模式下，需要计算均值和方差：

步骤：

1. 计算每个特征的均值：mean[j] = (1/batch_size) * sum_{b} x[b][j]

2. 计算方差：var[j] = (1/batch_size) * sum_{b} (x[b][j] - mean[j])^2

3. 计算标准化后的x_hat[b][j] = (x[b][j] - mean[j]) / sqrt(var[j] + eps)

4. 输出y[b][j] = gamma[j] * x_hat[b][j] + beta[j]

在CUDA中，这可能需要多个内核：

- 第一个内核计算均值。

- 第二个内核计算方差。

- 第三个内核进行标准化和应用gamma/beta.

或者，可以将均值和方差的计算合并到一个内核，但需要共享内存或原子操作。

计算均值：

可以将线程块组织为每个特征一个块，每个块中的线程处理每个batch样本的该特征。

例如：

__global__ void compute_mean(
    const float* x,
    float* mean,
    int batch_size,
    int out_features) {

    int j = blockIdx.x;
    if (j >= out_features) return;

    float sum = 0.0f;
    for (int b = 0; b < batch_size; ++b) {
        sum += x[b * out_features + j];
    }
    mean[j] = sum / batch_size;
}

然后，方差计算：

__global__ void compute_var(
    const float* x,
    const float* mean,
    float* var,
    float eps,
    int batch_size,
    int out_features) {

    int j = blockIdx.x;
    if (j >= out_features) return;

    float sum_sq_diff = 0.0f;
    for (int b = 0; b < batch_size; ++b) {
        float diff = x[b * out_features + j] - mean[j];
        sum_sq_diff += diff * diff;
    }
    var[j] = sum_sq_diff / batch_size;
}

然后，标准化：

__global__ void batch_norm_forward(
    const float* x,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    float eps,
    float* y,
    int batch_size,
    int out_features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int b = idx / out_features;
        int j = idx % out_features;
        float inv_std = 1.0f / sqrt(var[j] + eps);
        y[idx] = gamma[j] * (x[idx] - mean[j]) * inv_std + beta[j];
    }
}

这可能有效，但需要注意的是，均值和方差的计算需要所有线程块完成，因此需要同步。

因此，在调用这些内核时，需要确保计算均值和方差的内核完成，然后再进行标准化。

这可能在PyTorch的自定义扩展中实现。

综上，现在将所有这些内核整合到代码中。

现在，编写ModelNew的代码：

首先，导入必要的模块：

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

然后，编写内核的源代码：

对于scale乘法：

scale_mul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_mul_kernel(
    const float* x,
    const float* scale,
    float* out,
    int batch_size,
    int out_features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int j = idx % out_features;
        out[idx] = x[idx] * scale[j];
    }
}

torch::Tensor scale_mul_cuda(torch::Tensor x, torch::Tensor scale) {
    int batch_size = x.size(0);
    int out_features = x.size(1);
    auto out = torch::empty_like(x);

    const int threads = 256;
    const int blocks = (batch_size * out_features + threads - 1) / threads;

    scale_mul_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        scale.data_ptr<float>(),
        out.data_ptr<float>(),
        batch_size,
        out_features
    );

    return out;
}
"""

scale_mul_cpp_source = "torch::Tensor scale_mul_cuda(torch::Tensor x, torch::Tensor scale);"

然后，编译这个内核：

scale_mul = load_inline(
    name="scale_mul",
    cuda_sources=scale_mul_source,
    cpp_sources=scale_mul_cpp_source,
    functions=["scale_mul_cuda"],
    verbose=True
)

接下来，编写BatchNorm的内核：

batch_norm_forward_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void compute_mean(
    const float* x,
    float* mean,
    int batch_size,
    int out_features) {
    int j = blockIdx.x;
    if (j >= out_features) return;

    float sum = 0.0f;
    for (int b = 0; b < batch_size; ++b) {
        sum += x[b * out_features + j];
    }
    mean[j] = sum / batch_size;
}

__global__ void compute_var(
    const float* x,
    const float* mean,
    float* var,
    float eps,
    int batch_size,
    int out_features) {
    int j = blockIdx.x;
    if (j >= out_features) return;

    float sum_sq_diff = 0.0f;
    for (int b = 0; b < batch_size; ++b) {
        float diff = x[b * out_features + j] - mean[j];
        sum_sq_diff += diff * diff;
    }
    var[j] = sum_sq_diff / batch_size;
}

__global__ void batch_norm_forward_kernel(
    const float* x,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    float eps,
    float* y,
    int batch_size,
    int out_features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int b = idx / out_features;
        int j = idx % out_features;
        float inv_std = 1.0f / sqrt(var[j] + eps);
        y[idx] = gamma[j] * (x[idx] - mean[j]) * inv_std + beta[j];
    }
}

std::tuple<torch::Tensor, torch::Tensor> compute_mean_var(
    torch::Tensor x,
    int batch_size,
    int out_features,
    float eps) {

    auto mean = torch::empty({out_features}, x.options());
    auto var = torch::empty({out_features}, x.options());

    const int threads = 256;
    const int blocks = out_features;

    compute_mean<<<blocks, threads>>>(
        x.data_ptr<float>(),
        mean.data_ptr<float>(),
        batch_size,
        out_features
    );

    compute_var<<<blocks, threads>>>(
        x.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        eps,
        batch_size,
        out_features
    );

    return std::make_tuple(mean, var);
}

torch::Tensor batch_norm_forward_cuda(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    float eps) {

    int batch_size = x.size(0);
    int out_features = x.size(1);

    auto [mean, var] = compute_mean_var(x, batch_size, out_features, eps);

    auto y = torch::empty_like(x);

    const int threads = 256;
    const int blocks = (batch_size * out_features + threads - 1) / threads;

    batch_norm_forward_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        eps,
        y.data_ptr<float>(),
        batch_size,
        out_features
    );

    return y;
}
"""

batch_norm_forward_cpp_source = """
std::tuple<torch::Tensor, torch::Tensor> compute_mean_var(
    torch::Tensor x,
    int batch_size,
    int out_features,
    float eps);
torch::Tensor batch_norm_forward_cuda(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    float eps);
"""

然后，编译：

batch_norm_forward = load_inline(
    name="batch_norm_forward",
    cuda_sources=batch_norm_forward_source,
    cpp_sources=batch_norm_forward_cpp_source,
    functions=["batch_norm_forward_cuda"],
    verbose=True
)

注意，这里可能需要处理gamma和beta参数，这些在模型的BatchNorm模块中存在。

因此，在ModelNew中，需要访问BatchNorm的gamma和beta参数。

原模型中的BatchNorm是nn.BatchNorm1d，其参数是：

self.bn.weight (gamma)

self.bn.bias (beta)

因此，在forward中，可以获取这些参数，并传递给自定义的BatchNorm内核。

现在，编写ModelNew类：

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=True)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)
        self.scale_mul = scale_mul
        self.batch_norm_forward = batch_norm_forward

    def forward(self, x):
        x = self.gemm(x)
        # Apply scale
        x = self.scale_mul.scale_mul_cuda(x, self.scale)
        # Apply BatchNorm using custom kernel
        gamma = self.bn.weight
        beta = self.bn.bias
        eps = self.bn.eps
        x = self.batch_norm_forward.batch_norm_forward_cuda(x, gamma, beta, eps)
        return x

需要注意的是，必须确保所有张量都在CUDA设备上，并且内核正确处理了参数。

此外，需要在初始化时传递参数，但原get_init_inputs返回[in_features, out_features, scale_shape]，所以ModelNew的__init__需要接受这些参数。

现在，整合所有代码：

完整的代码：

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Scale multiplication kernel
scale_mul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_mul_kernel(
    const float* x,
    const float* scale,
    float* out,
    int batch_size,
    int out_features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int j = idx % out_features;
        out[idx] = x[idx] * scale[j];
    }
}

torch::Tensor scale_mul_cuda(torch::Tensor x, torch::Tensor scale) {
    int batch_size = x.size(0);
    int out_features = x.size(1);
    auto out = torch::empty_like(x);

    const int threads = 256;
    const int blocks = (batch_size * out_features + threads - 1) / threads;

    scale_mul_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        scale.data_ptr<float>(),
        out.data_ptr<float>(),
        batch_size,
        out_features
    );

    return out;
}
"""

scale_mul_cpp_source = "torch::Tensor scale_mul_cuda(torch::Tensor x, torch::Tensor scale);"

scale_mul = load_inline(
    name="scale_mul",
    cuda_sources=scale_mul_source,
    cpp_sources=scale_mul_cpp_source,
    functions=["scale_mul_cuda"],
    verbose=True
)

# BatchNorm forward kernel
batch_norm_forward_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void compute_mean(
    const float* x,
    float* mean,
    int batch_size,
    int out_features) {
    int j = blockIdx.x;
    if (j >= out_features) return;

    float sum = 0.0f;
    for (int b = 0; b < batch_size; ++b) {
        sum += x[b * out_features + j];
    }
    mean[j] = sum / batch_size;
}

__global__ void compute_var(
    const float* x,
    const float* mean,
    float* var,
    float eps,
    int batch_size,
    int out_features) {
    int j = blockIdx.x;
    if (j >= out_features) return;

    float sum_sq_diff = 0.0f;
    for (int b = 0; b < batch_size; ++b) {
        float diff = x[b * out_features + j] - mean[j];
        sum_sq_diff += diff * diff;
    }
    var[j] = sum_sq_diff / batch_size;
}

__global__ void batch_norm_forward_kernel(
    const float* x,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    float eps,
    float* y,
    int batch_size,
    int out_features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int b = idx / out_features;
        int j = idx % out_features;
        float inv_std = 1.0f / sqrt(var[j] + eps);
        y[idx] = gamma[j] * (x[idx] - mean[j]) * inv_std + beta[j];
    }
}

std::tuple<torch::Tensor, torch::Tensor> compute_mean_var(
    torch::Tensor x,
    int batch_size,
    int out_features,
    float eps) {

    auto mean = torch::empty({out_features}, x.options());
    auto var = torch::empty({out_features}, x.options());

    const int threads = 256;
    const int blocks = out_features;

    compute_mean<<<blocks, threads>>>(
        x.data_ptr<float>(),
        mean.data_ptr<float>(),
        batch_size,
        out_features
    );

    compute_var<<<blocks, threads>>>(
        x.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        eps,
        batch_size,
        out_features
    );

    return std::make_tuple