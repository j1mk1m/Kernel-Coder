The following operators are the ones you can choose to replace: 

1. The convolution operation (nn.Conv2d)
2. The tanh activation
3. The scaling (multiplication by a scalar)
4. The bias addition (addition of a tensor)
5. The max pooling (nn.MaxPool2d)

Please choose which operators to replace and implement their custom CUDA kernels, you can also combine some of them into a single kernel (operator fusion). 

You can also choose to implement all of them in one fused kernel. 

Note: The kernel implementation should be correct and efficient. You may want to take advantage of shared memory, thread coalescing, and other CUDA optimization techniques. 

Now, write the new code with custom CUDA operators.
Okay, I need to optimize the given Model architecture using custom CUDA kernels. The original model has several operations: convolution, tanh, scaling, bias addition, and max-pooling. The goal is to replace some or all of these with custom CUDA code to improve performance. 

First, I should consider which operators are worth replacing. Convolution is a compute-heavy operation, but implementing a custom Conv2D from scratch might be complex. However, perhaps combining it with the subsequent tanh, scaling, and bias addition could be beneficial. Similarly, the max-pooling might also be fused with those steps.

Wait, but fusing all of them into a single kernel could reduce memory access overhead. Let me think through the steps:

1. **Convolution**: The standard Conv2D involves a lot of computation. Custom implementation might use optimized memory access patterns and shared memory for better cache utilization. However, writing a full Conv2D kernel is non-trivial. Alternatively, maybe fusing it with the next operations could save time.

2. **Tanh Activation**: Applying tanh after convolution is straightforward. Since tanh is element-wise, it can be easily included in a kernel.

3. **Scaling (multiplication by scalar)**: Also element-wise, so combining this with tanh makes sense.

4. **Bias Addition**: The bias is a tensor of shape (out_channels, 1, 1), so it's added to each channel. This can be incorporated into the same kernel as the previous steps.

5. **Max-Pooling**: This is a separate operation that might be fused with the previous steps or handled in a separate kernel. However, max-pooling requires reading a window of values, which could be done efficiently if the data is already in shared memory from the convolution step.

Hmm, maybe it's better to separate the convolution from the rest. Alternatively, fuse the convolution with tanh, scaling, and bias addition into a single kernel. The max-pooling could be another kernel, possibly fused with the previous steps.

Alternatively, perhaps the max-pooling can be a separate kernel since it's a spatial operation. Let me outline possible approaches.

Option 1: Fused Convolution + Tanh + Scaling + Bias Addition.

Option 2: Fused all except max-pooling, then separate max-pool.

Option 3: Each step as separate kernels but optimized.

Starting with the first option, let's see. The convolution's output is then processed with tanh, scaled, and bias added. If I can compute all these steps in a single kernel after the convolution's computation, that might save memory bandwidth.

But implementing a custom convolution requires handling the input padding, stride, and the kernel weights. Since the original model uses nn.Conv2d, perhaps the weights and biases are already handled. Wait, in the original model, the bias is a separate parameter added after scaling. Wait, the model's forward has:

x = self.conv(x)  # this includes the convolution's bias if any?

Wait, no. The Conv2d layer has its own bias parameter. The original code defines the bias as a separate parameter (self.bias) added later. So the conv's bias is not used here. Wait, checking the Model's __init__: the Conv2d is initialized without specifying bias, so by default, it includes a bias term? Wait, the code has:

self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

By default, nn.Conv2d has bias=True unless specified otherwise. Wait, the user might have intended to use their own bias, but in the current code, the Conv2d's bias is present. Wait, the original code adds self.bias after scaling. So perhaps the model's Conv2d has its own bias, and then an additional bias is added. That's possible, but maybe that's an oversight. But the user's code says:

The model's description says: "applies tanh, scaling, adds a bias term". So the bias term is the self.bias, and the Conv2d may or may not have its own bias. Wait, the code's __init__ for the Model has the Conv2d without specifying bias, so it will have its own bias. So when the user writes "x = self.conv(x)", that includes the convolution and the Conv2d's bias. Then, after scaling and tanh, they add their own bias (self.bias). So the self.bias is a separate parameter. 

This complicates things. But for the purposes of optimization, perhaps the original code's self.bias is indeed a separate tensor. So, the process is:

conv (with its own bias) -> tanh -> scaling (by scaling_factor) -> add self.bias -> max_pool.

Wait, the order in the forward is:

x = self.conv(x)  # includes convolution and its own bias?

Then tanh, scaling (multiply by scaling_factor), then add self.bias, then max-pool.

Wait, the code's forward:

x = torch.tanh(x) after the conv, then scaling (x *= scaling_factor), then x += self.bias, then max_pool.

So the sequence is:

Convolution (including its own bias) -> tanh -> scaling (scalar multiplication) -> adding a tensor bias (self.bias) -> max_pool.

So the self.bias is a tensor of shape (out_channels, 1, 1). So when adding, it's broadcasted to match the output dimensions.

Now, the problem is to optimize this sequence. The key points are the operations after the convolution. Since the convolution is a heavy operation, maybe fusing it with the subsequent steps can help.

Alternatively, since the subsequent steps (tanh, scaling, bias addition) are all element-wise or channel-wise, perhaps they can be done in the same kernel as the convolution. Let me think of the steps:

The convolution produces an output tensor. Then, for each element, we apply tanh, multiply by scaling_factor, and add the bias (which is per-channel, since it's (out_channels, 1, 1)).

Wait, the bias is per-channel, so for each element in the output, the bias added is the value of self.bias[channel][0][0]. So, for a given output element (n, c, h, w), the bias is self.bias[c][0][0].

Therefore, after computing the convolution's output (including its own bias?), we need to:

1. Apply tanh to each element.

2. Multiply each element by scaling_factor.

3. Add the bias for its channel.

These can all be done in a single kernel after the convolution's computation.

Alternatively, if the convolution's output is computed and stored, then we can have a separate kernel for the tanh, scaling, and bias addition. But combining them into a single kernel would be more efficient.

However, implementing the convolution itself is complex. Since the user wants to replace operators, maybe the best approach is to first see which parts can be optimized.

Alternatively, maybe the user is allowed to replace the convolution with a custom one, but that's quite involved. Alternatively, perhaps fusing the tanh, scaling, and bias addition into a single kernel, and leaving the convolution as is. But that's a small optimization.

Alternatively, perhaps replacing the max_pooling with a custom kernel is easier. The max_pool is a 2x2 or larger kernel. Implementing a max-pool kernel could be manageable.

Alternatively, perhaps fusing the tanh, scaling, bias addition, and max-pooling into one kernel. But that might be complex.

Alternatively, let me consider which operators are most compute-heavy:

Convolution is by far the most expensive. However, writing a custom convolution is a lot of work, especially for a general 2D case. The user might not expect that, unless they have a specific structure.

Alternatively, maybe the user wants to focus on fusing the element-wise operations after convolution into a single kernel.

Let me think of the sequence after convolution:

- tanh(x) (element-wise)

- scaling: x = x * scaling_factor (element-wise)

- bias addition: x += self.bias (per-channel, so the bias is broadcasted)

These three can be done in a single kernel. Let's see.

The element-wise operations can be combined. So a kernel that takes the convolution output, applies tanh, scales, adds the bias, and returns the result. Then, the max_pooling can be a separate step. However, the max-pooling could also be fused into the same kernel, but that would complicate the kernel, since max-pooling requires reading a window of values.

Alternatively, the max-pooling can be a separate custom kernel.

So perhaps the best approach is:

1. Replace the tanh, scaling, and bias addition with a single fused kernel.

2. Replace the max-pooling with a custom kernel.

Alternatively, the max_pool could be replaced with a fused kernel with the previous steps.

Let me start with the element-wise part first.

The element-wise operations:

For each element in the tensor:

result = tanh( (conv_out * scaling_factor) + bias )

Wait, no, the order is tanh then scaling then adding bias?

Wait, in the original code:

x = self.conv(x) → tanh(x) → x * scaling_factor → x + self.bias.

Wait, the sequence is:

After convolution, tanh is applied, then scaled by scaling_factor (multiply), then add the bias tensor.

Wait, let's recheck:

Original forward:

x = torch.tanh(x) → applies to the convolution output.

Then x = x * scaling_factor → scales the tanh result.

Then x = x + self.bias → adds the bias to each element, but since the bias is (out_channels, 1, 1), it's per-channel.

So for each element (n, c, h, w), the value is tanh(conv_out) * scaling_factor + bias[c][0][0]

Therefore, the three operations can be combined into a single element-wise kernel.

So, the fused kernel would take the convolution output (assuming it includes its own bias?), compute the tanh, multiply by scaling, add the per-channel bias.

Wait, the convolution's own bias is already part of the Conv2d's computation. So the input to this kernel is the output of the Conv2d, which includes its bias.

Therefore, the kernel can process each element in parallel. Since all these operations are element-wise (except the bias is per-channel), the kernel can loop over all elements.

This seems manageable. Let's proceed.

The kernel would take the input tensor (conv output), the scaling factor, the bias tensor, and compute the fused operations.

Additionally, the max-pooling can be another kernel. Alternatively, maybe the max-pool can be fused with this, but that requires reading a window of values.

Alternatively, let's first implement the fused tanh-scaling-bias kernel, then handle the max-pooling.

Also, the original code's Conv2d may have a bias. The user's code's Model's __init__ for the Conv2d doesn't specify bias, so by default, it has bias=True. So the self.bias in the code is an additional bias term. Therefore, the convolution's output already includes its own bias. So the fused kernel needs to process that output.

Now, writing the fused kernel:

First, the kernel function:

The input tensor is the result of the convolution (including its own bias). Let's call this tensor 'conv_out'.

The steps:

for each element (n, c, h, w):

out[n][c][h][w] = tanh( conv_out[n][c][h][w] ) * scaling_factor + bias[c][0][0]

Wait, the scaling is applied after tanh, so the order is important.

Yes, so tanh is applied first, then scaled, then add the bias.

Wait, the code's steps are:

x = torch.tanh(x) → then scaled → then add bias. So yes.

So in the kernel, for each element:

value = tanh( input_value )

scaled = value * scaling_factor

result = scaled + bias_value (where bias_value is bias[c][0][0])

Thus, the kernel can take the input tensor (conv_out), the scaling factor, the bias tensor, and compute this.

Now, the kernel will need to handle the bias which is (out_channels, 1, 1). So for any position (c, h, w) in the tensor, the bias for that channel is at index (c, 0, 0) in the bias tensor.

Therefore, in the kernel, for each element, the thread can compute the channel index and fetch the corresponding bias value.

The kernel can be written as follows:

__global__ void fused_tanh_scale_bias(
    const float* input, float* output,
    const float scaling_factor,
    const float* bias,
    int out_channels, int height, int width, int batch_size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // Compute the indices
    if (idx >= (batch_size * out_channels * height * width)) return;

    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % out_channels;
    int n = idx / (out_channels * height * width);

    // Get the input value
    float val = input[idx];

    // Apply tanh
    val = tanhf(val); // or use __tanhf for CUDA?

    // Scale
    val *= scaling_factor;

    // Get the bias for this channel
    int bias_idx = c * height * width; // since bias is (out_channels,1,1), so for each c, the first element is at c * (1*1) ?

    // Wait, the bias tensor is of shape (out_channels, 1, 1). So the stored indices are (c, 0, 0). So for any h and w, the bias is bias[c*1*1 + 0*1 + 0] ?

    // So the index in the bias array for channel c is c * (1*1) + 0*1 + 0 = c.

    float b = bias[c]; // because bias is stored as a 1D array?

    val += b;

    output[idx] = val;
}

Wait, the bias is a 3D tensor (out_channels, 1, 1). When passed as a flat array, the storage is contiguous. So the linear index for bias at channel c is c * (1 * 1) + 0 * 1 + 0 = c. Therefore, bias[c] is correct.

This kernel would process each element in parallel. The parameters needed are:

- input tensor (convolution output)

- scaling factor (a scalar)

- bias tensor (as a 1D array, or 3D but accessed via channel index)

- the dimensions (out_channels, height, width, batch_size)

Wait, but to compute the indices, the kernel needs to know the dimensions of the output tensor. The input tensor's dimensions are [batch_size, out_channels, H, W], where H and W depend on the convolution's parameters (kernel size, stride, padding). But the kernel can get these from the input tensor's shape.

Wait, in the kernel, the input's dimensions are fixed based on the input's shape. So the parameters passed would need to include the out_channels, height, width, batch_size to compute the indices correctly. Alternatively, we can compute the indices using the linear index and the strides.

Alternatively, the kernel can be written to process each element as a linear index, and compute the channel from that.

Therefore, the kernel would require the out_channels, height, width, and batch_size as parameters.

The kernel can be called with the input tensor (conv_out), scaling_factor, bias tensor, and the dimensions.

Now, the kernel must be wrapped in a launcher function.

Now, for the max-pooling. The user's model uses nn.MaxPool2d(pool_kernel_size). Let's assume pool_kernel_size is 4, as per the given parameters. Wait, the parameters are pool_kernel_size =4, so the kernel size is 4x4? The default stride is equal to the kernel size in max-pool, but the user's code uses the default parameters, so stride would be 4, padding 0?

Alternatively, the max_pool is initialized with kernel_size=4, so stride=4, padding 0, and dilation 1, etc. So the pooling window is 4x4, moving in steps of 4, so the output dimensions are reduced by a factor of 4 in each spatial dimension.

Implementing a custom max-pool kernel is manageable. The kernel would process each output pixel by looking at the corresponding 4x4 window in the input, find the maximum value.

To implement this, the kernel can be structured as follows:

Each thread block handles a block of output pixels. Each thread in the block computes a single output pixel by checking the corresponding window.

Alternatively, each thread can process a single output element. The grid and block dimensions would be based on the output's spatial dimensions.

Suppose the input after the previous steps has dimensions (batch_size, out_channels, H, W). After max-pooling with kernel size 4, the output will be (batch_size, out_channels, H/4, W/4). Assuming H and W are divisible by 4.

The kernel would loop over each batch, channel, output height, and output width, then for each output element, check the 4x4 window in the input and find the max.

Alternatively, using a tiled approach where each thread handles a small portion.

Let me sketch the max-pool kernel.

First, the input tensor is the output of the previous fused kernel. The output dimensions are (B, C, H, W). The max-pool with kernel size K=4 reduces the spatial dimensions by K.

The max-pool kernel:

__global__ void max_pool_2d(
    const float* input, float* output,
    int batch_size, int channels,
    int input_h, int input_w,
    int kernel_size,
    int output_h, int output_w)
{
    // Each thread handles one output element (n, c, oh, ow)
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * output_h * output_w)
        return;

    int ow = idx % output_w;
    int oh = (idx / output_w) % output_h;
    int c = (idx / (output_w * output_h)) % channels;
    int n = idx / (channels * output_h * output_w);

    float max_val = -INFINITY;

    // Iterate over the kernel window
    for (int kh = 0; kh < kernel_size; ++kh) {
        for (int kw = 0; kw < kernel_size; ++kw) {
            int ih = oh * kernel_size + kh;
            int iw = ow * kernel_size + kw;
            if (ih < input_h && iw < input_w) {
                float val = input[get_index(n, c, ih, iw, input_h, input_w)];
                if (val > max_val)
                    max_val = val;
            }
        }
    }

    // Write the result
    output[get_index(n, c, oh, ow, output_h, output_w)] = max_val;
}

Wait, need a helper function to compute the linear index. Since the input is in (N, C, H, W) order, the linear index would be:

int get_index(int n, int c, int h, int w, int H, int W) {
    return n * C*H*W + c * H*W + h * W + w;
}

Alternatively, the dimensions can be passed as parameters.

Alternatively, to avoid the helper function, compute inline:

The input tensor has dimensions (B, C, H, W). So the stride for batch is C*H*W, for channel is H*W, for height is W, and width is 1.

Thus, the linear index for (n, c, h, w) is:

n * (C * H * W) + c * (H * W) + h * W + w.

But in the kernel, the input is a 1D array, so the code can compute it directly.

This kernel would process each output element in parallel. For each output element, it loops over the kernel window (4x4) to find the maximum.

The kernel_size is 4, so for each element, 4x4=16 elements are checked. Since this is small, it's manageable.

Now, putting this together.

The user's ModelNew would replace the tanh, scaling, bias addition, and max-pooling with the custom kernels.

But the convolution itself is still using PyTorch's nn.Conv2d. The user might want to replace that too, but that's more involved.

However, the question allows replacing any subset of the operators. Since the user's example replaced the element-wise add, which is small, but here the element-wise steps after convolution can be fused into a kernel, and the max-pool can be replaced with a custom kernel.

So, the plan is:

- Keep the convolution as is (using PyTorch's implementation).

- Replace the tanh, scaling, and bias addition with a fused kernel.

- Replace the max-pooling with a custom kernel.

This should give some speedup.

Now, let's code this step by step.

First, the fused kernel:

Implementing the fused_tanh_scale_bias kernel.

The launcher function in C++:

The kernel needs to take the input tensor, scaling factor, bias, and output tensor.

The parameters for the kernel are:

- input: tensor of shape (B, C, H, W)

- scaling_factor: float

- bias: tensor of shape (C, 1, 1) → stored as a 1D array of size C (since 1x1 is redundant)

Wait, in PyTorch, the bias is a 3D tensor (C,1,1). To pass it to the kernel, we can pass it as a 1D array, so we need to make sure it's contiguous and of the correct size.

The kernel's parameters are:

input (float*), output (float*), scaling_factor (float), bias (float*), out_channels (int), height (int), width (int), batch_size (int)

The launcher function:

torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias)
{
    auto output = torch::empty_like(input);
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int height = input.size(2);
    const int width = input.size(3);

    const int block_size = 256;
    const int num_elements = batch_size * channels * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    // Launch kernel
    fused_tanh_scale_bias<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        bias.data_ptr<float>(),
        channels, height, width, batch_size);

    return output;
}

Wait, the bias tensor must be a 1D tensor of size channels. Let me confirm:

The bias in the original code is a parameter of shape (out_channels,1,1). To pass it to the kernel, we can view it as a 1D tensor of size out_channels.

So in the Python code, before calling the kernel, we need to ensure that the bias is contiguous and of the correct type.

Now, the max-pool kernel.

Implementing the max_pool_2d kernel.

The kernel requires the input dimensions and the kernel size. Since the user's model uses pool_kernel_size=4, which is fixed, but in the code, the kernel can take it as a parameter.

Wait, the kernel_size is passed as an argument.

The launcher function for the max-pool:

torch::Tensor max_pool_cuda(
    torch::Tensor input,
    int kernel_size,
    int output_h,
    int output_w)
{
    auto output = torch::empty({input.size(0), input.size(1), output_h, output_w}, input.options());
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int input_h = input.size(2);
    const int input_w = input.size(3);

    const int block_size = 256;
    const int num_elements = batch_size * channels * output_h * output_w;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    max_pool_2d<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels,
        input_h, input_w,
        kernel_size,
        output_h, output_w);

    return output;
}

But how do we compute output_h and output_w?

In the original model, the max_pool is initialized with pool_kernel_size=4. The output dimensions depend on the input's spatial dimensions after the previous layers.

Wait, the input to the max-pool is the output of the previous steps (after convolution, tanh, scaling, bias). Let's denote that tensor as 'x' with shape (B, C, H, W). The max_pool with kernel_size=4 will output (B, C, H//4, W//4) assuming no padding and stride equal to kernel_size.

Therefore, in the Python code, when calling the max-pool function, the output_h and output_w can be computed as:

output_h = H // kernel_size

output_w = W // kernel_size

Therefore, in the ModelNew's forward function, after computing the previous steps, the dimensions can be calculated.

But when implementing the custom kernel, we need to pass these values.

Now, putting all together in the Python code.

First, define the CUDA kernels.

The fused kernel code:

The CUDA code for fused_tanh_scale_bias:

#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_tanh_scale_bias(
    const float* input, float* output,
    float scaling_factor,
    const float* bias,
    int channels, int height, int width, int batch_size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * height * width) return;

    // Compute indices
    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % channels;
    int n = idx / (channels * height * width);

    // Get input value
    float val = input[idx];

    // Apply tanh
    val = tanhf(val);

    // Scale
    val *= scaling_factor;

    // Add bias (bias is stored as 1D array of size channels)
    float b = bias[c];
    val += b;

    output[idx] = val;
}

torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias)
{
    // Ensure input and bias are on the same device
    auto output = torch::empty_like(input);
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int height = input.size(2);
    const int width = input.size(3);

    // Check that bias is 1D and has size channels
    TORCH_CHECK(bias.dim() == 1 && bias.size(0) == channels,
        "Bias must be a 1D tensor with size equal to number of channels");

    const int block_size = 256;
    const int num_elements = batch_size * channels * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_tanh_scale_bias<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        bias.data_ptr<float>(),
        channels, height, width, batch_size);

    return output;
}

The max-pool kernel:

__global__ void max_pool_2d(
    const float* input, float* output,
    int batch_size, int channels,
    int input_h, int input_w,
    int kernel_size,
    int output_h, int output_w)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * output_h * output_w) return;

    int ow = idx % output_w;
    int oh = (idx / output_w) % output_h;
    int c = (idx / (output_w * output_h)) % channels;
    int n = idx / (channels * output_h * output_w);

    float max_val = -FLT_MAX;

    for (int kh = 0; kh < kernel_size; ++kh) {
        for (int kw = 0; kw < kernel_size; ++kw) {
            int ih = oh * kernel_size + kh;
            int iw = ow * kernel_size + kw;
            if (ih < input_h && iw < input_w) {
                int input_idx = n * channels * input_h * input_w + c * input_h * input_w + ih * input_w + iw;
                float val = input[input_idx];
                if (val > max_val)
                    max_val = val;
            }
        }
    }

    // Write to output
    int output_idx = n * channels * output_h * output_w + c * output_h * output_w + oh * output_w + ow;
    output[output_idx] = max_val;
}

torch::Tensor max_pool_cuda(
    torch::Tensor input,
    int kernel_size,
    int output_h,
    int output_w)
{
    auto output = torch::empty({input.size(0), input.size(1), output_h, output_w}, input.options());
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int input_h = input.size(2);
    const int input_w = input.size(3);

    const int block_size = 256;
    const int num_elements = batch_size * channels * output_h * output_w;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    max_pool_2d<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels,
        input_h, input_w,
        kernel_size,
        output_h, output_w);

    return output;
}

Now, combining these into the Python code:

First, in the Python code, we need to load these kernels using load_inline. Since there are two separate kernels (fused_tanh_scale_bias and max_pool_cuda), we'll need to combine their source code.

Wait, the CUDA code for both kernels can be in a single string. Let me write the full CUDA source.

The full CUDA source code for the fused and max_pool kernels:

elementwise_fusion_and_pool_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// Fused kernel: tanh + scaling + bias addition
__global__ void fused_tanh_scale_bias(
    const float* input, float* output,
    float scaling_factor,
    const float* bias,
    int channels, int height, int width, int batch_size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * height * width) return;

    int w = idx % width;
    int h = (idx / width) % height;
    int c = (idx / (width * height)) % channels;
    int n = idx / (channels * height * width);

    float val = input[idx];
    val = tanhf(val);
    val *= scaling_factor;
    float b = bias[c];
    val += b;
    output[idx] = val;
}

// Max pooling kernel
__global__ void max_pool_2d(
    const float* input, float* output,
    int batch_size, int channels,
    int input_h, int input_w,
    int kernel_size,
    int output_h, int output_w)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * channels * output_h * output_w) return;

    int ow = idx % output_w;
    int oh = (idx / output_w) % output_h;
    int c = (idx / (output_w * output_h)) % channels;
    int n = idx / (channels * output_h * output_w);

    float max_val = -FLT_MAX;

    for (int kh = 0; kh < kernel_size; ++kh) {
        for (int kw = 0; kw < kernel_size; ++kw) {
            int ih = oh * kernel_size + kh;
            int iw = ow * kernel_size + kw;
            if (ih < input_h && iw < input_w) {
                int input_idx = n * channels * input_h * input_w + c * input_h * input_w + ih * input_w + iw;
                float val = input[input_idx];
                if (val > max_val)
                    max_val = val;
            }
        }
    }

    int output_idx = n * channels * output_h * output_w + c * output_h * output_w + oh * output_w + ow;
    output[output_idx] = max_val;
}

// Launcher functions
torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias)
{
    auto output = torch::empty_like(input);
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int height = input.size(2);
    const int width = input.size(3);

    TORCH_CHECK(bias.dim() == 1 && bias.size(0) == channels,
        "Bias must be a 1D tensor with size equal to number of channels");

    const int block_size = 256;
    const int num_elements = batch_size * channels * height * width;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    fused_tanh_scale_bias<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        bias.data_ptr<float>(),
        channels, height, width, batch_size);

    return output;
}

torch::Tensor max_pool_cuda(
    torch::Tensor input,
    int kernel_size,
    int output_h,
    int output_w)
{
    auto output = torch::empty({input.size(0), input.size(1), output_h, output_w}, input.options());
    const int batch_size = input.size(0);
    const int channels = input.size(1);
    const int input_h = input.size(2);
    const int input_w = input.size(3);

    const int block_size = 256;
    const int num_elements = batch_size * channels * output_h * output_w;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    max_pool_2d<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, channels,
        input_h, input_w,
        kernel_size,
        output_h, output_w);

    return output;
}
"""

Then, the CPP sources (header declarations for the launcher functions):

elementwise_fusion_and_pool_cpp = """
torch::Tensor fused_tanh_scale_bias_cuda(torch::Tensor input, float scaling_factor, torch::Tensor bias);
torch::Tensor max_pool_cuda(torch::Tensor input, int kernel_size, int output_h, int output_w);
"""

Then, in the Python code, load this inline:

elementwise_fusion_and_pool = load_inline(
    name="elementwise_fusion_and_pool",
    cpp_sources=elementwise_fusion_and_pool_cpp,
    cuda_sources=elementwise_fusion_and_pool_source,
    functions=["fused_tanh_scale_bias_cuda", "max_pool_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Now, in the ModelNew class:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape).squeeze(1).squeeze(1))  # convert to 1D tensor for the kernel
        self.pool_kernel_size = pool_kernel_size
        self.fused_tanh_scale_bias = elementwise_fusion_and_pool.fused_tanh_scale_bias_cuda
        self.max_pool_cuda = elementwise_fusion_and_pool.max_pool_cuda

    def forward(self, x):
        # Convolution
        x = self.conv(x)
        # Apply fused operations: tanh, scaling, bias addition
        x = self.fused_tanh_scale_bias(x, self.scaling_factor, self.bias)
        # Compute output dimensions for max-pooling
        output_h = x.size(2) // self.pool_kernel_size
        output_w = x.size(3) // self.pool_kernel_size
        # Max-pooling
        x = self.max_pool_cuda(x, self.pool_kernel_size, output_h, output_w)
        return x

Wait, but in the __init__ of ModelNew, the parameters are passed as