The code should be as concise as possible, but make sure the code is correct. 

When writing the code, please do the following:

    - Use the same order of operators as in the original architecture.

    - When possible, combine operators into a single fused kernel (for example, the example fused matmul+relu into a single kernel). For instance, the given architecture is Conv2D + GELU + adaptive_avg_pool2d + squeeze. You can choose to fuse the Conv2D and GELU into a single kernel, or fuse all the way except the squeeze, etc. It's up to you. 

    - The final code should work with the same get_inputs() and get_init_inputs() functions as the original code. 

    - The custom CUDA code must be inlined in the Python code (using torch.utils.cpp_extension.load_inline). 

    - Your code can call the native PyTorch functions for operators that you don't replace. 

    - The new code must not use any additional libraries (e.g., no cutlass, no thrust, etc) and must be self-contained. 

    - The new code must not introduce any new dependencies beyond PyTorch. 

    - The new code must be compatible with PyTorch 2.0 or newer. 

    - The code must use the same input and output shapes as the original architecture. 

    - The code should be as efficient as possible, but correctness is the priority. 

    - You can replace any operators except the Conv2d weights (since the user might want to keep the model parameters). 

    - The final fused kernel must be written in the most optimized way possible, with proper block and grid dimensions. 

    - Please use the same variable names as in the original code, but replace the PyTorch operators with your custom CUDA code.

    - The code must include the class definition of ModelNew, the get_inputs and get_init_inputs functions (if needed), and the custom CUDA code.

    - When writing the fused kernel, use float32 unless the original architecture uses a different dtype.

    - The kernel must be written for CUDA and must be efficient in terms of memory access and computation.

Now, proceed to optimize the given architecture with custom CUDA operators following the above guidelines.


<think>

Okay, I need to optimize the given PyTorch model by replacing some operators with custom CUDA kernels. The model's forward pass is Conv2d followed by GELU, then adaptive_avg_pool2d, and squeezing. The goal is to fuse these operations into a single kernel where possible for efficiency.

First, I'll look at the operators. The Conv2d is a big operation, but maybe I can combine it with GELU. The GELU activation can be applied right after the convolution computation, saving some memory and computation steps. The pooling and squeeze can probably stay separate since they are simpler, but maybe they can be combined with the GELU. Wait, the adaptive_avg_pool2d reduces the spatial dimensions to 1, so after convolution and GELU, the pooling is just averaging over the remaining spatial dimensions. Maybe combining all three (conv, gelu, pool) into a single kernel would be better.

Wait, the Convolution involves a lot of computation. Implementing a custom convolution is tricky because it's a complex operation with many parameters. Doing that from scratch might not be efficient. Maybe it's better to keep the convolution as a PyTorch op and fuse GELU and pooling? Or maybe just fuse GELU with the next steps. Alternatively, perhaps the GELU and pooling can be fused into the convolution's output. Hmm.

Alternatively, maybe the GELU can be fused with the convolution's computation. Since GELU is an element-wise function, after computing the convolution, applying GELU is straightforward. However, the convolution itself requires a lot of memory accesses and computations. Implementing a custom convolution kernel would be time-consuming and might not be worth it unless there's a significant gain. Since the user wants speedups, maybe it's better to focus on fusing GELU and pooling into a single kernel after the convolution.

Wait, the pooling is adaptive average over the entire spatial dimensions. So after the convolution and GELU, the tensor is (batch, out_channels, H, W). The adaptive_avg_pool2d(1) will average each channel across H and W, resulting in (batch, out_channels, 1, 1), then squeezing gives (batch, out_channels). So maybe combining GELU and the pooling into one kernel would be better.

Alternatively, can we compute the convolution, apply GELU, and then compute the average in the same kernel? That might be possible. Let's see:

The plan is to create a fused kernel that takes the input tensor, applies the convolution (using PyTorch's conv, but maybe not?), but wait, the user says we can replace any operators except the Conv2d weights. Wait the original model uses nn.Conv2d, which has parameters. The user says "replace the pytorch operators in the given architecture to get speedups" but "the user might want to keep the model parameters". So the convolution's weights can't be replaced, but the operator (the forward pass) can be replaced. Wait, the Conv2d's forward is a PyTorch operator. So maybe we can replace the entire forward pass of the convolution with a custom kernel that includes the GELU and pooling? Wait but the convolution's weights are part of the model's parameters. So the custom kernel would need to take the weights and bias as inputs, but that's possible.

Alternatively, the problem is that implementing a custom convolution is complicated. Maybe the best approach is to keep the convolution as is (using PyTorch's conv2d) but fuse GELU and pooling into a single kernel. Let me think about the steps:

Original steps:

x = self.conv(x) → convolves, applies weights and bias. This is a PyTorch op.

x = torch.nn.functional.gelu(x) → element-wise GELU activation.

x = F.adaptive_avg_pool2d(x, 1) → averages spatial dimensions.

x.squeeze → removes the last two dimensions.

The problem is that the GELU and pooling are separate steps. To fuse them into a single kernel, we can process each element after applying GELU and accumulate for pooling.

But the adaptive_avg_pool2d is equivalent to average over all spatial dimensions. So for each output channel, the result is the average over H and W.

So for the fused kernel, after applying GELU to the convolution output, we can compute the average in the same step. That way, we can avoid an intermediate storage for the GELU output, which saves memory and computation.

So the plan is:

1. Compute the convolution using PyTorch's conv2d (since replacing that with custom code is complex and maybe not worth it unless there's a big gain).

2. Then, apply a custom kernel that does GELU followed by the adaptive_avg_pool2d.

Wait, but the user wants to replace the operators in the given architecture. The original uses conv, gelu, and pool. So the GELU and pool can be fused into a single kernel.

Therefore, the steps would be:

- Keep the convolution as is (using PyTorch's Conv2d).

- Replace GELU and adaptive_avg_pool2d with a fused kernel that takes the convolution output, applies GELU, then computes the average over H and W, then returns the squeezed tensor.

This would save some memory and computation steps.

Now, to write that kernel:

The input to the kernel is the output of the convolution (shape B, C, H, W).

The output is B x C tensor.

The kernel needs to process each element of the input tensor (B, C, H, W):

- Apply GELU to each element: gelu(x) = x * 0.5 * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))).

But implementing GELU in CUDA might be a bit involved. Alternatively, use an approximation.

Alternatively, compute the GELU and then accumulate over H and W for each channel.

Wait, the GELU is applied element-wise, then the average over H and W is taken for each channel and batch. So for each channel c in C, and each batch b, the output[b, c] = (sum over h and w of gelu(x[b, c, h, w])) / (H * W).

Therefore, the kernel can process each element, compute the GELU, then accumulate into a per-channel sum, then divide by H*W.

So the steps for the kernel:

- For each element in the input tensor (after conv), compute GELU.

- Sum over H and W dimensions for each (b, c).

- Then divide by H*W.

But how to structure this in CUDA.

Possible approach:

The kernel can be written to process all elements in parallel. Each thread could handle one (b, c, h, w) element. But then the reduction over H and W would require atomic operations, which can be slow. Alternatively, use a grid that groups spatial dimensions and reduces per channel and batch.

Alternatively, use a grid where each block handles a (batch, channel), and threads process spatial elements.

Let me think of the dimensions:

Suppose the input is of size B x C x H x W.

Each output element is B x C.

So the kernel can be structured as follows:

- Each thread block handles a single (B, C) element.

- Each thread in the block handles a spatial position (h, w).

- The block computes the sum over h and w for that (B, C).

But how to handle the threads:

Alternatively, for each element (b, c, h, w):

Each thread can process a batch and channel, but need to handle all h and w.

Alternatively, for each batch and channel, the spatial dimensions can be divided among threads.

Alternatively, use a grid where each block is responsible for a single (b, c). The block then has enough threads to cover all h and w positions.

The block would have a shared memory array to accumulate the sum for that (b,c).

Each thread in the block takes a spatial position (h, w) and computes gelu(input[b,c,h,w]) and adds it to the shared sum.

Then, after all threads have done their part, the block computes the average and stores it in the output.

But this requires that the spatial dimensions (H x W) can be divided among threads in the block. Since H and W are 256 each, H*W = 65536. That's a big number, so each block would need a lot of threads. Since the maximum number of threads per block is 1024 (or 1024 for SM 3.5 and above), but 65536 is way larger. So this approach may not be feasible.

Alternative approach: Use a grid where each thread block handles a single channel and batch. Then, the threads in the block can process spatial elements in a loop.

Wait, perhaps a better approach is to have each thread handle a (b, c) and each thread compute the sum over all h and w for that (b,c). But how?

Alternatively, split the work so that each thread handles a portion of the spatial dimensions.

Alternatively, structure the problem as:

The kernel is launched with a grid of B * C blocks, each block is responsible for computing the sum for one (b, c) pair.

Each block has a number of threads, say 256.

The block's threads will loop over the spatial positions (h and w). Since H and W are 256 each, the total is 256*256 = 65536 elements per (b,c).

But 65536 is a lot, so each thread can process 65536 / 256 = 256 elements. So each thread can loop over 256 elements.

The steps:

For a given block (b, c):

- Initialize a shared memory variable to 0.

- Each thread processes a chunk of spatial positions:

   For each thread in the block (tid from 0 to 255):

      For i in 0 to 255 (since 256 elements per thread):

          h = (tid * 256 + i) / W

          w = (tid * 256 + i) % W

          Or wait, perhaps better to index (h, w) in a way that loops over all spatial positions.

Alternatively, each thread can process a spatial index:

total = H * W

per_thread = total / blockDim.x (which is 256)

start = tid * per_thread

end = start + per_thread

But this may not be exact, but for simplicity.

But in CUDA, handling this with loops and shared memory can work.

Wait, but 65536 elements divided by 256 threads gives 256 elements per thread. Each thread can loop over 256 elements.

The steps per block would be:

- Each thread loads its chunk of spatial positions, computes gelu(x[b,c,h,w]), accumulates into a per-thread partial sum.

- Then, the block reduces the per-thread partial sums into a shared variable.

- Finally, the block writes the average to the output.

This is manageable.

The GELU computation:

The GELU function can be approximated as 0.5 * x * (1 + tanh(sqrt(2/pi)*(x + 0.044715*x^3))).

Alternatively, use the exact formula. Since we are in CUDA, we can compute this for each element.

So the code outline:

__global__ void fused_gelu_avgpool_kernel(

    const float* input, float* output,

    int B, int C, int H, int W) {

    // Each block handles a (b, c) pair.

    int b = blockIdx.x / C;

    int c = blockIdx.x % C;

    // Each thread in the block processes some spatial positions.

    extern __shared__ float shared[];

    int tid = threadIdx.x;

    float sum = 0.0f;

    for (int idx = tid; idx < H*W; idx += blockDim.x) {

        int h = idx / W;

        int w = idx % W;

        float x = input[ get_offset(b, c, h, w) ]; // need to figure out the strides.

        // Compute GELU:

        float y = x * 0.5f * (1.0f + tanhf(sqrtf(2.0f/3.141592653589793f)*(x + 0.044715f*x*x*x)));

        sum += y;

    }

    // Write to shared memory.

    // Then perform reduction in the block.

    // Using shared memory for reduction.

    // Assuming blockDim.x is 256, which is a power of two.

    // Use a reduction approach with shared memory.

    shared[tid] = sum;

    __syncthreads();

    // perform reduction in the block.

    // ... (reduction steps)

    if (tid == 0) {

        // sum all the shared values.

        float total = 0.0f;

        for (int i =0; i < blockDim.x; i++) {

            total += shared[i];

        }

        // compute average and write to output.

        float avg = total / (H * W);

        output[get_output_offset(b, c)] = avg;

    }

}

Wait, but the input's memory layout is B x C x H x W. So the offset for input[b][c][h][w] would be calculated as:

int stride_b = C*H*W;

int stride_c = H*W;

int stride_h = W;

int offset = b * stride_b + c * stride_c + h * stride_h + w;

Alternatively, assuming contiguous memory (row-major order):

The strides would be:

input is stored as a 4D tensor, but in memory it's flattened.

Assuming the order is B, C, H, W, the offset for (b,c,h,w) would be:

b * (C*H*W) + c*(H*W) + h*W + w.

So in code, input is a pointer to the first element.

So for a given b, c, h, w:

float x = input[ b * C*H*W + c*H*W + h*W + w ];

But in CUDA, accessing this could be optimized with shared memory if possible, but for this kernel, since each block is processing a single (b,c), perhaps we can precompute the base offset for that (b,c):

base_offset = b * C * H * W + c * H * W;

Then, for h and w:

offset = base_offset + h*W + w;

So the loop over idx can iterate over all elements in H*W, and compute h and w accordingly.

Now, the shared memory reduction: since each thread has computed a partial sum, they are stored in shared[tid]. Then, the block needs to sum all the shared elements.

The reduction can be done with a loop, but in the kernel, to do this efficiently, we can use a binary reduction in shared memory.

Alternatively, since the number of threads is 256, which is a power of two, we can perform a reduction in steps:

for (int s=blockDim.x/2; s>0; s>>=1) {

    if (tid < s) {

        shared[tid] += shared[tid + s];

    }

    __syncthreads();

}

But this requires multiple steps. However, since blockDim.x is 256, log2(256) =8 steps. It's manageable.

Wait, but in this case, each thread is contributing a partial sum. So the initial partial sums are in shared[tid]. After the reduction loop, shared[0] will hold the total sum.

Alternatively, the code can be written as follows:

After each thread writes their partial sum to shared[tid], then perform the reduction.

But perhaps the initial approach with a loop over all threads in the block is acceptable for small block sizes.

Wait, the block size is 256 threads. To sum all the 256 elements in shared memory, a simple loop:

float total = 0.0f;

for (int i =0; i < blockDim.x; i++) {

    total += shared[i];

}

But this would require a loop of 256 iterations, which is 256 operations. On a GPU, this might be slow for each thread. Alternatively, only the thread 0 could do this, but that would require synchronization.

Wait, in the code above, only thread 0 writes to the output. So perhaps after the shared memory is written, we can have thread 0 compute the total.

Wait, let me restructure:

Each thread first computes their partial sum, writes to shared memory.

Then, all threads synchronize.

Then, if (threadIdx.x ==0), compute the total by looping over all elements in shared.

Then, write the total / (H*W) to output.

This approach would have thread 0 doing O(256) work, which is manageable since 256 is small.

So the code outline would be:

Inside the kernel:

...

sum = ...;

shared[tid] = sum;

__syncthreads();

if (tid ==0) {

    float total =0.0f;

    for (int i=0; i<blockDim.x; ++i) {

        total += shared[i];

    }

    total /= (H * W);

    output[ output_offset ] = total;

}

This way, only thread 0 does the summation and writes the output.

This approach works.

Now, the kernel needs to be launched with grid size B*C, block size 256, and shared memory per block equal to the block size (floats).

Wait, the shared memory array size is blockDim.x (256) floats, so the shared memory allocation should be 256 * sizeof(float).

Now, let's code this.

But also, need to handle the input and output dimensions.

In the Python code:

The output tensor from the convolution is of shape (B, C, H, W).

The fused kernel takes this as input and produces (B, C).

The kernel needs to know B, C, H, W. These can be passed as parameters.

Now, let's code the CUDA kernel.

First, the function to call the kernel:

torch::Tensor fused_gelu_avgpool_cuda(torch::Tensor input) {

    // Get the dimensions.

    int B = input.size(0);

    int C = input.size(1);

    int H = input.size(2);

    int W = input.size(3);

    // The output is B x C.

    auto output = torch::empty({B, C}, input.options());

    const int block_size = 256;

    const dim3 grid(B * C); // Each block handles a (b, c)

    const dim3 block(block_size);

    // Compute shared memory needed: block_size * sizeof(float)

    int shared_size = block_size * sizeof(float);

    // Launch the kernel

    fused_gelu_avgpool_kernel<<<grid, block, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W);

    return output;

}

Now, the kernel function:

__global__ void fused_gelu_avgpool_kernel(
    const float* input, float* output,
    int B, int C, int H, int W) {

    int block_idx = blockIdx.x;

    int b = block_idx / C;

    int c = block_idx % C;

    int tid = threadIdx.x;

    extern __shared__ float sdata[];

    float sum = 0.0f;

    for (int idx = tid; idx < H * W; idx += blockDim.x) {

        int h = idx / W;

        int w = idx % W;

        int offset = b * C * H * W + c * H * W + h * W + w;

        float x = input[offset];

        // Compute GELU

        const float sqrt_2_over_pi = 0.7978845608f;

        const float approximation_term = 0.044715f;

        float inner = sqrt_2_over_pi * (x + approximation_term * x * x * x);

        float tanh_val = tanhf(inner);

        float gelu_val = x * 0.5f * (1.0f + tanh_val);

        sum += gelu_val;

    }

    sdata[tid] = sum;

    __syncthreads();

    if (tid == 0) {

        float total = 0.0f;

        for (int i =0; i < blockDim.x; i++) {

            total += sdata[i];

        }

        int output_offset = b * C + c; // Assuming output is B*C in a contiguous array.

        output[output_offset] = total / (H * W);

    }

}

Wait, output's layout is B x C, so the output_offset would be b * C + c. Since the output is a 1D array? Or is it 2D? Actually, in Torch, a 2D tensor with B rows and C columns would have the same storage as a 1D tensor of B*C elements. So the calculation is correct.

Now, in the Python model:

The forward function would be:

x = self.conv(x)

x = fused_gelu_avgpool_cuda(x)

return x

Wait, but the original code has:

x = self.conv(x)

x = F.gelu(x)

x = F.adaptive_avg_pool2d(x, 1)

x = x.squeeze(-1).squeeze(-1)

So replacing those three lines with the fused kernel.

Thus, the ModelNew class would:

class ModelNew(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size):

        super().__init__()

        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)

        self.fused_gelu_avgpool = fused_gelu_avgpool  # loaded via load_inline

    def forward(self, x):

        x = self.conv(x)

        x = self.fused_gelu_avgpool.fused_gelu_avgpool_cuda(x)

        return x

Wait, but in the example provided earlier, the kernel is loaded as a module, and the functions are called via the module.

The code structure would be similar to the example:

Define the CUDA source code for the kernel, then load it with load_inline, then use the function in the model.

So putting it all together.

Now, check the parameters:

The get_init_inputs is [in_channels, out_channels, kernel_size], which is the same as the original. The ModelNew's __init__ should take these parameters and initialize the convolution layer, same as before.

Now, need to ensure that the CUDA kernel is properly inlined in the Python code.

Now, coding the CUDA source:

First, the CUDA source code for the kernel:

fused_gelu_avgpool_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <math.h>

#define THREADS_PER_BLOCK 256

__global__ void fused_gelu_avgpool_kernel(
    const float* input, float* output,
    int B, int C, int H, int W) {

    int block_idx = blockIdx.x;
    int b = block_idx / C;
    int c = block_idx % C;
    int tid = threadIdx.x;

    extern __shared__ float sdata[];

    float sum = 0.0f;

    for (int idx = tid; idx < H * W; idx += blockDim.x) {
        int h = idx / W;
        int w = idx % W;
        int offset = b * C * H * W + c * H * W + h * W + w;
        float x = input[offset];

        // Compute GELU
        const float sqrt_2_over_pi = 0.7978845608f;
        const float approximation_term = 0.044715f;
        float inner = sqrt_2_over_pi * (x + approximation_term * x * x * x);
        float tanh_val = tanhf(inner);
        float gelu_val = x * 0.5f * (1.0f + tanh_val);

        sum += gelu_val;
    }

    sdata[tid] = sum;
    __syncthreads();

    if (tid == 0) {
        float total = 0.0f;
        for (int i = 0; i < blockDim.x; ++i) {
            total += sdata[i];
        }
        int output_offset = b * C + c;
        output[output_offset] = total / (H * W);
    }
}

torch::Tensor fused_gelu_avgpool_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty({B, C}, input.options());

    const dim3 grid(B * C);
    const dim3 block(THREADS_PER_BLOCK);
    size_t shared_size = block.x * sizeof(float);

    fused_gelu_avgpool_kernel<<<grid, block, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W);

    return output;
}
"""

Then, the header and CPP source:

fused_gelu_avgpool_cpp = """
torch::Tensor fused_gelu_avgpool_cuda(torch::Tensor input);
"""

Then, loading the kernel:

fused_gelu_avgpool = load_inline(
    name="fused_gelu_avgpool",
    cpp_sources=fused_gelu_avgpool_cpp,
    cuda_sources=fused_gelu_avgpool_source,
    functions=["fused_gelu_avgpool_cuda"],
    verbose=True,
)

Now, the ModelNew class would have the Conv2d and the fused kernel:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.fused_gelu_avgpool = fused_gelu_avgpool  # the loaded module

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_gelu_avgpool.fused_gelu_avgpool_cuda(x)
        return x

Wait, but in the example, the elementwise_add was stored as an attribute, and called via self.elementwise_add.elementwise_add_cuda(a, b). Here, the fused kernel's function is called via self.fused_gelu_avgpool.fused_gelu_avgpool_cuda(x).

That's correct.

Now, need to check for any possible errors.

First, the kernel's grid is B*C blocks, each block handles a (b,c). Each block has THREADS_PER_BLOCK threads (256). The shared memory is 256 floats per block.

The loop over H*W is handled by each thread processing a chunk. The math expressions for GELU are correct.

The output offset is calculated as b*C + c, assuming that the output is a 1D tensor (but in Torch, when we create a tensor with torch.empty({B, C}), it's a 2D tensor, so the offset is correct.

Wait, the output tensor is B x C, so for output[b][c], the index is b * C + c, yes.

Another check: the input's strides. The code assumes that the input is stored in a contiguous manner with B, C, H, W dimensions. So the input must be contiguous. Since PyTorch's conv2d returns a contiguous tensor (assuming the input is contiguous), this should be okay. The get_inputs() function returns a random tensor, which is contiguous, so the code should work.

Now, testing if the kernel is correctly fused.

Original steps:

conv → gelu → avg_pool → squeeze.

The new code replaces the last three steps (gelu, avg_pool, squeeze) with the fused kernel.

The kernel's output is (B, C), same as the original code.

Thus, this should be correct.

Now, is there any possible optimization left?

Perhaps the convolution can also be fused, but that's a more complex task and might not be worth it unless necessary.

The user's instruction says to replace any operators except the Conv2d weights, so the convolution operator can be replaced if possible, but since that's complex, the current approach is acceptable.

Another point: the kernel's computation of GELU. The code uses an approximation term (0.044715) which is part of the GELU formula. The formula used here is the standard GELU implementation, so this should be correct.

Potential issues:

- The kernel uses tanhf, which is a standard function.

- The kernel may have a problem with very large H and W, but in the problem, H and W are 256 each, which is manageable.

- The kernel's block size is fixed at 256. Since the number of threads per block is 256, which is a common choice, but may need adjustment. However, the code should work as is.

- The shared memory size is computed correctly.

Thus, the code should be correct and functional.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused GELU + adaptive_avg_pool2d + squeeze kernel
fused_gelu_avgpool_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <math.h>

#define THREADS_PER_BLOCK 256

__global__ void fused_gelu_avgpool_kernel(
    const float* input, float* output,
    int B, int C, int H, int W) {

    int block_idx = blockIdx.x;
    int b = block_idx / C;
    int c = block_idx % C;
    int tid = threadIdx.x;

    extern __shared__ float sdata[];

    float sum = 0.0f;

    for (int idx = tid; idx < H * W; idx += blockDim.x) {
        int h = idx / W;
        int w = idx % W;
        int offset = b * C * H * W + c * H * W + h * W + w;
        float x = input[offset];

        // Compute GELU
        const float sqrt_2_over_pi = 0.7978845608f;
        const float approximation_term = 0.044715f;
        float inner = sqrt_2_over_pi * (x + approximation_term * x * x * x);
        float tanh_val = tanhf(inner);
        float gelu_val = x * 0.5f * (1.0f + tanh_val);

        sum += gelu_val;
    }

    sdata[tid] = sum;
    __syncthreads();

    if (tid == 0) {
        float total = 0.0f;
        for (int i = 0; i < blockDim.x; ++i) {
            total += sdata[i];
        }
        int output_offset = b * C + c;
        output[output_offset] = total / (H * W);
    }
}

torch::Tensor fused_gelu_avgpool_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty({B, C}, input.options());

    const dim3 grid(B * C);
    const dim3 block(THREADS_PER_BLOCK);
    size_t shared_size = block.x * sizeof(float);

    fused_gelu_avgpool_kernel<<<grid, block, shared_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W);

    return output;
}
"""

fused_gelu_avgpool_cpp = """
torch::Tensor fused_gelu_avgpool_cuda(torch::Tensor input);
"""

# Compile the fused kernel
fused_gelu_avgpool = load_inline(
    name="fused_gelu_avgpool",
    cpp_sources=fused_gelu_avgpool_cpp,
    cuda_sources=fused_gelu_avgpool_source,
    functions=["fused_gelu_avgpool_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.fused_gelu_avgpool = fused_gelu_avgpool

    def forward(self, x):
        x = self.conv(x)
        x = self.fused_gelu_avgpool.fused_gelu_avgpool_cuda(x)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
```