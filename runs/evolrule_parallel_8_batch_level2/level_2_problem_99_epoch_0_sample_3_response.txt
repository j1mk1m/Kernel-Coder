Note: Your implementation may replace parts of the code with your own kernels, but must retain the same functionality. 

You should also use the same method as the example to define the kernels. 

Your code may also choose to use the torch's existing CUDA kernels (i.e., not reimplement everything from scratch). 

If you are unsure about some details, make a reasonable assumption and proceed. 

You should not use any third-party libraries, only PyTorch and CUDA. 

You may assume that the input tensors are on GPU. 

Make sure the output code is valid and can be directly copy-pasted into a Python file.

Understood, I'll proceed to optimize the given Model architecture by replacing some operators with custom CUDA kernels. 

First, let me analyze the existing operations in the forward pass:

1. Linear layer (matrix multiplication + bias addition)
2. GELU activation
3. Softmax

The goal is to see where custom CUDA kernels can provide speedups. 

Possible optimizations:
- **Fusing the linear layer (matmul + bias) into a single kernel**. This can reduce memory overhead and kernel launch overhead.
- **Implementing GELU with a custom kernel**. PyTorch's GELU might already be optimized, but a fused kernel with the linear could be better.
- **Optimizing Softmax**, especially if combined with previous steps.

However, given that the linear layer involves a matrix multiplication and bias addition, fusing those into one kernel could be beneficial. Let me consider that first.

The linear layer's computation is: output = x @ weight^T + bias

So, to fuse this into a single CUDA kernel, we can compute both the matrix multiplication and the bias addition in a single kernel.

Additionally, the GELU and Softmax could be part of the same fused kernel, but that might complicate things. Let me start with fusing the linear layer (matmul + bias) first.

Wait, but the current code uses torch.nn.Linear which already includes the bias. So perhaps the existing linear layer is already efficient, but maybe we can make it faster by writing a custom fused kernel.

Alternatively, perhaps the GELU and Softmax can be fused with the linear layer's output. Let me think about the steps:

The model is: 

x = linear(x) --> (matmul(x, weight) + bias)
x = GELU(x)
x = Softmax(x, dim=1)

So, perhaps fusing matmul + bias + GELU into a single kernel? Or even all three steps into a single kernel?

However, each step has different computations. Let me see if combining matmul + bias is feasible.

The matrix multiplication for a linear layer with input x (batch_size, in_features), weight (out_features, in_features), and bias (out_features) can be expressed as:

for each output element y[i][j] = sum_{k} x[i][k] * weight[j][k] + bias[j]

Therefore, the computation can be done per output element, but the summation is over in_features terms. 

Implementing this in a CUDA kernel would require efficient memory access patterns. However, writing a custom matmul with bias might be beneficial for small in_features and out_features? Wait, in the given problem, in_features and out_features are 8192, which are large. So perhaps the existing PyTorch implementation is already optimized (e.g., using cuBLAS). 

Hmm, then maybe the matrix multiplication is already as fast as possible. However, the bias addition might be a separate step. Let me think: the linear layer in PyTorch is implemented with a matmul followed by a bias addition. So perhaps those two steps can be fused into a single kernel, which would reduce kernel launch overhead and perhaps memory traffic.

Similarly, the GELU activation can be applied element-wise. So if we can combine the bias addition and GELU into a single kernel, that would save some steps. But first, let's check if the matmul is the main bottleneck.

Alternatively, perhaps the GELU and Softmax can be optimized. For example, using a custom implementation of Softmax to avoid some steps (like computing the max and then exponentiating, etc.). But the existing PyTorch implementation is probably optimized.

Alternatively, perhaps fusing GELU and Softmax is possible? Not sure, since they are different operations.

Another idea: the GELU function can be expressed as 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))). This requires some element-wise operations, so maybe a custom kernel can compute this efficiently, especially if combined with the bias.

Alternatively, since the GELU is applied element-wise, perhaps a custom kernel can be written to compute the entire sequence: matmul + bias + GELU in a single kernel. But the matmul part is a reduction over in_features elements, which complicates things. 

Alternatively, perhaps fusing the linear layer's bias addition with GELU. Let's see:

After the matmul (which is x @ weight.T), we have to add bias, then apply GELU. So the sequence is:

y = matmul(x, weight) + bias

y = GELU(y)

So the GELU is element-wise. So perhaps the matmul is handled by cuBLAS, then the bias addition and GELU can be done in a single fused kernel. That would save one kernel launch.

Similarly, the Softmax can be done in another kernel. 

Alternatively, if the matmul is the main computation, perhaps it's not worth replacing, but maybe the bias + GELU can be fused. Let me proceed with that.

First, let me think of the steps:

1. Compute the matrix multiplication using PyTorch's matmul (since it's already using cuBLAS, which is optimized), then add bias and apply GELU in a custom kernel.

2. Then apply softmax.

Alternatively, maybe the GELU can be optimized by combining with the softmax? Probably not, since they are different functions.

Alternatively, perhaps the GELU and softmax can be done together? Not sure.

Let me first try to write a fused kernel for the bias addition and GELU.

The steps after the matmul are:

y = matmul_result + bias

y = GELU(y)

So, to fuse these into a single kernel:

We need to:

- Add the bias to each element of the matrix multiplication result (element-wise addition of the bias vector to each row)
- Apply the GELU to each element

This can be done in a CUDA kernel that takes the matmul_result, the bias, and applies both steps in a single pass.

Similarly, the softmax can be done in another kernel.

Alternatively, perhaps the GELU and softmax can be done in a single kernel.

But let's start with the first idea.

First, the matmul is done via PyTorch's linear layer, which is efficient. Then, the bias addition and GELU can be fused.

Wait, but the existing linear layer already includes the bias addition. Wait, no: the linear layer's implementation is:

linear(x) = x @ weight.T + bias. So if we use the PyTorch linear layer, then the bias is already added. Therefore, perhaps the GELU can be replaced with a custom kernel.

Wait, but the model is:

x = self.linear(x) --> which already includes the matmul and bias addition.

Then x is passed through GELU and softmax.

Therefore, the GELU is an element-wise function, so a custom kernel could be written for that, perhaps with some optimizations.

Alternatively, perhaps the GELU and Softmax can be combined into a single kernel to save memory and computation steps.

Alternatively, perhaps the GELU can be implemented in a more efficient way in a CUDA kernel.

First, let me check PyTorch's GELU implementation. The standard GELU is:

GELU(x) = x * 0.5 * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))

This requires a few operations per element. Implementing this in a CUDA kernel might be straightforward, and perhaps faster than the PyTorch implementation if it's not already optimized.

Similarly, the Softmax is:

softmax(x) = exp(x_i) / sum(exp(x_j)) over dim 1.

The standard implementation computes the max, subtracts it for numerical stability, then exponentiates, sums, and divides. 

Implementing a custom Softmax kernel could also be beneficial.

Alternatively, perhaps fusing GELU and Softmax into a single kernel would save time. Let's see:

The steps would be:

y = linear(x)

y = GELU(y)

y = Softmax(y, dim=1)

So, after the linear layer, each element is processed by GELU, then the entire row (dim=1) is processed by Softmax.

Therefore, fusing GELU and Softmax into a single kernel would require:

1. For each element in the row, compute GELU(y_i).

2. Then compute the Softmax over the row.

But since GELU is applied first, the order can't be changed. So, perhaps it's possible to first apply GELU and then compute the Softmax in the same kernel.

However, the Softmax requires a reduction (summing exponentials over the dimension), which complicates things. 

Alternatively, the GELU could be applied first in a kernel, then the Softmax in another kernel.

Alternatively, combining the GELU and Softmax steps into a single kernel might be challenging due to the reduction in Softmax.

Alternatively, perhaps the GELU can be optimized by using a faster approximation or algorithm.

Given that, perhaps the best approach is to replace the GELU and Softmax with custom CUDA kernels, either separate or fused.

First, let's try to write a custom GELU kernel.

The GELU function can be implemented as follows for each element:

def gelu(x):
    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))

So in CUDA, for each element, compute this.

Alternatively, there is an approximation called the "tanh" approximation which is faster and can be written as:

gelu(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))

Alternatively, some implementations use a polynomial approximation for speed.

Assuming we use the standard formula, the CUDA kernel would compute this per element.

Similarly, the Softmax can be implemented with a kernel that first computes the max of the row, subtracts it, exponentiates, sums the exponentials, then divides each element by the sum.

This requires a reduction step for each row. However, implementing this in CUDA can be done with atomic operations, but that might be slow. Alternatively, using shared memory for the reduction.

Alternatively, using the standard approach in CUDA:

For Softmax along dim=1 (each row):

For each row:

1. Find the max value (to prevent overflow)

2. Subtract max from all elements in the row

3. Exponentiate each element

4. Sum all exponentials in the row

5. Divide each element by the sum

This requires multiple steps, but can be implemented in a kernel.

However, the reduction (steps 1, 4) may require multiple passes or using atomic operations, which can be slow. Alternatively, using shared memory for each block to handle a row.

Assuming that the batch size is 1024 and out_features is 8192, so each row has 8192 elements. So for each row (of 8192 elements), we need to compute the max, then the exponentials, then the sum.

Alternatively, perhaps using existing PyTorch functions for Softmax is better, but if we can make a custom kernel faster, that's better.

Alternatively, perhaps combining the GELU and Softmax steps into a single kernel could help.

Alternatively, let's proceed step by step.

First, let me write a custom kernel for GELU.

Then, a custom kernel for Softmax.

Alternatively, since the GELU and Softmax are both element-wise (except for the Softmax's reduction), perhaps writing separate kernels is manageable.

Let me outline the plan:

1. Use the existing PyTorch Linear layer for the matrix multiplication and bias addition. (Assuming that this is already as fast as possible, given that it uses cuBLAS.)

2. Replace the GELU with a custom CUDA kernel to compute the GELU function.

3. Replace the Softmax with a custom CUDA kernel.

Alternatively, perhaps the GELU and Softmax can be fused into a single kernel to save memory and computation time. Let's explore that.

But first, let me write the code step by step.

Starting with the GELU kernel:

The input is a tensor of shape (batch_size, out_features). For each element, compute GELU.

CUDA kernel for GELU:

__global__ void gelu_kernel(float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        float tanh_out = tanh(sqrt(2.0f / M_PI) * (x + 0.044715f * pow(x, 3)));
        output[idx] = 0.5f * x * (1.0f + tanh_out);
    }
}

Wait, but pow(x,3) is x*x*x. Maybe better to compute that manually for speed.

Alternatively, use x*x*x.

So, perhaps:

float x_cubed = x * x * x;

Then tanh_out = tanh(sqrt(2/pi) * (x + 0.044715*x_cubed))

Also, M_PI is a constant, but in CUDA, perhaps it's defined. Alternatively, use a numerical value, like 3.141592653589793.

Alternatively, precompute sqrt(2/pi) as a constant.

sqrt(2/pi) ≈ sqrt(0.6366197723675814) ≈ 0.7978845608

So, perhaps precompute that constant to avoid calculating sqrt(2/M_PI) each time.

So constants:

const float sqrt_2_over_pi = 0.7978845608f;

const float constant = 0.044715f;

Then, the expression inside tanh is sqrt_2_over_pi * (x + constant * x*x*x).

This can be computed more efficiently.

Thus, the kernel can be optimized as:

float x_cubed = x * x * x;

float inside = x + constant * x_cubed;

inside *= sqrt_2_over_pi;

float tanh_val = tanhf(inside);

output[idx] = 0.5f * x * (1.0f + tanh_val);

This should be faster.

Now, compiling this into CUDA.

Next, the Softmax kernel.

The Softmax requires:

For each row (dim=1), compute the max, then exponentiate each element minus the max, sum those, then divide each element by the sum.

The steps for Softmax:

For each row i in 0..batch_size-1:

1. Find max_val = max over columns (0..out_features-1) of x[i][j]

2. Subtract max_val from all elements in the row to prevent overflow

3. Compute exponentials of each element

4. Sum all exponentials to get sum_val

5. Divide each element by sum_val

Implementing this in CUDA requires handling each row.

Given that the input is a 2D tensor of size (batch_size, out_features), we can process each row in a separate block or in parallel.

The kernel needs to handle each row independently, so we can launch one block per row.

Within each block, threads can process elements of the row.

The steps would be:

For each row:

- Compute the max (reduction over the row)

- Compute the exponentials minus the max

- Sum the exponentials (reduction over the row)

- Divide each element by the sum

Implementing this in a kernel requires shared memory for the row to perform reductions efficiently.

Alternatively, using atomic operations, but that might be slow.

Let's outline the steps in code:

First, for a given row, the threads in a block can compute the max, then the exponentials, then compute the sum.

But the max and the sum are reductions, which can be done with parallel reductions.

Alternatively, here's a possible approach:

Each block processes a row.

Each thread in the block handles a portion of the row's elements.

First, compute the max of the row:

Each thread loads a value from the input, and the block reduces to find the max.

Then, each thread computes the exponential of (x[i] - max), stores it in shared memory, and then the block reduces to compute the sum.

Finally, each thread computes the result by dividing the exponential by the sum.

The steps in code would be something like:

__global__ void softmax_kernel(float* input, float* output, int batch_size, int out_features) {
    int row = blockIdx.x;
    int tid = threadIdx.x;

    extern __shared__ float shared_memory[];

    float* smem = shared_memory;

    // Load the row into shared memory
    int num_threads = blockDim.x;
    int stride = blockDim.x;

    float max_val = -INFINITY;
    float sum_val = 0.0f;

    // First, compute the max of the row
    for (int i = tid; i < out_features; i += stride) {
        float val = input[row * out_features + i];
        if (val > max_val) {
            max_val = val;
        }
    }

    // Block reduction to find the max
    __syncthreads();
    // Perform reduction here...

    // Once max is found, compute exp(x_i - max)
    // Then compute the sum of all exp terms
    // Then divide each term by the sum

Wait, this is getting a bit involved. Let me structure this properly.

Alternatively, use a parallel reduction approach for the max and the sum.

Let me structure this in steps:

1. Each thread in the block is responsible for a portion of the row's elements.

2. Compute the max:

   - Each thread reads its portion of the row and computes the local max.

   - The block then reduces these local maxima to get the global max.

3. Compute the exponentials:

   - Each thread computes exp(x_i - max_val) for its portion.

4. Compute the sum of all exponentials:

   - Each thread accumulates its portion of the exponentials into a partial sum.

   - The block reduces these partial sums to get the total sum.

5. Compute the output values:

   - Each thread divides its exponential terms by the total sum.

This requires shared memory to hold intermediate values.

The block size should be chosen such that it can handle the number of elements in a row (out_features=8192). However, 8192 elements would require a large block size, which might be impractical. Alternatively, use a block size that can handle the row in chunks.

Alternatively, let's choose a block size of 256 threads. Then, for 8192 elements per row, each thread would handle 8192 / 256 = 32 elements.

Let me outline the code step by step.

First, the kernel function:

__global__ void softmax_kernel(float* input, float* output, int batch_size, int out_features) {
    int row = blockIdx.x;
    int tid = threadIdx.x;

    // Each thread in the block processes a portion of the row's elements
    // Number of elements per thread: elements_per_thread = ceil(out_features / blockDim.x)
    int elements_per_thread = (out_features + blockDim.x - 1) / blockDim.x;

    // Shared memory for intermediate values
    extern __shared__ float shared[];

    // Space for max and sum values
    // First 2 elements: shared[0] for max, shared[1] for sum
    // The rest for storing partial sums for the sum reduction

    // Compute max
    float local_max = -INFINITY;
    for (int i = tid; i < out_features; i += blockDim.x) {
        float val = input[row * out_features + i];
        if (val > local_max) {
            local_max = val;
        }
    }

    // Reduce to find the block's max
    __syncthreads();
    // Use parallel reduction here to find the global max
    // Using a reduction approach:
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            if (shared[tid + s] > shared[tid]) {
                shared[tid] = shared[tid + s];
            }
        }
        __syncthreads();
    }
    float global_max = shared[0];
    __syncthreads();

    // Compute exponentials and sum
    float local_sum = 0.0f;
    for (int i = tid; i < out_features; i += blockDim.x) {
        float val = input[row * out_features + i] - global_max;
        float exp_val = expf(val);
        output[row * out_features + i] = exp_val;
        local_sum += exp_val;
    }

    // Accumulate the local_sum into shared memory
    int offset = blockDim.x;
    shared[tid] = local_sum;
    __syncthreads();

    // Reduce the sum
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }
    float total_sum = shared[0];

    // Now divide each exponential by total_sum
    for (int i = tid; i < out_features; i += blockDim.x) {
        output[row * out_features + i] /= total_sum;
    }
}

Wait, this might have some errors, like the initial max reduction not being properly handled.

Alternatively, let's structure the code properly with steps for max, then exponentials and sum.

First, compute the max:

Each thread computes a local max over their portion of the row's elements.

Then, perform a block reduction to find the global max.

Then, compute the exponentials and accumulate the sum.

Wait, perhaps better to split the steps:

Step 1: Compute the max:

- Each thread processes their portion of the row.

- Use a block reduction to get the global max.

Step 2: Compute the exponentials and accumulate the sum:

- Each thread processes their portion again, computing exp(x_i - max), and accumulating into local_sum.

- Then perform a block reduction to get the total_sum.

Step 3: Divide each exponential by the total_sum.

This requires multiple passes over the data.

However, the code above may have some issues with shared memory usage and synchronization.

Alternatively, using a separate shared memory buffer to hold intermediate values.

Alternatively, perhaps using atomic operations for the max and sum, but that could be slow.

Alternatively, using the first part of shared memory to hold the max and the second part for the sum.

Alternatively, let's restructure the code:

First, compute the max:

Each thread loads their element, computes the local max, and then the block reduces to get the global max.

Then, each thread computes the exponential terms and accumulates the sum into shared memory.

Wait, here's a revised approach:

__global__ void softmax_kernel(float* input, float* output, int batch_size, int out_features) {
    int row = blockIdx.x;
    int tid = threadIdx.x;

    // Each thread in the block processes a portion of the row's elements
    int elements_per_thread = (out_features + blockDim.x - 1) / blockDim.x;

    // Shared memory for max and partial sums
    extern __shared__ float shared[];
    float* smem = shared;

    // Phase 1: Compute max
    float local_max = -FLT_MAX; // Using float's min
    for (int i = tid; i < out_features; i += blockDim.x) {
        float val = input[row * out_features + i];
        if (val > local_max) {
            local_max = val;
        }
    }

    // Write local_max to shared memory
    smem[tid] = local_max;
    __syncthreads();

    // Reduce to find the global max (using block-wide reduction)
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            if (smem[tid + s] > smem[tid]) {
                smem[tid] = smem[tid + s];
            }
        }
        __syncthreads();
    }
    float global_max = smem[0];
    __syncthreads();

    // Phase 2: Compute exponentials and accumulate sum
    float local_sum = 0.0f;
    for (int i = tid; i < out_features; i += blockDim.x) {
        float val = input[row * out_features + i] - global_max;
        float exp_val = expf(val);
        output[row * out_features + i] = exp_val; // Store intermediate result in output
        local_sum += exp_val;
    }

    // Write local_sum to shared memory
    smem[tid] = local_sum;
    __syncthreads();

    // Reduce to find total_sum
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            smem[tid] += smem[tid + s];
        }
        __syncthreads();
    }
    float total_sum = smem[0];
    __syncthreads();

    // Phase 3: Divide each element by total_sum
    for (int i = tid; i < out_features; i += blockDim.x) {
        output[row * out_features + i] /= total_sum;
    }
}

This code uses shared memory to first compute the max, then compute the exponentials and sum, then divide. Each phase requires synchronization.

The shared memory size needed is blockDim.x * sizeof(float) for storing the intermediate max and sums.

The block size should be chosen such that blockDim.x <= out_features. For out_features=8192, a block size of 256 is feasible.

The kernel is launched with one block per row (batch_size blocks), and each block uses blockDim.x threads (e.g., 256).

The shared memory size would be blockDim.x * sizeof(float). For 256 threads, that's 256 * 4 bytes = 1KB per block, which is acceptable.

Now, the parameters for launching the kernel would be:

block_size = 256

num_blocks = batch_size

The shared memory size per block is blockDim.x * sizeof(float), so we pass that via the kernel's extern __shared__ declaration.

Now, the Python wrapper for the Softmax kernel would be:

softmax_cuda = ... (the code for the kernel)

Then, the model's forward would be:

x = self.linear(x)
x = gelu_cuda(x)  # custom GELU
x = softmax_cuda(x)  # custom Softmax

Wait, but the input to the GELU is the output of the linear layer, which is (batch_size, out_features). The GELU kernel needs to process all elements.

The GELU kernel can be a simple element-wise kernel:

gelu_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void gelu_kernel(float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        float x_cubed = x * x * x;
        float inside = x + 0.044715f * x_cubed;
        inside *= 0.7978845608f;  // sqrt(2/pi)
        float tanh_val = tanhf(inside);
        output[idx] = 0.5f * x * (1.0f + tanh_val);
    }
}

torch::Tensor gelu_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    gelu_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
    return output;
}
"""

Then, the Softmax kernel needs to be written in a similar way.

The Softmax kernel's CUDA code would be as above, but in the Python code, we need to pass parameters like batch_size and out_features.

Wait, the Softmax kernel requires the batch_size and out_features as parameters, which can be extracted from the input tensor.

So the Softmax function in Python:

def softmax_cuda(input):
    batch_size = input.size(0)
    out_features = input.size(1)
    output = torch.empty_like(input)
    block_size = 256
    num_blocks = batch_size  # one block per row
    shared_size = block_size * sizeof(float)  # in bytes
    # But wait, in CUDA, we have to compute shared memory size as the total bytes.
    # The shared memory size per block is blockDim.x * sizeof(float)
    # So, for the kernel, we can compute it as:
    # shared_size = block_size * torch.cuda.FloatTensor().element_size()
    # However, in the Python wrapper, it might be easier to compute dynamically.

    # The kernel is launched with:
    # softmax_kernel<<<num_blocks, block_size, shared_size>>>(...)
    # So in the Python function, we need to compute the shared memory size.

    # Compute shared memory size (in bytes)
    shared_size = block_size * input.element_size()

    softmax_kernel<<<num_blocks, block_size, shared_size, 0>>>(
        input.data_ptr(), output.data_ptr(), batch_size, out_features)
    return output

Wait, but in the CUDA kernel, the parameters are input, output, batch_size, out_features.

Thus, the Python wrapper for the Softmax kernel would be:

softmax_source = """
#include <torch/extension.h>

__global__ void softmax_kernel(float* input, float* output, int batch_size, int out_features) {
    // the code as written earlier
}

torch::Tensor softmax_cuda(torch::Tensor input) {
    int batch_size = input.size(0);
    int out_features = input.size(1);
    auto output = torch::empty_like(input);
    const int block_size = 256;
    const int num_blocks = batch_size; // one block per row
    int shared_size = block_size * sizeof(float); // shared memory per block

    softmax_kernel<<<num_blocks, block_size, shared_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, out_features);
    return output;
}
"""

Wait, but in the kernel, the row is blockIdx.x, which goes up to batch_size-1. So the number of blocks is batch_size.

Now, putting it all together:

The ModelNew will use the PyTorch Linear layer for the matmul and bias, then apply the custom GELU and Softmax kernels.

Thus, the code would be structured as follows.

First, define the custom kernels for GELU and Softmax using the inline method.

The GELU and Softmax kernels are defined in strings, then compiled via load_inline.

Then, the ModelNew class uses these functions.

Putting it all together in code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom GELU kernel
gelu_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void gelu_kernel(float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        float x_cubed = x * x * x;
        float inside = x + 0.044715f * x_cubed;
        inside *= 0.7978845608f;  // sqrt(2/pi)
        float tanh_val = tanhf(inside);
        output[idx] = 0.5f * x * (1.0f + tanh_val);
    }
}

torch::Tensor gelu_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    gelu_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
    return output;
}
"""

# Custom Softmax kernel
softmax_source = """
#include <torch/extension.h>

__global__ void softmax_kernel(float* input, float* output, int batch_size, int out_features) {
    int row = blockIdx.x;
    int tid = threadIdx.x;

    extern __shared__ float shared[];

    float* smem = shared;

    // Phase 1: Compute max
    float local_max = -FLT_MAX;
    for (int i = tid; i < out_features; i += blockDim.x) {
        float val = input[row * out_features + i];
        if (val > local_max) {
            local_max = val;
        }
    }

    smem[tid] = local_max;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            if (smem[tid + s] > smem[tid]) {
                smem[tid] = smem[tid + s];
            }
        }
        __syncthreads();
    }
    float global_max = smem[0];
    __syncthreads();

    // Phase 2: Compute exponentials and accumulate sum
    float local_sum = 0.0f;
    for (int i = tid; i < out_features; i += blockDim.x) {
        float val = input[row * out_features + i] - global_max;
        float exp_val = expf(val);
        output[row * out_features + i] = exp_val;
        local_sum += exp_val;
    }

    smem[tid] = local_sum;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            smem[tid] += smem[tid + s];
        }
        __syncthreads();
    }
    float total_sum = smem[0];
    __syncthreads();

    // Phase 3: Divide by total_sum
    for (int i = tid; i < out_features; i += blockDim.x) {
        output[row * out_features + i] /= total_sum;
    }
}

torch::Tensor softmax_cuda(torch::Tensor input) {
    int batch_size = input.size(0);
    int out_features = input.size(1);
    auto output = torch::empty_like(input);
    const int block_size = 256;
    const int num_blocks = batch_size; // one block per row
    int shared_size = block_size * sizeof(float); // shared memory per block

    softmax_kernel<<<num_blocks, block_size, shared_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, out_features);
    return output;
}
"""

# Compile the custom kernels
gelu = load_inline(
    name="gelu",
    cpp_sources="",
    cuda_sources=gelu_source,
    functions=["gelu_cuda"],
    verbose=True,
)

softmax = load_inline(
    name="softmax",
    cpp_sources="",
    cuda_sources=softmax_source,
    functions=["softmax_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.gelu = gelu  # reference to the custom GELU function
        self.softmax = softmax  # reference to the custom Softmax function

    def forward(self, x):
        x = self.linear(x)
        x = self.gelu.gelu_cuda(x)
        x = self.softmax.softmax_cuda(x)
        return x

# The get_inputs and get_init_inputs functions remain the same as in the original code
def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features]
```

Wait, but in the original code, the input tensors are on GPU already? The problem statement says "You may assume that the input tensors are on GPU." So in get_inputs(), we should generate tensors on the GPU.

Hence, in the get_inputs function, we should add .cuda():

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

Similarly, in the original code's get_init_inputs, the parameters are passed as [in_features, out_features], which are scalars, so no issue.

Now, checking the code for possible errors:

In the Softmax kernel's phase 1, the initial local_max is set to -FLT_MAX (which is a C macro for the minimum float). However, in CUDA, we might need to use the appropriate constant. Alternatively, using -INFINITY. Let me check.

In CUDA, FLT_MIN is the smallest positive float, but FLT_MIN is not the minimum (negative) value. To initialize to negative infinity, it's better to use -INFINITY.

However, in the code above, it's written as -FLT_MAX. Let