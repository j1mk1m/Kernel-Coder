The code must work with PyTorch 2.3.0 and CUDA 12.1. Use the inline CUDA extension approach shown in the example. 

The code must be compatible with Python 3.10 or higher.

Your optimized code must NOT use any other third-party libraries other than PyTorch and native Python libraries. 

The code must import necessary modules. 

The code must be as efficient as possible, with fused kernels where possible. 

Avoid unnecessary memory allocations or copies. 

Your code must be correct and functionally identical to the original code, except for possible minor numerical differences due to floating point operation order. 

The code must have a `ModelNew` class with a constructor that takes the same arguments as `Model`: input_size, hidden_size, scaling_factor. The `get_init_inputs()` function must return the arguments required to initialize ModelNew.

You may not modify the get_inputs() and get_init_inputs() functions. 

You must inline the CUDA kernel source in the Python file using the load_inline function. 

You may only replace parts of the forward() method with your custom CUDA kernels. 

Make sure that the kernel code is correct and that the Python bindings are correctly implemented. 

All tensor operations in the forward() must be replaced with custom CUDA kernels where possible. 

Do not use any PyTorch native operators in the forward() method of your ModelNew. 

The ModelNew must be as fast as possible. 

The code must be compatible with the input and output shapes as given in the original code. 

All the CUDA kernels must be properly managed with respect to CUDA streams to ensure asynchronous execution where possible. 

The code should be compatible with Ampere architecture (sm_80) or higher. 

The code must include __global__ kernel functions and proper CUDA error checking. 

The code must handle all tensor sizes and shapes dynamically at runtime. 

Make sure that all CUDA kernels are launched with appropriate grid and block dimensions to maximize performance. 

Your kernels should use shared memory where beneficial. 

Your code must be correct and not crash. 

Please optimize the given architecture with these constraints in mind.



```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, scaling_factor):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))
        self.scaling_factor = scaling_factor

        # Define fused CUDA kernels
        self.fused_gemm_div_sum_scale = load_inline(
            name="fused_gemm_div_sum_scale",
            cuda_sources=f"""
            #include <torch/extension.h>
            #include <cuda_runtime.h>
            #include <cuda_fp16.h>
            #include <mma.h>

            template<typename T>
            __global__ void fused_gemm_div_sum_scale_kernel(
                const T* __restrict__ input,
                const T* __restrict__ weight,
                T* output,
                int batch_size,
                int input_size,
                int hidden_size,
                T scaling_factor,
                T divisor) {{
                extern __shared__ unsigned char smem[];
                T* sdata = reinterpret_cast<T*>(smem);

                int batch = blockIdx.x;
                int tid = threadIdx.x;

                T sum = 0.0;
                #pragma unroll
                for (int hid = 0; hid < hidden_size; hid += blockDim.x) {{
                    int hid_idx = hid + tid;
                    if (hid_idx < hidden_size) {{
                        T val = 0.0;
                        #pragma unroll
                        for (int in = 0; in < input_size; in++) {{
                            val += input[batch * input_size + in] * weight[hid_idx * input_size + in];
                        }}
                        val /= divisor;
                        sum += val;
                    }}
                }}
                __syncthreads();

                // Parallel reduction for sum
                for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {{
                    if (tid < stride) {{
                        sum += sdata[tid + stride];
                    }}
                    __syncthreads();
                }}

                if (tid == 0) {{
                    output[batch] = sum * scaling_factor;
                }}
            }}

            at::Tensor forward(
                at::Tensor input,
                at::Tensor weight,
                float scaling_factor,
                float divisor) {{
                const int batch_size = input.size(0);
                const int input_size = input.size(1);
                const int hidden_size = weight.size(0);
                auto output = at::empty({{batch_size, 1}}, input.options());

                int block_size = 256;
                int grid_size = batch_size;

                dim3 threads(block_size);
                dim3 blocks(grid_size);

                auto stream = at::cuda::getCurrentCUDAStream();
                fused_gemm_div_sum_scale_kernel<float>
                    <<<blocks, threads, 0, stream>>>(
                        input.data_ptr<float>(),
                        weight.data_ptr<float>(),
                        output.data_ptr<float>(),
                        batch_size,
                        input_size,
                        hidden_size,
                        scaling_factor,
                        divisor
                    );

                AT_CUDA_CHECK(cudaGetLastError());
                return output;
            }}
            """,
            functions=["forward"],
            verbose=True
        )

    def forward(self, x):
        # x: [batch_size, input_size]
        # weight: [hidden_size, input_size]
        # The fused kernel handles matmul, div, sum, and scaling
        return self.fused_gemm_div_sum_scale.forward(
            x,
            self.weight.t(),
            self.scaling_factor,
            2.0
        ).view(-1, 1)

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]

batch_size = 1024
input_size = 8192
hidden_size = 8192
scaling_factor = 1.5
```
</think>
