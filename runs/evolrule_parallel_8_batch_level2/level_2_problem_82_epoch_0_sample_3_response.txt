You may choose which operators to replace. For example, you might want to replace the tanh, scaling, bias addition steps as a single kernel, since they are all element-wise operations. Similarly, the convolution and max-pooling can be optimized as well, but note that convolution requires access to the kernel weights which are part of the model's parameters, making it more complex. 

When replacing operators, ensure that the replacement kernels handle all necessary aspects, such as memory allocation, CUDA stream management, and error checking. The kernels should support tensors of different shapes (not hard-coded to specific dimensions). 

You can also choose to combine operators in a kernel. For example, the tanh, scaling, and bias addition can be fused into a single kernel. The max-pooling is another candidate, but may be more complex. 

You may also choose to leave some operators as-is (e.g., the convolution, if it's already well-optimized by PyTorch's implementation) and focus on fusing the element-wise operations. 

Please provide a single ModelNew class that replaces parts of the original architecture with your custom CUDA kernels. The rest of the code should remain compatible with PyTorch's nn.Module and the given get_inputs and get_init_inputs functions. 

The problem is to write the new code, which should be a direct replacement of the original Model class, with some parts replaced by custom CUDA kernels. The code must compile and run correctly when used with the provided get_inputs and get_init_inputs functions. 

Now, please provide your optimized ModelNew code with custom CUDA kernels. 

Make sure you follow all the user's instructions, including syntax for inline CUDA extensions as shown in the example. Also, ensure the new code uses the same input/output signatures and parameters as the original. 

Also, make sure that the kernels you write are correct. For example, when fusing tanh, scaling, and bias addition into one kernel, you must implement the computation correctly (e.g., tanh(x) * scaling_factor + bias). 

The code must be fully functional and compatible with the provided input functions.



Okay, I need to optimize the given Model by replacing some operators with custom CUDA kernels. Let me look at the original architecture again. The model does a convolution, applies tanh, scales by a factor, adds a bias, then max-pools.

The user suggested fusing element-wise operations like tanh, scaling, and bias addition into a single kernel. That makes sense since those are all element-wise. The convolution and max-pooling might be more complex, especially since convolution involves weights, so maybe leave those as-is unless there's a good reason. Let's focus on the element-wise steps first.

First, I'll need to write a CUDA kernel that combines tanh(x), multiplies by scaling_factor, adds the bias. Wait, but the bias is a parameter with shape (out_channels, 1, 1). So when adding the bias, since the input tensor after conv and tanh would have shape (batch, out_channels, h, w), the bias is added across the spatial dimensions. So in the kernel, for each element, it would be (tanh(x) * scaling) + bias. Since the bias is a broadcasted tensor, the kernel needs to access the correct bias value for each channel.

The plan is:

1. Create a custom CUDA kernel that takes the input tensor (after conv), applies tanh, scales by the factor, adds the bias, and outputs the result. This combines three operations into one kernel.

2. The rest (conv and max_pool) can stay as PyTorch ops, since convolution might already be optimized, and max-pooling is a bit more complex to implement.

So, how to structure the kernel:

- The kernel function will process each element of the input tensor. For each element, compute the tanh, multiply by scaling, add the bias (taking the correct channel's bias value).

Wait, the bias is a 4D tensor (out_channels, 1, 1), so for an element at position (n, c, h, w), the bias value is self.bias[c, 0, 0]. So in the kernel, for each element, we can get the channel index from the position and index into the bias array.

The kernel needs to:

- Iterate over all elements of the input tensor.

- For each element:

   new_val = tanh(x) * scaling_factor + bias[c]

So, in CUDA, each thread handles one element. The thread's index can compute the position in the tensor.

First, I need to write the CUDA code. Let's think about the parameters. The kernel needs the input tensor, the scaling factor, the bias tensor, and the output tensor. The input and output are of the same shape. The bias is a tensor with shape (C, 1, 1).

Wait, the bias is a parameter, so in the model, it's stored as a PyTorch tensor. So in the kernel, we can pass the bias as a pointer. Since the bias is (C, 1, 1), the kernel can index it using the channel index. For example, if the thread's element is at position (n, c, h, w), then the bias value is bias[c * 1 * 1 + 0 * 1 + 0] (assuming it's stored in a contiguous buffer). So we can compute the channel index from the position.

Alternatively, the kernel can process each element with the following steps:

Each thread processes a linear index. The linear index can be converted into the 4D coordinates (n, c, h, w) based on the tensor's strides. But that might be complicated. Alternatively, since the tensor is 4D, but the bias is per channel, we can compute the channel index using the position.

Wait, for a tensor with dimensions (N, C, H, W), the total elements are N*C*H*W. Each element can be represented as a linear index idx. To get the channel index c, it's (idx / (H*W)) % C. Wait, maybe that's messy. Alternatively, let me think of the tensor as a 4D array. The linear index can be computed as:

Let's suppose the tensor is stored in row-major order (so the last dimension is contiguous). So for an element at position (n, c, h, w), the linear index would be n * C*H*W + c * H*W + h * W + w. So given the linear index, the channel can be calculated as (idx // (H*W)) % C. But maybe it's better to precompute the dimensions and handle the indexing.

Alternatively, in the kernel launch, we can pass the dimensions so that each thread can compute the coordinates. Hmm, this might complicate things, but perhaps necessary.

Wait, the input tensor after convolution is (N, C, H, W). The kernel will need to process each element. Let's say the input has dimensions N, C, H, W. So the total elements are N * C * H * W. Each thread can handle a single element, so the kernel can be launched with a block size of 256, and number of blocks as (total + block_size -1)/block_size. The thread index is threadIdx.x + blockIdx.x * blockDim.x.

Inside the kernel, for a given index, we need to compute the n, c, h, w indices. But to do that, we need the dimensions. So when launching the kernel, we have to pass the N, C, H, W as parameters. Alternatively, the kernel can compute the coordinates using the linear index.

Wait, let me see:

Given the linear index idx, the position can be calculated as:

w = idx % W

h = (idx // W) % H

c = (idx // (W*H)) % C

n = idx // (C*H*W)

So for example, if W=256, H=256, C=64, N=128:

C*H*W = 64*256*256 = 4194304. So for an index, say 12345678, it can be decomposed into n, c, h, w.

This way, inside the kernel, given the dimensions, we can compute the coordinates.

Thus, in the kernel function, we need to pass the input data pointer, output data pointer, scaling factor, bias data pointer, and the dimensions (N, C, H, W). Wait, but how do we pass the dimensions? Since CUDA kernels can have parameters, we can pass them as arguments.

So the kernel signature would be something like:

__global__ void fused_tanh_scale_bias_kernel(
    const float* input,
    float* output,
    float scaling_factor,
    const float* bias,
    int N, int C, int H, int W,
    int input_stride0, int input_stride1, int input_stride2, int input_stride3,
    int bias_stride0, int bias_stride1, int bias_stride2
) {
    // ... code here ...
}

Wait, but handling strides might complicate things. Alternatively, assuming the tensors are contiguous, so the strides are 1 for the last dimension. But for PyTorch tensors, we can't assume that. Hmm, this might be an issue. To avoid dealing with strides, perhaps the kernel should assume that the input and output are contiguous. Since in PyTorch, when you call .contiguous(), you can ensure that. So in the code that calls the CUDA function, we can ensure that the input is contiguous, then pass the dimensions as N, C, H, W.

Alternatively, the kernel can use the strides, but that complicates the code. Maybe for simplicity, we can assume contiguous tensors. So in the CUDA function, the input and output are contiguous. Then the kernel can process the data as a flat array, with the dimensions passed as parameters.

So inside the kernel, for each thread's index:

idx = threadIdx.x + blockIdx.x * blockDim.x;

if (idx >= N*C*H*W) return;

then compute n,c,h,w as above.

Then, compute the value:

float x = input[idx];

float tanh_x = tanh(x);

float scaled = tanh_x * scaling_factor;

// get the bias value for channel c

// bias is stored as (C, 1, 1), so the index into bias is c * 1 * 1 + 0*1 +0 = c.

// so bias[c]

float b_val = bias[c]; 

float result = scaled + b_val;

output[idx] = result;

Wait, but the bias is a 3D tensor (C, 1, 1), so the linear index for the bias at channel c would be c * (1*1) + 0*1 +0 = c. So yes, the bias array is stored as a 1D array of length C*1*1, so the value for channel c is at position c.

Thus, the code is manageable.

Now, in the CUDA code, the parameters for the kernel function would be:

input: pointer to input tensor data

output: pointer to output tensor data

scaling_factor: float

bias: pointer to bias data (which is a 1D array of length C)

dimensions N, C, H, W.

Wait, but in the code, how do we get the C from the bias? Because the bias has shape (C, 1, 1). The bias tensor's size can be obtained via bias.size(0). But in the CUDA function, we need to pass all the necessary parameters.

So the CUDA function (the host function) will need to take the input tensor, scaling factor, bias tensor. Then compute the dimensions:

N = input.size(0)

C = input.size(1)

H = input.size(2)

W = input.size(3)

Thus, the CUDA kernel function can be written as follows.

Now, implementing this in PyTorch's inline CUDA.

The CUDA kernel code would be:

#include <torch/extension.h>
#include <math.h>
#include <cuda_runtime.h>

__global__ void fused_tanh_scale_bias_kernel(
    const float* input,
    float* output,
    const float scaling_factor,
    const float* bias,
    int N, int C, int H, int W
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * C * H * W) return;

    // Compute n, c, h, w
    int w = idx % W;
    int h = (idx / W) % H;
    int c = (idx / (W * H)) % C;
    int n = idx / (C * H * W);

    float x = input[idx];
    float tanh_x = tanh(x);
    float scaled = tanh_x * scaling_factor;
    float b_val = bias[c];  // since bias is [C, 1, 1], so c is the correct index
    output[idx] = scaled + b_val;
}

Then, the host function in CUDA would be:

torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias
) {
    // Check that input is contiguous
    input = input.contiguous();
    bias = bias.contiguous();

    int N = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    // Check that bias has shape (C, 1, 1)
    // For brevity, assuming it's correct; error handling could be added
    auto output = torch::empty_like(input);

    const int block_size = 256;
    int total_elements = N * C * H * W;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    fused_tanh_scale_bias_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        bias.data_ptr<float>(),
        N, C, H, W
    );

    // Check for errors
    cudaDeviceSynchronize();
    // Add error checking here if needed (e.g., check for kernel errors)
    return output;
}

Wait, but in PyTorch, when using inline CUDA, the host function must be defined in the cpp_sources. The CUDA kernel is in the cuda_sources.

Now, in the Python code, we need to load this CUDA function as an inline extension. Following the example given in the problem's example, we can structure this.

The code structure would be:

Define the CUDA source code as a string variable.

Then, the header includes.

Wait, here's how the code should be written.

First, in the Python code:

We'll have:

fused_tanh_scale_bias_source = """
#include <torch/extension.h>
#include <math.h>
#include <cuda_runtime.h>

__global__ void fused_tanh_scale_bias_kernel(
    const float* input,
    float* output,
    const float scaling_factor,
    const float* bias,
    int N, int C, int H, int W
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * C * H * W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int c = (idx / (W * H)) % C;
    int n = idx / (C * H * W);

    float x = input[idx];
    float tanh_x = tanh(x);
    float scaled = tanh_x * scaling_factor;
    float b_val = bias[c];  // bias is [C, 1, 1], so c is correct
    output[idx] = scaled + b_val;
}

torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias
) {
    input = input.contiguous();
    bias = bias.contiguous();

    int N = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    int total_elements = N * C * H * W;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    fused_tanh_scale_bias_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        bias.data_ptr<float>(),
        N, C, H, W
    );

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));

    return output;
}
"""

Then, the corresponding cpp header declarations (since in the example they had a header for the function):

fused_tanh_scale_bias_cpp = """
torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias
);
"""

Then, load this with load_inline, passing the functions.

So in the code:

fused_tanh_scale_bias = load_inline(
    name="fused_tanh_scale_bias",
    cpp_sources=fused_tanh_scale_bias_cpp,
    cuda_sources=fused_tanh_scale_bias_source,
    functions=["fused_tanh_scale_bias_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_ldflags=[""]
)

Wait, but in the example, the extra_cflags were empty, but maybe using C++14 is better. Also, need to make sure the CUDA code is correct.

Now, in the ModelNew class, we can replace the tanh, scaling, and bias addition with this fused kernel.

The original Model's forward is:

def forward(self, x):
    x = self.conv(x)
    x = torch.tanh(x)
    x = x * self.scaling_factor
    x = x + self.bias
    x = self.max_pool(x)
    return x

The new code would replace the three lines with a single call to the fused kernel.

Thus, in ModelNew:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)
        # Load the fused kernel
        self.fused_tanh_scale_bias = fused_tanh_scale_bias  # the module we loaded

    def forward(self, x):
        x = self.conv(x)
        # Apply fused kernel
        x = self.fused_tanh_scale_bias.fused_tanh_scale_bias_cuda(
            x, self.scaling_factor, self.bias
        )
        x = self.max_pool(x)
        return x

Wait, but in the code, the fused function is stored as a module (similar to the example's elementwise_add). So the line would be:

x = self.fused_tanh_scale_bias.fused_tanh_scale_bias_cuda(x, self.scaling_factor, self.bias)

But need to check the parameters. The CUDA function requires input, scaling_factor (float), and bias (tensor). So that's correct.

Also, note that the bias is a parameter of the model, so in the __init__, we must initialize it as a Parameter, which is done in the original code.

Another thing to check: in the fused kernel, the bias is passed as a tensor. The kernel expects the bias to be contiguous. So in the CUDA function, we call bias.contiguous(). So even if the bias is stored as a 3D tensor (C, 1, 1), the .contiguous() will make sure it's stored in a contiguous buffer, so the kernel can read the first C elements as the bias for each channel.

Wait, the bias tensor is (out_channels, 1, 1). So when we call .contiguous(), it's stored in memory as a contiguous block. The first element is bias[0,0,0], next bias[0,0,1], but since the last two dimensions are 1, the next element is bias[1,0,0], so the first C elements correspond to the channels. Thus, in the kernel, bias[c] gives the correct value for each channel.

Yes, that should work.

Now, the rest of the code (conv and max_pool) remains as in the original. The get_inputs and get_init_inputs functions don't need to change, as the new model should accept the same parameters and inputs.

I need to ensure that all the code is correctly formatted, with the CUDA kernels properly included. Also, in the Python code, the parameters for the ModelNew's __init__ must match the original Model's, so that when the user initializes the new model using the get_init_inputs (which returns the parameters in order), it works.

Wait, in the original Model's __init__:

def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):

So the parameters are in_channels, etc. So in ModelNew's __init__, the parameters must be the same, so that when initialized with the parameters from get_init_inputs(), which returns [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size], the ModelNew can be initialized correctly.

Yes, the ModelNew's __init__ parameters are exactly the same as the original's, so that's correct.

Now, putting all together, the complete code would look like this.

Also, need to ensure that the CUDA kernel code is properly formatted in the Python strings. Let me check for any syntax errors.

In the kernel code, the division for h and c is integer division, which in C++ is okay. The CUDA code uses standard functions like tanh (from math.h), so that's okay.

The error checking in the CUDA function: after the kernel launch, it checks for CUDA errors via cudaGetLastError(). That's good for debugging, but in production, maybe not necessary, but including it is helpful.

Now, the complete code:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernel for tanh, scaling, and bias addition
fused_tanh_scale_bias_source = """
#include <torch/extension.h>
#include <math.h>
#include <cuda_runtime.h>

__global__ void fused_tanh_scale_bias_kernel(
    const float* input,
    float* output,
    const float scaling_factor,
    const float* bias,
    int N, int C, int H, int W
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * C * H * W) return;

    // Compute n, c, h, w from linear index
    int w = idx % W;
    int h = (idx / W) % H;
    int c = (idx / (W * H)) % C;
    int n = idx / (C * H * W);

    float x = input[idx];
    float tanh_x = tanh(x);
    float scaled = tanh_x * scaling_factor;
    float b_val = bias[c];  // bias is [C, 1, 1], so c is correct
    output[idx] = scaled + b_val;
}

torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias
) {
    // Ensure input and bias are contiguous
    input = input.contiguous();
    bias = bias.contiguous();

    int N = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    int total_elements = N * C * H * W;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    fused_tanh_scale_bias_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        bias.data_ptr<float>(),
        N, C, H, W
    );

    // Check for CUDA errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}
"""

fused_tanh_scale_bias_cpp = """
torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias
);
"""

# Load the fused CUDA kernel
fused_tanh_scale_bias = load_inline(
    name="fused_tanh_scale_bias",
    cpp_sources=fused_tanh_scale_bias_cpp,
    cuda_sources=fused_tanh_scale_bias_source,
    functions=["fused_tanh_scale_bias_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_cuda_cflags=["-arch=sm_75"]  # Adjust architecture if needed
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)
        self.fused_tanh_scale_bias = fused_tanh_scale_bias  # Access the loaded kernel

    def forward(self, x):
        x = self.conv(x)
        # Apply fused kernel
        x = self.fused_tanh_scale_bias.fused_tanh_scale_bias_cuda(x, self.scaling_factor, self.bias)
        x = self.max_pool(x)
        return x

# The rest of the functions (get_inputs, get_init_inputs) remain the same as in the original problem statement.
```

Wait, but in the original problem's code, the parameters for the Model are initialized with get_init_inputs() which returns the parameters. The ModelNew's __init__ must accept those parameters in the same order.

Looking back, the original Model's __init__ has parameters:

def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):

The ModelNew's __init__ is structured the same, so that's okay.

The user's get_init_inputs returns the parameters in the correct order, so when initializing the ModelNew, it should work.

Now, need to make sure that the kernel function is correctly called with the right parameters. The CUDA function requires:

input (the output of the conv layer), scaling_factor (a float), and the bias (a tensor). That's exactly what's passed in the forward method.

Testing the code in my mind: the conv output is a tensor of shape (batch, out_channels, H, W). The fused kernel processes each element, applies tanh, scales, adds bias. Then max_pool.

Yes, that should work.

Potential issues:

- The CUDA kernel's dimensions. Suppose the input has different dimensions. The code uses the input's size each time, so it's dynamic.

- The bias's shape must be (C, 1, 1). The user's code initializes it with bias_shape=(out_channels, 1, 1), which is correct. The kernel uses the first dimension (C) of the input to index the bias, which matches.

- The bias is passed as a contiguous tensor. Since in the CUDA function, we call .contiguous() on it, it's safe.

- Error handling in CUDA. The kernel checks for errors via cudaGetLastError(). This is good for debugging.

- The CUDA kernel uses a thread per element. For large tensors, the block size of 256 may not be optimal, but it's a standard choice and should work.

Overall, this code should be a valid optimization of the original model by fusing the three element-wise operations into a single kernel, which reduces the number of kernel launches and memory accesses between operations, leading to better performance.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernel for tanh, scaling, and bias addition
fused_tanh_scale_bias_source = """
#include <torch/extension.h>
#include <math.h>
#include <cuda_runtime.h>

__global__ void fused_tanh_scale_bias_kernel(
    const float* input,
    float* output,
    const float scaling_factor,
    const float* bias,
    int N, int C, int H, int W
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N * C * H * W) return;

    // Compute n, c, h, w from linear index
    int w = idx % W;
    int h = (idx / W) % H;
    int c = (idx / (W * H)) % C;
    int n = idx / (C * H * W);

    float x = input[idx];
    float tanh_x = tanh(x);
    float scaled = tanh_x * scaling_factor;
    float b_val = bias[c];  // bias is [C, 1, 1], so c is correct
    output[idx] = scaled + b_val;
}

torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias
) {
    // Ensure input and bias are contiguous
    input = input.contiguous();
    bias = bias.contiguous();

    int N = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty_like(input);

    const int block_size = 256;
    int total_elements = N * C * H * W;
    int num_blocks = (total_elements + block_size - 1) / block_size;

    fused_tanh_scale_bias_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        scaling_factor,
        bias.data_ptr<float>(),
        N, C, H, W
    );

    // Check for CUDA errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA error: " + std::string(cudaGetErrorString(err)));
    }

    return output;
}
"""

fused_tanh_scale_bias_cpp = """
torch::Tensor fused_tanh_scale_bias_cuda(
    torch::Tensor input,
    float scaling_factor,
    torch::Tensor bias
);
"""

# Load the fused CUDA kernel
fused_tanh_scale_bias = load_inline(
    name="fused_tanh_scale_bias",
    cpp_sources=fused_tanh_scale_bias_cpp,
    cuda_sources=fused_tanh_scale_bias_source,
    functions=["fused_tanh_scale_bias_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_cuda_cflags=["-arch=sm_75"]
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)
        self.fused_tanh_scale_bias = fused_tanh_scale_bias  # Access the loaded kernel

    def forward(self, x):
        x = self.conv(x)
        # Apply fused kernel
        x = self.fused_tanh_scale_bias.fused_tanh_scale_bias_cuda(x, self.scaling_factor, self.bias)
        x = self.max_pool(x)
        return x
```