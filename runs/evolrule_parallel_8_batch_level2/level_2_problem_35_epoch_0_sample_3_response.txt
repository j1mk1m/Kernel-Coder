When writing code, make sure that any function arguments that are tensors are contiguous in CUDA kernels. For non-tensor arguments like constants, you can pass them in as regular arguments and handle them in the kernel. Please only use the torch extension to write CUDA kernels (i.e., use torch's C++ API), do not use other libraries unless necessary. 

Please do not use any PyTorch native extensions (like torch.nn.functional) inside your CUDA kernels. 

Make sure that the compiled CUDA kernels are loaded with load_inline in the same file. 

The new code should be compatible with PyTorch 2.0 and above, and use the torchinductor compiler. 

The goal is to make the code as fast as possible, so please optimize all possible steps. 

You may replace multiple operators with fused kernels. 

Consider fusing convolutions, element-wise operations, and activation functions into a single kernel to maximize performance. 

Make sure that the fused kernels are memory-efficient and coalesce memory accesses. 

When possible, combine multiple operations into a single kernel launch to reduce kernel launch overhead.

I will give you the architecture above, and you must output the optimized ModelNew in codeblocks with the required CUDA kernels and inline compilation. 

I will check the code for correctness and performance. 

Please do not forget to handle all inputs and parameters correctly in the kernels. 

Make sure the code runs without errors when compiled with load_inline.



Okay, I need to optimize the given PyTorch model by replacing some operators with custom CUDA kernels. The original model has a convolution, subtraction, HardSwish, MaxPool, and Mish activation. Let me think about how to fuse these operations into fewer kernels for better performance.

First, the convolution is a big operation, but maybe I can combine it with the subtraction and the subsequent activations. Let's see the sequence: conv -> subtract -> hardswish -> maxpool -> mish. Wait, the MaxPool is after the hardswish, and then Mish comes after MaxPool. Since MaxPool is a non-trivial operation, maybe I can fuse up to the hardswish first, then handle the pool separately? Or maybe split into different fused kernels.

Alternatively, since the subtract is just an element-wise operation, maybe I can fuse that with the convolution's output. The convolution's output is a tensor, subtracting a scalar is easy to add into the same kernel. Then the HardSwish can be applied immediately after. So perhaps the first fused kernel is Conv + Subtract + HardSwish. Then, the MaxPool can be another kernel, but maybe even combine it with the next Mish?

Wait, the MaxPool is a spatial operation, which might be harder to fuse with other activations. Let me check the operations:

The sequence is:

1. Conv2d (with parameters: weights, bias)
2. Subtract a scalar (x = x - subtract_value)
3. HardSwish (element-wise: x * ReLU6(x + 3)/6 )
4. MaxPool2d (kernel_size=2)
5. Mish (element-wise: x * tanh(softplus(x)) )

Hmm. The first three steps can be done in one kernel. Let's see:

The convolution itself is a compute-heavy part. The subtract is straightforward. The HardSwish is an element-wise function. So fusing them into a single kernel after convolution would be good. The MaxPool is a pooling operation which requires taking max over a window, which is more complex. Then Mish is another element-wise operation. 

Alternatively, perhaps after the first fused kernel (conv + subtract + hardswish), then MaxPool, then another fused kernel for mish? But MaxPool is a separate step. Alternatively, can I combine MaxPool with Mish? Probably not straightforward, since MaxPool is a spatial reduction and Mish is element-wise. 

So, maybe the best approach is to fuse the first three steps (conv, subtract, hardswish) into one kernel, and then the MaxPool is separate but perhaps optimized, and then Mish is another fused kernel with MaxPool's output? Wait, but MaxPool is a separate operation that can't be combined with Mish unless they are both element-wise. 

Alternatively, maybe after the first fused kernel (conv + subtract + hardswish), the MaxPool is done via a custom kernel (since PyTorch's MaxPool is already optimized, but maybe a custom one can be faster?), but that's uncertain. Alternatively, leave MaxPool as is, but then combine MaxPool's output with Mish into another kernel? 

Alternatively, since the MaxPool is a standard operation, perhaps it's better to keep it as is but ensure that the data is contiguous. 

The main thing is to reduce kernel launches. The original model has multiple sequential operations, each with their own kernel launches. Let's see how many kernels are there in the original:

- Conv2d: one kernel.
- Subtract: scalar, probably done via element-wise, maybe via a kernel if the tensor isn't contiguous? Or maybe in-place.
- Hardswish: element-wise, another kernel.
- MaxPool2d: another kernel (but PyTorch's implementation might be optimized).
- Mish: element-wise, another kernel.

So in total, up to 5 kernel launches (if each step is a separate kernel). If we can fuse some of these steps into one kernel, we can reduce overhead.

So, let's focus on fusing the first three steps into a single kernel. The Convolution is the most involved. To do this, I'll need to write a CUDA kernel that performs the convolution, subtracts the value, and applies hardswish in one step.

First, I need to implement the convolution part. The standard convolution is a 2D convolution, so I need to handle the input, weights, and bias. The convolution can be implemented using a naive approach (which might not be as fast as optimized libraries, but for the sake of fusion, maybe it's worth trying). However, writing a highly optimized convolution kernel from scratch is quite involved and might not be feasible here. Alternatively, perhaps using PyTorch's own convolution implementation but that's not allowed. Hmm, but the user instructions say to use only torch's C++ API, not other libraries. So, perhaps it's better to leave the convolution as is, but combine the subsequent steps.

Wait, the problem allows replacing some operators. Maybe the convolution is best left as PyTorch's implementation, but then fuse the subtract, hardswish, and mish into a single kernel? Let's see:

Wait, after the convolution, there's subtract, hardswish, then maxpool, then mish. Maybe subtract and hardswish can be fused into one kernel, then maxpool is separate, then mish is another. That would reduce the number of kernels by 2 (from 3 to 1 for subtract and hardswish). Alternatively, perhaps the subtract and hardswish can be done in a single kernel, and then the mish can be done in another, but maxpool is in between.

Alternatively, perhaps after the convolution, subtract and hardswish can be in a single kernel, then maxpool is done via PyTorch's MaxPool2d (since it's optimized), then the mish can be done in another kernel. So total kernels: convolution (PyTorch), fused subtract+hardswish, maxpool (PyTorch), and mish (custom kernel). That might still be better.

Alternatively, let's think of fusing all the element-wise operations after convolution into a single kernel. The steps after convolution are:

x = conv(x)

x = x - subtract_value (element-wise)

x = hardswish(x) (element-wise)

x = maxpool(x)

x = mish(x) (element-wise)

So the subtract and hardswish can be done in a single kernel. Then, the mish after maxpool can be another kernel. That's two fused kernels. That's better.

So first, after the convolution, the subtract and hardswish can be combined into one kernel. Then, after maxpool, the mish can be another kernel. That's two custom kernels.

But the maxpool itself is a standard operation. Let's see: if the maxpool is done via PyTorch's built-in, which is already optimized, then it's probably better to keep it. So, the plan is:

- Keep the convolution as is (using PyTorch's Conv2d).
- Create a custom kernel for (subtract + hardswish) applied to the convolution's output.
- Keep the maxpool as is (PyTorch's MaxPool2d).
- Create a custom kernel for mish applied to the maxpooled output.

Alternatively, can we combine the subtract and hardswish into the same kernel as the convolution? That would be better, reducing the number of kernels even more. Let's see:

If I can write a kernel that does convolution, then subtract, then hardswish, all in one step. But convolution is a complex operation. To implement that, I need to handle the convolution computation first, which requires looping over the input and kernel dimensions. That's going to be a lot of code, but perhaps manageable.

The problem is that writing a custom convolution kernel might be inefficient compared to PyTorch's optimized implementation, but if we can combine the subsequent steps into the same kernel, maybe the total runtime is better. Alternatively, maybe the overhead of multiple kernel launches is worse than a slightly less optimized convolution. Let's consider that.

Suppose the original convolution is using PyTorch's implementation, which is very fast. So replacing it with a custom one might be slower. So perhaps better to keep the convolution as is, and fuse the subtract and hardswish into a single kernel.

Yes, that's probably a better approach.

So, the first step: the subtract and hardswish can be done in one kernel. Let's write that.

Then, the Mish activation after maxpool can be another kernel. Let me see:

The MaxPool2d with kernel_size=2 is a standard operation, and PyTorch's implementation is likely optimized. So leaving it as is.

Thus, the plan is to replace the subtract + hardswish with a single fused kernel, and the mish with another.

Alternatively, perhaps combining the subtract and hardswish is straightforward. Let's see how to implement that.

The subtract is just x[i] -= subtract_value (since it's a scalar), then the hardswish is x[i] *= max(0, min(6, x[i]+3)) / 6.

So combining these into a single kernel would be easy. The kernel would take the input tensor, subtract the value, then compute the hardswish function.

The same can be done for mish: x[i] = x[i] * tanh(log(1 + exp(x[i]))) (or equivalently, tanh(softplus(x))).

Wait, mish is x * tanh(softplus(x)), where softplus(x) is log(1 + exp(x)). So the calculation is:

mish(x) = x * tanh( log(1 + exp(x)) )

But calculating this in a kernel requires handling exponentials and logs, which might be computationally intensive, but necessary. Let's see:

So for each element:

temp = log(1 + exp(x)) → softplus

tanh(temp)

then multiply by x.

But in CUDA, we can compute this with the math functions available.

Alternatively, using the approximation for softplus:

For numerical stability, when x is large, exp(x) might overflow. But in practice, maybe it's manageable.

Alternatively, since exp(-abs(x)) is small for large x, so:

softplus(x) = max(x, 0) + log(1 + exp(-abs(x)))

But that might be better for numerical stability. Hmm, but for the kernel code, perhaps just use the straightforward computation, or use the built-in functions if possible.

Alternatively, using the formula:

tanh(softplus(x)) can be written as:

tanh( log(1 + exp(x)) )

Which can also be written as (exp(x) - exp(-x)) / (exp(x) + exp(-x)) + 1) ?

Wait, not sure. Let me think again:

Wait, tanh(softplus(x)) = tanh(log(1 + exp(x))).

Alternatively, note that softplus(x) is equal to log(1 + exp(x)), which for positive x approaches x, and for negative x approaches 0.

So, perhaps using the expression as is. But in code, for each element, compute:

float temp = exp(x[i]); // Wait no, x[i] is the value. Let me see:

Wait, the input is x, so for each element:

float s = log(1.0f + exp(x));

then the tanh(s) is tanh(s).

But this requires computing exp(x), which could overflow for large x. To prevent overflow, when x is large, exp(x) overflows, so log(1+exp(x)) ≈ x, so tanh(x) approaches 1. So we can handle this with conditionals:

if (x[i] > some threshold, like 20), then s = x[i], tanh(s) ~ 1, so the product x[i] * 1 = x[i]

Similarly for x[i] very negative, log(1+exp(x[i])) ≈ exp(x[i]), so s ≈ exp(x[i]), so tanh(s) ≈ exp(x[i])/(1 + exp(x[i])) which is exp(x[i])/(exp(x[i]) (1 + exp(-x[i]))) ) → 1/(1+exp(-x[i])). So:

Wait, perhaps it's better to use the formula:

mish(x) = x * tanh(softplus(x))

But to compute it numerically:

Compute softplus(x):

softplus = log(1 + exp(x))

But when x is large (like > 20), exp(x) is too big, so softplus(x) is approximately x, so tanh(softplus(x)) is ~ tanh(x) ~1, so mish(x) ≈ x*1 = x

When x is very negative (like < -20), exp(x) is near 0, so softplus(x) is ~ exp(x), so tanh(softplus(x)) is ~ tanh(exp(x)) ≈ exp(x) (since tanh(z) ≈ z for small z). So mish(x) ≈ x * exp(x), but for x very negative, this approaches zero. So in code, to handle these edge cases, perhaps we can use:

float softplus_x = std::log(1.0f + std::exp(x_val));
float tanh_softplus = std::tanh(softplus_x);
result = x_val * tanh_softplus;

But in CUDA, we need to use math functions from <math.h>. However, for very large x, exp(x) might overflow. To prevent that, we can use the following trick:

When x is large (x > 20), exp(x) is infinity, so log(1 + exp(x)) is x, so tanh(x) is 1, so mish(x) is x.

When x is very negative (x < -20), exp(x) is 0, so log(1+0) is 0, so tanh(0) is 0, so mish(x) is 0.

Thus, in code, we can handle these cases to avoid overflow:

if (x_val > 20.f) {
    result = x_val;
} else if (x_val < -20.f) {
    result = 0.f;
} else {
    float softplus_x = log(1.0f + exp(x_val));
    float tanh_softplus = tanh(softplus_x);
    result = x_val * tanh_softplus;
}

This would prevent overflow. So the kernel can compute mish this way.

Now, moving back to the plan:

First, the subtract + hardswish can be fused into one kernel.

Second, the mish can be another kernel.

Additionally, the MaxPool is kept as a PyTorch op.

Wait, but the MaxPool is a pooling layer which requires taking the maximum over a kernel. Since it's a 2x2 window, maybe we can also fuse it with the subsequent mish? Let me see: the MaxPool reduces the spatial dimensions, so the mish would be applied to the pooled feature maps. So, after MaxPool, the data is in a different shape. To combine MaxPool and Mish into a single kernel would require handling both operations in the same kernel, which complicates things. Since MaxPool is a spatial operation, it might not be worth the effort. So better to keep the MaxPool as is, and apply Mish via a custom kernel.

So the kernels to implement:

1. Subtract and HardSwish fused into one kernel.

2. Mish activation.

Additionally, perhaps the subtraction and HardSwish can be done in a single kernel, which would take the input tensor (output of conv) and the subtract value, then apply the operations.

Now, let's write the code for the first kernel (subtract + hardswish):

The hardswish function is defined as x * relu6(x + 3)/6, where relu6 is min(max(x + 3, 0), 6). So:

hardswish(x) = x * min(max(x + 3, 0), 6) / 6

But with the subtract first: x = x - subtract_value, then apply hardswish.

Wait the original sequence is:

x = conv_out

x = x - subtract_value

x = hardswish(x)

So the fused kernel would take conv_out and subtract_value, subtract the value, then apply hardswish.

So the kernel code would be:

for each element in the input tensor:

x = input[i] - subtract_value

then compute hardswish(x):

temp = x + 3

if temp < 0 → 0

else if temp >6 →6

else → temp

then result = (x * temp) / 6.0

Wait, let me re-derive:

hardswish formula:

hardswish(x) = x * ReLU6(x + 3) / 6

Where ReLU6(z) = min(max(z,0),6)

So:

z = x + 3

if z <0 →0, so result 0.

else if z>6 →6, so result x*6/6 = x

else → z, so result x*z /6

Thus, in code:

float temp = x - subtract_value; // after subtracting

float z = temp + 3;

if (z <0) {

    result = 0.f;

} else if (z >6) {

    result = temp;

} else {

    result = temp * z /6.f;

}

Wait, no, the formula is (temp * (min(max(z,0),6)) ) /6 ?

Wait, ReLU6(z) is min(max(z,0),6). So:

relu6 = max(0, z) → then min with 6.

Thus, ReLU6(z) is the clamped value between 0 and6.

So the result is temp * (ReLU6(z)) /6.

So:

z = temp +3.

relu6_z = clamp(z, 0.f, 6.f);

result = temp * relu6_z /6.f;

Yes.

So the kernel code for the first fused step is straightforward.

Now, the kernel for Mish is as discussed earlier.

Now, the code structure would be:

Implement the subtract+hardswish as a custom CUDA kernel, and Mish as another.

Now, the model's forward pass would be:

x = self.conv(x)

x = subtract_and_hardswish_cuda(x, subtract_value)

x = self.pool(x)

x = mish_cuda(x)

Thus, the custom kernels are used here.

Now, writing the CUDA code for these kernels.

First, the subtract_and_hardswish kernel:

The function would take the input tensor and the subtract_value (a float), and return the output tensor.

The CUDA kernel would be something like:

__global__ void subtract_and_hardswish_kernel(const float* input, float subtract_val, float* output, int size) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < size) {

        float temp = input[idx] - subtract_val;

        float z = temp + 3.f;

        float relu6_z = fmaxf(0.f, fminf(z, 6.f));

        output[idx] = temp * relu6_z / 6.f;

    }

}

Then the host function:

torch::Tensor subtract_and_hardswish_cuda(torch::Tensor input, float subtract_val) {

    auto output = torch::empty_like(input);

    int size = input.numel();

    const int block_size = 256;

    int num_blocks = (size + block_size -1)/block_size;

    subtract_and_hardswish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), subtract_val, output.data_ptr<float>(), size);

    return output;

}

Similarly, for the Mish kernel:

__global__ void mish_kernel(const float* input, float* output, int size) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < size) {

        float x_val = input[idx];

        if (x_val > 20.f) {

            output[idx] = x_val;

        } else if (x_val < -20.f) {

            output[idx] = 0.f;

        } else {

            float exp_x = expf(x_val);

            float softplus_x = logf(1.f + exp_x);

            float tanh_softplus = tanhf(softplus_x);

            output[idx] = x_val * tanh_softplus;

        }

    }

}

Host function:

torch::Tensor mish_cuda(torch::Tensor input) {

    auto output = torch::empty_like(input);

    int size = input.numel();

    const int block_size = 256;

    int num_blocks = (size + block_size -1)/block_size;

    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);

    return output;

}

Now, compiling these kernels with load_inline.

Now, putting this into the ModelNew class:

The ModelNew class will replace the subtract and hardswish with the fused kernel, and the mish with the custom kernel. The conv and pool are kept as PyTorch modules.

Wait, but in the original model, the subtract is a scalar stored in self.subtract_value. So in the new model, the subtract value is a parameter or just a constant? In the original code, it's a parameter (since it's passed in get_init_inputs as subtract_value). But in the new model, since we're using a kernel that takes the subtract_value as a float, we can pass it directly from the model's parameter.

Wait in the original Model's __init__, subtract_value is stored as an attribute. So in the new ModelNew, we can do the same.

Thus, in the forward:

def forward(self, x):

    x = self.conv(x)

    x = self.subtract_and_hardswish(x, self.subtract_value)

    x = self.pool(x)

    x = self.mish(x)

    return x

Wait, but the custom functions need to be called with the correct parameters.

Wait, in the code, the subtract_and_hardswish_cuda function is part of a loaded module. So perhaps:

The ModelNew will have the two custom functions as attributes.

Wait, in the example given, they used:

elementwise_add = load_inline(...)

and then in the ModelNew:

self.elementwise_add = elementwise_add

then in forward: self.elementwise_add.elementwise_add_cuda(a, b)

So similarly here, for the subtract_and_hardswish_cuda and mish_cuda functions, we can load them as modules.

Thus, the code structure would be:

First, write the CUDA source for subtract_and_hardswish, then for mish.

Then, load each inline, then in ModelNew, reference those functions.

Putting this all together:

The code would be:

First, define the two CUDA kernels as strings.

subtract_and_hardswish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void subtract_and_hardswish_kernel(
    const float* input, float subtract_val, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float temp = input[idx] - subtract_val;
        float z = temp + 3.0f;
        float relu6_z = fmaxf(0.0f, fminf(z, 6.0f));
        output[idx] = temp * relu6_z / 6.0f;
    }
}

torch::Tensor subtract_and_hardswish_cuda(
    torch::Tensor input, float subtract_val) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    subtract_and_hardswish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), subtract_val, output.data_ptr<float>(), size);
    return output;
}
"""

subtract_and_hardswish_cpp = "torch::Tensor subtract_and_hardswish_cuda(torch::Tensor input, float subtract_val);"

Then for the mish:

mish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void mish_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x_val = input[idx];
        if (x_val > 20.f) {
            output[idx] = x_val;
        } else if (x_val < -20.f) {
            output[idx] = 0.f;
        } else {
            float exp_x = expf(x_val);
            float softplus_x = logf(1.f + exp_x);
            float tanh_softplus = tanhf(softplus_x);
            output[idx] = x_val * tanh_softplus;
        }
    }
}

torch::Tensor mish_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
    return output;
}
"""

mish_cpp = "torch::Tensor mish_cuda(torch::Tensor input);"

Then load them inline:

subtract_and_hardswish = load_inline(
    name="subtract_and_hardswish",
    cuda_sources=subtract_and_hardswish_source,
    cpp_sources=subtract_and_hardswish_cpp,
    functions=["subtract_and_hardswish_cuda"],
    verbose=True,
)

mish = load_inline(
    name="mish",
    cuda_sources=mish_source,
    cpp_sources=mish_cpp,
    functions=["mish_cuda"],
    verbose=True,
)

Then in the ModelNew:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value = subtract_value
        self.pool = nn.MaxPool2d(pool_kernel_size)
        self.subtract_and_hardswish = subtract_and_hardswish
        self.mish = mish

    def forward(self, x):
        x = self.conv(x)
        x = self.subtract_and_hardswish.subtract_and_hardswish_cuda(x, self.subtract_value)
        x = self.pool(x)
        x = self.mish.mish_cuda(x)
        return x

Wait, but the parameters passed to the model are in_channels, etc. So the __init__ must match the original's get_init_inputs parameters. The original get_init_inputs returns [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]. So the ModelNew's __init__ must take those parameters.

So that should be correct.

Now, checking for any possible issues:

- The subtract_value is a float, so passed correctly to the kernel.

- The input tensors must be contiguous. The CUDA kernels assume that the input and output are contiguous. Since PyTorch's tensors may not be contiguous, the code uses .data_ptr(), which requires the tensors to be contiguous. Thus, in the functions, we can ensure that input is contiguous by using .contiguous().

Wait, the subtract_and_hardswish_cuda function creates an output tensor with the same shape as input, which is contiguous if input is. But if the input is not contiguous, the code may have issues. To ensure correctness, the input should be made contiguous before passing to the kernel. So perhaps in the function:

def subtract_and_hardswish_cuda(input, subtract_val):

    input = input.contiguous() 

    ... 

Similarly for the mish.

Wait, but in the CUDA code, the input is accessed via data_ptr(), which requires a contiguous tensor. So in the host function, we should make sure that the input is contiguous. So, in the code:

In subtract_and_hardswish_cuda function:

input = input.contiguous();

Similarly for the mish function.

Therefore, modifying the CUDA host functions to ensure input is contiguous.

Thus, in the subtract_and_hardswish_cuda function:

torch::Tensor subtract_and_hardswish_cuda(
    torch::Tensor input, float subtract_val) {
    input = input.contiguous(); // Ensure contiguous
    auto output = torch::empty_like(input);
    ...
}

Similarly for the mish_cuda function.

This is important to avoid memory access errors. So adding .contiguous() in the host functions.

Also, the input and output should be on the same device. Since the model is run on CUDA, the tensors are already on GPU. So the code is okay.

Now, putting it all together in codeblocks.

Also, note that the original model uses self.subtract_value, which is a float, so in the new model, the parameter is stored as a float, not a tensor. So that's fine, since the kernel takes a float.

Another thing to check: the MaxPool2d is kept as a PyTorch module, so no problem there.

Now, the final code should look like this.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused subtract + HardSwish CUDA kernel
subtract_and_hardswish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void subtract_and_hardswish_kernel(
    const float* input, float subtract_val, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float temp = input[idx] - subtract_val;
        float z = temp + 3.0f;
        float relu6_z = fmaxf(0.0f, fminf(z, 6.0f));
        output[idx] = temp * relu6_z / 6.0f;
    }
}

torch::Tensor subtract_and_hardswish_cuda(
    torch::Tensor input, float subtract_val) {
    input = input.contiguous();
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    subtract_and_hardswish_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), subtract_val, output.data_ptr<float>(), size);
    return output;
}
"""

subtract_and_hardswish_cpp = "torch::Tensor subtract_and_hardswish_cuda(torch::Tensor input, float subtract_val);"

# Define the Mish CUDA kernel
mish_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void mish_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x_val = input[idx];
        if (x_val > 20.f) {
            output[idx] = x_val;
        } else if (x_val < -20.f) {
            output[idx] = 0.f;
        } else {
            float exp_x = expf(x_val);
            float softplus_x = logf(1.f + exp_x);
            float tanh_softplus = tanhf(softplus_x);
            output[idx] = x_val * tanh_softplus;
        }
    }
}

torch::Tensor mish_cuda(torch::Tensor input) {
    input = input.contiguous();
    auto output = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    int num_blocks = (size + block_size - 1) / block_size;
    mish_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);
    return output;
}
"""

mish_cpp = "torch::Tensor mish_cuda(torch::Tensor input);"

# Compile the CUDA kernels
subtract_and_hardswish = load_inline(
    name="subtract_and_hardswish",
    cuda_sources=subtract_and_hardswish_source,
    cpp_sources=subtract_and_hardswish_cpp,
    functions=["subtract_and_hardswish_cuda"],
    verbose=True,
)

mish = load_inline(
    name="mish",
    cuda_sources=mish_source,
    cpp_sources=mish_cpp,
    functions=["mish_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value = subtract_value
        self.pool = nn.MaxPool2d(pool_kernel_size)
        self.subtract_and_hardswish = subtract_and_hardswish
        self.mish = mish

    def forward(self, x):
        x = self.conv(x)
        x = self.subtract_and_hardswish.subtract_and_hardswish_cuda(x, self.subtract_value)
        x = self.pool(x)
        x = self.mish.mish_cuda(x)
        return x

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract_value, pool_kernel_size]

# Global variables from the original setup
batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
subtract_value = 0.5
pool_kernel_size = 2
```

This code defines two custom CUDA kernels to fuse the subtract+HardSwish and Mish operations, reducing kernel launches and memory accesses. The convolution and MaxPool2d are kept as PyTorch modules for efficiency, while the element-wise operations are optimized into fused kernels to minimize overhead. All tensors are ensured to be contiguous for efficient memory access, and numerical stability is handled in the Mish kernel.