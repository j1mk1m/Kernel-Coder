The code you output must be compatible with Python 3.8+, PyTorch 1.13.1, CUDA 11.8. 

You may assume the following environment:
- CUDA_ARCH_LIST=8.6 (for nvcc)
- CUDA_ARCH_LIST=sm_86 (for nvcc)
- The code is compiled with PyTorch extensions using setup.py, but in your case, you can use the load_inline function to inline the CUDA code.

You can choose to either replace individual operators with CUDA kernels, fuse multiple operators into a single kernel (e.g., convolution + activation + bias), or implement algorithmic optimizations. Your goal is to maximize the speedup.



Okay, let's tackle optimizing the given Model with custom CUDA kernels. The model applies a 3D convolution followed by multiple activations and a bias addition. My plan is to fuse these operations into a single kernel to minimize memory transfers and maximize parallelism.

First, I'll look at the sequence of operations: Conv3D, ReLU, LeakyReLU, GELU, Sigmoid, and bias addition. Wait a second, some of these activations might not make sense together. For example, applying ReLU followed by LeakyReLU might be redundant. But the problem states the architecture as is, so I need to keep all the activations in order. Hmm, actually, ReLU is a special case of LeakyReLU with a slope of 0. So combining them might not be straightforward. Alternatively, maybe the order can be optimized, but I have to stick to the given order.

Wait, let's check the order again. The model does:
x = conv
ReLU
LeakyReLU (slope 0.01)
GELU
Sigmoid
Then adds bias. 

Wait, but applying ReLU first would set negative values to zero, then LeakyReLU would apply slope to those, but since ReLU already set them to 0, the LeakyReLU might not change them. Similarly, GELU is another activation. This might not be the most efficient sequence, but the problem requires preserving the architecture. So the goal is to fuse all these operations into a single kernel.

The main challenge is to implement the convolution followed by all these activations and the bias addition in a single CUDA kernel. However, convolution is a computationally heavy operation involving many input and output channels, and sliding window computations. Implementing a custom 3D convolution from scratch is quite complex and time-consuming. Maybe that's not feasible. Alternatively, perhaps we can use the existing convolution's output and then apply the activations and bias in a fused kernel.

Wait, maybe the convolution itself is the most time-consuming part, so fusing the subsequent activations and bias into a single kernel after the convolution could still provide some speedup. Alternatively, if possible, fusing the convolution with the activations might be better, but convolution's computation is more involved.

Alternatively, perhaps the order of the activations can be reordered or some can be combined. For instance, applying all the activations in sequence after the convolution. But according to the problem, the order must be preserved.

Another thought: the bias addition is a simple element-wise addition. So after the convolution and all activations, adding the bias could be fused into the last activation's computation. 

Let me think step by step:

1. The Conv3D is the first operation. Implementing a custom Conv3D is possible but requires handling the kernel weights and strides, which might be time-consuming. However, given the problem allows choosing which operators to replace, maybe we can leave the convolution as is and focus on fusing the activations and the bias.

Wait, but the problem allows replacing any operators, so perhaps replacing the sequence of activations and the bias addition with a fused kernel would be better.

So the plan is:

- Keep the Conv3D as is, since implementing a custom one might be too involved.
- Replace the sequence of activations (ReLU, LeakyReLU, GELU, Sigmoid) and the bias addition with a single fused kernel.

The fused kernel would take the output of the convolution, add the bias, then apply each activation in sequence. Wait, but the order must be preserved. Let me check the order again:

After convolution, the steps are:

x = torch.relu(x) → ReLU(x)
x = F.leaky_relu(x, 0.01) → LeakyReLU with slope 0.01 on the result of ReLU
x = F.gelu(x) → GELU
x = torch.sigmoid(x) → Sigmoid
x += self.bias → Add bias

Wait, the order is conv → ReLU → LeakyReLU → GELU → Sigmoid → bias addition. Wait, the bias is added after all the activations. Wait no, the code shows:

After convolution:

x = ReLU(x)

x = LeakyReLU(x)

x = GELU(x)

x = Sigmoid(x)

Then x = x + self.bias.

So the bias is added after all the activations. But the order of the activations must be maintained.

Wait, but the order of ReLU followed by LeakyReLU might not make sense. Because ReLU already sets negative values to zero. So LeakyReLU would apply slope to negative inputs, but ReLU already forced them to 0. So after ReLU, all values are non-negative, so LeakyReLU would leave them as is, since the slope is only applied to negative parts. So applying LeakyReLU after ReLU is redundant. However, the problem states the architecture as is, so I must keep that sequence. However, maybe the user made a mistake, but I have to follow the given code.

Assuming that the code is correct, then the order must be preserved. But this might be an error in the problem's setup. Anyway, proceeding as per given.

The fused kernel would need to:

Take the input tensor (after convolution), apply ReLU, then LeakyReLU, GELU, Sigmoid, and finally add the bias. Wait, but the bias is added after all activations. Wait the code has:

After the activations, x += self.bias. So the bias is added at the end. Therefore, in the fused kernel, after all activations, add the bias.

Therefore, the kernel steps per element:

out = sigmoid(gelu(leaky_relu(relu(x_conv) , slope) ) ) + bias

Wait, but the order is:

After ReLU, the next activation is LeakyReLU, which takes the output of ReLU and applies the slope to the negative parts. But since ReLU output is non-negative, the LeakyReLU will leave it as is. So the LeakyReLU after ReLU is redundant, but per the problem's code, it must be kept.

Similarly, the GELU and Sigmoid are applied. So the fused kernel must compute:

1. Apply ReLU: x = max(x, 0)
2. Apply LeakyReLU: x = (x >=0 ? x : 0.01*x) → but after ReLU, x is non-negative, so this does nothing. So this is redundant, but the code requires it.
3. Apply GELU: GELU is a smooth approximation, which is more complex.
4. Apply Sigmoid: 1/(1+exp(-x))
5. Add the bias.

So the fused kernel would compute all these steps in sequence for each element. Since the convolution output is the input to this sequence, the fused kernel can process each element independently, so it's amenable to a CUDA kernel.

Therefore, the plan is:

- Keep the Conv3D layer as is, since writing a custom one would be very time-consuming and perhaps not worth the effort for the speedup. The bulk of computation is in the convolution, but if we can't replace it, then the next step is to fuse the activations and bias into a single kernel.

Thus, the custom kernel will process the output of the convolution, apply all the activations in sequence, and then add the bias. Since all these are element-wise operations except for the convolution, this fused kernel can be written efficiently in CUDA.

So, the steps for the fused kernel are:

- For each element in the input tensor (after convolution):

1. Apply ReLU: max(0, x)
2. Apply LeakyReLU (slope 0.01): but since ReLU already set it to non-negative, this is same as x.
3. Apply GELU: GELU(x). The formula for GELU is 0.5 * x * (1 + tanh(√(2/π)(x + 0.044715x³)))
4. Apply Sigmoid: 1/(1 + exp(-x))
5. Add the bias value (element-wise addition, since the bias is a parameter of shape (out_channels, 1, 1, 1), so it's broadcastable to the tensor's dimensions).

Wait, the bias_shape is (out_channels, 1, 1, 1). So when adding, the bias is broadcasted over the spatial dimensions (depth, height, width). So in the kernel, for each element, the bias is the corresponding out_channel's value. So for a tensor of shape (batch, out_channels, depth, height, width), each element at (b, c, d, h, w) would add bias[c, 0, 0, 0].

Therefore, the kernel needs to have access to the bias tensor. Since the bias is a parameter, we can pass it as an argument to the kernel function.

Therefore, the fused kernel's function signature would be something like:

def fused_activations_and_bias(input, bias, output):

Wait, in CUDA terms, the kernel will process each element. The input is the output of the convolution, the bias is a 4D tensor (since it's (out_channels, 1, 1, 1)), and the output is the result after all activations and bias addition.

Now, writing the CUDA kernel:

The kernel function will loop over each element of the input. Let's see:

The input tensor has shape (batch_size, out_channels, depth, height, width). Let's say the total number of elements is N = batch_size * out_channels * depth * height * width.

The kernel can be written as:

__global__ void fused_activations_kernel(
    const float* input, const float* bias, float* output,
    int batch_size, int out_channels, int depth, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= N) return;

    // Compute the indices
    int w = idx % width;
    int h = (idx / width) % height;
    int d = (idx / (width * height)) % depth;
    int c = (idx / (width * height * depth)) % out_channels;
    int b = idx / (width * height * depth * out_channels);

    float x = input[idx];
    // Apply ReLU
    x = max(x, 0.0f);
    // Apply LeakyReLU (but since ReLU is applied first, this does nothing)
    // x = max(x, 0.01f * x); → same as before.
    // So can skip, but per problem's code, we have to apply it.
    // Wait, the LeakyReLU after ReLU is redundant, but we must apply it as per the given code.
    // Wait LeakyReLU with negative slope 0.01 applied after ReLU (which sets negatives to zero). The LeakyReLU's formula is:
    // y = x if x >=0 else 0.01*x. But after ReLU, x is non-negative, so LeakyReLU leaves it as x. So effectively, it does nothing.
    // But the code has it, so we have to compute it. However, since it's redundant, perhaps the code can omit it, but since the problem says to preserve the architecture, we must include it. So in code:

    // Apply LeakyReLU (slope 0.01)
    if (x < 0) x = 0.01f * x;
    else x = x;

    // But since after ReLU, x is >=0, so this is redundant. So perhaps the problem has a mistake, but we must follow the given code.

    // Apply GELU. Let's compute the GELU function here.
    // GELU(x) = 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3) ) )
    const float sqrt_2_over_pi = 0.7978845608f;
    const float approximation_term = 0.044715f;
    float inner = sqrt_2_over_pi * (x + approximation_term * x*x*x);
    float tanh_val = tanhf(inner);
    x = 0.5f * x * (1.0f + tanh_val);

    // Apply Sigmoid
    x = 1.0f / (1.0f + expf(-x));

    // Add bias. The bias is (out_channels, 1, 1, 1), so for each c, the bias value is at bias[c * 1*1*1 + 0]
    // The bias tensor is stored as a 4D tensor, but in memory, it's contiguous. So the index into bias is c (since the other dimensions are 1)
    float bias_val = bias[c];
    x += bias_val;

    output[idx] = x;
}

This would handle each element. The kernel must be launched with enough threads to cover all elements.

The CUDA kernel function would need to handle the indices correctly. To compute the indices, perhaps it's easier to compute the linear index and then compute the position.

Now, implementing this in PyTorch via load_inline.

First, writing the CUDA code as a string. Also, note that the bias is a parameter, which is a tensor. So in the Python function, we need to pass it as an argument to the kernel.

The function in Python would be:

def fused_activation_cuda(input, bias):
    # compute the necessary dimensions
    batch_size, out_channels, depth, height, width = input.shape
    N = batch_size * out_channels * depth * height * width

    output = torch.zeros_like(input)

    block_size = 256
    num_blocks = (N + block_size - 1) // block_size

    fused_activations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr(), bias.data_ptr(), output.data_ptr(),
        batch_size, out_channels, depth, height, width
    )

    return output

Wait, but the CUDA kernel needs to be defined with the correct parameters. Let's write the CUDA code.

Putting this together, here's the CUDA kernel code:

#include <torch/extension.h>
#include <math.h>

__global__ void fused_activations_kernel(
    const float* input, const float* bias, float* output,
    int batch_size, int out_channels, int depth, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_channels * depth * height * width) return;

    int w = idx % width;
    int h = (idx / width) % height;
    int d = (idx / (width * height)) % depth;
    int c = (idx / (width * height * depth)) % out_channels;
    int b = idx / (width * height * depth * out_channels);

    float x = input[idx];

    // Apply ReLU
    x = fmaxf(x, 0.0f);

    // Apply LeakyReLU (slope 0.01)
    // Since after ReLU, x is non-negative, this is redundant but per the code, must apply
    // But the LeakyReLU with negative_slope=0.01 is:
    // x = max(x, 0.01*x) when x is negative, but since x is already non-negative, it's same as x
    // So the code can be:
    // x = (x >=0) ? x : 0.01*x;
    // But after ReLU, x is >=0, so this is redundant. However, the original code applies it, so we must include it.
    // So perhaps the problem's code has a mistake, but we must proceed as per given.
    // Alternatively, maybe the order is different. Wait, in the original code, the LeakyReLU is applied after ReLU, so the input to LeakyReLU is already non-negative. Hence, this step does nothing. So the code can omit it, but the problem requires us to follow the given architecture. Therefore, we must include it in the kernel, even though it's redundant.

    // Compute LeakyReLU
    if (x < 0) {
        x = 0.01f * x;
    } else {
        x = x;
    }

    // Apply GELU
    const float sqrt_2_over_pi = 0.7978845608f;
    const float approximation_term = 0.044715f;
    float inner = sqrt_2_over_pi * (x + approximation_term * x * x * x);
    float tanh_val = tanhf(inner);
    x = 0.5f * x * (1.0f + tanh_val);

    // Apply Sigmoid
    x = 1.0f / (1.0f + expf(-x));

    // Add bias. The bias is (out_channels, 1, 1, 1)
    float bias_val = bias[c]; // since the other dimensions are 1, the index is c
    x += bias_val;

    output[idx] = x;
}

Then the wrapper function in C++:

torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor bias) {
    // Check shapes
    TORCH_CHECK(input.dim() == 5, "Input must be 5D tensor");
    TORCH_CHECK(bias.dim() == 4, "Bias must be 4D tensor");
    int64_t out_channels = input.size(1);
    TORCH_CHECK(bias.size(0) == out_channels, "Bias out_channels must match input's");
    TORCH_CHECK(bias.size(1) == 1 && bias.size(2) == 1 && bias.size(3) == 1, "Bias must have shape (out_channels, 1, 1, 1)");

    int batch_size = input.size(0);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);

    const int N = batch_size * out_channels * depth * height * width;
    const int block_size = 256;
    const int num_blocks = (N + block_size - 1) / block_size;

    auto output = torch::empty_like(input);

    // Launch kernel
    fused_activations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), 
        bias.data_ptr<float>(), 
        output.data_ptr<float>(),
        batch_size, out_channels, depth, height, width
    );

    return output;
}

Wait, but the CUDA code needs to be written as a string in Python. Also, the function parameters must match.

Putting this into the Python code:

The CUDA source code string (elementwise_add_source in the example):

fused_activation_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_activations_kernel(
    const float* input, const float* bias, float* output,
    int batch_size, int out_channels, int depth, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_channels * depth * height * width) return;

    int w = idx % width;
    int h = (idx / width) % height;
    int d = (idx / (width * height)) % depth;
    int c = (idx / (width * height * depth)) % out_channels;
    int b = idx / (width * height * depth * out_channels);

    float x = input[idx];

    // Apply ReLU
    x = fmaxf(x, 0.0f);

    // Apply LeakyReLU (slope 0.01)
    // Since after ReLU, x is non-negative, this is redundant but per the code, must apply
    // But the LeakyReLU with negative_slope=0.01 is:
    // x = max(x, 0.01*x) when x is negative, but since x is already non-negative, it's same as x
    // So the code can be:
    // x = (x >=0) ? x : 0.01*x;
    // But after ReLU, x is >=0, so this is redundant. However, the original code applies it, so we must include it.
    // Alternatively, maybe the problem has a mistake, but we proceed as per given.
    if (x < 0) {
        x = 0.01f * x;
    } else {
        x = x;
    }

    // Apply GELU
    const float sqrt_2_over_pi = 0.7978845608f;
    const float approximation_term = 0.044715f;
    float inner = sqrt_2_over_pi * (x + approximation_term * x * x * x);
    float tanh_val = tanhf(inner);
    x = 0.5f * x * (1.0f + tanh_val);

    // Apply Sigmoid
    x = 1.0f / (1.0f + expf(-x));

    // Add bias. The bias is (out_channels, 1, 1, 1)
    float bias_val = bias[c]; // since the other dimensions are 1, the index is c
    x += bias_val;

    output[idx] = x;
}

torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor bias) {
    // Check shapes
    if(input.dim() !=5) {
        std::cerr << "Input must be 5D tensor" << std::endl;
        exit(-1);
    }
    if(bias.dim() !=4) {
        std::cerr << "Bias must be 4D tensor" << std::endl;
        exit(-1);
    }
    int64_t out_channels = input.size(1);
    if(bias.size(0) != out_channels) {
        std::cerr << "Bias out_channels must match input's" << std::endl;
        exit(-1);
    }
    if(bias.size(1)!=1 || bias.size(2)!=1 || bias.size(3)!=1) {
        std::cerr << "Bias must have shape (out_channels, 1, 1, 1)" << std::endl;
        exit(-1);
    }

    int batch_size = input.size(0);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);

    const int N = batch_size * out_channels * depth * height * width;
    const int block_size = 256;
    const int num_blocks = (N + block_size - 1) / block_size;

    auto output = torch::empty_like(input);

    // Launch kernel
    fused_activations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(), 
        bias.data_ptr<float>(), 
        output.data_ptr<float>(),
        batch_size, out_channels, depth, height, width
    );

    return output;
}
"""

Wait, but in C++, the error checking could be more robust, but for simplicity, we can do it this way. Also, in the code, using exit might not be ideal, but for the inline code, perhaps it's okay.

Then the header:

fused_activation_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor bias);"
)

Then, load the kernel:

fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

Now, the model_new would replace the sequence of activations and the bias addition with this fused kernel.

The original Model has:

def forward(self, x):
    x = self.conv(x)
    x = torch.relu(x)
    x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)
    x = torch.nn.functional.gelu(x)
    x = torch.sigmoid(x)
    x = x + self.bias
    return x

The new ModelNew would do:

def forward(self, x):
    x = self.conv(x)
    x = self.fused_activation(x, self.bias)
    return x

Wait, but the fused_activation_cuda function takes input and bias as arguments, so in the forward, we need to pass the conv output and the bias parameter.

So the code for ModelNew would be:

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_activation = fused_activation

    def forward(self, x):
        x = self.conv(x)
        # The fused activation takes input and bias as arguments
        return self.fused_activation.fused_activation_cuda(x, self.bias)

Wait, the 'fused_activation' is the module returned by load_inline, which has the 'fused_activation_cuda' function. So in the forward, we call self.fused_activation.fused_activation_cuda(x, self.bias).

Putting it all together:

The full code would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernel
fused_activation_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_activations_kernel(
    const float* input, const float* bias, float* output,
    int batch_size, int out_channels, int depth, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_channels * depth * height * width) return;

    int w = idx % width;
    int h = (idx / width) % height;
    int d = (idx / (width * height)) % depth;
    int c = (idx / (width * height * depth)) % out_channels;
    int b = idx / (width * height * depth * out_channels);

    float x = input[idx];

    // Apply ReLU
    x = fmaxf(x, 0.0f);

    // Apply LeakyReLU (slope 0.01)
    if (x < 0) {
        x = 0.01f * x;
    } else {
        x = x;
    }

    // Apply GELU
    const float sqrt_2_over_pi = 0.7978845608f;
    const float approximation_term = 0.044715f;
    float inner = sqrt_2_over_pi * (x + approximation_term * x * x * x);
    float tanh_val = tanhf(inner);
    x = 0.5f * x * (1.0f + tanh_val);

    // Apply Sigmoid
    x = 1.0f / (1.0f + expf(-x));

    // Add bias
    float bias_val = bias[c];
    x += bias_val;

    output[idx] = x;
}

torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor bias) {
    // Check input dimensions
    if (input.dim() != 5) {
        std::cerr << "Input must be a 5D tensor" << std::endl;
        exit(EXIT_FAILURE);
    }
    if (bias.dim() != 4) {
        std::cerr << "Bias must be a 4D tensor" << std::endl;
        exit(EXIT_FAILURE);
    }
    int64_t out_channels = input.size(1);
    if (bias.size(0) != out_channels) {
        std::cerr << "Bias out_channels must match input's" << std::endl;
        exit(EXIT_FAILURE);
    }
    if (bias.size(1) != 1 || bias.size(2) != 1 || bias.size(3) != 1) {
        std::cerr << "Bias must have shape (out_channels, 1, 1, 1)" << std::endl;
        exit(EXIT_FAILURE);
    }

    int batch_size = input.size(0);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);

    const int N = batch_size * out_channels * depth * height * width;
    const int block_size = 256;
    const int num_blocks = (N + block_size - 1) / block_size;

    auto output = torch::empty_like(input);

    fused_activations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, out_channels, depth, height, width
    );

    return output;
}
"""

fused_activation_cpp_source = (
    "torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor bias);"
)

# Compile the fused kernel
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_cuda_cflags=["-std=c++14"],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_activation = fused_activation  # The loaded module

    def forward(self, x):
        x = self.conv(x)
        # Call the fused CUDA function
        return self.fused_activation.fused_activation_cuda(x, self.bias)

# The rest of the code (batch_size, etc.) remains the same as in the original problem.

Wait, but in the original problem's get_init_inputs function, the parameters are passed as [in_channels, out_channels, kernel_size, bias_shape]. So the __init__ of ModelNew must take these parameters.

Wait the original Model's __init__ is:

def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
    super().__init__()
    self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
    self.bias = nn.Parameter(torch.randn(bias_shape))

Thus, the ModelNew should take the same parameters.

Testing the code for compilation:

The CUDA kernel uses threadIdx and blockIdx, and the kernel is correctly launched. The fused_activation_cuda function checks the tensor dimensions and launches the kernel.

Potential issues:

- The LeakyReLU step after ReLU is redundant, but the problem requires it, so it's kept.
- The GELU approximation uses the standard formula with the cubic term, which is correct.
- The bias is added correctly based on the channel index.
- The kernel uses threadIdx correctly and handles the indices properly.

This should provide a speedup by fusing all the element-wise operations into a single kernel, thus reducing the number of memory copies and kernel launches.
</think>

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the fused CUDA kernel for activations and bias addition
fused_activation_source = """
#include <torch/extension.h>
#include <math.h>

__global__ void fused_activations_kernel(
    const float* input, const float* bias, float* output,
    int batch_size, int out_channels, int depth, int height, int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * out_channels * depth * height * width) return;

    // Compute indices
    int w = idx % width;
    int h = (idx / width) % height;
    int d = (idx / (width * height)) % depth;
    int c = (idx / (width * height * depth)) % out_channels;
    int b = idx / (width * height * depth * out_channels);

    float x = input[idx];

    // Apply ReLU
    x = fmaxf(x, 0.0f);

    // Apply LeakyReLU (slope 0.01)
    if (x < 0) {
        x = 0.01f * x;
    }

    // Apply GELU approximation
    const float sqrt_2_over_pi = 0.7978845608f;
    const float approximation_term = 0.044715f;
    float inner = sqrt_2_over_pi * (x + approximation_term * x * x * x);
    float tanh_val = tanhf(inner);
    x = 0.5f * x * (1.0f + tanh_val);

    // Apply Sigmoid
    x = 1.0f / (1.0f + expf(-x));

    // Add bias (shape: [out_channels, 1, 1, 1])
    x += bias[c];

    output[idx] = x;
}

torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor bias) {
    if (input.dim() != 5) {
        std::cerr << "Input must be a 5D tensor" << std::endl;
        exit(EXIT_FAILURE);
    }
    if (bias.dim() != 4) {
        std::cerr << "Bias must be a 4D tensor" << std::endl;
        exit(EXIT_FAILURE);
    }
    if (input.size(1) != bias.size(0)) {
        std::cerr << "Bias channels must match input's" << std::endl;
        exit(EXIT_FAILURE);
    }
    if (bias.size(1) != 1 || bias.size(2) != 1 || bias.size(3) != 1) {
        std::cerr << "Bias must have shape (C, 1, 1, 1)" << std::endl;
        exit(EXIT_FAILURE);
    }

    int batch_size = input.size(0);
    int out_channels = input.size(1);
    int depth = input.size(2);
    int height = input.size(3);
    int width = input.size(4);
    int N = batch_size * out_channels * depth * height * width;

    const int block_size = 256;
    const int num_blocks = (N + block_size - 1) / block_size;

    auto output = torch::empty_like(input);

    fused_activations_kernel<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size, out_channels, depth, height, width
    );

    return output;
}
"""

fused_activation_cpp_source = "torch::Tensor fused_activation_cuda(torch::Tensor input, torch::Tensor bias);"

# Compile the fused kernel
fused_activation = load_inline(
    name="fused_activation",
    cpp_sources=fused_activation_cpp_source,
    cuda_sources=fused_activation_source,
    functions=["fused_activation_cuda"],
    verbose=True,
    extra_cflags=["-std=c++14"],
    extra_cuda_cflags=["-std=c++14"],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.fused_activation = fused_activation

    def forward(self, x):
        x = self.conv(x)
        return self.fused_activation.fused_activation_cuda(x, self.bias)

# Environment setup remains the same as original
batch_size = 64
in_channels = 8
out_channels = 32
depth, height, width = 32, 64, 64
kernel_size = 3
bias_shape = (out_channels, 1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, bias_shape]
```