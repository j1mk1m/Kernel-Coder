To optimize the given architecture, I'll focus on fusing multiple operators into a single CUDA kernel to reduce memory overhead and kernel launch overhead. The steps to be fused are:

1. **Convolution Transpose** followed by **Mean Pooling**: Since mean pooling is applied over the depth dimension, this can be integrated into the convolution transpose computation by accumulating the contributions across depth.
2. **Bias Addition** and **Softmax** over channels: After computing the mean, adding the bias and then applying softmax can be done in the same kernel.
3. **Tanh** and **Scaling**: Finally, the tanh and scaling can be combined into a single operation.

However, due to complexity, I'll start by fusing the most compute-heavy parts first. Let's focus on fusing the Convolution Transpose and Mean Pooling first. But implementing a fused ConvTranspose3d with Mean Pooling might be very complex. Alternatively, perhaps it's better to first implement the mean pooling and bias addition together, followed by softmax and scaling. Alternatively, maybe the addition of bias and the mean pooling can be combined. Let me think through the steps.

Looking at the forward pass:

1. **ConvTranspose3d**: This is a major compute operation. Implementing a custom version would require handling the transposed convolution math, which is non-trivial. Maybe it's better to keep this as is since PyTorch's implementation is optimized. Alternatively, if we can find that the mean pooling can be incorporated into the convolution computation, but that might be complex.

2. **Mean Pooling over depth**: This reduces the depth dimension to 1. Since it's a mean over depth, perhaps this can be integrated into the convolution transpose kernel. Let me think: the output depth after ConvTranspose is computed based on input depth and stride. The mean pooling is then over that depth, which might not be straightforward to combine. Maybe better to handle the mean pooling in a separate kernel but optimized.

3. **Bias addition**: This is a simple addition, which can be done in the same kernel as the next operations.

4. **Softmax over channels**: The softmax can be implemented in a custom kernel, especially since it's over the channel dimension which may have a manageable size.

5. **Tanh and scaling**: These are element-wise operations and can be combined into a single kernel.

So perhaps the most effective fusion is to combine the **bias addition**, **softmax**, **tanh**, and **scaling** into a single kernel. Let's see:

The steps after ConvTranspose and Mean Pooling are:

x = x.mean(...) → shape (B, C, 1, H, W)

Then:

x += bias → (B, C, 1, H, W)

softmax over channels → (B, C, 1, H, W)

tanh → same shape

scale → same shape.

These steps can all be done in a single kernel, which reduces the number of memory copies and kernel launches.

Similarly, the mean pooling can be optimized. The current code does x.mean(dim=2, keepdim=True), which is a reduction over depth. The mean can be computed as sum over depth divided by depth. So, perhaps we can write a custom kernel that does this mean pooling.

So the plan is:

- Keep the ConvTranspose3d as is (since it's a complex operation, but perhaps PyTorch's implementation is already efficient). However, if we can find a way to optimize it, that's better. But for now, proceed with the other steps.

- Implement a custom kernel for the mean pooling over depth (step 2).

- Implement a custom kernel that fuses bias addition, softmax over channels, tanh, and scaling (steps 3,4,5,6).

Therefore, two custom kernels: one for mean pooling, another for the fused operations.

Alternatively, perhaps even fuse mean pooling with the next steps? Let me see:

Mean pooling is over depth (dim=2), which reduces it to 1. The next steps after that are adding bias (which is per channel, so independent of the spatial dims), then softmax over channels, then tanh and scaling. The bias is added per channel, which is independent of the spatial dimensions. The softmax is over the channel dimension.

Wait, after the mean pooling, the output is of shape (B, C, 1, H, W). The bias is of shape (1, C, 1, 1, 1), so adding it to the tensor would broadcast over the spatial dimensions, resulting in the same shape.

Therefore, the operations after convolution transpose can be expressed as:

x = mean_pooling(x) → (B, C, 1, H, W)

x += bias → (B, C, 1, H, W)

x = softmax(x, dim=1) → (B, C, 1, H, W)

x = tanh(x) → same shape

x = x * scaling_factor → same shape.

These steps can all be combined into a single kernel. Let's see:

First, the mean pooling over depth is a reduction operation. So the first kernel would compute the mean, then proceed to do the rest.

Alternatively, perhaps the mean pooling can be written as a separate kernel, then the rest fused.

Let me outline the steps:

First, the mean pooling over depth:

Original input after ConvTranspose: (B, C, D_out, H_out, W_out)

After mean pooling over dim=2 (depth), the output is (B, C, 1, H_out, W_out).

This can be done by a kernel that for each (B, C, h, w), computes the average over the D dimension.

The kernel would loop over each element in B, C, h, w, and for each, compute sum over D and divide by D.

This is a reduction kernel, which can be optimized.

Then, the remaining operations can be done in a single kernel:

Given the output of the mean pooling (B, C, 1, H, W), perform:

1. Add bias (shape 1xCx1x1x1)

2. Softmax over channels (dim=1)

3. Tanh

4. Multiply by scaling factor.

All these can be done in a single kernel since they are element-wise operations (except softmax which requires normalization across channels).

Wait, the softmax over channels requires computing the exponential of each element, summing over channels, then dividing by the sum. This is not element-wise, but can be done per channel.

Let me think of the computation steps for the fused kernel:

For a given spatial position (h, w):

For each batch, and for each spatial position, we have a vector of C elements (since depth is 1 now). We need to compute softmax over those C elements, then apply tanh and scaling.

So, the steps are:

Compute for each position (b, c, 1, h, w):

value = x[b, c, 0, h, w]

Then:

value += bias[c]

Compute softmax over c:

Compute the max value for all c in this position (to prevent overflow).

Compute exp(value - max) for all c, sum over c to get sum_exp.

Then, value = exp(value - max) / sum_exp

Then apply tanh(value) * scaling_factor.

These steps can be done per spatial position, so the kernel can process each (b, h, w) and compute the softmax across channels.

However, the softmax requires a reduction over the channels for each spatial position. To do this efficiently, the kernel can process each spatial position (b, h, w) and for each, compute the max, compute the exp terms, accumulate the sum, then compute the normalized exponents, then apply the rest.

This requires synchronization within a block for the reduction. Alternatively, implement a block-wise reduction for the max and sum.

This could be done with a kernel that uses shared memory for the max and sum, but it might be a bit complex.

Alternatively, given that the number of channels is 64 (as per the test config), the softmax can be handled by a thread per channel, but this may not be efficient. Alternatively, we can have a thread block per spatial position (b, h, w), and have threads handle each channel.

Alternatively, here's a possible approach:

The kernel processes each spatial position (h, w) for a given batch and channel. Let's structure the kernel as follows:

- Each thread block handles a spatial position (h, w).

- Within the block, each thread corresponds to a channel index.

- Compute the max over all channels (each thread has its value, and they can compute the max using a reduction within the block).

- Then compute the exp for each thread's value minus the max, accumulate the sum of exps (using a reduction again).

- Then each thread computes its value as exp(val - max) / sum_exp, applies tanh and scaling.

But this would require atomic operations for the sum, or a proper reduction.

Alternatively, since the number of channels is 64, which is manageable, we can use a block of threads equal to the number of channels. Let me see:

Assume that for each spatial position (h, w), we have a block of threads. Each thread is responsible for a channel.

But in 3D, the spatial dimensions are H x W. So the grid size would be B x H x W, but B is batch size (16), H and W are 128 each. The block size would be 64 (number of channels). However, 16 * 128 * 128 = 245,760 blocks, each of size 64 threads. That's a lot of blocks, but maybe manageable on a GPU.

Alternatively, group spatial positions into blocks. This is a bit tricky.

Alternatively, the kernel could be structured as follows:

For the fused operations (bias + softmax + tanh + scale):

Input is a tensor of shape (B, C, 1, H, W)

The kernel can process each element as follows:

Each thread handles a single element (b, c, 0, h, w). So the total number of elements is B*C*H*W.

But for the softmax, the normalization is over the channels for each (b, h, w). Thus, for a given (b, h, w), all C channels must be processed together.

This suggests that the threads must be organized in such a way that they can collaborate on a per-(b,h,w) basis.

An efficient approach would be to have each block handle a (b, h, w) group. The block size would be the number of channels (C), so each thread in the block handles one channel.

The steps for the block would be:

1. Load all C values (for this b,h,w) into shared memory.

2. Compute the max over the C values.

3. Compute the exponentials and accumulate the sum.

4. Compute normalized values (exp(val - max) / sum_exp).

5. Apply tanh and scaling.

This requires synchronization and shared memory usage.

Let me sketch the CUDA kernel code for this.

First, the mean pooling kernel:

Mean pooling over depth (dim=2). The input has shape (B, C, D, H, W). The output is (B, C, 1, H, W). Each element is the average over the D dimension.

The kernel can be written as:

Each thread processes a single output element (b, c, 0, h, w). To compute this, the thread needs to sum over all d in 0..D-1 the input[b,c,d,h,w], then divide by D.

But with D being the input depth (after ConvTranspose), which in the test case is 32 (depth=32). So the kernel would need to loop over the depth dimension.

Alternatively, the kernel can be structured as follows:

The grid is B x C x H x W. Each thread block handles a single output element (b,c,h,w). The block has a number of threads equal to the depth (D). Each thread in the block loads one element from depth d and accumulates the sum.

But this may not be optimal since D can be 32 (as in the test case). Alternatively, the kernel can have each thread handle a single output element, and compute the sum in a loop over D.

Let's see:

Kernel for mean pooling:

__global__ void mean_pooling_3d_kernel(
    const float* input,
    float* output,
    int B, int C, int D, int H, int W)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*H*W) return;

    int b = idx / (C*H*W);
    int c = (idx % (C*H*W)) / (H*W);
    int h = (idx % (H*W)) / W;
    int w = idx % W;

    float sum = 0.0f;
    for (int d = 0; d < D; ++d) {
        sum += input[get_index(input, b, c, d, h, w)];
    }
    output[get_index(output, b, c, 0, h, W)] = sum / D;
}

Wait, but the output is (B,C,1,H,W), so the index for output is straightforward.

But how do we index into the input? The input is (B, C, D, H, W). So the index is:

input_offset = b * C*D*H*W + c * D*H*W + d * H*W + h * W + w.

But this could be done with a helper function.

Alternatively, compute it inline.

This kernel would have a grid of B*C*H*W, and each thread computes the sum over D elements. The loop over D is done in the thread.

This might be acceptable for D=32 (only 32 iterations per thread). But if D is large, it could be slow. However, in the test case, D is 32, so it's manageable.

Alternatively, using multiple threads per output element to parallelize the sum over D. But the above approach is simpler.

Now, the fused kernel for bias, softmax, tanh, scaling:

The input is (B, C, 1, H, W). The bias is (1, C, 1, 1, 1). The output is same as input.

The kernel must process each spatial position (b, h, w), across all channels.

Structure:

Each block handles a (b, h, w) position. The block size is C (number of channels). Each thread in the block handles a channel.

The steps:

1. Each thread loads its channel's value (x[b][c][0][h][w]) and the bias (bias[c][0][0][0]).

2. Add the bias to the value.

3. Compute the max value across all channels in the block (using a reduction in shared memory).

4. Compute exponentials of (value - max) for each channel.

5. Compute the sum of all exponentials (another reduction in shared memory).

6. Each thread computes its normalized exponential (exp_val / sum_exp).

7. Apply tanh and scaling.

This requires shared memory for the max and sum, and synchronization points.

Let me outline the code:

First, the kernel signature:

__global__ void fused_operations_kernel(
    const float* input,
    const float* bias,
    float* output,
    int B,
    int C,
    int H,
    int W,
    float scaling_factor)
{
    // blockIdx.x: batch index
    // blockIdx.y: h index
    // blockIdx.z: w index
    // But since B can be up to 16, H and W up to 128, this may be a problem.

    // Alternatively, the block index is (b, h, w) in a flattened index.

    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // Each block corresponds to a (b, h, w) position.

    // Wait, let's structure the block as follows:

    // Each block is for a (b, h, w) position.

    // blockIdx.x = b
    // blockIdx.y = h
    // blockIdx.z = w

    // Then, the block size is C (number of channels).

    // So thread index is c.

    int b = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int c = threadIdx.x;

    if (c >= C) return;

    extern __shared__ float shared[];

    float value = input[b * C * H * W + c * H * W + h * W + w] + bias[c];

    // Step 1: Compute max across channels

    // Use shared memory to hold all values.

    if (threadIdx.x == 0) {
        for (int i = 0; i < C; i++) {
            shared[i] = 0.0f;
        }
    }
    __syncthreads();

    shared[c] = value;
    __syncthreads();

    // Now compute max using a reduction in shared memory.

    // Implement a parallel reduction to find the maximum.

    int stride = 1;
    while (stride < C) {
        if (threadIdx.x % (2*stride) == 0) {
            if (shared[threadIdx.x + stride] > shared[threadIdx.x]) {
                shared[threadIdx.x] = shared[threadIdx.x + stride];
            }
        }
        stride *= 2;
        __syncthreads();
    }

    float max_val = shared[0];
    __syncthreads();

    // Compute exp(value - max_val)

    float exp_val = expf(value - max_val);

    // Accumulate the sum into shared memory.

    if (threadIdx.x == 0) {
        shared[0] = 0.0f;
    }
    __syncthreads();

    // Use atomicAdd for the sum?

    // Alternatively, do a parallel reduction for the sum.

    // First, write exp_val into shared memory.

    shared[threadIdx.x] = exp_val;
    __syncthreads();

    stride = 1;
    while (stride < C) {
        if (threadIdx.x % (2*stride) == 0) {
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        }
        stride *= 2;
        __syncthreads();
    }

    float sum_exp = shared[0];
    __syncthreads();

    // Compute normalized value.

    float normalized = exp_val / sum_exp;

    // Apply tanh and scaling.

    normalized = tanhf(normalized) * scaling_factor;

    // Write to output.

    output[b * C * H * W + c * H * W + h * W + w] = normalized;
}

Wait, but this requires that the block size is exactly C (64), so that each thread can handle a channel. Also, the shared memory must be at least of size C floats (64). So the kernel requires:

- Block dimensions: (C, 1, 1)

- Grid dimensions: (B, H, W)

The block size must be exactly C, which is 64 here. So the kernel can be launched with:

dim3 block_size(C);
dim3 grid_size(B, H, W);

sharedMemSize = C * sizeof(float) * 2 (since we need space for both max and sum steps? Or maybe we can reuse the same shared memory).

Wait in the code above, the shared memory is used first to store the values for max calculation, then for the exponentials for sum calculation.

Alternatively, the shared memory can be reused. Let's see:

The first part stores value in shared[c], then the max reduction is done.

Then, after max, the shared array is used to store exp_val, then sum is calculated.

Thus, the shared memory needs to be of size C floats.

The shared memory allocation is done via:

kernel<<<grid_size, block_size, C * sizeof(float)>>>(...);

In the kernel call.

This approach should work. However, there might be some off-by-one errors or indexing issues.

Now, putting all together.

The first step is to replace the mean pooling with a custom kernel.

But let me see:

The original code has:

x = self.conv_transpose(x) → (B, C, D_out, H_out, W_out)

x = x.mean(dim=2, keepdim=True) → (B, C, 1, H_out, W_out)

So the mean pooling is over depth (dim=2).

Thus, to implement the mean pooling, we need to write a CUDA kernel that takes the output of conv_transpose and produces the mean over dim=2.

The dimensions:

Input shape for mean pooling: (B, C, D, H, W)

Output shape: (B, C, 1, H, W)

The kernel for this can be written as:

The kernel function:

__global__ void mean_pooling_3d(
    const float* input,
    float* output,
    int B, int C, int D, int H, int W)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*H*W) return;

    int b = idx / (C*H*W);
    int c = (idx % (C*H*W)) / (H*W);
    int h = (idx % (H*W)) / W;
    int w = idx % W;

    float sum = 0.0f;
    for (int d = 0; d < D; ++d) {
        sum += input[b * C * D * H * W + c * D * H * W + d * H * W + h * W + w];
    }
    output[idx] = sum / D;
}

Wait, the output's index is (B, C, 1, H, W), so the output tensor is stored as B*C*H*W elements. The output index can be mapped as:

output_index = b * C*H*W + c * H*W + h * W + w.

Which is exactly the same as the input's idx, since D is summed over.

Thus, the kernel is correct.

Now, the fused kernel:

The parameters are:

input is of shape (B, C, 1, H, W), stored as B*C*H*W elements.

The bias is of shape (1, C, 1, 1, 1), so stored as C elements.

The kernel:

As above.

Now, compiling these kernels and integrating into the model.

In the ModelNew class, replace the mean pooling, bias addition, softmax, tanh, and scaling with the custom kernels.

First, for the mean pooling:

We can create a function that calls the kernel.

Then, the fused kernel.

Now, the problem is to write the Python code with the custom kernels.

First, the mean pooling kernel:

elementwise_add in the example was a simple kernel. Here, the mean_pooling_3d requires knowing the dimensions B, C, D, H, W.

Wait, in the code, when using the kernel, we need to pass these parameters.

Alternatively, the kernel can be written to take the input and output tensors, and the parameters can be inferred from the tensor's shape.

Wait in the CUDA kernel, the parameters B, C, D, H, W must be passed as arguments, since the kernel can't know the tensor's shape.

Thus, the Python function for the mean pooling would need to extract these dimensions from the input tensor.

For example, in the Python code:

def mean_pooling_3d_cuda(input):
    B, C, D, H, W = input.shape
    output = torch.empty(B, C, 1, H, W, device=input.device)
    # launch the kernel with B*C*H*W threads
    block_size = 256
    num_blocks = (B*C*H*W + block_size -1) // block_size
    mean_pooling_3d[blocks_per_grid=num_blocks, threads_per_block=block_size](
        input.data_ptr(), output.data_ptr(), B, C, D, H, W)
    return output

Wait but in CUDA, the kernel launch syntax in Python via torch.utils.cpp_extension.load_inline is a bit different.

Wait, the example used:

elementwise_add_kernel<<<num_blocks, block_size>>>(...);

But when using the load_inline function, the kernel is called via the generated function.

Wait in the example, the Python function elementwise_add_cuda takes the tensors and calls the kernel.

Thus, for the mean_pooling_3d, the Python function would be:

def mean_pooling_cuda(input):
    B, C, D, H, W = input.shape
    output = torch.zeros(B, C, 1, H, W, dtype=input.dtype, device=input.device)
    kernel = ... 
    # launch the kernel with the appropriate parameters.
    # Need to calculate grid and block size.

    # The kernel requires B*C*H*W elements processed.

    threads_per_block = 256
    blocks_per_grid = (B*C*H*W + threads_per_block - 1) // threads_per_block

    mean_pooling_3d[blocks_per_grid, threads_per_block](
        input.data_ptr(),
        output.data_ptr(),
        B, C, D, H, W
    )
    return output

But in the CUDA code, the kernel must be declared as:

extern "C" __global__ void mean_pooling_3d(...)

So the CUDA source code for the mean pooling kernel is:

mean_pooling_3d.cu:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_pooling_3d(
    const float* input,
    float* output,
    int B, int C, int D, int H, int W)
{
    // code as above
}

Then the Python function would need to call this kernel.

However, in the inline code, we can write the CUDA code as a string and load it.

Now, integrating all these into the ModelNew class:

First, define the CUDA kernels as strings.

Then, compile them via load_inline.

Now, let's proceed step by step.

First, the mean pooling kernel:

mean_pooling_3d_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_pooling_3d(
    const float* input,
    float* output,
    int B, int C, int D, int H, int W)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * C * H * W)
        return;

    int b = idx / (C * H * W);
    int c = (idx % (C * H * W)) / (H * W);
    int h = (idx % (H * W)) / W;
    int w = idx % W;

    float sum = 0.0f;
    for (int d = 0; d < D; ++d) {
        sum += input[b * C * D * H * W + c * D * H * W + d * H * W + h * W + w];
    }
    output[idx] = sum / D;
}
"""

The corresponding Python function would be:

def mean_pooling_cuda(input):
    B, C, D, H, W = input.shape
    output = torch.empty(B, C, 1, H, W, dtype=input.dtype, device=input.device)
    threads_per_block = 256
    blocks_per_grid = (B * C * H * W + threads_per_block - 1) // threads_per_block
    mean_pooling_3d[blocks_per_grid, threads_per_block](
        input.data_ptr(),
        output.data_ptr(),
        B, C, D, H, W
    )
    return output

But to make this inline, we need to wrap this into a function that can be called.

Wait, when using load_inline, we need to have a .cpp function that calls the kernel. So similar to the example:

The .cpp function would be:

torch::Tensor mean_pooling_cuda(torch::Tensor input) {
    B, C, D, H, W = input.sizes();
    auto output = torch::empty({B, C, 1, H, W}, input.options());
    // launch kernel here
    // but in C++, not Python
    // so the kernel must be called with appropriate parameters
    int threads_per_block = 256;
    int blocks_per_grid = (B*C*H*W + threads_per_block -1)/threads_per_block;
    mean_pooling_3d<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(), output.data_ptr<float>(),
        B, C, D, H, W);
    return output;
}

Wait, but in C++ code:

The input tensor's shape can be obtained via input.sizes(). So the code would need to extract B, C, D, H, W.

Therefore, the CUDA source code for the mean_pooling is as above, and the C++ wrapper function is as follows.

Therefore, the mean_pooling_3d_source would include both the kernel and the wrapper.

Wait, in the example, the elementwise_add_cuda function is written in C++:

torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {
    auto size = a.numel();
    auto out = torch::zeros_like(a);
    ... launch kernel ...
    return out;
}

Similarly, for mean_pooling_cuda:

So the full CUDA code for mean_pooling is:

mean_pooling_3d.cu:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_pooling_3d(
    const float* input,
    float* output,
    int B, int C, int D, int H, int W)
{
    // kernel code as before
}

torch::Tensor mean_pooling_cuda(torch::Tensor input) {
    // Get dimensions
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty({B, C, 1, H, W}, input.options());

    int threads_per_block = 256;
    int blocks_per_grid = (B * C * H * W + threads_per_block - 1) / threads_per_block;

    mean_pooling_3d<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, D, H, W
    );

    return output;
}

So the full CUDA source is:

mean_pooling_3d_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void mean_pooling_3d(
    const float* input,
    float* output,
    int B, int C, int D, int H, int W)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B * C * H * W)
        return;

    int b = idx / (C * H * W);
    int c = (idx % (C * H * W)) / (H * W);
    int h = (idx % (H * W)) / W;
    int w = idx % W;

    float sum = 0.0f;
    for (int d = 0; d < D; ++d) {
        sum += input[b * C * D * H * W + c * D * H * W + d * H * W + h * W + w];
    }
    output[idx] = sum / D;
}

torch::Tensor mean_pooling_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty({B, C, 1, H, W}, input.options());

    int threads_per_block = 256;
    int blocks_per_grid = (B * C * H * W + threads_per_block - 1) / threads_per_block;

    mean_pooling_3d<<<blocks_per_grid, threads_per_block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, D, H, W
    );

    return output;
}
"""

Then the header for the C++ function is:

mean_pooling_3d_cpp = "torch::Tensor mean_pooling_cuda(torch::Tensor input);"

Similarly, for the fused kernel.

Now, the fused kernel:

The fused_operations_kernel requires the input tensor (after mean pooling), the bias tensor, and the scaling factor.

The fused kernel is:

First, the CUDA code for the kernel.

The fused_operations.cu:

#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_operations(
    const float* input,
    const float* bias,
    float* output,
    int B,
    int C,
    int H,
    int W,
    float scaling_factor)
{
    // blockIdx.x: b
    // blockIdx.y: h
    // blockIdx.z: w
    // Each block corresponds to (b, h, w)
    // Threads per block: C (number of channels)

    int b = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int c = threadIdx.x;

    if (c >= C) return;

    extern __shared__ float shared[];

    float value = input[b * C * H * W + c * H * W + h * W + w] + bias[c];

    // Compute max across channels in the block

    // Step 1: load into shared memory
    shared[c] = value;
    __syncthreads();

    // reduction to find max
    int stride = 1;
    while (stride < C) {
        if (threadIdx.x % (2 * stride) == 0) {
            if (shared[threadIdx.x + stride] > shared[threadIdx.x]) {
                shared[threadIdx.x] = shared[threadIdx.x + stride];
            }
        }
        stride *= 2;
        __syncthreads();
    }

    float max_val = shared[0];
    __syncthreads();

    // compute exp(value - max_val)
    float exp_val = expf(value - max_val);

    // compute sum of exp_val across channels

    shared[threadIdx.x] = exp_val;
    __syncthreads();

    stride = 1;
    while (stride < C) {
        if (threadIdx.x % (2 * stride) == 0) {
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        }
        stride *= 2;
        __syncthreads();
    }

    float sum_exp = shared[0];

    float normalized = exp_val / sum_exp;
    normalized = tanhf(normalized) * scaling_factor;

    output[b * C * H * W + c * H * W + h * W + w] = normalized;
}

// C++ wrapper function
torch::Tensor fused_operations_cuda(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling_factor)
{
    int B = input.size(0);
    int C = input.size(1);
    int H = input.size(3);
    int W = input.size(4);

    // The input after mean pooling has dimensions (B, C, 1, H, W)
    // So H and W are input.size(3) and input.size(4)

    auto output = torch::empty_like(input);

    // Grid dimensions: (B, H, W)
    dim3 grid(B, H, W);

    // Block dimensions: (C, 1, 1)
    dim3 block(C, 1, 1);

    // Shared memory needed: C floats
    int shared_mem_size = C * sizeof(float);

    fused_operations<<<grid, block, shared_mem_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        B, C, H, W,
        scaling_factor
    );

    return output;
}

Thus, the CUDA source for the fused operations:

fused_operations_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_operations(
    const float* input,
    const float* bias,
    float* output,
    int B,
    int C,
    int H,
    int W,
    float scaling_factor)
{
    int b = blockIdx.x;
    int h = blockIdx.y;
    int w = blockIdx.z;
    int c = threadIdx.x;

    if (c >= C) return;

    extern __shared__ float shared[];

    float value = input[b * C * H * W + c * H * W + h * W + w] + bias[c];

    // Compute max across channels
    shared[c] = value;
    __syncthreads();

    int stride = 1;
    while (stride < C) {
        if (threadIdx.x % (2 * stride) == 0) {
            if (shared[threadIdx.x + stride] > shared[threadIdx.x]) {
                shared[threadIdx.x] = shared[threadIdx.x + stride];
            }
        }
        stride *= 2;
        __syncthreads();
    }

    float max_val = shared[0];
    __syncthreads();

    float exp_val = expf(value - max_val);

    shared[threadIdx.x] = exp_val;
    __syncthreads();

    stride = 1;
    while (stride < C) {
        if (threadIdx.x % (2 * stride) == 0) {
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        }
        stride *= 2;
        __syncthreads();
    }

    float sum_exp = shared[0];

    float normalized = exp_val / sum_exp;
    normalized = tanhf(normalized) * scaling_factor;

    output[b * C * H * W + c * H * W + h * W + w] = normalized;
