Also, in your code, ensure that all the CUDA kernel functions are **inline** with torch's load_inline function. 

        Here's the original architecture's forward function:

```python
def forward(self, x):
    x = self.matmul(x)
    x = torch.sigmoid(x) * x  # Swish activation
    x = x + self.bias
    x = self.group_norm(x)
    return x
```

The goal is to create an optimized forward path that can handle large batch sizes (like 32768) efficiently. Let's see your optimization! Okay, let's see. I need to optimize the given PyTorch model using custom CUDA kernels. The model does a matrix multiply, applies Swish activation (which is x * sigmoid(x)), adds a bias, then applies GroupNorm. The batch size is pretty big (32768), so efficiency is key here.

First, I should identify which parts of the forward pass can be optimized with CUDA. The main operations here are the linear layer (which includes a matrix multiplication and possibly a bias addition, but the bias is added separately here), the Swish activation, adding the bias term, and the GroupNorm.

Looking at the current steps:

1. MatMul (nn.Linear): This is a dense layer, which is a matrix multiply plus a bias. Wait, but in the given code, the nn.Linear might already include a bias, but in the Model's __init__, the bias is a separate parameter. Wait, checking the code again: the Linear layer's bias is controlled by its own initialization. The model's __init__ has self.bias as a separate parameter, so the Linear's bias is probably set to False. Let me check the parameters. The user's code defines self.matmul = nn.Linear(in_features, out_features), but they also have self.bias as a separate parameter. So the Linear's bias is not used, and they add the bias after the Swish? Wait no: the code says after Swish, they add self.bias. Wait the forward is:

x = self.matmul(x)  # this is x @ W^T + bias (if the Linear has bias). Wait, but in the code, the user's Model's __init__ doesn't mention the Linear's bias parameter. The default for nn.Linear is bias=True, but here maybe the user is using the Linear without bias, since they have a separate self.bias? Wait, no, the code shows:

Wait, in the given code, the Model's __init__ has:

self.matmul = nn.Linear(in_features, out_features)

The Linear by default has a bias. So the bias in the linear is included. However, the user then adds another self.bias (a parameter). Wait, that's conflicting. Wait, maybe the user made a mistake here? Because the Linear's bias and the self.bias would both be added. But in the forward:

x = self.matmul(x) --> which includes the Linear's bias. Then, after Swish, they add self.bias (the separate parameter). That's probably intentional, but maybe the Linear's bias is set to False? Wait the user's code doesn't specify, so perhaps the Linear has its own bias, but the user is adding an extra bias term. Hmm, but that's unusual. Maybe it's a typo, but assuming that's correct.

Anyway, the user wants to optimize the forward path. Let me focus on the steps:

The main operations are:

1. Linear (matrix multiply + optional bias)
2. Swish activation (sigmoid(x) * x)
3. Add bias (another bias term)
4. GroupNorm (normalizes over groups)

The GroupNorm is applied after the Swish and the bias addition. Since the batch size is very large, maybe we can find ways to fuse some of these operations into a single kernel to reduce memory copies and kernel launches, which can be slow.

First, let's look at the operations:

- The Linear layer's forward is x @ W^T + bias_linear. But if the Linear has its own bias, then the first step includes that. Then, after Swish, the user adds another bias term (self.bias). So overall, the bias terms are added in two places. That's a bit odd, but perhaps it's intended. Maybe the user wants to have a separate bias after Swish. 

Alternatively, maybe there's a mistake here, but I'll proceed as per the given code.

Now, the main idea for optimization is to fuse as many operations as possible into a single CUDA kernel to minimize the number of kernel launches and memory transfers. Since each CUDA kernel launch has some overhead, combining steps can help. Let's see which operations can be fused.

Looking at the steps:

1. Matrix multiply (matmul) â€“ this is the main compute-heavy operation. The matrix multiply is O(N^3) but here it's a linear layer, so it's batched. The batch size is large, so this might be a good candidate to handle efficiently. But the standard nn.Linear is already optimized, so maybe not much gain here unless we can fuse it with other steps.

2. Swish activation: this is an element-wise operation. Can be fused with other element-wise steps.

3. Add the self.bias (another element-wise addition)

4. GroupNorm: this is a bit more complex, but perhaps can be fused with some steps.

Wait, let's think about the sequence:

The order is:

linear_out = x @ W^T + linear.bias (if exists)

then swish = linear_out * sigmoid(linear_out)

then add_bias = swish + self.bias

then groupnorm = groupnorm(add_bias)

The group norm's computation is:

GroupNorm divides the channels into groups and normalizes each group across the batch. The formula is:

y = (x - mean) / sqrt(variance + eps) * gamma + beta

where gamma and beta are learned parameters. The parameters for GroupNorm are stored in the group_norm module (the self.group_norm instance). So, the group norm involves computing mean and variance per group, then scaling and shifting.

Hmm, integrating the group norm into a fused kernel might be challenging, but perhaps possible.

Alternatively, maybe we can combine the matmul with the Swish and the bias addition. Let's see:

The matmul is a dense matrix multiplication. The Swish is element-wise, as is the addition of the bias. So perhaps after the matrix multiply, we can perform Swish, add the bias, and then compute group norm all in one kernel.

But the group norm requires computing means and variances over groups. That might require a reduction step which complicates things.

Alternatively, perhaps we can first fuse the matmul with the Swish and bias addition, then handle the group norm separately. Let me think.

Let me consider the steps that can be fused:

- The matmul (matrix multiply) is O(N) in terms of batch size. The output dimensions are batch_size x out_features.

The Swish is element-wise: for each element x_i, compute x_i * sigmoid(x_i). The sigmoid is 1/(1+exp(-x_i)), but perhaps we can approximate it or find a way to compute efficiently. However, for a custom kernel, implementing sigmoid is straightforward.

The addition of self.bias is also element-wise.

So steps 2 and 3 are element-wise. So perhaps the matmul can be followed by a kernel that computes Swish and adds the bias. However, the matmul is a separate step. Since the matmul is already a dense operation, perhaps it's best to keep that as is, but perhaps use a custom kernel that combines matmul with the subsequent element-wise steps.

Alternatively, the standard PyTorch matmul (linear layer) is already highly optimized, so maybe not worth replacing. Let's think of other steps.

Another possibility is to combine the Swish and the bias addition into a single kernel, but that's trivial. However, perhaps combining all steps except matmul into a single kernel.

Alternatively, the group norm might be a candidate for optimization, especially with a large batch size. Let me think about how group norm works.

GroupNorm divides the channels into groups. For example, if out_features is 4096 and num_groups is 64, each group has 64 channels (since 4096 / 64 = 64). Wait no, 4096 divided by 64 is 64. So each group has 64 channels. Then, for each group, the mean and variance are computed over the entire batch and the group's channels.

Wait the input to GroupNorm is (batch_size, channels, ...). Since in this case, the input is (batch_size, out_features), the channels are out_features. So when using GroupNorm with num_groups=64, the channels are split into 64 groups, each of size 64 (since 4096 / 64 = 64). 

The computation for each group is:

For each group (of 64 channels):

Compute the mean and variance across the batch and the group's channels. 

Then, normalize each element in the group by (x - mean) / sqrt(var + eps), then scale by gamma and add beta.

The gamma and beta are parameters of the GroupNorm layer, stored in self.group_norm.weight and self.group_norm.bias.

The parameters gamma and beta have the same shape as the number of channels (4096 here).

The standard implementation of GroupNorm is already optimized, but perhaps for very large batch sizes, a custom kernel can compute the mean and variance more efficiently, especially since the batch is large (32768). The standard implementation might not be optimized for such large batches, so a custom kernel could help.

Alternatively, perhaps we can combine the Swish, bias addition, and group norm into a single kernel, which would reduce the number of memory accesses.

Alternatively, perhaps fusing the entire forward path into a single kernel is the way to go. Let me outline possible fusion points.

Option 1: Combine matmul, Swish, bias addition, and group norm into a single kernel. But matmul is a separate operation which is O(N^3), so maybe that's too much. The matmul's computational cost is much higher than the element-wise operations. So perhaps it's better to handle matmul as a separate step (using the existing optimized implementation) and then fuse the remaining steps.

Option 2: Fuse Swish, bias addition, and group norm into a single kernel. Since these are all element-wise or reduction-based steps, this might be feasible.

Alternatively, perhaps first fuse Swish and bias addition into a single kernel, then apply group norm with a custom kernel.

Let me think about the steps after the matmul:

After matmul, we have a tensor of shape (batch_size, out_features).

Then, Swish activation: element-wise x * sigmoid(x)

Then add the bias (element-wise addition of the bias tensor of shape (out_features,))

Then group norm: requires per-group computation of mean and variance over the batch and group channels.

The group norm's computation for each group:

For each group g (there are num_groups groups):

- Take the subset of channels in group g (each group has C_g = out_features / num_groups channels)

- For each element in these channels, compute the mean and variance over the batch dimension and the channels in the group.

Wait, actually, in GroupNorm, the mean and variance are computed over the batch and the group's channels. The dimensions for a group's data would be (batch_size, C_g), so the mean and variance are computed over all batch elements and all channels in the group. 

So for each group, the data is a matrix of size (batch_size, C_g). The mean is the mean over all batch_size * C_g elements. Similarly, variance is over those elements.

This computation can be done in parallel for each group.

Now, the problem with the standard implementation is that for large batch sizes (like 32768), the computation of mean and variance for each group might involve a lot of data, but with optimized reductions.

However, if we can compute this efficiently in a custom kernel, perhaps with better memory access patterns or using shared memory for reductions, it could be faster.

Alternatively, maybe the current PyTorch implementation is already efficient, but let's proceed.

Let me plan to create a custom kernel that combines the Swish activation, bias addition, and the group norm computation.

Alternatively, let's see if the Swish and bias addition can be done in a single kernel, then the group norm in another. But combining them all into one kernel might save some time.

Alternatively, the group norm could be a candidate for optimization because it involves reduction operations, which can be parallelized.

So let's think about writing a custom kernel for the group norm. Let's first outline the steps:

1. For each group, split the channels into groups.

2. For each group, compute mean and variance over the batch and the group's channels.

3. Normalize each element in the group using the computed mean and variance.

4. Apply the learned parameters (gamma and beta) for each channel.

The group norm parameters (gamma and beta) are of size equal to the number of channels (out_features).

The input to the group norm is the tensor after Swish and bias addition. Let's call that tensor 'x'.

The process for each group:

Suppose group g has channels from c_start to c_end (exclusive). For each element in the batch and in those channels, compute the mean and variance over all batch elements and all channels in the group.

Wait, the mean is the mean over all the elements in the group's channels across the entire batch. Similarly for variance.

So for group g:

data_group = x[:, c_start:c_end]

mean_g = mean(data_group.view(-1))

var_g = var(data_group.view(-1))

Then, normalized = (data_group - mean_g) / sqrt(var_g + eps)

Then, the output is normalized * gamma[c_start:c_end] + beta[c_start:c_end]

Thus, the gamma and beta are per-channel.

So, in a CUDA kernel, how can we compute this efficiently?

First, for each group, we need to compute the mean and variance. These are reductions over the elements in that group's channels across the batch.

The group's channels are contiguous in memory, so perhaps we can process each group in parallel.

The plan is:

- For each group:

   - Compute the sum and sum of squares of all elements in the group's channels across the batch.

   - Then compute mean and variance from those sums.

- Then, for each element in each group, apply the normalization.

The key challenge is efficiently computing the sums and sum of squares for each group. Since groups are processed independently, we can parallelize across groups. However, for each group, the computation requires traversing all elements in that group's channels over the batch.

Given that the batch size is 32768 and the group's channel count is 64 (since 4096/64=64), each group has 32768 * 64 elements. For 64 groups, that's manageable.

Alternatively, for each group, we can have a thread block handle the computation of the sum and sum of squares. Each thread in the block can process a chunk of elements. Then, using parallel reduction within the block to compute the total sum and sum_squares.

Once those are computed, the normalization can be done in a separate kernel.

Alternatively, maybe a single kernel can handle both the reduction and the normalization, but that might be more complex.

Alternatively, let's first compute the mean and variance for each group, then do the normalization.

Let me structure this as two steps:

1. Compute mean and variance for each group.

2. Normalize each element using the computed means and variances, and apply gamma and beta.

First, let's focus on step 1.

To compute the mean and variance for each group:

Each group has:

num_elements = batch_size * channels_per_group

For each group:

sum_val = sum of all elements in group's channels

sum_squares = sum of squares of all elements in group's channels

mean = sum_val / num_elements

var = (sum_squares / num_elements) - mean^2

Then, variance is var + eps (assuming eps is a small constant, say 1e-5).

Now, in CUDA, we can loop over each group and compute these sums. However, the problem is that for large batch sizes, the number of elements per group is large, so we need to do this efficiently.

Let me think of a kernel that processes each group in a block.

Suppose we have a grid where each block corresponds to a group. Each block processes all elements in its group's channels across the batch.

Each thread in the block can process a chunk of the elements. For example, with 256 threads per block, each thread could process (num_elements / 256) elements.

The kernel would look something like this:

__global__ void compute_group_stats(float *data, int batch_size, int num_groups, int out_features, float *means, float *vars) {

    int group_id = blockIdx.x;

    int channels_per_group = out_features / num_groups;

    int c_start = group_id * channels_per_group;

    int c_end = c_start + channels_per_group;

    // Each group has batch_size * channels_per_group elements

    int num_elements = batch_size * channels_per_group;

    // Each thread processes a portion of the elements

    int tid = threadIdx.x;

    __shared__ float s_sum[256]; // or some fixed size?

    float sum = 0.0f;

    float sum_sq = 0.0f;

    for (int i = tid; i < num_elements; i += blockDim.x) {

        int batch_idx = i / channels_per_group;

        int channel_in_group = i % channels_per_group;

        int channel = c_start + channel_in_group;

        float val = data[batch_idx * out_features + channel];

        sum += val;

        sum_sq += val * val;

    }

    // Now perform a block reduction to sum all partial sums

    // Use shared memory to accumulate

    s_sum[threadIdx.x] = sum;

    __syncthreads();

    // Apply reduction to get total sum

    for (int s = blockDim.x/2; s > 0; s >>= 1) {

        if (threadIdx.x < s) {

            s_sum[threadIdx.x] += s_sum[threadIdx.x + s];

        }

        __syncthreads();

    }

    if (threadIdx.x == 0) {

        float total_sum = s_sum[0];

        // Similarly for sum_sq?

        // Wait, perhaps we need to do this for both sums and sum_sq

        // Wait, the above loop only handles sum. Need to also track sum_sq.

        // Hmm, need to track two variables.

        // Maybe better to handle them separately.

    }

    // This is getting complicated. Maybe better to have separate arrays for sum and sum_sq.

    // Alternatively, use two shared memory arrays.

    // Alternatively, process sum and sum_sq in the same loop.

    // Alternatively, use two separate variables in the thread's local storage.

    // Wait, perhaps the initial approach was to compute sum and sum_sq in each thread's local variables.

    // Then perform two reductions: one for sum and one for sum_sq.

    // So after the initial loop, each thread has their partial sum and sum_sq.

    // Then, perform a block-wide reduction for sum and sum_sq.

    // But how?

    // Maybe first compute the sum reduction, then do the same for sum_sq.

    // Or interleave them.

    // Alternatively, use two separate shared arrays.

    __shared__ float s_sum[256];

    __shared__ float s_sq[256];

    s_sum[tid] = sum;

    s_sq[tid] = sum_sq;

    __syncthreads();

    // Now reduce s_sum and s_sq separately.

    // Reduction for sum:

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {

        if (threadIdx.x < s) {

            s_sum[threadIdx.x] += s_sum[threadIdx.x + s];

            s_sq[threadIdx.x] += s_sq[threadIdx.x + s];

        }

        __syncthreads();

    }

    if (threadIdx.x == 0) {

        float total_sum = s_sum[0];

        float total_sq = s_sq[0];

        float mean = total_sum / num_elements;

        float var = (total_sq / num_elements) - (mean * mean);

        means[group_id] = mean;

        vars[group_id] = var;

    }

}

Wait, but this might have a problem with the thread indices. Let me think again. Each block handles one group. The threads in the block process the elements of that group. The loop over i (the index into the elements of the group) uses a stride of blockDim.x, so each thread processes blockDim.x elements. But the elements are in the batch and channel dimensions. So the indexing might need to be handled carefully.

Alternatively, since the data is stored in row-major order (batch first, then channels), for a given group, the elements for the group are spread across each batch element's channels. So for example, for group 0, channels 0-63, each batch element has those channels, so the elements for the group in batch 0 are at positions 0-63, batch 1 at out_features +0 to +63, etc.

Alternatively, it's easier to think of the data as a 2D array (batch_size, out_features). For a group, the channels are contiguous, so each batch element has a slice of channels for the group. So the data for group g is a matrix of size (batch_size, channels_per_group).

The total elements are batch_size * channels_per_group.

So in the kernel, for each group, the threads can process the entire batch and channels in parallel.

The code above may have some errors, but the general idea is to compute the sums for each group in parallel.

Once the means and variances are computed, the normalization can be done in another kernel.

The normalization kernel would:

For each element in the group, subtract the mean, divide by sqrt(var + eps), multiply by gamma[channel], add beta[channel].

Wait, the gamma and beta are per-channel. So for each group's channels, the gamma and beta are per-channel. So for group g, channels c_start to c_end, each channel has its own gamma and beta.

So the normalization kernel would look like:

__global__ void apply_group_norm(float *data, float *means, float *vars, float *gamma, float *beta, int batch_size, int num_groups, int out_features, float eps) {

    int group_id = blockIdx.x / (gridDim.y); // Wait, not sure. Maybe better to loop over each element.

    Wait, perhaps each thread processes a single element.

    Alternatively, for each element (batch, channel):

        find group g: group_id = channel / channels_per_group

        mean = means[g]

        var = vars[g]

        denom = rsqrt(var + eps)

        data[batch][channel] = (data[batch][channel] - mean) * denom * gamma[channel] + beta[channel]

    So this can be done per element.

    To parallelize, each thread can handle a single element.

    So the kernel can be:

    extern "C" __global__ void apply_group_norm_kernel(float *input, float *means, float *vars, float *gamma, float *beta, int batch_size, int num_features, int num_groups, float eps) {

        int idx = blockIdx.x * blockDim.x + threadIdx.x;

        if (idx >= batch_size * num_features) return;

        int channel = idx % num_features;

        int batch = idx / num_features;

        int channels_per_group = num_features / num_groups;

        int group_id = channel / channels_per_group;

        float mean = means[group_id];

        float var = vars[group_id];

        float denom = rsqrtf(var + eps);

        float value = input[idx] - mean;

        value = value * denom;

        value = value * gamma[channel] + beta[channel];

        input[idx] = value;

    }

}

Wait, but this requires having the means and vars computed for each group, stored in arrays. 

So, the overall process for the group norm would be:

1. Allocate temporary storage for means and variances (size num_groups each).

2. Launch compute_group_stats kernel to compute means and vars for each group.

3. Launch apply_group_norm kernel to apply the normalization.

This could be done as a custom CUDA function.

Alternatively, perhaps the Swish and bias addition can be fused with this process. Let's think:

The Swish and bias addition are element-wise operations, so they can be done in the same kernel that processes the input before the group norm.

Wait, the steps after the matmul are:

x = matmul(x)

x = Swish(x) = x * sigmoid(x)

x = x + self.bias

x = group_norm(x)

So, after the matmul, the next three steps can be done in a single kernel.

So, a custom kernel that combines Swish, bias addition, and group norm might be possible.

Let me outline the steps in such a kernel:

1. For each element in x (after matmul), compute swish: x * sigmoid(x).

2. Add the bias (element-wise addition of self.bias).

3. Then, perform group norm.

But steps 1 and 2 are element-wise, and step 3 requires reductions.

Wait, but step 3 (group norm) requires reductions over groups. So we can't combine all three into a single kernel, because the reductions need to happen over the entire tensor after steps 1 and 2.

So the order is important. So perhaps first compute the swish and bias addition in a single kernel, then compute group norm as a separate step with custom kernels.

Therefore, the plan is:

- Create a custom kernel that combines Swish and bias addition. 

- Then, create custom kernels for group norm (compute stats and apply norm).

Alternatively, perhaps the Swish and bias can be fused into the group norm's computation, but it's unclear.

Alternatively, let's first handle the Swish and bias addition.

The Swish activation is x * sigmoid(x). The sigmoid can be implemented as 1/(1 + exp(-x)). However, for very large x, exp(-x) becomes 0, so sigmoid approaches 1. For very negative x, exp(-x) becomes very large, so sigmoid approaches 0. But computing this in CUDA can be done straightforwardly.

The kernel for Swish + bias addition would be:

__global__ void swish_add_bias_kernel(float* input, float* bias, float* output, int batch_size, int num_features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx >= batch_size * num_features) return;

    float x = input[idx];

    float sigmoid_x = 1.0f / (1.0f + expf(-x));

    output[idx] = (x * sigmoid_x) + bias[idx % num_features]; // since bias is (num_features, )

}

Wait, but the bias is of shape (out_features, ), so for each batch element, the same bias is added. So for the bias, the index should be idx % num_features. Because for each element (batch, channel), the channel is fixed, so the bias is bias[channel].

Alternatively, in the kernel:

int channel = idx % num_features;

output[idx] = (x * sigmoid_x) + bias[channel];

This way, each channel has its own bias term.

This is a simple kernel. The standard PyTorch might already have optimized element-wise operations, but perhaps combining Swish and bias addition into a single kernel can save some overhead.

Then, the group norm is handled with the two-step approach.

So the overall plan is:

- After matmul, apply the custom Swish + bias kernel.

- Then, apply the custom group norm (compute stats and apply norm).

This would reduce the number of CUDA kernel launches compared to using PyTorch's default functions.

Now, let's consider the matmul itself. The nn.Linear is already optimized, but perhaps we can fuse it with the Swish and bias addition into a single kernel. However, matmul is a dense matrix multiply, which is O(batch_size * in_features * out_features). That's a huge computation. It's unlikely that we can write a better kernel than PyTorch's cuBLAS implementation. So probably better to leave the matmul as is, and focus on fusing the subsequent steps.

Therefore, the steps to optimize are:

1. Swish + bias addition.

2. Group norm.

Now, the group norm requires two kernels: one to compute means and variances, and another to apply the normalization.

Putting this together, here's the plan for the new ModelNew:

- Use the existing nn.Linear for the matmul part.

- Then, use a custom kernel to compute Swish and add bias.

- Then, use custom kernels to compute group norm.

Additionally, maybe the group norm can be optimized further. Let's see.

Now, coding this into the ModelNew.

First, let's code the Swish + bias kernel.

The input to this kernel is the output of the Linear layer (matmul), which is a tensor of shape (batch_size, out_features).

The code for the custom kernel would be:

#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void swish_add_bias_kernel(const float* input, const float* bias, float* output, int batch_size, int num_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * num_features) return;

    float x = input[idx];
    float sigmoid_x = 1.0f / (1.0f + expf(-x));
    int channel = idx % num_features;
    output[idx] = x * sigmoid_x + bias[channel];
}

torch::Tensor swish_add_bias_cuda(torch::Tensor input, torch::Tensor bias) {
    int batch_size = input.size(0);
    int num_features = input.size(1);
    auto output = torch::empty_like(input);

    const int block_size = 256;
    const int num_elements = batch_size * num_features;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    swish_add_bias_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), bias.data_ptr<float>(), output.data_ptr<float>(), batch_size, num_features);

    return output;
}

This function takes the input tensor from the Linear layer and the bias tensor, applies Swish and adds the bias, returning the result.

Next, the group norm custom kernels.

First, the kernel to compute the group means and variances:

__global__ void compute_group_stats_kernel(
    const float* input,
    float* means,
    float* vars,
    int batch_size,
    int num_features,
    int num_groups,
    float eps
) {
    int group_id = blockIdx.x;

    int channels_per_group = num_features / num_groups;
    int c_start = group_id * channels_per_group;
    int c_end = c_start + channels_per_group;

    int num_elements = batch_size * channels_per_group;

    // Each thread processes a portion of the elements
    int tid = threadIdx.x;
    __shared__ float s_sum[256];
    __shared__ float s_sq[256];

    float sum = 0.0f;
    float sum_sq = 0.0f;

    for (int i = tid; i < num_elements; i += blockDim.x) {
        int batch_idx = i / channels_per_group;
        int channel_in_group = i % channels_per_group;
        int channel = c_start + channel_in_group;
        float val = input[batch_idx * num_features + channel];

        sum += val;
        sum_sq += val * val;
    }

    s_sum[tid] = sum;
    s_sq[tid] = sum_sq;
    __syncthreads();

    // Reduction within the block
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_sum[tid] += s_sum[tid + s];
            s_sq[tid] += s_sq[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total_sum = s_sum[0];
        float total_sq = s_sq[0];
        float mean = total_sum / num_elements;
        float var = (total_sq / num_elements) - (mean * mean);
        means[group_id] = mean;
        vars[group_id] = var + eps; // Adding eps here to var?
        // Wait, actually, the formula is var + eps, but in the variance term:

        // var = (sum_sq / n) - (mean)^2

        // So the variance is var, then in the normalization we have sqrt(var + eps).

        // So the vars array should store var, and when computing denom, we add eps.

        // So the code above should compute var as is, not adding eps yet.

        // Correction:

        vars[group_id] = var;
    }
}

Then, the kernel to apply the normalization and scaling with gamma and beta:

__global__ void apply_group_norm_kernel(
    const float* input,
    const float* means,
    const float* vars,
    const float* gamma,
    const float* beta,
    float* output,
    int batch_size,
    int num_features,
    int num_groups,
    float eps
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * num_features) return;

    int channel = idx % num_features;
    int batch = idx / num_features;

    int channels_per_group = num_features / num_groups;
    int group_id = channel / channels_per_group;

    float mean = means[group_id];
    float var = vars[group_id];

    float denom = rsqrtf(var + eps);

    float value = input[idx] - mean;
    value *= denom;
    value = value * gamma[channel] + beta[channel];

    output[idx] = value;
}

Then, the wrapper function for group norm would be:

torch::Tensor group_norm_cuda(
    torch::Tensor input,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps
) {
    int batch_size = input.size(0);
    int num_features = input.size(1);

    const int num_groups_ = num_groups;
    const int channels_per_group = num_features / num_groups_;
    // Ensure that num_features is divisible by num_groups
    assert(num_features % num_groups_ == 0);

    // Allocate temporary tensors for means and variances
    auto means = torch::empty({num_groups_}, input.options());
    auto vars = torch::empty({num_groups_}, input.options());

    // Compute group means and variances
    const int block_size_stats = 256;
    const int num_blocks_stats = num_groups_; // each group is a block
    compute_group_stats_kernel<<<num_blocks_stats, block_size_stats>>>(
        input.data_ptr<float>(),
        means.data_ptr<float>(),
        vars.data_ptr<float>(),
        batch_size,
        num_features,
        num_groups_,
        eps
    );

    // Apply normalization
    auto output = torch::empty_like(input);
    const int block_size_apply = 256;
    const int num_elements = batch_size * num_features;
    const int num_blocks_apply = (num_elements + block_size_apply - 1) / block_size_apply;

    apply_group_norm_kernel<<<num_blocks_apply, block_size_apply>>>(
        input.data_ptr<float>(),
        means.data_ptr<float>(),
        vars.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        num_features,
        num_groups_,
        eps
    );

    return output;
}

Now, putting this all together in the Python code.

The ModelNew class would replace the forward steps with these kernels.

First, define the custom CUDA functions with load_inline.

But since we have multiple kernels (swish_add_bias, compute_group_stats, apply_group_norm), we need to write them in one CUDA source block.

Wait, when using load_inline, all the CUDA code must be in one string, or multiple?

The load_inline function can handle multiple kernels in one source.

So, putting all the CUDA code into a single string.

First, the Swish and bias addition kernel.

Then the group norm kernels.

Then the wrapper functions.

Let me write the full CUDA source code as a string:

elementwise_add_swish_bias_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Kernel for Swish activation and bias addition
__global__ void swish_add_bias_kernel(const float* input, const float* bias, float* output, int batch_size, int num_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * num_features) return;

    float x = input[idx];
    float sigmoid_x = 1.0f / (1.0f + expf(-x));
    int channel = idx % num_features;
    output[idx] = x * sigmoid_x + bias[channel];
}

// Group Norm kernels

__global__ void compute_group_stats_kernel(
    const float* input,
    float* means,
    float* vars,
    int batch_size,
    int num_features,
    int num_groups,
    float eps
) {
    int group_id = blockIdx.x;

    int channels_per_group = num_features / num_groups;
    int c_start = group_id * channels_per_group;
    int c_end = c_start + channels_per_group;

    int num_elements = batch_size * channels_per_group;

    int tid = threadIdx.x;
    __shared__ float s_sum[256];
    __shared__ float s_sq[256];

    float sum = 0.0f;
    float sum_sq = 0.0f;

    for (int i = tid; i < num_elements; i += blockDim.x) {
        int batch_idx = i / channels_per_group;
        int channel_in_group = i % channels_per_group;
        int channel = c_start + channel_in_group;
        float val = input[batch_idx