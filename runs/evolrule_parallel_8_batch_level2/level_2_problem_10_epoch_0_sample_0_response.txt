You can make any modifications you like to the architecture as long as the final output is the same as the original Model. The modified code must be a drop-in replacement for the original. The function signature of the ModelNew must be compatible with the original Model class (same arguments in __init__ and forward function). 

The code should be written in the same style as the example provided. 

You can make the following assumptions:

- The batch size is fixed to 128
- All tensors are on the same device and contiguous
- The input tensors to each operator have fixed shapes
- The operators in the model can be replaced with custom CUDA kernels
- The model is run on a single GPU

Make sure your code is correct and passes unit tests. 

I will compile and run your code with the given input dimensions. If there is a bug, you will be penalized. 

You are allowed to import the following external libraries in the code:
- torch
- math
- numpy
- torch.utils.cpp_extension.load_inline
- torch.cuda

But you cannot use any other libraries. Also, avoid using any other CUDA libraries besides torch and the ones provided in the example. 

You can use any PyTorch operators in the forward function (even inside the custom CUDA kernel). 

You can choose to replace a subset of the operators, but the more you replace the better (as long as it is correct). 

The goal is to maximize the speedup while maintaining correctness. 

Please note that you can also combine multiple operators into one kernel (operator fusion) to reduce memory traffic and kernel launch overhead. This might yield better performance than replacing individual operators. 

You can also choose to do algorithmic optimizations (like online softmax, but in this case, for applicable operators). 

I will now wait for your optimized code.

Okay, let's tackle this optimization. The original model has a ConvTranspose2d, MaxPool2d, Hardtanh, mean, and tanh. The goal is to replace some or all of these with custom CUDA kernels for speed.

First, I need to understand each operation's computational pattern and see where fusing them would help. Let's break it down:

1. **ConvTranspose2d**: This is a transposed convolution, which is compute-intensive. Implementing this in CUDA might be complex, but maybe possible. However, fusing it with subsequent operations could help.

2. **MaxPool2d**: Max pooling is a local maximum over a window. It's relatively straightforward to implement in CUDA and could be fused with the next activation function (Hardtanh).

3. **Hardtanh**: This is an element-wise clamp between min and max values. Combining this with the MaxPool output would be easy.

4. **Mean over dimensions 2 and 3**: This reduces spatial dimensions to 1, so the mean is sum divided by the product of height and width after pooling. Calculating this in a kernel might be efficient.

5. **Tanh**: Another element-wise operation that can be fused with previous steps if possible.

Fusing the MaxPool, Hardtanh, and mean steps into a single kernel could reduce memory traffic. Let's see:

- After ConvTranspose, the output is passed to MaxPool. The MaxPool can be done, then immediately followed by Hardtanh in the same kernel. Then compute the mean and apply tanh.

Wait, but the mean is a reduction, which might complicate things. Alternatively, perhaps the MaxPool and Hardtanh can be fused, then the mean and tanh can be done in another kernel.

Alternatively, maybe the entire sequence from MaxPool onward can be fused into a single kernel. Let me think step by step.

First, the ConvTranspose is a big operation. Since PyTorch's implementation might already be optimized, replacing it might not be worth it unless there's a significant gain. So perhaps focus on fusing the later steps.

Looking at the forward steps:

x = self.conv_transpose(x)  # transposed conv
x = self.maxpool(x)         # maxpool
x = self.hardtanh(x)        # clamp between -1 and 1
x = torch.mean(...)         # mean over spatial dims
x = torch.tanh(x)           # apply tanh

The mean and tanh could be combined. Since tanh is applied after the mean, which is a scalar per channel (since mean over height/width), perhaps the mean can be calculated in a way that also applies tanh immediately. 

Alternatively, the mean is a reduction, so maybe compute that first, then apply tanh. But combining the maxpool, hardtanh, and mean might be the key.

Wait, the maxpool is 2x2 with stride 2, so it halves the spatial dimensions. The hardtanh is element-wise. The mean is over the remaining spatial dimensions (after maxpool). Let me see the input sizes:

Original input to ConvTranspose is (batch, in_channels, H, W). Let's see:

Suppose the input to ConvTranspose is (128,64,256,256). The ConvTranspose parameters are kernel_size=3, stride=1, padding=1. The output spatial dimensions for ConvTranspose2d when stride=1 and padding=1 is same as input: 256x256. So after ConvTranspose, the tensor is (128,64,256,256).

Then MaxPool2d with kernel 2, stride 2: output spatial dims become 128x128. So after MaxPool, it's (128,64,128,128).

Then Hardtanh (no change in shape). Then the mean over 2 and 3 (height and width) gives (128,64,1,1). Then tanh.

So the main steps after ConvTranspose are MaxPool, Hardtanh, Mean, Tanh. Fusing these would be ideal.

Let me think of a kernel that does MaxPool + Hardtanh + Mean + Tanh. Wait, but MaxPool is a spatial operation, which is a bit involved. Alternatively, can we combine MaxPool and Hardtanh in one kernel, then do the mean and tanh in another?

Alternatively, since MaxPool is 2x2, maybe we can process each 2x2 window, compute the max, clamp it, then accumulate for the mean, and finally apply tanh.

Wait, the mean is over the entire spatial dimensions after pooling. So the mean reduces each channel to a scalar. Then tanh is applied to that scalar.

Wait, the mean is taken over the 2nd and 3rd dimensions (height and width), so the output after mean is (128,64,1,1). Then tanh is applied to each element. So for each channel, the mean is a single value, then tanh of that.

So for each channel, the tanh is just applied to the mean value. So the order is: compute maxpool, then clamp (hardtanh), then compute the mean over spatial dimensions, then apply tanh.

So fusing the maxpool and hardtanh into one kernel would be a good start. Then, the mean and tanh could be done in another kernel. Alternatively, even better, compute the maxpool and clamp, then compute the mean and tanh in a single step, but how?

Wait, the mean is a sum divided by the number of elements. So perhaps during the MaxPool and clamp steps, we can accumulate the sum, then divide and apply tanh. Let me see:

Suppose for each output position of the MaxPool (after 2x2 pooling), we have a max value clamped between -1 and 1. The mean over the entire spatial dimensions would require summing all those values and dividing by the total number of elements (after pooling). Since the spatial dimensions after pooling are 128x128, total elements per channel is 128*128 = 16384.

So, perhaps we can compute the maxpool, clamp, and accumulate the sum in a kernel, then divide by 16384 and apply tanh in a separate kernel.

But even better, maybe all these steps can be done in a single kernel. Let's think:

The MaxPool + clamp can be done per 2x2 window. Then, instead of storing the max values, just accumulate their sum (after clamping). Since all elements contribute to the sum, we can process each input element in the MaxPool window, compute the max (but actually, the max is only one element per window), but since we need the max for each window, perhaps it's better to first compute the max per window, clamp it, then add to the sum. Alternatively, maybe process all elements, but track the max for each window.

Hmm, this might be tricky. Let me think of the steps in detail.

Let me first think of a kernel that does MaxPool, then clamp, then compute the sum over the spatial dimensions, then divide and apply tanh. To do this efficiently, perhaps:

1. For each output spatial position (after MaxPool), compute the max over the 2x2 window of the input.
2. Clamp that max value between -1 and 1.
3. Sum all these clamped max values across all spatial positions and channels (or per channel? Wait, the mean is per channel).

Wait, the mean is per channel, so for each channel, we need to compute the sum over its spatial dimensions (after MaxPool) of the clamped max values, then divide by 128*128, then apply tanh to each of those per-channel means.

So perhaps:

The first part is to compute the MaxPool and clamp, storing the intermediate tensor. Then compute the sum over spatial dimensions, then divide by 16384 and apply tanh. But this requires storing the intermediate tensor, which might be memory-heavy. Alternatively, we can compute the sum in a single pass over the input.

Wait, the MaxPool is over 2x2 windows. For each output position (after MaxPool), the input window is 2x2. The max is taken over those 4 elements. Then the clamped value is stored. The sum over all these clamped max values for each channel gives the total needed for the mean.

Alternatively, the sum can be computed on the fly during the MaxPool step. Since for each input window (2x2), we can compute the max, clamp it, and add to a running sum for that channel. However, the input is large (128,64,256,256), so the MaxPool reduces it to (128,64,128,128). The sum over all spatial positions (after pooling) for each channel would require 128*128 elements per channel. 

Hmm, perhaps it's better to split the process into two steps: first compute the MaxPool and clamp, then compute the sum for the mean and apply tanh. But this would require two kernels.

Alternatively, let's first consider fusing the MaxPool + clamp into one kernel, then the mean and tanh in another kernel. That way, we can reduce the number of memory copies.

First, MaxPool + clamp:

The MaxPool is straightforward: for each 2x2 window in the input (after ConvTranspose), take the max, then clamp between -1 and 1.

Then, the next step is to compute the mean over spatial dimensions (128x128) for each channel, then apply tanh. The mean is sum / (128*128), so sum all elements in the channel, divide by 16384, then tanh that value.

The sum over spatial dimensions can be done using a reduction kernel. Since each channel's spatial elements are 128x128, perhaps a per-channel sum can be done in parallel.

So, let's plan:

First, the ConvTranspose2d: probably not worth replacing, as PyTorch's implementation is optimized. So leave that as is.

Then, the MaxPool + clamp: replace with a custom kernel.

Then, compute the mean over spatial dimensions and apply tanh: another custom kernel.

So, steps for the forward pass:

x = self.conv_transpose(x)  # use PyTorch's implementation

x = maxpool_clamp_cuda(x)  # custom kernel combining MaxPool and clamp

x = mean_tanh_cuda(x)      # custom kernel to compute mean and apply tanh

The output shape would be (128,64,1,1) as desired.

Now, let's think of the kernels:

First, the MaxPool + clamp kernel:

Input is a tensor of shape (128, 64, 256, 256). The kernel needs to process each 2x2 window in spatial dimensions (h and w), take the max, clamp between -1 and 1, and store the result in an output tensor of size (128,64,128,128).

This can be done with a CUDA kernel that loops over the output indices. For each output position (n, c, i, j), the input indices would be (n, c, 2*i, 2*j) to (n, c, 2*i+1, 2*j+1). The max of those 4 values, then clamped.

Second, the mean and tanh kernel:

Input is (128,64,128,128). For each channel c in 0..63, the spatial elements (over 128x128) must be summed, then divided by 16384 (the number of elements per channel). Then tanh is applied to each of the 64 values (per batch and channel), resulting in a tensor (128,64,1,1).

This can be done with a kernel that, for each channel, sums all elements in that channel's spatial dimensions. Since each batch is independent, we can process per batch and per channel.

Alternatively, use a reduction approach where each thread processes a block of elements and accumulates partial sums, then combine them.

Alternatively, for each output element (n,c,0,0), compute the sum over (n,c,:,:):

sum = 0
for i in 0..127:
   for j in 0..127:
       sum += input[n][c][i][j]
then output[n][c][0][0] = tanh( sum / 16384 )

But doing this naively would be slow for 128*128 elements. So, to parallelize this, we can have each thread handle a portion of the elements. For example, for each channel, split the spatial elements among threads.

Let's think of the first kernel (maxpool and clamp):

The kernel would process each output element. The output has dimensions (128,64,128,128). The input has (128,64,256,256). The kernel can be structured as follows:

Each thread block handles a batch and channel (since there are 128 batches and 64 channels, but maybe better to tile differently). Alternatively, for each output (i,j), have a thread compute it.

But in terms of CUDA dimensions, perhaps we can have grid dimensions as (128,64,128,128), but that's too big. Instead, we can flatten the indices.

Alternatively, the kernel can have a 3D grid: blocks per batch, channel, and spatial dimensions. Hmm, perhaps better to compute per output pixel.

The kernel might look like this:

__global__ void maxpool_clamp_kernel(
    const float* input, float* output,
    int batch, int channels, int input_h, int input_w,
    int output_h, int output_w
) {
    // Each thread handles an output position (b, c, i, j)
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels * output_h * output_w) return;

    int b = idx / (channels * output_h * output_w);
    int c = (idx / (output_h * output_w)) % channels;
    int ij = idx % (output_h * output_w);
    int i = ij / output_w;
    int j = ij % output_w;

    // Compute input indices
    int in_i = i * 2;
    int in_j = j * 2;

    // Initialize max_val to -inf
    float max_val = -FLT_MAX;
    for (int di=0; di<2; di++) {
        for (int dj=0; dj<2; dj++) {
            int in_h = in_i + di;
            int in_w = in_j + dj;
            float val = input[b * channels * input_h * input_w + c * input_h * input_w + in_h * input_w + in_w];
            if (val > max_val) max_val = val;
        }
    }

    // Clamp between -1 and 1
    if (max_val < -1.0f) max_val = -1.0f;
    else if (max_val > 1.0f) max_val = 1.0f;

    // Write to output
    int out_offset = b * channels * output_h * output_w + c * output_h * output_w + i * output_w + j;
    output[out_offset] = max_val;
}

Wait, but the input and output are 4D tensors. The strides matter here. However, assuming that the tensors are contiguous, the calculation is okay. But in PyTorch, the tensors are in channels_first (NCHW), so the formula should be correct.

The parameters for the kernel would need to include the batch, channels, input height and width (256,256), output height and width (128,128).

Now, the second kernel for mean and tanh.

The output tensor here is (batch_size, channels, 1, 1). So each element (b,c,0,0) is tanh(mean_val).

The mean_val is the sum of all elements in input[b][c][:][ : ] divided by (128*128).

So, the kernel can be written to process each (b,c):

__global__ void mean_tanh_kernel(
    const float* input, float* output,
    int batch, int channels, int input_h, int input_w
) {
    // Each thread handles a (b, c) pair
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels) return;

    int b = idx / channels;
    int c = idx % channels;

    float sum = 0.0f;
    for (int i=0; i<input_h; i++) {
        for (int j=0; j<input_w; j++) {
            int in_offset = b * channels * input_h * input_w + c * input_h * input_w + i * input_w + j;
            sum += input[in_offset];
        }
    }
    float mean = sum / (input_h * input_w); // 128*128 = 16384
    float tanh_val = tanhf(mean);

    // Write to output
    int out_offset = b * channels * 1 * 1 + c * 1 * 1;
    output[out_offset] = tanh_val;
}

Wait, but this loops over all elements for each (b,c). Since 128x128 is 16384 elements, this could be slow if done per thread. To make this more efficient, we need to parallelize the summation across threads.

So, better to use a parallel reduction approach for the sum.

Perhaps a better way is to have each thread process a chunk of the spatial elements and accumulate partial sums, then combine them.

Alternatively, for each (b,c), we can have a block of threads compute the sum.

Let me structure it as follows:

Each block handles a (b,c). The block has multiple threads. Each thread processes a portion of the spatial elements.

The block reduces the sum across threads.

The kernel would be:

__global__ void mean_tanh_kernel(
    const float* input, float* output,
    int batch, int channels, int spatial_dim // 128
) {
    extern __shared__ float shared[];
    int b = blockIdx.x;
    int c = blockIdx.y;

    int tid = threadIdx.x;
    float sum = 0.0f;

    // Each thread processes a tile of spatial elements
    for (int i = tid; i < spatial_dim * spatial_dim; i += blockDim.x) {
        int ii = i / spatial_dim;
        int jj = i % spatial_dim;
        int in_offset = b * channels * spatial_dim * spatial_dim + c * spatial_dim * spatial_dim + ii * spatial_dim + jj;
        sum += input[in_offset];
    }

    // Block reduction
    __syncthreads();
    for (int s = blockDim.x/2; s > 0; s >>=1) {
        if (tid < s) {
            sum += shared[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float mean = sum / (spatial_dim * spatial_dim);
        float tanh_val = tanhf(mean);
        int out_offset = b * channels + c;
        output[out_offset] = tanh_val;
    }
}

Wait, but the shared memory is needed for the reduction. Let me rework this.

Actually, the above code uses shared memory but the partial sums are stored in registers first. Let's see:

Perhaps a better approach is:

Each block is for (b,c). The block has a certain number of threads (e.g., 256).

Each thread processes a part of the spatial elements. The threads accumulate their partial sums into a shared memory array, then the block reduces that.

Let me restructure:

__global__ void mean_tanh_kernel(
    const float* input, float* output,
    int batch, int channels, int spatial_dim
) {
    extern __shared__ float sdata[];
    int b = blockIdx.x;
    int c = blockIdx.y;
    int tid = threadIdx.x;

    float sum = 0.0f;

    // Process all elements in the spatial dimensions
    for (int i = tid; i < spatial_dim * spatial_dim; i += blockDim.x) {
        int ii = i / spatial_dim;
        int jj = i % spatial_dim;
        int in_offset = b * channels * spatial_dim * spatial_dim + c * spatial_dim * spatial_dim + ii * spatial_dim + jj;
        sum += input[in_offset];
    }

    // Write to shared memory
    sdata[tid] = sum;
    __syncthreads();

    // Perform block reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float mean = sdata[0] / (spatial_dim * spatial_dim);
        float tanh_val = tanhf(mean);
        int out_offset = b * channels + c;
        output[out_offset] = tanh_val;
    }
}

The shared memory size would need to be at least blockDim.x. Let's say we use 256 threads per block, so sdata should be size 256. The kernel is launched with grid dimensions (batch, channels), and block size blockDim.x (e.g., 256). The shared memory size is blockDim.x * sizeof(float).

This way, each (b,c) pair is processed in a block, and the threads within the block handle the spatial elements. The reduction is done in shared memory.

Now, putting this into code using load_inline.

Additionally, the original code uses nn.Hardtanh with min_val=-1 and max_val=1. Our kernel applies the clamp between those values, so that's correct.

Now, putting all together:

The ModelNew class would use the ConvTranspose2d as before, then replace the MaxPool, Hardtanh, mean, and tanh with the two custom kernels.

Wait, the original code has:

x = self.maxpool(x)
x = self.hardtanh(x)
x = torch.mean(x, dim=(2, 3), keepdim=True)
x = torch.tanh(x)

We need to replace the combination of these with the custom kernels.

So in the forward function of ModelNew:

def forward(self, x):
    x = self.conv_transpose(x)
    x = self.maxpool_clamp(x)  # first custom op
    x = self.mean_tanh(x)      # second custom op
    return x

Now, implementing the two kernels with load_inline.

First, the maxpool_clamp kernel:

elementwise_add was an example, but here we have two separate kernels.

Wait, the first kernel (maxpool and clamp) requires input and output tensors of certain dimensions. So we need to write the CUDA code for that.

Let me start writing the code step by step.

First, define the first kernel (maxpool and clamp):

maxpool_clamp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

__global__ void maxpool_clamp_kernel(
    const float* input, float* output,
    int batch, int channels, int input_h, int input_w,
    int output_h, int input_w_step
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels * output_h * output_w) return;

    int b = idx / (channels * output_h * output_w);
    int c = (idx / (output_h * output_w)) % channels;
    int ij = idx % (output_h * output_w);
    int i = ij / output_w;
    int j = ij % output_w;

    int in_i = i * 2;
    int in_j = j * 2;

    float max_val = -std::numeric_limits<float>::infinity();

    for (int di=0; di<2; di++) {
        for (int dj=0; dj<2; dj++) {
            int ii = in_i + di;
            int jj = in_j + dj;
            int in_offset = b * channels * input_h * input_w + c * input_h * input_w + ii * input_w + jj;
            float val = input[in_offset];
            if (val > max_val) {
                max_val = val;
            }
        }
    }

    // Clamp between -1 and 1
    if (max_val < -1.0f) max_val = -1.0f;
    else if (max_val > 1.0f) max_val = 1.0f;

    int out_offset = b * channels * output_h * output_w + c * output_h * output_w + i * output_w + j;
    output[out_offset] = max_val;
}

torch::Tensor maxpool_clamp_cuda(torch::Tensor input) {
    // Get dimensions
    int batch = input.size(0);
    int channels = input.size(1);
    int input_h = input.size(2);
    int input_w = input.size(3);
    int output_h = input_h / 2; // since kernel 2 and stride 2
    int output_w = input_w / 2;

    // Output tensor has size (batch, channels, output_h, output_w)
    auto output = torch::empty({batch, channels, output_h, output_w}, input.options());

    const int threads = 256;
    int total_elements = batch * channels * output_h * output_w;
    int blocks = (total_elements + threads - 1) / threads;

    maxpool_clamp_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch, channels, input_h, input_w,
        output_h, output_w // wait, the parameter list had input_w_step before, but perhaps I can fix the parameters here
    );

    return output;
}
"""

Wait, in the kernel's parameters, there was an 'input_w_step' which I might have mistakenly included. Let me check the parameters again. The kernel's parameters should be:

batch, channels, input_h, input_w, output_h, output_w.

Wait, in the kernel function:

__global__ void maxpool_clamp_kernel(
    const float* input, float* output,
    int batch, int channels, int input_h, int input_w,
    int output_h, int output_w
) 

So in the launch, the parameters after input and output would be:

batch, channels, input_h, input_w, output_h, output_w.

So in the code above, when launching, the arguments should be:

maxpool_clamp_kernel<<<blocks, threads>>>(
    input.data_ptr<float>(),
    output.data_ptr<float>(),
    batch, channels, input_h, input_w,
    output_h, output_w
);

Yes, that's correct. So the code above is okay except for the parameter 'input_w_step' which is a mistake. Let's correct that.

Second kernel for mean and tanh:

mean_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void mean_tanh_kernel(
    const float* input, float* output,
    int batch, int channels, int spatial_dim
) {
    extern __shared__ float sdata[];
    int b = blockIdx.x;
    int c = blockIdx.y;
    int tid = threadIdx.x;

    float sum = 0.0f;

    // Iterate over all spatial elements (spatial_dim x spatial_dim)
    for (int i = tid; i < spatial_dim * spatial_dim; i += blockDim.x) {
        int ii = i / spatial_dim;
        int jj = i % spatial_dim;
        int in_offset = b * channels * spatial_dim * spatial_dim + c * spatial_dim * spatial_dim + ii * spatial_dim + jj;
        sum += input[in_offset];
    }

    // Save to shared memory
    sdata[tid] = sum;
    __syncthreads();

    // Block reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total = sdata[0];
        float mean = total / (spatial_dim * spatial_dim);
        float tanh_val = tanhf(mean);
        int out_offset = b * channels + c;
        output[out_offset] = tanh_val;
    }
}

torch::Tensor mean_tanh_cuda(torch::Tensor input) {
    int batch = input.size(0);
    int channels = input.size(1);
    int spatial_dim = input.size(2); // since it's square

    // Output is (batch, channels, 1, 1)
    auto output = torch::empty({batch, channels, 1, 1}, input.options());

    // Launch configuration
    const int threads = 256; // or another number, needs to be a power of 2
    int blocks_x = batch;
    int blocks_y = channels;

    dim3 grid(blocks_x, blocks_y);
    dim3 block(threads);

    // shared memory size is blockDim.x * sizeof(float)
    int smem_size = threads * sizeof(float);

    mean_tanh_kernel<<<grid, block, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch, channels, spatial_dim
    );

    return output;
}
"""

Wait, in the mean_tanh_cuda function, after computing the output, it's of size (batch, channels, 1,1). So each element is stored as (b, c, 0, 0).

In the kernel, the output_offset is calculated as b * channels + c, but since the output is 4D, the actual offset should be:

output is (batch, channels, 1, 1). So for a given (b,c), the offset is:

b * channels * 1 * 1 + c * 1 * 1 + 0 * 1 + 0 = b * channels + c.

Wait, the output is NCHW, so yes. The kernel is correct.

Now, compiling these kernels with load_inline.

Then, the ModelNew class would need to have these functions.

Putting it all together:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the first custom CUDA kernel for MaxPool + Clamp
maxpool_clamp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <limits>

__global__ void maxpool_clamp_kernel(
    const float* input, float* output,
    int batch, int channels, int input_h, int input_w,
    int output_h, int output_w
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels * output_h * output_w) return;

    int b = idx / (channels * output_h * output_w);
    int c = (idx / (output_h * output_w)) % channels;
    int ij = idx % (output_h * output_w);
    int i = ij / output_w;
    int j = ij % output_w;

    int in_i = i * 2;
    int in_j = j * 2;

    float max_val = -std::numeric_limits<float>::infinity();

    for (int di=0; di<2; di++) {
        for (int dj=0; dj<2; dj++) {
            int ii = in_i + di;
            int jj = in_j + dj;
            int in_offset = b * channels * input_h * input_w + c * input_h * input_w + ii * input_w + jj;
            float val = input[in_offset];
            if (val > max_val) {
                max_val = val;
            }
        }
    }

    // Clamp between -1 and 1
    if (max_val < -1.0f) max_val = -1.0f;
    else if (max_val > 1.0f) max_val = 1.0f;

    int out_offset = b * channels * output_h * output_w + c * output_h * output_w + i * output_w + j;
    output[out_offset] = max_val;
}

torch::Tensor maxpool_clamp_cuda(torch::Tensor input) {
    int batch = input.size(0);
    int channels = input.size(1);
    int input_h = input.size(2);
    int input_w = input.size(3);
    int output_h = input_h / 2;
    int output_w = input_w / 2;

    auto output = torch::empty({batch, channels, output_h, output_w}, input.options());

    const int threads = 256;
    int total_elements = batch * channels * output_h * output_w;
    int blocks = (total_elements + threads - 1) / threads;

    maxpool_clamp_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch, channels, input_h, input_w,
        output_h, output_w
    );

    return output;
}
"""

maxpool_clamp_cpp = "torch::Tensor maxpool_clamp_cuda(torch::Tensor input);"

maxpool_clamp = load_inline(
    name="maxpool_clamp",
    cpp_sources=[maxpool_clamp_cpp],
    cuda_sources=[maxpool_clamp_source],
    functions=["maxpool_clamp_cuda"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_ldflags=[""]
)

# Define the second custom CUDA kernel for Mean + Tanh
mean_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void mean_tanh_kernel(
    const float* input, float* output,
    int batch, int channels, int spatial_dim
) {
    extern __shared__ float sdata[];
    int b = blockIdx.x;
    int c = blockIdx.y;
    int tid = threadIdx.x;

    float sum = 0.0f;

    for (int i = tid; i < spatial_dim * spatial_dim; i += blockDim.x) {
        int ii = i / spatial_dim;
        int jj = i % spatial_dim;
        int in_offset = b * channels * spatial_dim * spatial_dim + c * spatial_dim * spatial_dim + ii * spatial_dim + jj;
        sum += input[in_offset];
    }

    sdata[tid] = sum;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total = sdata[0];
        float mean = total / (spatial_dim * spatial_dim);
        float tanh_val = tanhf(mean);
        int out_offset = b * channels + c;
        output[out_offset] = tanh_val;
    }
}

torch::Tensor mean_tanh_cuda(torch::Tensor input) {
    int batch = input.size(0);
    int channels = input.size(1);
    int spatial_dim = input.size(2); // assuming square

    auto output = torch::empty({batch, channels, 1, 1}, input.options());

    const int threads = 256;
    dim3 grid(batch, channels);
    dim3 block(threads);
    int smem_size = threads * sizeof(float);

    mean_tanh_kernel<<<grid, block, smem_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch, channels, spatial_dim
