    You can replace any operators with your custom CUDA kernels. You can also combine operators together into a fused kernel. You can choose to implement any operator or combination of operators. You can use any pytorch extension API (custom CUDA kernels, custom ops, etc.). You can also replace operators with other pytorch operators if it results in a faster runtime.
        
The input tensors for the model are randomly generated with get_inputs() and get_init_inputs(), which you must use when writing your code. The get_inputs() and get_init_inputs() are already provided in the code for the original model, so you can just copy them over. The model is initialized with the get_init_inputs() as the arguments. So the user can do:

model = Model(*get_init_inputs()) 

The outputs of your code must have the same numerical accuracy (up to floating point rounding errors) as the original code. 

You can assume that all inputs are on the same device (probably GPU). 

The optimization should target the forward pass. You can ignore the backward pass if it's a feedforward network. 

    You can use any PyTorch extension APIs such as load_inline, custom backward, etc. 

    You can also use any third-party libraries such as CUTLASS, but since they may not be available everywhere, it's better to stick to native CUDA.

    You should generate a new ModelNew class that has the same inputs, outputs, and interfaces as the original Model class. The new class should use your custom CUDA kernels where applicable. The rest can be implemented with standard PyTorch operators. 

    The goal is to make the forward pass as fast as possible. 

Now, I want you to optimize the given architecture for speed. You can replace any operators with custom CUDA kernels. 

    You are free to choose which operators to replace, but you should aim for the best possible speedup. 

    You should also consider operator fusion. For example, fusing the Gemm, BatchNorm, scaling, and Softmax into a single kernel could lead to significant speedup by reducing memory traffic and kernel launch overhead.

    Please make sure that the code you write is correct and will actually run faster than the original code. 

    Avoid redundant computations and memory copies. 

Please write the optimized ModelNew class and all necessary code in a single Python file, using the same structure as the original code. You can refer to the example provided above for syntax. 

The get_inputs() and get_init_inputs() functions should remain the same as in the original code. 

You must use the same initialization parameters for the model. 

The output must be compatible with the original model's interface.



Okay, let's tackle this optimization problem. The user wants me to replace parts of the given PyTorch model with custom CUDA kernels to speed up the forward pass. The original model has a linear layer (Gemm), batch normalization (BatchNorm), scaling, and softmax. My goal is to fuse as many operations as possible into a single kernel to minimize memory traffic and kernel launch overhead.

First, I need to understand each component's computation. The linear layer does a matrix multiplication (x @ weight^T + bias). Then batch normalization applies (x - mean)/(sqrt(var + eps)), then scales by gamma, adds beta. But wait, in PyTorch's BatchNorm, the parameters gamma and beta are applied after the normalization. The scaling here is an additional parameter 'scale' multiplied by the output, followed by softmax.

Hmm, since all these operations are element-wise except the matrix multiplication, maybe I can combine the linear layer's output with the subsequent steps. Let's see:

The sequence is:
1. Gemm: x = x * weight + bias
2. BatchNorm: (x - running_mean) / sqrt(running_var + eps) * gamma + beta
3. Scaling: x * scale
4. Softmax: exp(x) / sum(exp(x))

Wait, actually, during forward pass in training mode, batch norm uses mini-batch stats, but since the user didn't specify training, maybe it's in eval mode? The problem doesn't mention training, so probably the model is in inference mode, using the running_mean and running_var stored in the BatchNorm layer.

So, assuming inference mode, the BN computation can be expressed as:

x = (x - running_mean) * inv_std * gamma + (beta - running_mean * inv_std * gamma)

where inv_std = 1/sqrt(running_var + eps)

Then scaling is multiplied, so x = scale * [ (x - running_mean) * inv_std * gamma + beta ] 

Wait, actually, the order after BN is scale * (BN output). 

So putting all together, after Gemm:

x = Gemm(x) = (x @ weight.T) + bias 

Then BN:

x = (x - running_mean) * inv_std * gamma + beta 

Then scaling: x = scale * x 

Then softmax.

Wait, the scaling is applied to the BN output. 

So if we can combine all these steps into a single kernel, that would be ideal. The matrix multiply is the only non-element-wise step, so the first part is the gemm, then the rest are element-wise operations except the softmax, which is row-wise (since dim=1).

The plan is to combine the gemm with all subsequent element-wise operations except the softmax. However, the softmax requires an exponential and a reduction over the features, so that might be a separate step but still fused into a single kernel.

Alternatively, maybe we can combine the gemm with the element-wise steps (BN, scaling) into a single kernel, and then handle the softmax in another fused kernel. Or even combine all steps except gemm into a kernel, but the gemm is a different operation.

Alternatively, can we do all in one kernel? Let's think:

The Gemm is a matrix multiplication. So each output feature is a dot product between the input vector and the weight's row (since it's a linear layer). Then, each output element goes through BN, scaling, and then into softmax.

The problem is that the Gemm is O(N*M*K) where N=batch, M=input features, K=output features. The subsequent steps are O(N*K) operations. So the bulk of computation is the Gemm. However, for speed, memory access patterns matter. If we can compute the Gemm and then apply all the element-wise operations in a single kernel, that would reduce memory transfers.

So the idea is to fuse the Gemm with the element-wise operations (BN, scaling) into one kernel, then handle the softmax in another fused kernel, or combine all into a single kernel. However, the softmax requires an all-reduce over each row, which complicates things because each element in the row must be summed to compute the denominator. That might be difficult to do in a single kernel step.

Alternatively, perhaps the first kernel does the Gemm plus the element-wise steps (BN, scaling), and then another kernel does the softmax, but even that reduces kernel launches.

Alternatively, let's see:

The first step is the matrix multiply. Let me think of the steps:

For each output element (batch, feature):

- Gemm: sum_{i} x_input[i] * weight[feature][i] + bias[feature]

Then BN:

- (value - running_mean[feature]) / sqrt(running_var[feature] + eps) * gamma[feature] + beta[feature]

Scaling: multiply by scale (shape (1,)) so just scale * (result from BN)

Then Softmax: exp of all elements in a row, sum, divide each by sum.

The matrix multiply is the big part. The element-wise steps after that can be done in a single kernel. The softmax is a bit more involved.

Perhaps, first, combine the Gemm with the element-wise steps except the softmax, then do the softmax in a separate kernel. But even better, can we combine all into a single kernel?

Wait, let's think: the Gemm is O(N*M*K) operations. The element-wise steps are O(N*K). So the element-wise steps are much smaller in terms of computation. The bulk is the matrix multiply. So perhaps the best way is to combine the element-wise steps with the Gemm. But how?

Alternatively, if we can structure the matrix multiply in a way that allows us to apply the element-wise transformations on the fly. For example, when computing each output element (output row), compute the value, apply the BN, scaling, then store intermediate values, and then compute the softmax.

Alternatively, let's see the steps in order:

Let me think of the matrix multiply as:

for each batch, for each output feature f:

    gemm_val = bias[f]

    for each input feature i:

        gemm_val += x[i] * weight[f][i]

Then apply BN, scaling, etc.

Wait, but in CUDA, we can compute this in parallel. Each thread could handle a single output feature for a batch element. 

Wait, for a batch of 1024 and output features 8192, the total number of elements is 1024*8192. 

The matrix multiply can be done with a kernel where each thread computes one output element. So each thread would loop over the input features (8192) to compute the sum. That's 8192 iterations per thread, which might be slow. Alternatively, using shared memory or tiled matrix multiplication could be better, but that's more complex.

Alternatively, using CUDA's cuBLAS for the matrix multiply and then doing the element-wise steps. However, the user might get better performance by combining steps. Since cuBLAS is already highly optimized, perhaps it's better to use it for the matrix multiply and then optimize the rest.

Alternatively, since the user wants to replace operators with custom kernels, perhaps replacing the entire forward path with a single fused kernel would be better.

Wait, the original code uses nn.Linear, which under the hood uses a matrix multiply plus bias addition. So the first step is to replace the Linear layer with a custom kernel that includes the matrix multiply, bias addition, BN, scaling, and then softmax. Or at least as much as possible.

Alternatively, let's break down the steps into manageable parts.

Let me consider possible fusion points:

1. Gemm (matrix multiply + bias) + BN + scaling: these are all element-wise after the matrix multiply. So if we can compute the matrix multiply, then apply the BN and scaling in the same kernel, that would save a memory copy.

2. The Softmax is a bit more involved because it requires an all-reduce (summing the exponentials across the features for each batch element). So perhaps the first kernel handles the first three steps (Gemm, BN, scaling) and the second kernel handles the softmax.

Alternatively, maybe the first kernel can do the first three steps, then compute the exponentials and partial sums for the softmax, then another kernel for the final division. But that might be complicated.

Alternatively, here's the plan:

First, write a kernel that fuses the Gemm (with bias), then applies BN, scaling, and then computes the exponentials for the softmax. Then another kernel to compute the row-wise sums of the exponents and divide each element by the sum. But even that is two kernels, which might still be better than four separate steps.

Alternatively, can we combine everything into a single kernel? Let's think:

The softmax requires computing the max per row, then exp, then sum, then divide. To do this in a single kernel, we might need to handle the reduction steps. But that's tricky because the reduction needs to be done across the features for each batch element.

Hmm, perhaps it's better to first handle the first three steps (Gemm, BN, scaling) in one kernel, then handle the softmax in another.

Let me start by considering the first part: fusing the Gemm, BN, and scaling into a single kernel.

The Gemm computation is:

output_gemm = x * weight + bias 

Then, the BN steps:

output_bn = (output_gemm - running_mean) / sqrt(running_var + eps) * gamma + beta 

Then scaling: output_scaled = scale * output_bn 

So all of these can be done in a single kernel after computing the Gemm result.

Wait, but the Gemm itself requires a matrix multiply, which is O(N*M*K), so the key is to compute that efficiently.

Alternatively, perhaps using cuBLAS for the matrix multiply is still better, but then we can combine the BN, scaling, and softmax into a separate kernel.

Wait, but the user might want to replace all operators with custom CUDA kernels. Let me think of the steps again.

Original code's forward:

def forward(self, x):
    x = self.gemm(x) # Gemm (matrix multiply + bias)
    x = self.bn(x) # BatchNorm (inference mode, using running stats)
    x = self.scale * x # scaling
    x = self.softmax(x) # softmax

The goal is to replace these with custom kernels. The first step is to replace the entire sequence with a single kernel or multiple fused kernels.

Let me first consider the first three steps (Gemm + BN + scaling). Since BN and scaling are element-wise, perhaps after computing the Gemm result, we can apply those in the same kernel.

But the Gemm is a matrix multiply. So let's think of the first kernel doing the Gemm (matrix multiply plus bias), then applying BN and scaling. Then the second kernel does the softmax.

Alternatively, here's a plan for the first kernel:

The first kernel takes the input x (batch_size x in_features), the weight matrix (out_features x in_features), bias (out_features), the batch norm parameters (running_mean, running_var, gamma, beta), and the scale parameter (shape (1,)), and computes the result after Gemm, BN, scaling.

The steps for each output element (for each batch and feature):

1. Compute the Gemm: out_gemm = x[i] * weight[f][i] summed over i (input features) plus bias[f].

2. Apply BN:

out_bn = (out_gemm - running_mean[f]) * (gamma[f]/sqrt(running_var[f] + eps)) + beta[f]

Wait, since the BN formula in inference is:

y = (x - running_mean) / sqrt(running_var + eps) * gamma + beta

Which can be rewritten as:

scale_factor = gamma / sqrt(running_var + eps)

bias_term = beta - running_mean * scale_factor 

Then y = x * scale_factor + bias_term 

So maybe precompute the scale and bias terms for each feature, so that during computation, it's just a multiply-add.

That way, in the kernel, for each output feature f, we have precomputed scale_bn[f] = gamma[f]/sqrt(running_var[f] + eps), and bias_bn[f] = beta[f] - running_mean[f] * scale_bn[f].

Then, the BN step becomes: out_gemm * scale_bn[f] + bias_bn[f]

Then scaling is multiplied by the scale parameter (which is a scalar here, since scale_shape is (1,)), so total becomes (out_gemm * scale_bn + bias_bn) * scale 

So, in the kernel, after computing the Gemm value, the rest is element-wise operations.

Therefore, the first kernel can do:

Compute the Gemm, then compute the scaled value with BN and scaling, all in one step.

So the first kernel can be written as:

For each output feature f, and each batch element b:

gemm_val = bias[f]

for i in 0 to in_features-1:

    gemm_val += x[b][i] * weight[f][i]

then:

result = (gemm_val * scale_bn[f] + bias_bn[f]) * scale 

Wait, but the order is:

BN: (gemm_val - running_mean) * (gamma / sqrt(var + eps)) + beta 

Then scaling: result = scale * (BN result) 

So the formula is:

result = scale * [ (gemm_val - running_mean) * (gamma/sqrt(var+eps)) + beta ]

Which can be rearranged as:

scale * (gamma/sqrt(var+eps)) * (gemm_val - running_mean) + scale * beta 

But perhaps it's better to precompute the terms as:

scale_bn = scale * (gamma / sqrt(var + eps))

bias_bn = scale * (beta - running_mean * gamma / sqrt(var + eps))

Then the result becomes:

gemm_val * scale_bn + bias_bn 

That way, all the terms can be precomputed once, and the per-element computation is just a multiply-add.

Yes, that's better. So precompute for each feature f:

scale_bn[f] = scale * (gamma[f] / sqrt(running_var[f] + bn_eps))

bias_bn[f] = scale * (beta[f] - running_mean[f] * (gamma[f]/sqrt(running_var[f]+bn_eps)))

Then, for each output element (b, f):

result = (x @ weight + bias)[f] * scale_bn[f] + bias_bn[f]

So the first kernel (Gemm + BN + scaling) can be done as:

Compute the matrix multiply plus bias, then apply the scale and bias terms precomputed for each feature.

So the first part is the matrix multiply, which is the big computation. To implement this efficiently in CUDA, perhaps using a tiled approach or leveraging shared memory for better memory access.

Alternatively, using CUDA's built-in functions might be faster, but the user wants to replace with custom kernels. So let's proceed.

The kernel structure would be:

Each thread block processes a block of output features for all batch elements. Or each thread handles one output element (batch, feature).

Let me think of the grid and block dimensions. Suppose the output features are 8192, and batch is 1024. That's 1024 * 8192 = ~8 million elements. 

If we have a thread per output element, that's 8 million threads. CUDA can handle that, but the matrix multiply per thread requires looping over 8192 input features (the in_features is 8192). Each thread would loop 8192 times, which is 8K iterations per thread. That's a lot. That might be too slow because of the high number of iterations.

Ah, here's a problem. The matrix multiply step, when implemented in a naive way with each thread handling a single output element, would have each thread perform 8192 multiplications and additions. That's 8192 * 1024 * 8192 operations. That's a lot, but maybe using a more optimized approach is necessary.

Alternatively, perhaps we can transpose the weight matrix so that each thread processes a row of the weight matrix. Alternatively, using shared memory to cache the weight matrix.

Alternatively, maybe it's better to use the cuBLAS GEMM for the matrix multiply, then apply the element-wise transformations (BN and scaling) with a separate kernel. Because cuBLAS is highly optimized for matrix multiplication. That would reduce the computational burden of the matrix multiply.

The problem is that the user wants to replace operators with custom CUDA kernels. But if using cuBLAS is allowed as part of the custom code, then maybe that's acceptable.

Wait, in PyTorch, the nn.Linear uses cuBLAS internally for the matrix multiplication. So replacing the Linear layer with a custom kernel that uses cuBLAS might not provide a speedup. However, the subsequent steps (BN and scaling) can be fused into the same kernel after the matrix multiply.

Alternatively, perhaps using a custom kernel to combine the matrix multiply with the element-wise steps would be better. Let's think.

Alternatively, let's consider that the matrix multiply can be done with a custom kernel, but the problem is the loop over the input features (8192) per thread. That's 8K iterations, which would be very slow because of the loop overhead.

Therefore, the matrix multiply is better handled by cuBLAS, so perhaps the first step is to let PyTorch's Linear layer do the matrix multiply (using cuBLAS), then apply the rest in a custom kernel.

Wait, but then the user wants to replace the operators with custom kernels. The original code's Linear layer uses the built-in matrix multiply, so to replace it, we can use a custom kernel for the matrix multiply, but that's probably not faster than cuBLAS. So maybe it's better to just keep the matrix multiply as is, and fuse the subsequent steps into a custom kernel.

Hmm, this is a bit of a dilemma. Let me think of another approach.

Suppose we leave the matrix multiply (Linear layer) as is, but replace the subsequent BN, scaling, and softmax with a custom kernel. That way, we can fuse the element-wise steps into a single kernel, saving memory copies and kernel launches.

Alternatively, combining all steps except the matrix multiply into one kernel. Let's see:

The steps after the matrix multiply are:

1. BN (element-wise)
2. scaling (element-wise)
3. softmax (row-wise, involves all elements in a row)

The first two steps can be done in a single kernel. The softmax requires per-row reduction and division. 

So first, after the matrix multiply (which is handled by the Linear layer), we can have a custom kernel that does the BN and scaling. Then, another kernel for the softmax.

Alternatively, combine BN, scaling, and softmax into a single kernel.

But the softmax requires computing the sum of exponentials for each row. To do this in a single kernel, each thread would need to compute the exponential, then accumulate the sum. Then, do a reduction to get the row sum. But that's tricky.

Alternatively, here's a plan:

First kernel (post-Gemm):

Compute for each output element the value after BN and scaling. Let's call this intermediate array 'scaled_bn'.

Then, compute the exponentials of each element in scaled_bn, then compute the row sums.

The second kernel would compute the exponentials and accumulate the row sums using atomic operations or parallel reduction. However, atomic operations can be slow.

Alternatively, use CUDA's cooperative groups or a parallel reduction approach for the row sums.

Alternatively, use a separate kernel for the softmax that first computes the exponentials, then uses a reduction kernel to compute the row sums, then divides each element by the sum. This would involve three steps, which may not be ideal.

Alternatively, using the fast softmax implementation that combines exponentials, max subtraction, and reductions in a single kernel.

Wait, here's a standard approach for softmax:

softmax(x_i) = exp(x_i) / sum(exp(x_j) for j in features)

To compute this, you can first compute the maximum of each row (to prevent overflow), subtract the max from each element, compute exp, sum the exps, then divide by the sum.

So steps:

1. Compute the max for each row.

2. Subtract the max from each element in the row (to compute x_i - max).

3. Compute exponentials of each element.

4. Compute the sum of exponentials for each row.

5. Divide each element by the row sum.

Steps 1 and 4 can be done via reductions.

But this requires multiple kernel launches and intermediate buffers.

Alternatively, can we do this in a single kernel for the softmax?

Perhaps not easily, but maybe possible with a combination of shared memory and thread synchronization.

Alternatively, since the softmax is a standard function, perhaps using PyTorch's implementation is better, but the user wants to replace operators with custom kernels. So we have to implement it ourselves.

Putting this together, the plan is:

1. Keep the matrix multiply (Linear layer) as is, since it's using cuBLAS.

2. Replace the BN, scaling, and softmax with custom kernels.

But the BN, scaling can be done in a single kernel.

Let's structure the code as follows:

- The first kernel (BN and scaling) takes the output of the Linear layer (x), applies the BN formula using the precomputed parameters, then scales by the 'scale' parameter, and stores the result in an intermediate tensor.

- The second kernel computes the softmax by first computing the exponentials, then the row sums, then dividing.

Wait, but how to handle the reduction steps in the second kernel?

Alternatively, here's the code steps for the fused BN and scaling kernel:

The parameters needed are:

- running_mean, running_var from the BN layer.

- gamma and beta from BN.

- scale (the nn.Parameter).

These parameters are all stored in the model. So in the custom kernel, we need to pass them as arguments.

So in the ModelNew class, we'll need to have access to these parameters, so we can pass them to the kernel.

The code structure would be:

class ModelNew(nn.Module):

    def __init__(self, in_features, out_features, bn_eps, bn_momentum, scale_shape):
        super().__init__()
        self.gemm = nn.Linear(in_features, out_features)  # Keep the Linear layer as is.

        # Initialize BN parameters (copied from original model's initialization)
        # Wait, but in the original code, the Model uses the parameters from the BatchNorm layer. So in ModelNew, we need to replicate the same parameters.

        # Wait, actually, in the original code, the model's __init__ passes the bn_eps and bn_momentum to the BatchNorm, so in ModelNew, we need to have the same.

        # Wait, but the user's original code's initialization is:

        # model = Model(*get_init_inputs())

        # get_init_inputs() returns [in_features, out_features, bn_eps, bn_momentum, scale_shape]

        # So the ModelNew must have the same parameters.

        # So in ModelNew, we have to replicate the BN parameters. Since the original model's BN has parameters (gamma, beta), and running_mean, running_var.

        # However, since we are fusing the BN into a custom kernel, we can store those parameters as buffers or tensors in the model.

        # Therefore, in ModelNew's __init__:

        self.bn = nn.BatchNorm1d(out_features, eps=bn_eps, momentum=bn_momentum)

        self.scale = nn.Parameter(torch.ones(scale_shape))

        # But to use them in the kernel, we need to pass them as arguments. Alternatively, the kernel can access them via the model's attributes, but in PyTorch, when using load_inline, the kernel functions are separate and don't have access to the model's attributes. So the kernel functions must be given the parameters as arguments.

        # Therefore, in the kernel function's parameters, we'll need to pass running_mean, running_var, gamma, beta, and scale.

        # So in the forward function, we'll extract these parameters from the bn layer and scale.

        # So, perhaps the code is structured as follows.

    def forward(self, x):
        x = self.gemm(x)
        # Now, apply BN, scaling, and softmax using custom kernels.

        # Extract parameters from BN layer:
        running_mean = self.bn.running_mean
        running_var = self.bn.running_var
        gamma = self.bn.weight  # gamma is the weight parameter of BN
        beta = self.bn.bias     # beta is the bias parameter
        eps = self.bn.eps

        scale = self.scale

        # Now, compute the scaled and BN'ed result using a custom kernel.

        # Let's create a custom kernel for BN and scaling.

        # The first kernel (BN + scaling) will take x as input and output the scaled_bn tensor.

        scaled_bn = bn_scale_forward(x, running_mean, running_var, gamma, beta, eps, scale)

        # Then, apply softmax using another custom kernel.

        output = softmax_forward(scaled_bn)

        return output

So the kernels need to be defined as functions that can be called in the forward pass.

Now, let's think about writing the BN and scaling kernel.

The kernel will need to compute for each element:

scaled_bn = scale * [ (x - running_mean) * (gamma / sqrt(running_var + eps)) + beta ]

Wait, let's compute this step by step.

First, for each feature f:

inv_std = 1 / sqrt(running_var[f] + eps)

scale_factor = gamma[f] * inv_std 

bias_term = beta[f] - running_mean[f] * scale_factor 

Then, the result for each element in feature f is: (x[b, f] * scale_factor) + bias_term 

Multiply by scale (the scalar parameter):

scaled_bn[b, f] = scale * ( (x[b,f] - running_mean[f]) * inv_std * gamma[f] + beta[f] )

Which can be rearranged as:

scale * (x[b,f] * scale_factor + bias_term )

where scale_factor is gamma/(inv_std), etc.

Alternatively, precompute for each feature:

scale_bn[f] = scale * (gamma[f]/sqrt(running_var[f]+eps))

bias_bn[f] = scale * (beta[f] - running_mean[f] * (gamma[f]/sqrt(running_var[f]+eps)) )

Then the formula becomes:

scaled_bn[b,f] = x[b,f] * scale_bn[f] + bias_bn[f]

This way, the kernel can compute this per element.

Therefore, in the kernel, for each element (b, f):

result = x[b,f] * scale_bn[f] + bias_bn[f]

So the kernel can be written as:

__global__ void bn_scale_kernel(const float* x, float* out,
    const float* scale_bn, const float* bias_bn, int batch_size, int out_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * out_features) {
        int f = idx % out_features;
        out[idx] = x[idx] * scale_bn[f] + bias_bn[f];
    }
}

But to compute scale_bn and bias_bn, we need to precompute them once based on the BN parameters and the scale.

Wait, but in PyTorch, the parameters are stored as tensors. So in the forward function, before calling the kernel, we can precompute scale_bn and bias_bn as tensors.

So in the forward function:

def forward(self, x):
    # ... get all parameters as before
    # Precompute scale_bn and bias_bn:
    eps = self.bn.eps
    running_var = self.bn.running_var
    inv_std = 1 / torch.sqrt(running_var + eps)
    scale_factor = self.bn.weight * inv_std 
    bias_term = self.bn.bias - self.bn.running_mean * scale_factor 

    scale_bn = self.scale * scale_factor 
    bias_bn = self.scale * bias_term 

    # Then, pass these to the kernel.

    scaled_bn = bn_scale_forward(x, scale_bn, bias_bn)

    # The kernel just multiplies each element by scale_bn[f] and adds bias_bn[f].

This way, the kernel can be a simple element-wise multiplication and addition.

Now, the kernel for bn_scale is straightforward.

Next, the softmax kernel.

The softmax requires computing for each row:

sum_exp = sum(exp(scaled_bn[b, f] for f in 0..out_features-1))

result[b,f] = exp(scaled_bn[b,f]) / sum_exp

But to compute this efficiently:

First, compute the exponentials.

Second, compute the row sums.

Third, divide each element by the row sum.

We can do this in a few steps.

First, compute the exponentials and accumulate the row sums.

Let me think of the kernel structure.

We can have a kernel that, for each row (each batch element), computes the exponentials and the sum.

But how to do this in parallel.

Option 1: Use a kernel where each thread handles a row.

Each thread (per batch element) can compute the exponentials and sum for that row.

For batch_size=1024, this requires 1024 threads. Each thread must loop over 8192 features.

So each thread would:

- Initialize max_val to -infinity.

- Loop over all features to find the max (to prevent overflow).

- Then loop again to compute exp(x - max), accumulate the sum.

- Finally store the exps and the sum.

But storing the exponentials in a separate array may be memory intensive.

Alternatively, the steps can be done in one kernel:

The first step: compute exponentials and accumulate row sums.

The second step: divide each element by the row sum.

But the first kernel can output both the exponentials and the row sums.

Alternatively, here's a plan for the first kernel:

Compute the exponentials and accumulate the row sums in shared memory.

But for each row, the number of features is 8192, so using shared memory may not be feasible.

Alternatively, each thread handles a row. The thread loops over the features, computes exp, adds to the sum, and stores the exp value in an output array.

The code:

__global__ void exp_and_sum_kernel(
    const float* input, float* exp_output, float* row_sums,
    int batch_size, int features) {

    int b = blockIdx.x * blockDim.x + threadIdx.x;
    if (b >= batch_size) return;

    float sum = 0;
    float max_val = -FLT_MAX;

    // First, find the max for the row
    for (int f = 0; f < features; ++f) {
        float val = input[b * features + f];
        if (val > max_val) {
            max_val = val;
        }
    }

    // Now compute exp and accumulate
    sum = 0;
    for (int f = 0; f < features; ++f) {
        float val = input[b * features + f];
        float e = expf(val - max_val);
        exp_output[b * features + f] = e;
        sum += e;
    }

    row_sums[b] = sum;
}

This kernel would process each batch element in a thread (or block). Since batch_size is 1024, we can launch 1024 threads (each handling one batch).

However, each thread has to process 8192 features, which is 8K iterations. For 1024 threads, this may be manageable? Maybe not, because 8K * 1024 = 8 million operations, but with 8192 features, each thread has to loop 8192 times, which is expensive.

Alternatively, use blocks of threads to process each row's features.

For example, each block handles one batch element. The block has multiple threads, each handling a portion of the features.

For example, for a batch element, split the 8192 features into chunks per thread.

Each block has 256 threads. Each thread handles 32 features (since 256*32=8192). 

Wait, 256 threads * 32 features per thread = 8192. So each block is per batch element.

The kernel would be structured as:

__global__ void exp_and_sum_kernel(
    const float* input, float* exp_output, float* row_sums,
    int batch_size, int features) {

    int b = blockIdx.x; // Each block handles one batch element
    int tid = threadIdx.x;
    int nthreads = blockDim.x;

    float max_val = -FLT_MAX;
    float local_max = -FLT_MAX;

    // First pass to compute max
    for (int f = tid; f < features; f += nthreads) {
        float val = input[b * features + f];
        if (val > local_max) local_max = val;
    }

    // Use shared memory to find the block's max
    __shared__ float shared_max[256]; // assuming block size <= 256

    shared_max[tid] = local_max;
    __syncthreads();

    // Reduce the shared_max to find the global max
    for (int s = nthreads/2; s > 0; s >>=1) {
        if (tid < s) {
            if (shared_max[tid] < shared_max[tid + s]) {
                shared_max[tid] = shared_max[tid + s];
            }
        }
        __syncthreads();
    }
    max_val = shared_max[0];

    // Now compute exp and accumulate sum
    float sum = 0;
    __shared__ float shared_sum[256]; // per thread's partial sum
    shared_sum[tid] = 0;

    for (int f = tid; f < features; f += nthreads) {
        float val = input[b * features + f];
        float e = expf(val - max_val);
        exp_output[b * features + f] = e;
        shared_sum[tid] += e;
    }

    __syncthreads();

    // Sum the partial sums
    for (int s = nthreads/2; s >0; s >>=1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
        }
        __syncthreads();
    }
    if (tid ==0) {
        row_sums[b] = shared_sum[0];
    }
}

This way, each block (per batch element) uses a block of threads to compute the max and the sum efficiently.

The block size can be chosen as 256 threads, which is a common block size.

This approach reduces the per-thread loop count from 8192 to 32 (for 256 threads). That's manageable.

Then, after computing exp_output and row_sums, the final step is to divide each element by the row sum.

Another kernel can handle this:

__global__ void softmax_divide_kernel(
    const float* exp_vals, const float* row_sums,
    float* output, int batch_size, int features) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * features) return;

    int b = idx / features;
    int f = idx % features;

    float sum = row_sums[b];
    output[idx] = exp_vals[idx] / sum;
}

This kernel is straightforward.

Putting it all together, the code would be:

First, define the BN and scaling kernel:

elementwise_bnscale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void bn_scale_kernel(const float* x, float* out,
    const float* scale_bn, const float* bias_bn, int batch_size, int out_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx <