You can write as many CUDA kernels as you need. 

**Note:**
- Please avoid using any third-party libraries. Only use PyTorch's native extensions.
- You can combine multiple operators into a single kernel (e.g., elementwise operations after convolutions) to reduce memory overhead and kernel launch overhead.
- The code should be compatible with PyTorch's autograd by properly defining backward passes or using in-place operations where applicable.
- Your code must compile and work with PyTorch's CUDA extensions.
- Ensure that the final ModelNew class has the same interface as the original Model class, including initialization and forward passes.
- All the inputs to the model are generated by get_inputs() and get_init_inputs().
- The generated ModelNew must be a drop-in replacement for the original Model, so that users can just replace "Model" with "ModelNew" without changing any other code. 

Make sure to provide the complete code including the definition of get_inputs(), get_init_inputs(), and the ModelNew class with all necessary CUDA kernels.




Okay, so I need to optimize this PyTorch model using custom CUDA kernels. Let me start by understanding the model's architecture first. The original Model has a 3D transpose convolution, then an element-wise multiplication by a scalar, followed by max pooling, global average pooling, and a clamp operation. The goal is to replace some or all of these operations with custom CUDA kernels to get speedups.

Hmm, the user mentioned that combining operators into a single kernel can help reduce memory overhead and kernel launch overhead. So maybe I can fuse some of these operations. Let's see which parts can be fused.

First, the transpose convolution is a big operation. Replacing that with a custom kernel might be tricky because it's a complex convolution. Maybe I can leave that as is, since writing a custom ConvTranspose3d would require handling the backward pass and might not be worth it unless I can fuse it with subsequent steps.

The element-wise multiplication by a scalar is straightforward. That's just x * scale. Combining that with the next operations might be possible. The next step is max pooling, which is a non-trivial operation. Then comes the global average pooling and clamp.

Wait, the global average pooling reduces the spatial dimensions to 1, so after that, the tensor is small. The clamp is an element-wise operation on a small tensor. Maybe those can be combined into a single kernel?

Alternatively, perhaps combining the scalar multiplication with the max pooling? Or maybe the max pooling and the global average pooling can be fused?

Alternatively, after the transpose convolution, the scalar multiplication is easy. Let me see:

Original forward steps:

1. ConvTranspose3d (output is a 5D tensor)
2. Multiply by scale (element-wise)
3. MaxPool3d (downsamples spatial dimensions)
4. GlobalAvgPool3d (reduces to 1x1x1)
5. Clamp (on the resulting small tensor)

The transpose convolution is the most compute-heavy, so maybe it's better to keep it as a PyTorch op. But the rest could be fused.

Looking at steps 2,3,4,5: The element-wise multiplication is simple, so maybe combine it with the max pooling? Wait, but max pooling is a reduction over a window, so combining multiplication before max pooling might be possible, but the multiplication is just a scalar, so that doesn't interfere. So the multiplication can stay as part of the kernel.

Wait, actually, the multiplication is just x * scale, so it's an element-wise operation. So after the transpose convolution, each element is scaled. So maybe the max pooling can be modified to take the scaled values. But perhaps combining the scaling and the max pooling into a single kernel?

Wait, the scaling is just multiplying each element by a scalar. So, instead of doing it as a separate step, we can include the scaling inside the max pooling kernel. However, max pooling is a sliding window operation, so the scaling doesn't affect the max, except that each element is scaled. Since scaling is a scalar, the order doesn't matter. So, whether you scale first then max, or max then scale, the result would be the same. Wait, no: scaling first then max is (scale * x_i), then take the max over the window. The max of scaled elements is scale * max(x_i). So scaling can be factored out. Wait, if the scalar is a positive number, then scaling first or after doesn't change the max's relative value. But the actual max value would be scaled. So, if you compute max first, then scale, that's equivalent to scaling first then taking max. So the scaling can be applied before or after the max pooling. 

Wait, but in the current code, the scaling is applied before max pooling, so to replicate that, the scaling must be applied before. So to combine scaling and max pooling into a single kernel, perhaps that's possible. Let me think: the max pooling kernel can take the input tensor, multiply each element by the scalar, then compute the max in each window. That way, we eliminate the separate scaling step.

Similarly, after max pooling, the global average pooling reduces to 1x1x1. The clamp is then applied. Since global average pooling is a reduction over all spatial dimensions, perhaps that can be done in a kernel that also applies the clamp.

Alternatively, maybe fuse the max pooling, global average pooling, and clamp into a single kernel. Let me see:

The steps after scaling would be:

- Max pool with kernel size 2 (since maxpool_kernel_size is 2?), so downsample each spatial dimension by 2. Wait, the kernel size is given as maxpool_kernel_size=2. The original code uses stride equal to kernel size by default for MaxPool3d? Wait, in PyTorch's MaxPool3d, the stride is by default equal to the kernel_size, unless specified. Let me check the original code: in the initialization, the model's parameters include maxpool_kernel_size. The code says self.maxpool = nn.MaxPool3d(kernel_size=maxpool_kernel_size). So the stride is the same as kernel_size, so the spatial dimensions are halved each time (if kernel is 2 and stride 2).

Then, the global average pooling reduces to 1x1x1. So after max pooling, the tensor's shape is (batch, out_channels, depth', height', width'), where depth', height', width' are halved. Then, global average pooling averages all spatial dimensions, so the output is (batch, out_channels, 1, 1, 1).

Then the clamp is applied, which is per-element.

So, combining the max pooling, global average pooling, and clamp into a single kernel might be possible. Let's see:

The max pooling reduces spatial dimensions, but the global average pooling would further reduce them. Wait, but max pooling is a local reduction (over a kernel window), while global average pooling is a global reduction over the entire spatial dimensions. So perhaps it's possible to combine max pooling and global average pooling into one step, but that might complicate the kernel.

Alternatively, after the max pooling, the global average pooling can be optimized as a separate kernel. Let me think.

Alternatively, perhaps the entire sequence after the transpose convolution can be done in a single kernel. Let's see:

The steps after conv_transpose are:

scaled = x * scale

maxpooled = maxpool(scaled)

global_avg = global_avg_pool(maxpooled)

clamped = clamp(global_avg, 0, 1)

The problem is that the max pooling is a spatial reduction, then the global average pooling is another reduction. So to combine them, perhaps the maxpool is a local reduction, then the global average is over the remaining spatial dimensions. However, the maxpooling is applied first, so you can't combine those steps directly.

Alternatively, since the global average is over all spatial dimensions, perhaps after maxpooling, the global average is equivalent to averaging over all the maxpooled features. So maybe we can compute the max pooling and then compute the average in a kernel that also applies the clamp.

But how to do that efficiently?

Alternatively, let's consider that the max pooling is followed by global average pooling. The global average pooling is equivalent to averaging over all elements in the spatial dimensions. So after the max pooling, the tensor's spatial dimensions are smaller, but the global average would still require a summation over all elements and dividing by the number of elements.

Hmm. Alternatively, the entire process from the transpose convolution's output to the clamp can be broken into parts where some steps can be fused.

Wait, perhaps the most straightforward approach is to combine the scaling and the max pooling into a single kernel. Let's see:

The scaling is an element-wise operation, so it can be integrated into the max pooling's computation. The max pooling for a 3D kernel would look at a 3D window (since it's 3D max pool), but in the problem's parameters, the kernel size is 2, so it's a 2x2x2 window? Or maybe the kernel_size is a single integer, which is applied to all dimensions? Let me check the parameters given. The user provided maxpool_kernel_size = 2, so the kernel_size for MaxPool3d is 2, which would be a cube of 2x2x2? Because the MaxPool3d takes a single integer as kernel_size, which is treated as (kW, kH, kD). So the kernel is 2x2x2.

So, the max pooling over a 2x2x2 window, with stride equal to kernel_size (so stride 2 in each dimension), so the output is half the size in each spatial dimension.

Now, to combine the scaling (multiply by 0.5) with the max pooling, the kernel would process each window, multiply each element by 0.5, then take the maximum. But since scaling is a scalar, the maximum of (scaled elements) is equal to 0.5 times the maximum of the original elements. Wait, that's true. So scaling before or after the max pooling would give different results. Wait, the original code does scaling first, then max. So the maximum is taken on the scaled values. So if you scale after, the maximum would be max(x)*0.5. But the original approach is (x*0.5) maxed. So it's equivalent. Therefore, the scaling can be done after the max pooling, but that would save computation, but here the order is fixed. So to replicate the original code's behavior, we must do scaling first, then max pooling.

So the kernel for max pooling with scaling would first apply the scaling to each element in the window, then take the maximum. Alternatively, since scaling is a scalar, we can compute the maximum first, then multiply by the scale. But that would not be the same as the original. Because scaling first then taking the max is (x1 * 0.5, x2 *0.5, ...) then max over those. The maximum of scaled elements is 0.5 * max(x_i). So if we first compute the max of the original elements, then multiply by 0.5, that's the same as doing the scaling first and then max. So the order doesn't matter. Wait, yes! Because multiplication by a positive scalar is commutative with the max operation. Since 0.5 is positive, then max(a * 0.5, b * 0.5) = (max(a, b)) * 0.5. Therefore, the scaling can be done after the max pool. Therefore, the scaling can be omitted from the max pooling step and done later. 

Ah, that's a good point! This means that the scaling can be combined with later steps. Let me think again. Since scaling can be applied after the max, then the order doesn't matter. So the max pooling can be done without scaling, and then we can multiply by the scale once, after the max, which might save computation steps. Wait, but in the original code, the scaling is done before the max. So the max is over scaled values. But since scaling is a scalar, the max of scaled elements is the same as scaling the max of the elements. Therefore, we can choose to apply scaling after the max pool, which might be more efficient because the max pool reduces the number of elements, so scaling once is cheaper. Wait, but the order in the original code is scale first, so the user's code is (x * scale) then max pool. But mathematically, (x * scale) max pooled is equivalent to (max_pooled_x) * scale. Therefore, the scaling can be done after the max pooling. Therefore, the original code's result can be obtained by doing the scaling after the max pooling. Therefore, to save computation, perhaps we can do the max pooling first, then multiply by the scale. That way, the scaling is done on a smaller tensor, which is more efficient. 

Wait, this is a key insight. That's a good optimization! Because the scaling is a scalar multiplication, it can be applied after the max pooling. Therefore, the element-wise multiplication can be moved after the max pool, which reduces the number of elements we have to scale. So that's a smart optimization. So, in the original code, the scaling is done before max pooling, but we can move it after the max pool without changing the result. 

That's a big win. Let me verify with an example. Suppose x is a 3D tensor with elements 2 and 3. The scaling is 0.5. 

Original approach: (2 *0.5, 3*0.5) → [1,1.5]. The max would be 1.5. 

If we first take the max (max of 2 and 3 is 3), then multiply by 0.5 → 1.5. So same result. Therefore, the order doesn't matter here. So this works for any scalar positive value. 

Therefore, the scaling can be moved after the max pool. That means that the scaling can be done once on the max-pooled tensor, which is smaller, saving computation.

Therefore, the sequence becomes:

conv_transpose → max_pool → scale → global_avg_pool → clamp.

Wait, but then global_avg_pool comes next. So scaling before or after the global_avg_pool?

Wait, let's see. The original sequence is: after scaling (before max pool), then max, then global_avg, then clamp. 

But moving scaling to after max pool, then scaling, then global_avg_pool, then clamp is equivalent. 

Alternatively, perhaps even move the scaling to after global_avg_pool?

The scaling is a scalar, so it can be applied at any point before the clamp. Since scaling is multiplication by a scalar, it commutes with any linear operation. 

The global average pooling is a linear operation (sum divided by count). So scaling before or after the global average is equivalent. 

Therefore, scaling can be done at any point between max_pool and the clamp. The optimal place would be to do it after the max pool, but before the global_avg_pool, so that the scaling is applied to a smaller tensor (after max pool), but even better, perhaps do it after the global average pool so that it's applied to a single value per channel. 

Wait, the global average pooling reduces each spatial dimension to 1. So the output of the global_avg_pool is a tensor of shape (batch, channels, 1, 1, 1). So scaling that is very cheap. 

Therefore, moving the scaling to after the global_avg_pool would be the most efficient, since it's only applied to a very small tensor. 

Wait, but let's check the original steps again. 

Original steps after conv_transpose:

scaled_x = x * scale → full size.

maxpooled = max_pool(scaled_x) → smaller size.

global_avg = global_avg(maxpooled) → very small.

clamped = clamp(global_avg, 0,1).

But if we reorder:

maxpooled = max_pool(x) → then scaled_max = maxpooled * scale → then global_avg(scaled_max) → then clamp.

This would be same as original.

Alternatively, if we move scaling to after global_avg, then:

global_avg_x = global_avg(max_pool(x)) → scaled_global = global_avg_x * scale → then clamp.

Yes, that also gives the same result. 

Therefore, the scaling can be done after the global average pool. Since the global average is the smallest tensor (size batch x channels x 1x1x1), this is the most efficient place for the scaling. 

Therefore, the sequence can be reorganized as:

conv_transpose → max_pool → global_avg → scale → clamp.

This way, the scaling is done on the smallest possible tensor, which is just a few elements. 

Therefore, this is a good optimization. So the element-wise multiplication can be moved all the way to after the global average pooling. That allows us to eliminate the scaling step from the earlier operations, making it easier to fuse with the clamp.

Therefore, the steps to be fused are max_pool, global_avg_pool, scaling, and clamp. Let me see:

The max pool reduces the spatial dimensions, then global_avg_pool reduces them further to 1x1x1. Then scaling and clamp are applied. 

Perhaps these steps can be combined into a single kernel. Let me think about how to do that.

Wait, the max pooling and global average pooling are two different operations. The max pooling is a local reduction (each window's maximum), while the global average is a global reduction over the entire tensor except the channel dimension. 

Hmm, fusing those two might be challenging. Alternatively, perhaps first perform the max pooling, then the global average, then the scaling and clamp. But to combine them into a single kernel might require handling both reductions. That could be complex, but maybe possible.

Alternatively, perhaps it's easier to first implement the max pooling and global average in separate steps, but fused with scaling and clamp. Let's think step by step.

Alternatively, first, let's see the computational steps after the conv_transpose:

Original path:

conv_out = conv_transpose(x)

scaled_conv_out = conv_out * scale

maxpooled = max_pool(scaled_conv_out)

global_avg = global_avg(maxpooled)

clamped = clamp(global_avg, 0,1)

Reordered path:

conv_out = conv_transpose(x)

maxpooled = max_pool(conv_out)

global_avg = global_avg(maxpooled)

scaled_global = global_avg * scale

clamped = clamp(scaled_global, 0,1)

This is equivalent. Now, the scaled_global is a tensor of shape (batch, channels, 1,1,1). The clamp is applied to each element of this tensor, clamping between 0 and 1. 

Therefore, the sequence from maxpooled to clamped can be represented as: maxpooled → global_avg → scale → clamp. 

These steps can be combined into a single kernel. Let me see:

The global average pooling can be implemented as sum over all spatial dimensions, divided by the number of elements. Then multiply by scale, then clamp each element.

So for each channel, compute the average over all spatial dimensions, multiply by scale, then clamp between 0 and 1. 

This can be done in a single kernel. Let's outline the steps for the kernel:

1. For each element in the input tensor (after max pooling), compute the sum over all spatial dimensions (depth, height, width). 

Wait, but the maxpooled tensor has dimensions (batch, channels, depth', height', width'), where depth' = (original depth + padding - kernel_size)/stride +1. Assuming the maxpool kernel size is 2, and stride 2, then depth' = depth /2 (assuming even dimensions). 

The global average pooling reduces each channel to a single value by averaging over all spatial dimensions. 

So the global_avg is computed as sum over all spatial elements for each channel and batch, then divided by the total number of spatial elements (depth' * height' * width').

So for each batch and channel, the value is (sum of all elements in spatial dimensions) / (depth' * height' * width').

Then, multiply by scale, then clamp between 0 and 1.

Therefore, the computation can be structured as:

for each batch and channel:

   compute the sum of all spatial elements.

   average = sum / (depth' * height' * width') 

   scaled_avg = average * scale 

   clamped = min(max(scaled_avg, 0), 1)

Therefore, the kernel can be written to compute this for all elements.

But how to implement this efficiently in CUDA?

Let me think about the steps. The input to this kernel is the output of the max pooling layer (maxpooled tensor), which has shape (B, C, D, H, W). 

The kernel needs to compute, for each (B, C) pair, the sum over D, H, W. 

So this is a reduction over the spatial dimensions. Then, after reduction, multiply by scale, divide by (D*H*W), then clamp.

So first, we need to compute the sum for each (B,C). 

This can be done with a kernel that, for each thread block, handles a single (B,C) pair. Each thread in the block can process a spatial element (d, h, w) and accumulate the sum. 

Alternatively, use atomicAdd, but that's slow. Maybe better to use a parallel reduction approach. 

Alternatively, use CUDA's built-in reduction functions, but since we are writing a custom kernel, perhaps it's better to structure it as a kernel that does the summation efficiently. 

Let me outline the steps:

First, the kernel for the combined maxpool and other steps. Wait, no. The max pool is a separate operation. So first, the max pool must be performed as a separate step, but perhaps the max pool can be fused with the previous steps. 

Wait, the original problem is to replace operators with custom CUDA kernels. The user mentioned that operator fusion can be done, so maybe the max pool can be replaced with a custom CUDA kernel, and then fused with the global average, scaling, and clamp. 

Alternatively, replace the max pool with a custom kernel, and then the rest can be done in another kernel. 

Alternatively, perhaps the entire sequence after conv_transpose can be replaced with a single kernel. Let me see:

The sequence after conv_transpose is:

conv_out → max_pool → global_avg → scale → clamp.

Wait, but the max pool is a complex operation. Writing a custom max pool kernel would require handling the kernel windows and sliding. That might be time-consuming. However, perhaps it's better to use the PyTorch's max pool and then combine the subsequent steps into a single kernel. 

Alternatively, perhaps the max pool is left as a PyTorch op, but the rest (global_avg, scale, clamp) can be fused into a single kernel. Let me think about that.

Suppose we use PyTorch's MaxPool3d, then the output is a tensor of shape (B, C, D', H', W'). Then, the rest can be done in a single kernel.

So the plan would be:

- Keep the conv_transpose as a PyTorch op.

- Keep the max_pool as a PyTorch op.

- Replace the global_avg_pool, scaling, and clamp with a custom kernel.

Alternatively, even the scaling and clamp can be done in the kernel.

Alternatively, combine the global_avg, scaling, and clamp into a single kernel. 

Let me proceed with that.

So the global_avg, scaling, and clamp can be fused. Let me write a CUDA kernel for that.

First, the inputs to this kernel are:

- The output of the max_pool, which is a 5D tensor (B, C, D, H, W).

The output should be a 5D tensor (B, C, 1,1,1) with each (B,C) element computed as clamp( (sum / (D*H*W)) * scale, 0,1).

Wait, actually, the global_avg_pool reduces each spatial dimension to 1, so the output is (B, C, 1,1,1). 

So the kernel can process each batch and channel independently. 

The steps in the kernel would be:

For each batch in B:

   For each channel in C:

      Compute sum of all elements in (D, H, W) for this channel and batch.

      Compute average = sum / (D*H*W)

      scaled = average * scale 

      clamped = max(0, min(scaled, 1))

      store the clamped value in the output tensor at (B, C, 0,0,0)

So the kernel needs to loop over B and C, compute the sum, then apply the operations.

Implementing this in CUDA can be done with a kernel that uses one thread block per (B, C) pair. Each thread in the block processes a spatial position (d, h, w) and accumulates the sum. Then, after reduction, the final average is computed.

Alternatively, use a more efficient approach with shared memory for the reduction.

Let me think of the dimensions. Suppose the input tensor after max_pool has dimensions (B, C, D, H, W). Let's say B is 128, C is 16, D, H, W are 8, 16, 16 (assuming original dimensions were 16,32,32 and kernel size 2, stride 2). 

So for each (B, C), the spatial elements are D*H*W = 8*16*16 = 2048 elements. 

Each thread block could handle a single (B,C), and have enough threads to process all the spatial elements. 

Alternatively, using a grid of blocks where each block handles a single (B,C). 

The kernel code might look like this:

__global__ void fused_global_avg_scale_clamp(
    const float* input, 
    float* output, 
    int B, int C, int D, int H, int W, 
    float scale, float clamp_min, float clamp_max
) {
    // Each block handles one (B, C) pair
    int b = blockIdx.x;
    int c = blockIdx.y;
    
    if (b >= B || c >= C) return;

    // Each thread in the block processes a spatial element
    // Maybe use a grid that tiles the spatial dimensions?

    // Alternatively, use a parallel reduction approach.
    // For simplicity, let's use a single thread block to compute the sum.

    // Shared memory for partial sums
    extern __shared__ float shared[];
    int tid = threadIdx.x;
    
    float sum = 0.0f;
    for (int d = 0; d < D; ++d) {
        for (int h = 0; h < H; ++h) {
            for (int w = 0; w < W; ++w) {
                // Compute index in input tensor
                int idx = ((b * C + c) * D + d) * H * W + h * W + w;
                sum += input[idx];
            }
        }
    }

    // Use atomic add or reduction within the block
    // Since threads in the block can cooperate, we can do a reduction here.
    // But since all threads are processing in a single block, maybe better to have each thread process a portion.

    // Alternatively, since this is per (B,C), and each thread in the block can contribute a partial sum.

    // Wait, the above loop is done by each thread individually, but that's inefficient. 

    // Maybe better to have each thread process a chunk of the spatial elements.

    // Let me try a different approach. 

    // Each thread in the block handles a spatial element. 

    // So total number of threads per block should be at least D*H*W.

    // But for D=8, H=16, W=16 → 2048 elements, so a block of 1024 threads would need two blocks, but maybe it's better to use a more optimized approach.

    // Alternatively, using a grid where each block is for a (B,C), and each thread in the block processes a spatial position.

    // So the block size is blockDim.x = number of threads per block. Let's say we have a block size of 256, then each block can process up to 256 elements per thread.

    // Hmm, perhaps it's better to use a tiled approach.

    // Alternatively, use a 1D grid where each thread is responsible for a single spatial element.

    // Wait, perhaps the kernel should be structured as follows:

    // For each (B, C):

    //   Initialize sum to 0.

    //   Iterate over all d, h, w and accumulate the input[b][c][d][h][w].

    //   Then compute the average, scale, clamp, and store.

    // To do this in parallel, we can have each thread handle a different (d, h, w) for a given (B,C).

    // So the grid is B * C, and each block processes a single (B,C).

    // The block's threads can process each spatial element. 

    // For example, each thread in the block handles a spatial position (d, h, w) mapped to a thread index.

    // So the block size needs to be at least D*H*W. 

    // For D=8, H=16, W=16 → 2048 elements. So a block of 2048 threads. 

    // That's possible, but may not be optimal. Alternatively, use a block size of 1024, and have two threads per element, but that would require synchronization and atomic adds, which might be slower.

    // Alternatively, use shared memory for reduction within the block.

    // Let me think of using a block with a number of threads equal to the number of elements in D*H*W. Each thread loads one element and sums into a shared memory array.

    // Here's a possible approach:

    __shared__ float partial_sums[THREADS_PER_BLOCK]; // Not sure, but maybe better to use a single sum.

    // Wait, but for D*H*W elements, each thread can contribute one value. 

    // Each thread in the block computes one element's value, adds to a global partial sum.

    // The first thread (thread 0) can accumulate all the partial sums.

    // Here's the outline:

    // The block has D*H*W threads. Each thread i corresponds to spatial index (d, h, w) = (i / (H*W), (i / W) % H, i % W).

    // Each thread reads input[b][c][d][h][w], and stores it in shared memory.

    // Then, we perform a parallel reduction in shared memory to compute the sum.

    // However, this requires knowing the spatial dimensions at compile time, which may not be feasible.

    // Alternatively, the kernel can be written with dynamic spatial dimensions.

    // Let me proceed step by step.

    // The kernel:

    __global__ void fused_global_avg_scale_clamp(
        const float* input, 
        float* output, 
        int B, int C, int D, int H, int W, 
        float scale, float clamp_min, float clamp_max
    ) {
        // Each block handles a single (b, c) pair
        int b = blockIdx.x;
        int c = blockIdx.y;
        if (b >= B || c >= C) return;

        // Each thread in the block contributes to the sum
        float sum = 0.0f;
        for (int idx = threadIdx.x; idx < D * H * W; idx += blockDim.x) {
            int d = idx / (H * W);
            int rem = idx % (H * W);
            int h = rem / W;
            int w = rem % W;

            int input_idx = (((b * C + c) * D) + d) * H * W + h * W + w;
            sum += input[input_idx];
        }

        // Perform a reduction within the block to compute the total sum
        __shared__ float shared_sum[THREADS_PER_BLOCK]; // Maybe not necessary, use warp-based reduction?

        // Use a reduction within the block
        for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
            if (threadIdx.x < stride) {
                sum += shared_sum[threadIdx.x + stride];
            }
            __syncthreads();
        }

        // Only thread 0 writes the result
        if (threadIdx.x == 0) {
            float avg = sum / (D * H * W);
            float scaled = avg * scale;
            float clamped = fmaxf(clamp_min, fminf(scaled, clamp_max));
            output[(((b * C) + c) * 1) * 1 * 1 + 0 * 1 + 0] = clamped;
        }
    }

Wait, but this is a bit rough. Maybe a better approach would be to use a block size of 256 threads, and have each thread process a chunk of the spatial elements. Then perform a reduction using shared memory.

Alternatively, using a single thread per block to compute the sum via atomic operations, but that could be slow due to contention.

Hmm, perhaps the best approach is to have each block handle one (B, C) pair. The threads in the block process a portion of the spatial elements, accumulate partial sums into shared memory, then reduce those partial sums.

Here's a possible implementation:

The block size can be 256 threads. The spatial elements are divided among the threads. Each thread processes a chunk of elements, accumulating their sum into a private variable. Then, using shared memory, they perform a parallel reduction.

Let me outline this:

// Each block handles one (B, C) pair
int b = blockIdx.x;
int c = blockIdx.y;
if (b >= B || c >= C) return;

// Shared memory for partial sums
extern __shared__ float shared_sums[];
int tid = threadIdx.x;
float sum = 0.0f;

// Each thread processes a range of elements
int total_elements = D * H * W;
int elements_per_thread = (total_elements + blockDim.x - 1) / blockDim.x;
for (int i = tid * elements_per_thread; i < (tid + 1)*elements_per_thread && i < total_elements; i++) {
    int d = i / (H * W);
    int rem = i % (H * W);
    int h = rem / W;
    int w = rem % W;

    int input_idx = (((b * C + c) * D) + d) * H * W + h * W + w;
    sum += input[input_idx];
}

// Write the partial sum to shared memory
shared_sums[tid] = sum;
__syncthreads();

// Perform a parallel reduction in shared memory
for (int s = blockDim.x / 2; s > 0; s >>= 1) {
    if (tid < s) {
        shared_sums[tid] += shared_sums[tid + s];
    }
    __syncthreads();
}

// The total sum is in shared_sums[0]
sum = (tid == 0) ? shared_sums[0] : 0.0f;

// Only thread 0 proceeds
if (tid == 0) {
    float avg = sum / (D * H * W);
    float scaled = avg * scale;
    float clamped = max(clamp_min, min(scaled, clamp_max));
    // Write to output tensor
    output[(((b * C) + c) * 1 + 0) * 1 + 0] = clamped;
}

But the indexing for the output might need to be adjusted. The output is of shape (B, C, 1, 1, 1), so the index for output[b][c][0][0][0] would be:

output_index = b * C * 1*1*1 + c * 1*1*1 + 0*1*1 + 0*1 + 0 → but actually, the storage is contiguous, so:

The output tensor is stored as (b, c, d, h, w), so for each element (b, c, 0, 0, 0), the linear index is (b * (C * 1 * 1 * 1) ) + (c * (1*1*1)) + (0*1*1) + (0*1) +0 → which simplifies to b * C + c.

Thus, the output index can be written as (b * C + c) * 1*1*1 (since the last dimensions are 1). 

Therefore, the output index is (((b * C) + c) * 1) * 1 * 1 + 0 → so just (b * C + c).

Wait, the output is 5D, so to get the correct offset, it's:

output[b][c][0][0][0] → the linear index is computed as:

The strides for a 5D tensor are typically [C*D*H*W, D*H*W, H*W, W, 1]. But since the output's last three dimensions are 1, the strides would be [C, 1, 1, 1, 1], but perhaps I'm overcomplicating. Since the last three dimensions are 1, the linear index for (b,c,0,0,0) is:

index = b * (C * 1 * 1 * 1) + c * (1 * 1 * 1) + 0*1*1 + 0*1 +0 → b*C + c.

So the output index can be written as (b * C + c) * 1*1*1 → which is just (b * C + c). 

Thus, the output address is output + (b * C + c).

Therefore, the code for writing would be:

output[b * C + c] = clamped;

Wait, but the output is a 5D tensor. However, in Py