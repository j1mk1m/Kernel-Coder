In your code, inline the custom CUDA kernels with the torch.utils.cpp_extension.load_inline function. The code should be self-contained. The functions must have the same name as the original ones. You must not use PyTorch's native functions for the replaced operations. For example, in the example given, torch.add was not used. 

When writing kernels, consider the following tips:
- Consider operator fusion to combine multiple operations into a single CUDA kernel for efficiency. For instance, combining linear (matmul + bias addition) and activation functions.
- For LogSumExp, compute exponentials, sums, logs, and maxes in a fused kernel to reduce memory transactions.
- Make sure to handle edge cases and numerical stability (e.g., subtracting the max before exponentiating in LogSumExp).
- Avoid unnecessary memory allocations by reusing memory where possible.
- Use appropriate block and grid dimensions for your CUDA kernels to maximize occupancy.

**Final Answer**
The user wants me to optimize the provided PyTorch model using custom CUDA kernels. Let me start by understanding the original model architecture.

The model has two linear layers followed by a sigmoid activation and a LogSumExp operation. The forward pass is: Linear1 -> Sigmoid -> Linear2 -> LogSumExp. The goal is to replace some or all of these operations with custom CUDA kernels for better performance.

First, I need to identify which operators can be optimized. The linear layers involve matrix multiplications and bias additions. Since these are common operations, fusing them with the activation function (sigmoid) might reduce overhead. Also, the LogSumExp is a combination of multiple steps (exp, sum, log, max), so fusing those into a single kernel could help.

Let me break down each part:

1. **Linear Layers (matmul + bias):** 
   The linear layer computes `x = Wx + b`. PyTorch's `nn.Linear` already does this, but writing a fused kernel could eliminate intermediate steps. Maybe combine the two linear layers with their activations?

Wait, the first linear is followed by a sigmoid, and the second linear is followed by LogSumExp. So perhaps fusing each linear with its subsequent activation?

Alternatively, combine the two linear operations into a single kernel if possible? But since they are separate layers with different weights, that might not be feasible. Alternatively, fuse each linear + activation into a single kernel.

2. **Sigmoid Activation:** 
   After the first linear layer, applying sigmoid. If we can combine the linear computation and sigmoid into a single kernel, that would save time by not having to write intermediate results to memory.

3. **LogSumExp:**
   This is a reduction operation. The steps are: compute max, subtract max from each element, exponentiate, sum, take log, add back the max. This is a good candidate for a fused kernel to minimize memory accesses.

Let me consider operator fusion:

- **First Linear + Sigmoid:**
   Create a kernel that does the matrix multiplication, adds the bias, applies sigmoid, and stores the result. This avoids the intermediate tensor storage between the linear and sigmoid.

- **Second Linear + LogSumExp:**
   After the second linear, compute LogSumExp in one step. The LogSumExp requires the entire output vector for each sample, so maybe we can compute it directly after the second linear's computation, avoiding an intermediate tensor.

Wait, the second linear's output is passed to LogSumExp. The LogSumExp is applied over dim=1, which for a batch of samples, each sample's features are summed. So the second linear's output is a (batch_size x output_size) tensor, and LogSumExp reduces each row to a scalar.

Alternatively, perhaps the second linear can be fused with the LogSumExp? Let me think:

The second linear is `x = W2 * x + b2`, then LogSumExp over dim=1. The LogSumExp can be written as log(sum(exp(x))) - max(x). Wait, actually, the formula is:

log( sum_{i} exp(x_i) ) - max(x). But actually, the correct formula is:

log( sum_{i} exp(x_i - max(x)) ) + max(x). This is to prevent overflow.

So perhaps during the computation of the second linear's output, we can compute the necessary components for LogSumExp in a single kernel pass. That way, we can avoid storing the intermediate activations and compute the final result in a single step.

This might be challenging because the LogSumExp requires the entire vector for each sample to compute the max and sum. So, for each sample in the batch, after computing the linear output, we need to compute its LogSumExp.

So, the plan is:

1. Implement a fused kernel for the first linear (matrix multiply + bias add) followed by sigmoid.

2. Implement a fused kernel for the second linear (matrix multiply + bias add) followed by LogSumExp over dim=1.

This way, we can reduce the number of memory accesses and kernel launches, which should improve performance.

Now, let's start writing code.

First, define the fused Linear + Sigmoid kernel.

The first linear layer: input is (batch_size x input_size), weight1 is (hidden_size x input_size), bias1 is (hidden_size,).

The computation is:

hidden = x @ weight1.T + bias1

Then apply sigmoid.

The fused kernel can compute this in one pass.

CUDA Kernel for Linear1 + Sigmoid:

The kernel will process each element of the output. The input is a 2D tensor (batch x input). The output is (batch x hidden_size).

The matrix multiplication can be done using tiled matrix multiply, but given that the batch size is large (16384), it might be better to use a batched GEMM approach. Alternatively, for simplicity, we can loop over the batch and compute each element.

Wait, but writing a custom matmul for large matrices might be time-consuming. Alternatively, using cuBLAS for the matrix multiplication and then adding bias and applying sigmoid in the same kernel.

But since the user wants us to replace the operators with custom CUDA kernels, perhaps we need to implement the matmul from scratch. However, that's a lot of work and may not be efficient. Alternatively, using cuBLAS within the kernel is not straightforward.

Alternatively, maybe use cuBLAS for the matrix multiply part and then handle the bias and sigmoid in a separate kernel. But the problem requires replacing operators with custom kernels, so perhaps we have to implement the matmul ourselves.

Wait, but that's a big task. Maybe it's better to use the existing cuBLAS functions for the matmul, then add bias and apply activation in a custom kernel.

Alternatively, the user might accept using cuBLAS for the matrix multiplication but then combine the rest into a single kernel.

Hmm. The problem says to replace pytorch operators with custom CUDA kernels. The pytorch operators here are the Linear layers (which include matmul + bias) and the Sigmoid, LogSumExp.

So, perhaps for the first Linear layer, the user expects us to replace it with a custom kernel that does the matmul + bias add + sigmoid, all in one kernel.

Similarly for the second Linear and LogSumExp.

Therefore, we need to implement matrix multiplication ourselves.

But writing an efficient matrix multiplication kernel is non-trivial. Let me think about the dimensions.

First Linear: input is (batch_size, input_size) = (16384, 2048)

Weight1 is (hidden_size, input_size) = (4096, 2048)

So, the output is (16384, 4096). 

The matrix multiply is 16384 x 2048 multiplied by 4096 x 2048 (transposed). Wait, actually, in PyTorch, the weight is (input_size, hidden_size), so when you do x @ weight1, it would be (batch, input) @ (input, hidden) = (batch, hidden).

So, the first Linear is a batched matrix multiply of (batch x input) with (input x hidden), resulting in (batch x hidden).

Implementing this with a custom kernel requires handling all the elements.

Alternatively, perhaps we can use shared memory for tiling, but given the time constraints, maybe a simpler approach is better, even if less efficient, as the user just needs the code to be functional.

Alternatively, use a kernel that for each output element (i,j), computes the dot product of row i of input and column j of weight, then adds bias[j].

But that would be O(batch * hidden * input) operations, which is 16384 * 4096 * 2048. That's way too big. So perhaps we need a better approach.

Alternatively, for each thread, compute a single element of the output matrix. Let me think about how to structure the kernel.

Suppose the grid is dim3(batch_size, hidden_size), and each thread computes one element. But 16384 * 4096 = ~65 million threads, which is too many. CUDA can handle up to 65,536 threads per block, so with 65 million threads, it would need a lot of blocks, which might not be efficient.

Alternatively, use a tiled approach where each block computes a tile of the output matrix.

Alternatively, for each row in the input (each sample), compute the entire row's output (hidden_size elements) in parallel. Since the batch is large (16384), perhaps each block handles one sample, and threads handle the hidden elements.

Wait, perhaps the first linear can be handled as follows:

Each sample in the batch is independent, so each block can process a sample's output.

For the first Linear:

Input is (batch x input), weight is (input x hidden). So for each sample, the output is a vector of hidden_size elements.

For each sample, the output vector is the input vector (input_size) multiplied by the weight matrix (input x hidden) plus the bias.

So, for a single sample, the computation is:

output[j] = sum_{k=0 to input_size-1} (input[k] * weight[j][k]) + bias[j]

Wait, actually, the weight matrix is (input x hidden), so each row of the weight corresponds to a feature. Wait, perhaps the weight is stored as (hidden, input), so each row is a hidden unit's weights across all inputs. So the multiplication is:

For sample i, output_j = sum_{k=0}^{input_size-1} (input_i[k] * weight_j[k]) + bias_j.

Therefore, for each sample and each hidden unit, we need to compute this sum.

Thus, for each sample, the output vector can be computed with a kernel that for each thread computes one hidden unit's value.

Let me structure this:

The grid can have one block per sample (since batch_size=16384 is manageable with multiple blocks). Each block processes a sample. The block size can be the hidden size divided by threads per block.

Wait, but the hidden size is 4096. Let's say we use 256 threads per block, then 4096 / 256 = 16 blocks per sample? No, actually each block would process one sample, and each thread within the block would handle a hidden unit.

Wait, each block processes one sample. The number of threads per block would be the hidden size (4096). But 4096 threads per block may exceed the maximum threads per block (usually 1024). So that's not possible.

Hmm, so perhaps we need to use a different approach. Maybe split the computation into multiple threads per output element.

Alternatively, use a block per hidden unit, but that might not be efficient either.

Alternatively, use a grid where each block processes a sample, and each thread handles a portion of the hidden units.

Let me think of a thread per hidden unit:

Each block is a sample. The number of threads per block is 4096 (hidden_size). But since CUDA limits the maximum threads per block to 1024 (for most architectures), this won't work. So perhaps need to have multiple threads per block handling multiple hidden units.

Alternatively, use a 2D block structure or use a tiling approach.

Alternatively, let me think of the first kernel as follows:

The first kernel (Linear1 + Sigmoid) can be written as follows:

For each sample in the batch (16384):

   For each hidden unit j (4096):

       output[j] = bias[j] + sum_{k=0 to input_size-1} (input[k] * weight[j][k])

       output[j] = 1 / (1 + exp(-output[j]))  // sigmoid

To compute this efficiently, perhaps each thread processes a sample and a hidden unit, but the batch is 16384 which is manageable.

Alternatively, the block size can be 256 threads, and the grid size is (batch_size, hidden_size / 256). Not sure.

Alternatively, use a block per sample, and threads per block compute a portion of the hidden units.

Suppose each block has 256 threads, then for a hidden size of 4096, each block would need 4096 / 256 = 16 threads. Wait, no, if each thread handles multiple hidden units.

Alternatively, each thread in a block (per sample) computes a portion of the hidden units.

Let me think of the first kernel's code structure:

Kernel for Linear1 + Sigmoid:

Parameters:

input: tensor of size (batch, input_size)

weight1: (hidden_size, input_size)

bias1: (hidden_size, )

output: (batch, hidden_size)

The kernel will process each element of the output.

But given the large size, this might be slow. However, for the sake of writing code, let's proceed.

The CUDA kernel might look like:

__global__ void fused_linear_sigmoid(
    const float* input,
    const float* weight,
    const float* bias,
    float* output,
    int batch_size,
    int input_size,
    int hidden_size
) {
    int sample_idx = blockIdx.x;  // each block handles one sample
    int hidden_idx = threadIdx.x + blockDim.x * blockIdx.y;

    if (sample_idx >= batch_size || hidden_idx >= hidden_size)
        return;

    // Compute the linear part for this sample and hidden unit
    float sum = bias[hidden_idx];
    for (int k = 0; k < input_size; ++k) {
        sum += input[sample_idx * input_size + k] * weight[hidden_idx * input_size + k];
    }

    // Apply sigmoid
    float z = sum;
    output[sample_idx * hidden_size + hidden_idx] = 1.0f / (1.0f + expf(-z));
}

Wait, but this is a very naive implementation. The loop over k (input_size=2048) would be done per thread, leading to 2048 iterations per thread. This would be slow because of the high latency of loops and lack of parallelism across the input dimensions.

This approach is not efficient because each thread is doing a loop over 2048 elements, leading to high computation time. A better approach would be to use shared memory and tiled matrix multiplication.

However, writing an efficient matrix multiplication kernel is complex. Since this is an exercise to write code, perhaps proceed with a simpler (though not optimal) kernel, even if it's slower, but functional.

Alternatively, we can use the cuBLAS GEMM function within the kernel? No, cuBLAS is a library and needs to be called from host code.

Wait, the problem requires writing custom CUDA kernels, so perhaps the user expects us to use cuBLAS for the matrix multiplication, then handle the bias and activation in a separate kernel.

Alternatively, use the cuBLAS functions for the matrix multiplication and then do the rest in a custom kernel.

Let me think of that approach.

First, for the first Linear layer:

Compute the matrix multiply using cuBLAS:

handle = cublasCreate();

float alpha = 1.0f;
float beta = 0.0f;

cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, 
    hidden_size, batch_size, input_size,
    &alpha,
    weight1.data_ptr(), input_size,
    input.data_ptr(), input_size,
    &beta,
    output.data_ptr(), hidden_size);

Wait, but the dimensions need to be correct. The input is (batch, input), the weight is (hidden, input), so the output should be (hidden, batch). But we need it as (batch, hidden). So perhaps transpose the result.

Alternatively, arrange the parameters correctly. Let's see:

The standard GEMM function computes C = alpha * op(A) * op(B) + beta * C.

We want C = input * weight^T. So op(A) is input (batch x input), op(B) is weight (input x hidden). The result is (batch x hidden).

So in terms of cublas:

- op(A) is input (not transposed, since it's stored in row-major, so the dimensions are batch rows, input columns)

- op(B) is weight (input rows, hidden columns). To do A * B^T, we need to transpose B.

Wait, actually, to compute input * weight.T:

input is (batch, input)

weight is (hidden, input), so weight^T is (input, hidden)

Thus, the multiplication is (batch x input) * (input x hidden) = batch x hidden.

Therefore, in cublas:

A is input, not transposed (CUBLAS_OP_N)

B is weight, transposed (CUBLAS_OP_T)

The dimensions:

m = batch_size (rows of A)

n = hidden_size (columns of B^T)

k = input_size (columns of A and rows of B^T)

Thus:

cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_T,
    batch_size, hidden_size, input_size,
    &alpha,
    input.data_ptr(), input_size,
    weight1.data_ptr(), hidden_size,  // rows of B is hidden_size (since B is weight's dimensions are (hidden, input) so when transposed, it's (input, hidden). So the rows of B (transposed) is input_size, columns is hidden_size. Wait, the cublas parameters for B are (lda, ldb), where lda is the leading dimension (rows stored in memory). The weight is stored as (hidden x input), so when transposed, the leading dimension becomes input_size.

Wait, this is getting complicated. Let me double-check:

The weight tensor is stored as (hidden_size, input_size). So when transposed, the dimensions are (input_size, hidden_size), but the storage is the same as the original. The leading dimension (lda) for B in cublas would be the first dimension of the matrix in memory. So if B is stored as (hidden_size, input_size), then when transposed, the leading dimension would be input_size, and the rows are input_size elements.

Therefore, the parameters for cublasSgemm would be:

cublasSgemm(
    handle,
    CUBLAS_OP_N,  // op(A) is not transposed (input is batch x input)
    CUBLAS_OP_T,  // op(B) is transposed (weight is hidden x input, so transposed is input x hidden)
    batch_size,   // m: rows of A (batch_size)
    hidden_size,  // n: columns of B (hidden_size)
    input_size,   // k: columns of A and rows of B (input_size)
    &alpha,
    input.data_ptr(), input_size,  // lda for A is input_size (columns of A)
    weight1.data_ptr(), hidden_size,  // ldb for B is hidden_size (rows of B before transposing)
    &beta,
    output.data_ptr(), hidden_size  // ldc is hidden_size (columns of C)
);

Wait, the output is batch_size x hidden_size, so the leading dimension (ldc) is hidden_size (since rows are batch_size, each row has hidden_size elements). So the output tensor needs to be of size batch x hidden, with leading dimension hidden_size.

Wait, no, in row-major storage, the leading dimension (lda) is the number of columns. So if the output is stored as batch x hidden, then ldc is hidden_size.

This should compute the matrix multiplication correctly.

Once we have the result in the output tensor, then we can add the bias. The bias is a vector of size hidden_size, so we need to add it to each row.

Adding the bias can be done with a kernel that for each element (sample, hidden):

output[sample][hidden] += bias[hidden]

This can be done in parallel.

Then apply sigmoid.

The sigmoid can be applied element-wise with another kernel.

Alternatively, combine the bias addition and sigmoid into a single kernel.

So the first fused kernel could be:

After the matrix multiplication, launch a kernel that adds the bias and applies sigmoid.

Similarly for the second linear layer and LogSumExp.

This approach uses cuBLAS for the matrix multiplication, which is efficient, and then custom kernels for the remaining parts.

Since the problem allows replacing operators, using cuBLAS for matmul is acceptable as long as we replace the entire Linear operator with a custom kernel that includes matmul + bias + activation.

Therefore, the steps for the first Linear + Sigmoid would be:

1. Use cuBLAS to compute the matmul part (input @ weight^T).

2. Add the bias using a custom kernel.

3. Apply the sigmoid using another custom kernel, or combine with step 2.

Alternatively, combine steps 2 and 3 into a single kernel.

Similarly for the second linear and LogSumExp.

Now, let's proceed to write the code.

First, for the first fused kernel (Linear1 + Sigmoid):

The kernel for adding bias and applying sigmoid:

__global__ void apply_bias_sigmoid(
    const float* input,  // result of matmul, size (batch x hidden)
    const float* bias,
    float* output,
    int batch_size,
    int hidden_size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * hidden_size)
        return;
    int sample = idx / hidden_size;
    int hidden = idx % hidden_size;

    float val = input[idx] + bias[hidden];
    output[idx] = 1.0f / (1.0f + expf(-val));
}

Then, the code would:

- Perform the matrix multiplication using cuBLAS.

- Call the kernel to add bias and apply sigmoid.

But integrating this into the model requires defining the custom function.

Similarly for the second part:

Second linear layer (matmul2 + bias2), followed by LogSumExp.

The second matmul is:

x = x @ weight2^T + bias2

Then compute LogSumExp over dim=1.

The LogSumExp can be done with a fused kernel that:

For each sample:

1. Compute the max of the current row (over hidden_size elements).

2. Subtract max from each element, exponentiate, sum, take log, add back max.

This can be done in a single kernel.

The steps in code:

Define a kernel that:

- For each sample in the batch, compute the LogSumExp of the row.

This requires, for each sample, reading all elements of the row, computing max, then the sum of exp(x_i - max), then log(sum) + max.

The kernel can be structured with each thread handling a sample.

The code for LogSumExp kernel:

__global__ void fused_linear_logsumexp(
    const float* input,  // input is (batch x output_size)
    const float* weight,  // weight2 (output_size x hidden_size), or hidden_size x output_size?
    const float* bias,
    float* output,  // size (batch)
    int batch_size,
    int hidden_size,
    int output_size
) {
    int sample_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (sample_idx >= batch_size)
        return;

    // Compute the linear part first: input (sample's hidden vector) * weight2^T + bias
    // input is (batch x hidden_size)
    // weight2 is (output_size x hidden_size), assuming weight2's dimensions are (hidden, output)
    // so to compute input * weight2^T: (hidden x 1) * (hidden x output) = output x 1

    // First, compute the linear part for this sample's output vector
    // The output vector is of size output_size
    // Allocate space for intermediate results
    float linear_output[output_size];  // assuming output_size is small enough for registers?

Wait, but if output_size is 1024, this array would take 4KB per thread, which is too much. So instead, compute each element on the fly.

Alternatively, compute each element of the output vector and accumulate the LogSumExp.

Wait, the goal is to compute the LogSumExp over the output vector of the second linear layer, which is of size output_size.

Wait, the second linear layer's output is (batch_size x output_size). The LogSumExp is over dim=1, so for each sample, we have a vector of output_size elements, and we need to compute log(sum(exp(x_i))).

Thus, the process is:

For each sample:

1. Compute the linear output: output_linear[i] = input[i] * weight2 + bias2 (but need to compute the full vector first).

Wait, the second linear is:

x = weight2 * x_prev + bias2. Wait, need to get the matrix multiplication correct.

Assuming the second linear's weight is (output_size x hidden_size), so the matrix multiply is:

input (batch x hidden_size) multiplied by weight2^T (hidden_size x output_size), resulting in (batch x output_size).

Then add bias2 (output_size).

Therefore, the second linear's computation is:

linear_out = input @ weight2.T + bias2

Then, LogSumExp over each row (dim=1) gives a (batch) vector.

Thus, the kernel needs to:

For each sample:

   1. Compute linear_out = input_row * weight2 + bias2 (but how?)

   2. Compute max_val of linear_out.

   3. Compute sum(exp(linear_out - max_val)).

   4. Compute log(sum) + max_val.

   Store this in output[sample].

To do this efficiently, we can compute the linear_out on the fly and compute the max and sum.

But storing linear_out is not feasible if output_size is 1024. So we need to compute it element-wise.

Let me structure this kernel:

For each sample, each thread processes it, and for each output element (output_size elements):

   compute the linear value for that element (sum over hidden_size elements of input * weight).

   keep track of max_val and the accumulated sum of exp(linear_val - max_val).

This requires per-sample shared memory to accumulate the max and sum.

Alternatively, compute the linear values for each element, track the max and sum.

But since each element of the output requires a dot product over hidden_size elements, this is computationally intensive.

Alternatively, structure the kernel as follows:

Each thread handles one sample. For that sample, iterate over all output elements (1024), and for each output element, compute the dot product between input vector and the corresponding weight column.

Wait, but for each output element, the dot product is:

linear_val = sum_{h=0 to hidden_size-1} (input[sample][h] * weight2[h][output_element]) + bias2[output_element]

Wait, if weight2 is (output_size, hidden_size), then each row of weight2 corresponds to an output element's weights over the hidden units.

Wait, actually, if weight2 is (output_size, hidden_size), then to compute the output for sample i and output_element j:

linear_val_j = bias2[j] + sum_{h=0}^{hidden_size-1} input[i][h] * weight2[j][h]

Because the matrix multiplication is input (batch x hidden) * weight2^T (hidden x output_size), so each column of weight2^T is a row of weight2.

Therefore, the kernel can proceed as follows:

For each sample (thread):

   Initialize max_val to -infinity.

   Initialize sum_exp to 0.0.

   For each output_element from 0 to output_size-1:

       linear_val = bias2[output_element]

       for h in 0 to hidden_size-1:

           linear_val += input[sample * hidden_size + h] * weight2[output_element * hidden_size + h]

       // compute exp(linear_val - current_max_val), but need to track the max_val

       // but need to track the max across all output elements

       if (linear_val > max_val):

           max_val = linear_val

       // accumulate the exponentials

       exp_val = exp(linear_val - max_val)

       sum_exp += exp_val

   // after all output elements:

   log_sum_exp = log(sum_exp) + max_val

   output[sample] = log_sum_exp

This approach is very compute-heavy because for each output element, it's looping over hidden_size elements (4096) and each output element is 1024. So per sample, it's 1024 * 4096 = ~4 million operations. With 16384 samples, that's ~65 billion operations. This would be very slow.

Clearly, this approach is not feasible. Therefore, we need a better way.

Alternative idea: use cuBLAS for the second matrix multiplication as well, then compute LogSumExp in a separate kernel.

Let me think:

Second linear layer:

Use cuBLAS to compute the matmul, add bias, then compute LogSumExp.

The second matmul:

input is (batch x hidden), weight2 is (output_size x hidden). So the matrix multiply is:

output_linear = input @ weight2^T + bias2.

Then, the LogSumExp is applied over dim=1 (each row).

Thus, the code can be structured as:

- Use cuBLAS to compute the matmul.

- Add the bias.

- Compute LogSumExp in a custom kernel.

But the adding bias can be done with the same kernel as the LogSumExp?

Alternatively:

The steps would be:

1. Matrix multiply using cuBLAS.

2. Add bias using a kernel (similar to the first part).

3. Compute LogSumExp using another kernel.

Now, let's tackle the LogSumExp kernel.

The LogSumExp requires, for each sample, to:

- Find the maximum value in its row.

- Subtract the max from each element, exponentiate, sum all, take log, add back the max.

To compute this efficiently:

The kernel can be structured as follows:

Each thread handles one sample.

For each sample, the thread processes all output_size elements.

First, compute the max of the row.

Then compute the sum of exponentials.

Finally compute the log_sum_exp.

But how to do this efficiently.

The steps in code:

__global__ void logsumexp_kernel(
    const float* input,
    float* output,
    int batch_size,
    int output_size
) {
    int sample_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (sample_idx >= batch_size)
        return;

    float max_val = -FLT_MAX;
    float sum_exp = 0.0f;

    // Iterate over each element in the sample's row
    for (int i = 0; i < output_size; ++i) {
        float val = input[sample_idx * output_size + i];
        if (val > max_val) {
            max_val = val;
        }
    }

    // Now compute sum of exp(val - max_val)
    for (int i = 0; i < output_size; ++i) {
        float val = input[sample_idx * output_size + i];
        sum_exp += expf(val - max_val);
    }

    output[sample_idx] = logf(sum_exp) + max_val;
}

This kernel requires two passes over the data for each sample, which is okay as long as the output_size (1024) isn't too large. Since 1024 is manageable, this should work.

Now, putting it all together.

First, for the first fused kernel (Linear1 + Sigmoid):

We'll have a custom function that:

- Uses cuBLAS for the matrix multiply.

- Uses a kernel to add bias and apply sigmoid.

Second, for the second part:

- cuBLAS for second matrix multiply.

- Add bias, then compute LogSumExp.

Now, let's write the code step by step.

First, define the first fused kernel.

We need to write the CUDA code for the first part.

First, the code for the first fused Linear1 + Sigmoid:

The first part:

class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.linear1_weight = nn.Parameter(torch.empty(hidden_size, input_size))
        self.linear1_bias = nn.Parameter(torch.empty(hidden_size))
        self.linear2_weight = nn.Parameter(torch.empty(output_size, hidden_size))
        self.linear2_bias = nn.Parameter(torch.empty(output_size))

        # Initialize weights and biases (assuming using the original model's initialization)
        # But to make it compatible, perhaps load from the original model?
        # For simplicity, assume that we'll initialize them here properly.

        # Initialize weights and biases (random for now)
        nn.init.xavier_normal_(self.linear1_weight)
        nn.init.zeros_(self.linear1_bias)
        nn.init.xavier_normal_(self.linear2_weight)
        nn.init.zeros_(self.linear2_bias)

        # Define custom CUDA kernels

        # First kernel: fused linear1 + sigmoid
        fused_linear_sigmoid_source = """
        #include <torch/extension.h>
        #include <cublas_v2.h>
        #include <cuda_runtime.h>

        __global__ void apply_bias_sigmoid(
            const float* matmul_result,
            const float* bias,
            float* output,
            int batch_size,
            int hidden_size
        ) {
            int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= batch_size * hidden_size) return;
            int sample = idx / hidden_size;
            int hidden = idx % hidden_size;
            float val = matmul_result[idx] + bias[hidden];
            output[idx] = 1.0f / (1.0f + expf(-val));
        }

        torch::Tensor fused_linear_sigmoid(
            torch::Tensor input,
            torch::Tensor weight,
            torch::Tensor bias
        ) {
            int batch_size = input.size(0);
            int input_size = input.size(1);
            int hidden_size = weight.size(0);

            // Perform matrix multiplication using cuBLAS
            cublasHandle_t handle;
            CUBLAS_CHECK(cublasCreate(&handle));

            float alpha = 1.0f;
            float beta = 0.0f;
            auto matmul_result = torch::empty({batch_size, hidden_size}, input.options());

            CUBLAS_CHECK(cublasSgemm(
                handle,
                CUBLAS_OP_N, CUBLAS_OP_T,
                batch_size, hidden_size, input_size,
                &alpha,
                input.data_ptr<float>(), input_size,
                weight.data_ptr<float>(), weight.size(0), // lda for B is hidden_size (since weight is stored as (hidden, input))
                &beta,
                matmul_result.data_ptr<float>(), hidden_size // ldc is hidden_size
            ));

            CUBLAS_CHECK(cublasDestroy(handle));

            // Launch kernel to add bias and apply sigmoid
            int threads_per_block = 256;
            int blocks = (batch_size * hidden_size + threads_per_block - 1) / threads_per_block;

            apply_bias_sigmoid<<<blocks, threads_per_block>>>(
                matmul_result.data_ptr<float>(),
                bias.data_ptr<float>(),
                output.data_ptr<float>(),
                batch_size,
                hidden_size
            );

            return output;
        }
        """
        # Similarly define the second kernel for linear2 + logsumexp

        # Second kernel: fused linear2 + logsumexp
        fused_linear_logsumexp_source = """
        #include <torch/extension.h>
        #include <cublas_v2.h>
        #include <cuda_runtime.h>
        #include <math.h>

        __global__ void apply_logsumexp(
            const float* matmul_result,
            const float* bias,
            float* output,
            int batch_size,
            int hidden_size,
            int output_size
        ) {
            int sample_idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (sample_idx >= batch_size) return;

            float max_val = -FLT_MAX;
            float sum_exp = 0.0f;

            for (int o = 0; o < output_size; ++o) {
                float val = 0.0f;
                val += bias[o];
                for (int h = 0; h < hidden_size; ++h) {
                    val += matmul_result[sample_idx * hidden_size + h] * 
                        weight_linear2[o * hidden_size + h]; // Wait, weight is a parameter, not accessible here.
                }
                // This approach is not feasible because the weight is not passed into the kernel.

                // Oops, this is a problem. The kernel can't access the weight parameters directly.

                // Alternative approach: the kernel can't access the weight, so need to use cuBLAS again.

                // So perhaps this approach is incorrect.
            }

            // So this method is not feasible. Need to rethink.
        }

        // Alternative approach: use cuBLAS for the second matmul, then compute LogSumExp.

        __global__ void compute_logsumexp(
            const float* input,  // input is (batch x output_size)
            float* output,
            int batch_size,
            int output