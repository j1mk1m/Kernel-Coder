{
  "rule": "Layer normalization and activations (e.g., GELU) are optimized with warp-based reductions and PyTorch's native implementations.",
  "total": 560,
  "aligned": 38,
  "alignment_rate": 0.2289156626506024,
  "both_false": 365,
  "both_true": 29
}