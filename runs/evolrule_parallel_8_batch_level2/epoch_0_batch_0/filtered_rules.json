[
  "The kernel uses operator fusion to combine multiple operations (e.g., convolution, activation, normalization) into a single CUDA kernel for reduced overhead.",
  "Thread block sizes are multiples of warp size (32) and do not exceed CUDA's maximum limit (1024 threads per block).",
  "The kernel leverages shared memory for tiling and reductions (e.g., global memory access reduction, cross-channel computations).",
  "GEMM and complex operations (e.g., convolution, normalization) are delegated to PyTorch's optimized native functions.",
  "Contiguous memory access patterns and coalesced global memory transactions are enforced (e.g., using __ldg for constant regions).",
  "Grid and block dimensions are optimized for compute architecture (e.g., 1D grid, warp-alignment, minimizing excessive grid sizes).",
  "Reductions (mean, variance, sum) use block-wide shared memory and thread-local accumulators before global writes.",
  "Bitwise operations (__ballot_sync, __popc, __ffs) and vectorized memory accesses (e.g., float4) are used for optimization.",
  "All __syncthreads() calls are strategically placed to avoid stalls and maintain data dependencies.",
]
