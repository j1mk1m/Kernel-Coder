Below is a large list of rule-like statements regarding the behavior of CUDA kernels. Some of these rules might be duplicates or very similar.
Please merge them so that there are no duplicates or very similar rules. Condense the rules into at most 25 rules.
Return the merged list as a JSON array of strings. Do not use ``json``, just output the JSON array directly. 
[Rules]
The kernel uses a single fused CUDA kernel to reduce kernel launch overhead.
The kernel is compiled with optimization flags such as -O3.
The kernel ensures all tensor operations are performed on the same CUDA device.
The kernel uses shared memory tiling to reduce global memory access.
The kernel uses thread block sizes that are multiples of warp size (32).
The kernel performs operator fusion between multiple operations.
The kernel fuses convolution with multiple activation functions and bias into a single operation to reduce kernel launches.
The kernel implements activation functions without branching (e.g., using fmax/fmin) to optimize performance.
The kernel calculates output dimensions dynamically based on input tensor shapes to ensure correctness.
The kernel uses shared memory tiling to reduce global memory access.
The kernel is designed for memory-bound operations to improve throughput.
The kernel maintains the correct computational order of layers to preserve forward pass integrity.
Kernel 4 and 7 use operator fusion between scaling and LeakyReLU operations.
Kernel 4 and 7 rely on PyTorch's optimized nn.Linear for GEMM operations.
Kernel 3 incorrectly uses shared memory with an invalid reduction loop structure.
The kernel uses shared memory to store intermediate values for cross-channel reductions (e.g., softmax computation).
The kernel uses thread block sizes that do not exceed CUDA's maximum limit of 1024 threads per block.
The kernel accesses the bias tensor using only the channel index, not spatial dimensions.
The kernel uses shared memory for mean and variance reduction within threads.
The kernel incorporates learnable parameters (weight and bias) from the instance normalization layer.
The kernel combines instance normalization and division into a single step to minimize memory access.
The kernel uses thread block sizes that are multiples of warp size (32).
The kernel uses shared memory tiling to reduce global memory access.
The kernel implements operator fusion between multiple operations.
The kernel uses parallel reduction with warp shuffle instructions (e.g., __shfl_down_sync) to optimize reduction steps.
The kernel correctly implements the log-sum-exp operation by subtracting the maximum value before exponentiation to prevent overflow.
The kernel avoids incorrect bias terms when none exist in the original PyTorch layer.
The kernel uses a 1D grid for thread block distribution.
The kernel uses thread block sizes that are multiples of warp size (32).
The kernel avoids excessively large grid dimensions (number of blocks) to reduce launch overhead.
The kernel uses shared memory tiling to reduce global memory access.
The kernel uses thread block sizes that are multiples of warp size (32).
The kernel performs operator fusion between multiple operations.
The kernel fuses element-wise addition and multiplication operations into a single kernel.
The kernel uses thread block sizes that are multiples of the warp size (32).
The kernel delegates complex operations (e.g., instance normalization) to pre-optimized libraries.
- The kernel uses a 1D thread-block structure with grid-stride loops to ensure all spatial dimensions are covered.
- The kernel flattens the bias tensor to allow contiguous memory access, improving performance.
- The kernel avoids explicit synchronization calls (e.g., cudaDeviceSynchronize()), relying on PyTorch's event-based dependency management instead.
The kernel uses a 1D grid to parallelize spatial positions across batches.
The kernel employs shared memory for reduction operations.
The kernel uses block dimensions equal to the number of output channels.
The kernel uses a fused CUDA kernel for Swish activation followed by bias addition.
The kernel correctly separates matrix multiplication bias and post-Swish bias parameters.
The kernel avoids nested loops over input dimensions for matrix multiplication, leveraging thread-wise parallelism.
The kernel uses shared memory reductions for max and sum calculations during softmax.
The kernel performs operator fusion between clamping, softmax, and scaling.
The kernel uses thread block sizes of 256 threads.
The kernel uses hard-coded constants for parameters instead of dynamically computed values.
The kernel lacks proper error checking for CUDA runtime API calls.
The kernel does not correctly compute output dimensions based on input and convolution parameters.
The kernel uses numerical approximations for extreme values to avoid overflow/underflow.
The kernel avoids branching by replacing conditionals with arithmetic expressions.
The kernel fuses multiple activation operations into a single CUDA kernel to reduce overhead.
The kernel fuses consecutive operations that share memory access patterns to reduce overhead.
The kernel avoids excessive kernel launches for simple scalar operations (e.g., scaling by scale1).
The kernel correctly calculates output dimensions using the transposed convolution formula.
The kernel uses thread block sizes that are not multiples of the warp size (32), causing inefficient execution.
The kernel performs matrix multiplication with nested loops inside a single thread, leading to high computational cost and poor parallelism.
The kernel does not ensure all tensors are on the same CUDA device, leading to runtime errors due to device mismatches.
The kernel performs operator fusion between multiple operations.
The kernel uses thread block sizes that are multiples of warp size (32).
The kernel implements the GELU activation using the tanh-based approximation with coefficients 0.79788 and 0.044715.
The kernel uses tanh-based GELU approximation for improved computational efficiency.
The kernel performs operator fusion between element-wise operations (multiplication, LeakyReLU, GELU) after convolution.
The kernel avoids fusing convolution operations due to potential implementation complexity and errors.
The kernel fuses multiple operations (matrix multiplication, max pooling, summation, and scaling) into a single CUDA kernel to reduce intermediate memory copies.
The kernel uses a thread block size of 256 to better utilize GPU resources compared to smaller thread block sizes.
The kernel processes feature dimensions in steps of the kernel size (e.g., 2) to efficiently implement max pooling without intermediate tensors.
The kernel fuses ReLU and HardSwish into a single CUDA kernel operation.
The kernel uses PyTorch's optimized nn.Conv2d layer instead of reimplementing convolution.
The kernel avoids unnecessary branch divergence in activation computations by using clamping functions instead of if-statements.
The kernel performs operator fusion between the Swish activation and scaling operation.
The kernel uses PyTorch's built-in Linear layer for matrix multiplication instead of implementing it manually.
The kernel ensures all tensors are moved to the CUDA device before execution.
The kernel applies padding to retain the input's spatial dimensions.
The kernel correctly orders operations as ReLU followed by GroupNorm.
The kernel uses ConvTranspose3d instead of Conv3d for upsampling.
The kernel misses the fully connected layer (matrix multiplication) computation, only handling subsequent steps like group norm or activation.
The kernel incorrectly calculates group mean and variance using per-thread computations without proper reduction across all elements in the group.
The kernel uses sequential PyTorch operations instead of a fused CUDA kernel, increasing kernel launch overhead and memory traffic.
The kernel uses fused operations to combine multiple sequential average pooling layers into a single step.
The kernel omits learnable parameters (gamma and beta) in the batch normalization computation.
The kernel uses a flat index for memory access, leading to potential non-coalesced reads from global memory.
The kernel fuses bias addition and ReLU activation into a single CUDA kernel.
The kernel uses a 1D thread grid covering all output elements (batch_size × out_features).
The kernel launches with thread block sizes that are powers of two (e.g., 256).
The kernel does not implement the transposed convolution layer required by the model.
The kernel lacks a parallelized implementation of global average pooling for spatial dimensions.
The kernel computes mean and variance using CPU-based reductions after partial GPU accumulation, causing unnecessary host-device data transfers.
The kernel uses loops matching the kernel_size parameter of the replaced layer (e.g., loops up to 2 for kernel_size=2).
The kernel correctly computes output dimensions as (input_size + 1) // 2 to match default PyTorch max pooling behavior.
The kernel explicitly avoids hardcoded stride values to maintain compatibility with dynamic input configurations.
The kernel fuses clamp and multiplication into a single step.
The kernel uses thread indices to iterate over all dimensions deterministically.
The kernel combines max reduction over channels into one pass.
The kernel uses thread block sizes exceeding CUDA's maximum thread limit per block.
The kernel incorrectly computes the mean by dividing the max value by batch_size instead of the feature dimension size.
The kernel uses a naive GEMM implementation without shared memory tiling or parallelization optimizations.
- The kernel does not use tiled matrix multiplication with shared memory for GEMM.
- The kernel performs reductions for GroupNorm with serial computation instead of parallelized block/warp reductions.
- The kernel uses a grid configuration that results in excessive block count for batch × groups.
The kernel performs operator fusion between multiple operations
The kernel uses contiguous memory access to ensure coalesced memory transactions
The kernel uses thread block sizes that are multiples of warp size (32)
The kernel performs operator fusion between multiple operations.
The kernel uses in-place operations to reduce intermediate tensor creation.
The kernel leverages PyTorch's TorchScript to reduce kernel launch overhead.
The kernel implements GELU approximation using either erf-based or tanh-based method.
The kernel uses operator fusion to combine multiple arithmetic operations into a single CUDA kernel.
The kernel ensures contiguous memory access for input/output tensors.
The kernel uses thread block sizes that are multiples of warp size (32).
The kernel fuses multiple activation functions (Swish, Tanh, GELU, Hardtanh) into a single kernel launch.
The kernel implements GELU using an exact erf-based calculation instead of the standard tanh approximation.
The kernel uses shared memory tiling to reduce global memory access.
The kernel must sequence Matmul, AvgPool, GELU, Scale, and Max in the specified order.
The kernel must handle tensor dimension adjustments for AvgPool (unsqueeze/squeeze).
The kernel must apply the final Max reduction along the correct dimension (dim=1).
