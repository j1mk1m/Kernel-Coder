[
  "- The kernel uses a 1D thread-block structure with grid-stride loops to ensure all spatial dimensions are covered.",
  "- The kernel flattens the bias tensor to allow contiguous memory access, improving performance.",
  "- The kernel avoids explicit synchronization calls (e.g., cudaDeviceSynchronize()), relying on PyTorch's event-based dependency management instead."
]