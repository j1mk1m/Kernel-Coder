[
  "The kernel uses operator fusion to combine multiple operations (e.g., convolution, activation, normalization) into a single CUDA kernel for reduced overhead.",
  "Thread block sizes are multiples of warp size (32) and do not exceed CUDA's maximum limit (1024 threads per block).",
  "The kernel leverages shared memory for tiling and reductions (e.g., global memory access reduction, cross-channel computations).",
  "Warp-based parallel reductions use shuffle instructions (e.g., __shfl_down_sync, __shfl_xor_sync) to minimize latency.",
  "All tensors (inputs, outputs, parameters) are on the same CUDA device to ensure compatibility and avoid runtime errors.",
  "GEMM and complex operations (e.g., convolution, normalization) are delegated to PyTorch's optimized native functions.",
  "Non-branching implementations (e.g., using arithmetic expressions instead of conditionals) are used for activation functions and reductions.",
  "Contiguous memory access patterns and coalesced global memory transactions are enforced (e.g., using __ldg for constant regions).",
  "Numerical stability is maintained via techniques like log-sum-exp, padding with max values before exponentiation, and canonicalization.",
  "Grid and block dimensions are optimized for compute architecture (e.g., 1D grid, warp-alignment, minimizing excessive grid sizes).",
  "In-place operations and contiguous tensors are preferred to reduce memory allocation and copies.",
  "Output dimensions and padding are dynamically calculated using PyTorch's formulas and tensor metadata.",
  "Reductions (mean, variance, sum) use block-wide shared memory and thread-local accumulators before global writes.",
  "CUDA Streams are utilized for overlapping computation and memory transfers where applicable.",
  "Critical operations (e.g., attention, feedforward layers) are implemented as fused kernels to avoid intermediate storage.",
  "Bitwise operations (__ballot_sync, __popc, __ffs) and vectorized memory accesses (e.g., float4) are used for optimization.",
  "All thread indices are properly mapped to data elements using blockIdx, blockDim, and threadIdx.",
  "Numerical precision is preserved using fused multiply-add (__fma) for key computations.",
  "Error checking for CUDA runtime API calls and PyTorch's exception system ensure robustness.",
  "Layer normalization and activations (e.g., GELU) are optimized with warp-based reductions and PyTorch's native implementations.",
  "Grid-stride loops and parallel reductions (e.g., binary reduction) minimize kernel execution time.",
  "All __syncthreads() calls are strategically placed to avoid stalls and maintain data dependencies.",
  "Shared memory alignment and padding are used to avoid bank conflicts.",
  "The kernel processes spatial dimensions (height, width), channels, and batches in parallel for maximum throughput.",
  "All operations are implemented as a single fused kernel to eliminate intermediate tensor storage and kernel launch overhead."
]