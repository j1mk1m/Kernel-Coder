Additionally, in your code submission, you must include the import statements for any required modules. 

Also, please explain your optimization choices in a concise manner in the markdown.
Alright, let's tackle this optimization problem. The given model has a series of operations: a 3D transposed convolution, scaling by a factor, batch normalization, and global average pooling. The goal is to replace some or all of these with custom CUDA kernels for better performance.

First, I need to analyze which operators might benefit from custom CUDA. The transposed convolution (ConvTranspose3d) is a compute-heavy operation. Implementing a custom kernel here could help, but it's complex. Scaling (x * scale_factor) is a simple element-wise multiplication, which is trivial in CUDA and might have overhead in PyTorch due to separate kernels. BatchNorm3d involves mean and variance computations, which can be fused with the scaling and convolution. Global average pooling is also element-wise but can be combined with other steps.

Operator fusion is key here. Let's consider fusing the convolution, scaling, and batch norm into a single kernel. However, batch norm requires normalization, which involves reduction operations (computing mean and variance across dimensions). These reductions can be done in the same kernel as the convolution to save memory transfers. However, this might complicate the kernel design.

Alternatively, the scaling (element-wise multiply) is straightforward. Let's start there. The existing scaling is x * scale_factor. A custom CUDA kernel for element-wise multiplication could replace this. However, PyTorch's implementation is already optimized, so maybe not much gain here unless we can fuse it with another operation.

Next, the batch norm: the forward pass computes (x - mean)/(sqrt(var + eps)) * gamma + beta. If we can combine this with the scaling, perhaps in a fused kernel, that could reduce overhead. For example, after the convolution, multiply by scale, then apply batch norm. The fused kernel would handle scaling and batch norm together.

The global average pooling is also an element-wise operation but over the spatial dimensions. If we can combine this with the previous steps, like in a final step that accumulates the values, that might be beneficial.

However, the most impactful optimization might be the transposed convolution itself. Implementing a custom ConvTranspose3d kernel could offer significant speedups, especially for 3D tensors. PyTorch's implementation might not be optimized for specific cases, and a custom kernel can be tailored for the given parameters (kernel size 5, etc.), potentially using shared memory or optimized memory access patterns.

Let me outline possible steps:

1. **Fusing ConvTranspose3d with scaling and batch norm**: This is complex because ConvTranspose3d involves a lot of computation and memory accesses. However, fusing the scaling (multiplication by scale_factor) and batch norm into the same kernel could reduce the number of memory operations. Since the scaling is just a scalar multiply, it can be incorporated into the convolution's output computation.

2. **Optimizing the convolution kernel**: Writing a custom ConvTranspose3d kernel with optimized memory access (e.g., using shared memory for weight tiles) and compute utilization. For a kernel size of 5 in 3D, the receptive field is manageable.

3. **Global Average Pooling**: Implement a custom kernel that efficiently computes the average across depth, height, and width. Since it's a reduction, a parallel reduction approach could be faster.

Alternatively, maybe the transposed convolution is the main bottleneck, so focusing there first makes sense.

However, implementing a custom 3D transposed convolution kernel is quite involved. Let me consider if there's a way to simplify. Maybe using a combination of existing libraries or patterns, but the user wants custom CUDA.

Alternatively, perhaps the batch norm and scaling can be fused into a custom kernel that's applied after the convolution. Since the batch norm requires normalization parameters computed from the convolution output, which involves reductions, this requires calculating the mean and variance in the same kernel as the convolution?

Wait, the batch norm's mean and variance are computed over the batch and spatial dimensions. To fuse with the convolution, the kernel would need to compute these statistics on the fly, which complicates things. Alternatively, maybe precompute the mean and variance in separate steps but use a fused kernel for the normalization and scaling.

Alternatively, perhaps the most straightforward optimization is to fuse the scaling (x * scale_factor) and batch norm into a single kernel. Let's see:

After the convolution, the output is x. The next steps are:

x = x * scale_factor  
x = batch_norm(x)

The batch norm requires computing mean and var over the channels and spatial dims. So first, compute the mean and variance for batch norm, then apply the scaling and normalization.

But the scaling is just a multiplication by a scalar, so maybe we can combine scaling and the batch norm's gamma (which is per-channel). Wait, the batch norm's formula is:

y = gamma * (x_scaled - mean) / sqrt(var + eps) + beta

Where x_scaled = x * scale_factor.

So the scaling is part of the input to batch norm. So perhaps we can combine the scaling into the batch norm computation.

Alternatively, perhaps the scaling can be incorporated into the gamma parameters, but that might not be straightforward if scale_factor is a parameter of the model.

Hmm, perhaps the element-wise scaling is a simple operation that can be implemented in a custom kernel with minimal overhead, but in practice, PyTorch's element-wise operations are already highly optimized, so the benefit might be marginal.

The transposed convolution is likely the biggest candidate. Let me think about how to write a custom kernel for 3D transposed convolution. The 3D transposed convolution (also known as a deconvolution) involves upsampling and convolving. The forward pass for a transposed convolution can be thought of as a convolution with the kernel flipped and the input upsampled via stride.

Implementing a custom kernel for ConvTranspose3d requires handling the input strides, kernel padding, and output dimensions. Since the user's kernel_size is 5, which is odd, padding might be symmetric.

The standard approach for a transposed convolution kernel involves:

- For each output position, compute the input position based on the stride and kernel size.
- Apply the kernel weights to the input, accumulating into the output.

The challenge is to efficiently traverse the 3D indices and manage memory access patterns to minimize latency.

However, writing such a kernel from scratch is quite involved. Given the time constraints, perhaps it's better to focus on fusing the scaling and batch norm into a single kernel, along with the global average pooling, but I need to see if that's feasible.

Alternatively, maybe the global average pooling can be optimized. The current implementation uses AdaptiveAvgPool3d((1,1,1)), which averages over all spatial dimensions. A custom kernel for this could compute the average in parallel, using a reduction across the depth, height, and width dimensions. Since it's a reduction, a parallel approach where each thread handles a block of data, sums, and then reduces could be faster, especially for large tensors.

Let me outline the plan:

1. **Replace the scaling operation (x * scale_factor) with a custom CUDA kernel.** This is trivial but might save some overhead from PyTorch's operator dispatch. However, given that this is a simple element-wise operation, the speedup might be minimal. Let's proceed for the sake of example.

2. **Fuse BatchNorm3d and scaling into a custom kernel.** Wait, the scaling is before the batch norm. Let me recheck the model:

The forward is:

x = self.conv_transpose(x)  
x = x * self.scale_factor  
x = self.batch_norm(x)  

So the scaling is applied before batch norm. To fuse these two steps (scaling and batch norm), we can compute the scaled x, then compute the batch norm parameters (mean/var) on the scaled x, then apply batch norm. However, the batch norm computation requires computing the mean and variance of the scaled x. This can be done in a single kernel that first computes the scaled x, then computes mean and variance, then applies normalization. However, this requires multiple steps and might be more complex.

Alternatively, perhaps the batch norm can be adjusted to account for the scaling factor. Since scaling by a factor affects the mean and variance:

If x_scaled = x * scale, then the mean of x_scaled is mean(x) * scale, and the variance is var(x) * scale^2. So, if we know the scale factor is constant, we can compute the mean and variance of x first, then adjust them using the scale factor. However, the batch norm parameters (gamma and beta) are also part of the model, so the batch norm's computation would be:

y = gamma * (x_scaled - mean_x_scaled) / sqrt(var_x_scaled + eps) + beta  
= gamma * (x * scale - mean_x * scale) / (scale * sqrt(var_x + eps/scale^2)) + beta  
Hmm, not sure if that helps. Alternatively, the batch norm's parameters could be adjusted to absorb the scaling factor. Since the scaling is a scalar, maybe the batch norm can be reparameterized to combine with it, but that might require changing the model's parameters, which might not be desired.

Alternatively, perhaps the batch norm can be fused with the scaling by reordering steps. However, this might not be straightforward.

Given the complexity, perhaps the best approach is to focus on the transposed convolution and the global average pooling.

**Option 1:** Implement a custom ConvTranspose3d kernel.

**Option 2:** Implement a custom kernel for global average pooling.

Let me try to proceed with the global average pooling first, as it's simpler.

The global average pooling over depth, height, width for each channel and batch.

The input is (batch, channels, depth, height, width).

The output is (batch, channels, 1, 1, 1).

The computation for each output element (for a specific channel and batch) is the average over all depth, height, width elements of that channel.

A custom kernel can compute this by:

- For each channel and batch, sum all elements in the spatial dimensions, then divide by the total number of elements.

To parallelize this, each thread can handle a (batch, channel) pair. The number of threads would be batch * channels. Each thread accumulates the sum over depth, height, width.

But the spatial dimensions might be large, so each thread would have to loop over depth*height*width elements. For example, if depth=16, height=32, width=32, then each thread has to process 16*32*32 = 16384 elements. That might be too much per thread.

Alternatively, use a block of threads per (batch, channel), and each thread processes a portion of the spatial elements.

Alternatively, use a parallel reduction approach where multiple threads contribute to the sum.

Alternatively, use a kernel where each thread handles a single element of the input and contributes to the corresponding channel's sum.

Let me outline the code for global average pool:

The kernel would:

- For each input element at position (b, c, d, h, w), add its value to a global sum for (b, c).

- After all elements are processed, divide each sum by the total number of elements (depth * height * width) to get the average.

To implement this efficiently, perhaps use atomic operations to accumulate the sums, but that can introduce contention. Alternatively, use a reduction pattern with shared memory.

Alternatively, use a tiled approach where each thread block handles a chunk of the input.

Given the complexity, let me sketch a possible kernel:

First, precompute the total elements per channel: total = depth * height * width.

Each thread can compute a partial sum. Let's have a grid of blocks where each block handles a batch and channel.

Each block has threads processing a chunk of the spatial dimensions.

Alternatively, launch a kernel where each thread processes one input element and contributes to the sum for its (b, c).

Here's an example:

```cpp
__global__ void global_avg_pool3d_forward_kernel(
    const float* input, float* output,
    int batch, int channels, int depth, int height, int width,
    int total_elements) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels) return;

    // Each thread handles a (batch, channel) pair
    int b = idx / channels;
    int c = idx % channels;

    float sum = 0.0f;
    for (int d = 0; d < depth; ++d) {
        for (int h = 0; h < height; ++h) {
            for (int w = 0; w < width; ++w) {
                int offset = ((b * channels + c) * depth + d) * height + h;
                offset *= width + w;
                // Wait, need to get the correct indexing. Let's use the strides.
                // Assuming input is stored as (batch, channels, depth, height, width)
                int input_idx = b * channels * depth * height * width +
                                c * depth * height * width +
                                d * height * width +
                                h * width +
                                w;
                float val = input[input_idx];
                sum += val;
            }
        }
    }
    output[b * channels + c] = sum / total_elements;
}
```

But this has each thread looping over all spatial elements, which is O(DHW) per thread. For DHW=16*32*32=16384, this would be 16k iterations per thread, which is not efficient. This might be too slow due to high arithmetic intensity and loop overhead.

Alternative approach: Use a kernel where each thread processes a single element and writes to a shared memory array for the batch and channel sums, then reduce with atomicAdd.

Alternatively, use a grid where each thread block is responsible for a specific (batch, channel), and within the block, threads process a portion of the spatial dimensions.

Let me try this approach:

Each thread block handles one (b, c). The block has multiple threads.

The block size could be, say, 256 threads. Each thread in the block processes a portion of the spatial indices (d, h, w).

Compute the total spatial elements: DHW = D*H*W.

Each thread in the block processes DHW / blockDim.x elements.

Each thread accumulates their portion into a shared memory array, then perform a block reduction to get the total sum for (b,c).

This approach would be better because the work is split among threads.

Here's a sketch:

```cpp
__global__ void global_avg_pool3d_forward_kernel(
    const float* input, float* output,
    int batch, int channels, int depth, int height, int width) {

    __shared__ float shared_sums[256]; // assuming max block size 256

    int b = blockIdx.y;
    int c = blockIdx.x;

    int tid = threadIdx.x;
    int total_spatial = depth * height * width;

    float sum = 0.0f;
    for (int i = tid; i < total_spatial; i += blockDim.x) {
        int d = i / (height * width);
        int rem = i % (height * width);
        int h = rem / width;
        int w = rem % width;

        // Compute input index
        int idx = b * channels * depth * height * width +
                  c * depth * height * width +
                  d * height * width +
                  h * width +
                  w;
        sum += input[idx];
    }

    // Write to shared memory
    shared_sums[tid] = sum;
    __syncthreads();

    // Block reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sums[tid] += shared_sums[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total = shared_sums[0];
        output[b * channels + c] = total / (depth * height * width);
    }
}
```

This way, each block handles one (b,c), and the spatial elements are divided among threads in the block. The shared memory is used to accumulate partial sums, then a block reduction is performed. The final sum is written by the first thread in the block.

This should be more efficient as each thread only processes a portion of the spatial elements.

The kernel would be launched with:

gridDim.x = channels  
gridDim.y = batch  
blockDim.x = 256 (or a suitable number)

But need to make sure that the blockDim.x divides into total_spatial evenly, but the loop with i += blockDim.x handles that.

This approach reduces the number of iterations per thread and uses shared memory for partial sums, which should be faster.

So, implementing this kernel would be a good optimization for the global average pooling.

Next, the transposed convolution. Writing a custom ConvTranspose3d kernel is quite involved, but let's see if we can sketch it out.

The forward pass for transposed convolution involves:

Given input tensor X of shape (N, C_in, D_in, H_in, W_in), the output Y has shape (N, C_out, D_out, H_out, W_out), computed as:

Y[n, k, d, h, w] = sum_{c=0}^{C_in-1} sum_{p=0}^{kernel_depth-1} sum_{q=0}^{kernel_height-1} sum_{r=0}^{kernel_width-1} X[n, c, d' , h', w'] * weight[k, c, p, q, r]

Where d', h', w' are the input positions determined by the output positions and the stride, padding, etc.

The exact indices depend on the padding and stride. Since the user's model uses default padding (as kernel_size is given but no padding specified), we might need to assume that the output dimensions are computed accordingly.

Assuming stride=1 and padding such that output dimensions are as per PyTorch's default (padding = kernel_size // 2 for symmetric padding), but the user's kernel_size is 5, so padding would be 2 in each spatial dimension. However, since the user didn't specify padding, we need to check.

Wait, in the given model, the ConvTranspose3d is initialized with kernel_size, but the parameters for stride, padding, etc. are not specified. The user's example uses scale_factor=2.0, which is a scalar multiplier after convolution. The convolution's parameters (stride, padding) are not given in the Model's __init__ except for in_channels, out_channels, kernel_size. Thus, the convolution uses PyTorch's default parameters, which for ConvTranspose3d:

The default stride is (1, 1, 1), and padding is computed as (kernel_size - 1) // 2 for same padding if output_padding is zero. But without knowing the exact parameters, it's hard to proceed.

Alternatively, perhaps the model's ConvTranspose3d is using default stride 1 and padding, leading to output dimensions computed as:

D_out = (D_in - 1)*stride - 2*padding + kernel_size

But since stride is 1, padding is (kernel_size-1)/2 (assuming same padding), then D_out = D_in + (kernel_size-1) - 2*padding = D_in + kernel_size -1 - (kernel_size-1) = D_in. Wait, maybe the user's model isn't using padding, so the convolution parameters may vary. This uncertainty complicates writing a kernel.

Alternatively, maybe the key is to write a generic kernel that can handle any parameters, but that's more complex.

Given time constraints, perhaps the best path is to focus on fusing the scaling and batch norm into a single kernel, along with the global average pool.

Wait, let's think again: the batch norm's forward pass requires:

1. Compute mean over the spatial dimensions (for each channel and batch).
2. Compute variance over the spatial dimensions.
3. Normalize and scale.

The scaling by scale_factor is applied before batch norm, so the mean and variance are computed on the scaled data.

If we can fuse the scaling and batch norm into a single kernel, that would save memory and computation time.

Here's how:

- For each element in the input (after convolution), multiply by scale_factor, then accumulate into mean and variance computation.

The mean and variance can be computed on the scaled data, then the batch norm is applied.

Thus, the kernel would process the scaled data, compute mean and variance, then apply the batch norm formula.

This can be done in a single kernel pass over the data.

The steps in the kernel would be:

1. For each element in the input:

   a. Scale by scale_factor.

   b. Accumulate sum and sum of squares for each channel.

2. After all elements are processed, compute the mean and variance for each channel.

3. Apply the batch norm formula: (x_scaled - mean) / sqrt(var + eps) * gamma + beta.

However, steps 2 and 3 require global reductions (sum across all spatial elements for each channel), so this can't be done in a single kernel pass without using atomic operations or a two-step approach.

Perhaps split into two kernels:

1. First kernel: compute the scaled values and accumulate sums and squares per channel.

   - Use atomicAdd to accumulate the sums and sums of squares.

2. Second kernel: apply the batch norm using the computed mean and variance.

But this might not be faster than the PyTorch implementation, which is optimized.

Alternatively, use a shared memory approach for the first step within a block.

Suppose we have a grid of blocks where each block handles a channel. Each block processes a portion of the data for that channel, computes local sums and squares, then performs a block reduction to get the total.

This approach would require launching as many blocks as there are channels.

Let me outline the steps:

Kernel 1: Compute scaled values and accumulate sums and squares per channel.

Each thread block handles a specific channel (c). The block processes all spatial elements for that channel across all batches.

Wait, but batch normalization is computed per channel across all batches and spatial dimensions.

So for each channel c:

Total elements = batch_size * depth * height * width.

Each thread in the block processes a portion of these elements.

The kernel would:

- For each thread in the block, compute a portion of the scaled elements and accumulate the sum and sum of squares into shared memory.

Then, after reduction within the block, the block's sum and sum_sq can be stored in global memory.

This requires a kernel like:

```cpp
__global__ void compute_mean_var(
    const float* input,
    float* scaled_output,
    float* sum,
    float* sum_sq,
    int batch, int channels,
    int depth, int height, int width,
    float scale_factor) {

    int c = blockIdx.x; // each block handles a channel
    int tid = threadIdx.x;

    // Compute the total number of elements per channel
    int total = batch * depth * height * width;

    // Initialize shared memory for partial sums
    __shared__ float shared_sum[THREADS_PER_BLOCK];
    __shared__ float shared_sum_sq[THREADS_PER_BLOCK];

    shared_sum[tid] = 0.0f;
    shared_sum_sq[tid] = 0.0f;

    for (int i = tid; i < total; i += blockDim.x) {
        // Compute indices
        int b = i / (depth * height * width);
        int rem = i % (depth * height * width);
        int d = rem / (height * width);
        rem %= (height * width);
        int h = rem / width;
        int w = rem % width;

        int input_idx = b * channels * depth * height * width +
                        c * depth * height * width +
                        d * height * width +
                        h * width +
                        w;

        float val = input[input_idx] * scale_factor;
        scaled_output[input_idx] = val; // store scaled value

        shared_sum[tid] += val;
        shared_sum_sq[tid] += val * val;
    }

    __syncthreads();

    // Block reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
            shared_sum_sq[tid] += shared_sum_sq[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        sum[c] = shared_sum[0];
        sum_sq[c] = shared_sum_sq[0];
    }
}
```

After this kernel, we have for each channel c, the total sum and sum of squares.

Then, compute the mean and variance per channel:

mean[c] = sum[c] / total  
var[c] = (sum_sq[c] - sum[c]^2 / total) / (total - 1) // or total? Need to check batch norm implementation.

Assuming PyTorch uses the biased variance (divided by N):

var[c] = (sum_sq[c] - mean[c]^2 * total) / total

Wait, variance = E[x^2] - (E[x])^2

Thus:

mean = sum / total  
var = (sum_sq - mean * sum) / total

Wait, sum_sq is the sum of x^2, and sum is sum of x. Thus:

var = (sum_sq - (sum)^2 / total) / total ?

Wait, variance is (sum(x^2) - (sum x)^2 / N) / (N)

Wait:

Var = E[x^2] - (E[x])^2 = (sum(x^2)/N) - (sum(x)/N)^2  
= (sum(x^2) - (sum(x))^2 / N ) / N  
Wait no: (sum(x^2) - (sum(x))^2/N ) / N ?

Wait:

Var = (sum(x_i^2) - (sum(x_i))^2 / N) / N

Yes. So variance = [sum_sq - (sum^2)/total] / total

Thus, once sum and sum_sq are computed per channel, we can compute mean and var.

Then, the batch norm step can be applied.

The next kernel would apply the normalization:

y = (x_scaled - mean) / sqrt(var + eps) * gamma + beta

Assuming gamma and beta are per-channel parameters.

Thus, a second kernel would take the scaled output (already stored in scaled_output), and compute the normalized values.

```cpp
__global__ void apply_batch_norm(
    const float* scaled_output,
    float* out,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    int batch, int channels,
    int depth, int height, int width,
    float eps) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels * depth * height * width) return;

    // Compute indices
    int c = (idx / (depth * height * width)) % channels;
    int b = idx / (channels * depth * height * width);
    int rem = idx % (channels * depth * height * width);
    rem %= (depth * height * width);
    int d = rem / (height * width);
    rem %= (height * width);
    int h = rem / width;
    int w = rem % width;

    float x_scaled = scaled_output[idx];
    float m = mean[c];
    float v = var[c];
    float inv_std = 1.0f / sqrtf(v + eps);
    float gamma_val = gamma[c];
    float beta_val = beta[c];

    out[idx] = (x_scaled - m) * inv_std * gamma_val + beta_val;
}
```

This approach would combine the scaling and batch norm into two custom kernels, which could save some overhead compared to separate PyTorch operations.

However, this requires managing the intermediate scaled_output tensor, which consumes memory, but it's better than multiple separate operations.

Now, putting this all together with the ConvTranspose3d.

Alternatively, the most impactful optimization is to replace the ConvTranspose3d with a custom kernel. Given that it's the most compute-intensive step, this could yield the biggest speedup.

But writing a custom ConvTranspose3d kernel is quite involved. Let's outline the steps.

The transposed convolution (deconvolution) can be implemented by:

For each output position (n, k, d, h, w):

Y[n][k][d][h][w] = sum_{c=0}^{C_in-1} sum_{p=0}^{kernel_depth-1} sum_{q=0}^{kernel_height-1} sum_{r=0}^{kernel_width-1} X[n][c][d'][h'][w'] * W[k][c][p][q][r]

Where the input position (d', h', w') is determined by the output position and the stride, padding, etc.

Assuming stride=1 and padding such that output dimensions are correctly computed.

The kernel needs to efficiently loop over the input and kernel dimensions.

Given the complexity, perhaps it's best to use PyTorch's implementation for the convolution and focus on optimizing the other steps.

Alternatively, use a fused kernel that combines the convolution with scaling and batch norm.

However, given time constraints and the need to produce working code, perhaps the best approach is to implement the global average pool and the fused scaling + batch norm.

So the optimized ModelNew would replace the global_avg_pool with a custom kernel and combine the scaling and batch norm steps into two custom CUDA kernels.

Let me structure the code accordingly.

First, define the custom global average pool kernel.

Then, define the fused scaling and batch norm steps with custom kernels.

Additionally, the model's parameters for batch norm (gamma, beta, running_mean, running_var) need to be accessible. Since the original model uses nn.BatchNorm3d, these parameters are stored in the module. Thus, in the custom kernel, we need to pass gamma, beta, and the computed mean and var.

Putting this all together:

The code will have several CUDA kernels:

1. element-wise scaling and sum for batch norm mean/var computation.

2. apply batch norm.

3. global average pool.

The ModelNew class will replace the relevant steps with these kernels.

Now, writing the code step by step.

First, the global_avg_pool custom kernel.

The kernel is as follows:

```cpp
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void global_avg_pool3d_forward(
    const float* input,
    float* output,
    int batch, int channels,
    int depth, int height, int width) {

    __shared__ float shared_sums[256]; // Assuming block size <=256

    int b = blockIdx.y;
    int c = blockIdx.x;
    int tid = threadIdx.x;

    int total_spatial = depth * height * width;
    float sum = 0.0f;

    for (int i = tid; i < total_spatial; i += blockDim.x) {
        int d = i / (height * width);
        int rem = i % (height * width);
        int h = rem / width;
        int w = rem % width;

        int idx = b * channels * depth * height * width +
                  c * depth * height * width +
                  d * height * width +
                  h * width +
                  w;

        sum += input[idx];
    }

    shared_sums[tid] = sum;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sums[tid] += shared_sums[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        float total = shared_sums[0];
        output[b * channels + c] = total / (depth * height * width);
    }
}

torch::Tensor global_avg_pool3d_cuda(torch::Tensor input) {
    auto batch = input.size(0);
    auto channels = input.size(1);
    auto depth = input.size(2);
    auto height = input.size(3);
    auto width = input.size(4);

    auto output_size = {batch, channels};
    auto output = torch::zeros(output_size, input.options());

    dim3 block(256);
    dim3 grid(channels, batch);

    global_avg_pool3d_forward<<<grid, block>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch, channels, depth, height, width);

    return output.view({batch, channels, 1, 1, 1});
}
```

Note: The output is reshaped to match the expected dimensions (1,1,1).

Next, the scaling and batch norm kernels.

First, the kernel to compute mean and var:

```cpp
__global__ void compute_mean_var(
    const float* input,
    float* scaled,
    float* sum,
    float* sum_sq,
    int batch, int channels,
    int depth, int height, int width,
    float scale_factor) {

    int c = blockIdx.x;
    int tid = threadIdx.x;

    int total = batch * depth * height * width;
    __shared__ float shared_sum[256];
    __shared__ float shared_sum_sq[256];

    shared_sum[tid] = 0.0f;
    shared_sum_sq[tid] = 0.0f;

    for (int i = tid; i < total; i += blockDim.x) {
        int b = i / (depth * height * width);
        int rem = i % (depth * height * width);
        int d = rem / (height * width);
        rem %= (height * width);
        int h = rem / width;
        int w = rem % width;

        int in_idx = b * channels * depth * height * width +
                     c * depth * height * width +
                     d * height * width +
                     h * width +
                     w;

        float val = input[in_idx] * scale_factor;
        scaled[in_idx] = val;

        shared_sum[tid] += val;
        shared_sum_sq[tid] += val * val;
    }

    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
            shared_sum_sq[tid] += shared_sum_sq[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        sum[c] = shared_sum[0];
        sum_sq[c] = shared_sum_sq[0];
    }
}

// Then apply batch norm
__global__ void apply_batch_norm(
    const float* scaled,
    float* out,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    int batch, int channels,
    int depth, int height, int width,
    float eps) {

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch * channels * depth * height * width) return;

    int c = (idx / (depth * height * width)) % channels;
    int b = idx / (channels * depth * height * width);
    int rem = idx % (channels * depth * height * width);
    rem %= (depth * height * width);
    int d = rem / (height * width);
    rem %= (height * width);
    int h = rem / width;
    int w = rem % width;

    float x_scaled = scaled[idx];
    float m = mean[c];
    float v = var[c];
    float inv_std = 1.0f / sqrtf(v + eps);
    float gamma_val = gamma[c];
    float beta_val = beta[c];

    out[idx] = (x_scaled - m) * inv_std * gamma_val + beta_val;
}

// Define the functions in C++
torch::Tensor scaled_batch_norm_cuda(
    torch::Tensor input,
    float scale_factor,
    torch::Tensor batch_norm_gamma,
    torch::Tensor batch_norm_beta,
    torch::Tensor batch_norm_running_mean,
    torch::Tensor batch_norm_running_var,
    float eps) {

    auto batch = input.size(0);
    auto channels = input.size(1);
    auto depth = input.size(2);
    auto height = input.size(3);
    auto width = input.size(4);

    // Allocate tensors for intermediate results
    auto scaled = torch::empty_like(input);
    auto sum = torch::zeros({channels}, input.options());
    auto sum_sq = torch::zeros({channels}, input.options());

    dim3 block(256);
    dim3 grid(channels); // each block handles a channel

    compute_mean_var<<<grid, block>>>(
        input.data_ptr<float>(),
        scaled.data_ptr<float>(),
        sum.data_ptr<float>(),
        sum_sq.data_ptr<float>(),
        batch, channels, depth, height, width,
        scale_factor);

    // Compute mean and var on CPU (since these are per-channel and small)
    auto mean = sum / (batch * depth * height * width);
    auto var = (sum_sq - sum.pow(2) / (batch * depth * height * width)) / (batch * depth * height * width);

    // Prepare mean and var as tensors on GPU
    auto mean_gpu = mean.cuda();
    auto var_gpu = var.cuda();

    // Launch apply batch norm kernel
    auto total_elements = batch * channels * depth * height * width;
    dim3 grid2((total_elements + block.x - 1) / block.x);
    apply_batch_norm<<<grid2, block>>>(
        scaled.data_ptr<float>(),
        scaled.data_ptr<float>(), // output overwrites scaled
        mean_gpu.data_ptr<float>(),
        var_gpu.data_ptr<float>(),
        batch_norm_gamma.data_ptr<float>(),
        batch_norm_beta.data_ptr<float>(),
        batch, channels, depth, height, width,
        eps);

    return scaled;
}
```

Wait, but the batch norm's parameters (gamma and beta) are part of the model. In the original model, they are stored in the batch_norm