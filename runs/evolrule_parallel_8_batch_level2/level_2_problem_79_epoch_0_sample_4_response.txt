I need you to write the entire ModelNew class and any other necessary code with the custom CUDA kernels, and the get_init_inputs and get_inputs functions. 

The new architecture must have the same input and output dimensions as the original. The new architecture must also have the same initialization parameters. 

If the original architecture has parameters, the new architecture must have parameters that can be trained in the same way (i.e. the same shape and type). 

Your code must import all the necessary libraries. The code should be self-contained and ready to run. 

Your code must use torch.utils.cpp_extension.load_inline to define the CUDA kernels. 

You can replace any operators with custom CUDA kernels, but you are not required to replace all. You can also fuse multiple operators into a single kernel if possible. 

The goal is to optimize for speed. 

The code must be compatible with PyTorch >= 2.0.0. 

Please optimize the given architecture. 

Okay, I need to optimize the provided Model architecture using custom CUDA kernels. Let me start by understanding the original code.

The original Model has a series of operations: Conv3d, multiplication by a parameter, InstanceNorm3d, clamp, another multiplication, and a max operation. The goal is to speed this up by replacing some operators with custom CUDA kernels.

First, I should look for operations that can be fused or optimized. The multiplication by the multiplier and the clamping could be combined into a single kernel, as well as the second multiplication. Maybe even the InstanceNorm3d could be fused with some of these steps.

Let me think about each step:

1. Conv3d: This is a standard layer. Since Conv3d is already optimized in PyTorch, maybe it's best to leave it as is unless there's a way to combine it with subsequent steps. But combining convolution with element-wise operations might be complex. So perhaps leave Conv3d untouched.

2. Multiply by self.multiplier: This is an element-wise multiplication. The multiplier has shape (out_channels, 1, 1, 1), so it's applied across spatial dimensions for each channel. This is a simple element-wise op, but doing it in a custom kernel might save overhead.

3. InstanceNorm3d: Instance normalization applies normalization across each channel independently. The computation involves mean and variance calculation for each instance and channel. This might be a good candidate for fusion with the next steps if possible, but it's a bit involved. Alternatively, perhaps the multiplication and clamp can be done before or after the normalization.

4. Clamp between min and max: This is another element-wise operation. Combining this with the multiplication steps could be beneficial.

5. Second multiplication by self.multiplier: Same as step 2, so perhaps both multiplications can be fused with other steps.

6. Max over dimension 1: This is a reduction operation. Maybe the max can be combined with earlier steps if possible, but that might require rethinking the computation.

Fusion Strategy:

The steps 2, 3, 4, 5 are all element-wise or per-channel operations. Let's see:

After Conv3d, the output is x. Then:

x = x * self.multiplier (element-wise, channel-wise scaling)

x = instance_norm(x) (normalization)

x = clamp(x, min, max)

x = x * self.multiplier (another scaling)

Perhaps the two multiplications and the clamp can be combined into a single kernel. The instance norm is a bit more complex, but maybe if we can compute the normalization along with the scaling and clamping in a fused kernel.

Alternatively, perhaps the first multiplication and the first clamp can be done in a single kernel, then instance norm, then the second multiplication and clamp again? But clamping is applied after the instance norm. Hmm.

Wait, the original steps are:

x = self.conv(x)

x = x * self.multiplier

x = self.instance_norm(x)

x = torch.clamp(x, self.clamp_min, self.clamp_max)

x = x * self.multiplier

x = torch.max(x, dim=1)[0]

So after the first multiplication, then instance norm, then clamp, then multiply again. So the instance norm is between the first multiplication and the clamp.

Hmm, so maybe the first multiplication is before instance norm. Let's see:

The first multiply by multiplier is element-wise, so it's easy to do in a kernel. Then instance norm. But instance norm requires calculating mean and variance, so that can't be easily fused into an element-wise kernel.

The clamp is next, which is element-wise. Then another multiply by the same multiplier. The final step is max over dimension 1.

Perhaps the first multiplication can be fused with the second multiplication and the clamp. Let's see:

Wait the second multiply is after the clamp. So the sequence is:

conv -> multiply1 -> instance norm -> clamp -> multiply2 -> max.

Hmm, so between multiply1 and multiply2 there's instance norm and clamp.

It might be challenging to fuse the instance norm into the other steps. Maybe the instance norm is a separate step but can be optimized, but I don't think so. Let me think of which parts can be fused.

The first multiply (multiply1) can be done in a kernel. The instance norm requires per-channel mean and variance. The clamp is element-wise. The second multiply (multiply2) and the clamp can be done in a single kernel.

Alternatively, perhaps the clamp and second multiply can be fused. Let's see:

Original steps after instance norm:

x = torch.clamp(x, min, max)

x = x * self.multiplier

These two can be done in a single element-wise kernel, since clamp and multiply are both per-element operations. So combining those into one kernel would save some memory transfers and overhead.

So maybe:

- Create a kernel that does element-wise multiply (multiply1) after convolution.

- The instance norm remains as is, since it's a standard layer.

- Create a fused kernel for clamp and multiply2.

- The max operation could also be replaced with a custom kernel, but it's a reduction, which might be already optimized.

Alternatively, the max over dimension 1 can be done with a custom kernel if needed, but I'm not sure if that's worth it. Let me check the PyTorch documentation. The max function has a CUDA implementation, so perhaps it's already optimized. Maybe focusing on the element-wise operations would give better returns.

So the plan is to replace the first multiply, the clamp and second multiply with custom kernels.

First, the first multiply (multiply1):

This is x * self.multiplier, where multiplier has shape (out_channels, 1, 1, 1). The multiplication is channel-wise, so for each element, it's multiplied by the corresponding channel's multiplier value.

This can be implemented in a CUDA kernel. Let's see:

The input is a tensor of shape (batch, channels, depth, height, width). The multiplier is (channels, 1, 1, 1). So for each element (b, c, d, h, w), the value is input[b][c][d][h][w] * multiplier[c][0][0][0].

To implement this in CUDA, we can loop over all elements, and for each, multiply by the channel's multiplier value.

Similarly, the second step is clamp followed by multiply2. The clamp is between clamp_min and clamp_max, and then multiplied by the same multiplier. So the fused kernel would take an input tensor, clamp each element between min and max, then multiply by the multiplier (again channel-wise). Wait, the multiplier in the second step is the same as the first? Yes, according to the original code.

Wait, the first multiplication is by self.multiplier, and the second is also by self.multiplier. So in the second multiply, same multiplier. So the combined operation after instance norm is: clamp then multiply by self.multiplier again.

Therefore, the second multiply can also be done in a kernel, but perhaps fused with the clamp.

Thus, let's create two custom kernels:

1. First multiply (after conv): element-wise channel-wise multiplication.

2. Clamp and multiply (after instance norm and clamp): element-wise operations.

Additionally, perhaps the max operation can be replaced with a custom kernel for speed. Let me think.

The max over dimension 1 (the channel dimension) for each spatial position. The input to max is (batch, channels, depth, height, width), and the output is (batch, depth, height, width). The max over channels.

The standard PyTorch implementation might already be optimized, but if we can write a kernel that does this efficiently, maybe it can be faster. Let's see. The CUDA kernel would need to process each spatial element (d, h, w) for each batch, and find the max across channels. The implementation could be done by looping over the batch, then each spatial position, then compute max over the channels. However, this might not be as efficient as the existing implementation, but let's try.

Alternatively, perhaps the max can be left as is since it's a simple reduction.

So proceeding with replacing the first multiply and the clamp+second multiply with custom kernels.

Now, writing the CUDA code:

First, the first multiplication (multiply1):

CUDA kernel for element-wise multiplication with a channel-wise multiplier:

The multiplier is a tensor of shape (C, 1, 1, 1). The input is (B, C, D, H, W). The output is (B, C, D, H, W).

The kernel would need to access the multiplier for each channel. Since the multiplier is 1D (only varying with channel), we can store it in a 1D array and for each element, multiply by the multiplier[c].

CUDA code:

__global__ void channelwise_mult(const float* input, const float* multiplier, float* output, int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*D*H*W) return;
    
    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int c = (idx / (W*H*D)) % C;
    int b = (idx / (W*H*D*C)); // since B is the first dimension
    
    float val = input[b * C*D*H*W + c*D*H*W + d*H*W + h*W + w];
    float m = multiplier[c]; // since multiplier is (C, 1, 1, 1), so multiplier[c] is correct
    output[idx] = val * m;
}

Wait, but in CUDA, the indexing is done in flattened arrays. The input is stored in a contiguous array, so the index can be calculated as:

Each element can be represented as:

index = b * (C*D*H*W) + c * (D*H*W) + d * (H*W) + h * W + w

But in the flattened array, the elements are stored in the order of B first, then C, D, H, W. So the calculation above is correct.

Alternatively, using a flattened index, perhaps it's better to compute it as:

The total number of elements is B*C*D*H*W. The index is the linear index. So for each index, the coordinates can be computed as follows:

w = idx % W

h = (idx / W) % H

d = (idx / (W*H)) % D

c = (idx / (W*H*D)) % C

b = (idx / (W*H*D*C))

This should work. Then the multiplier is stored as a 1D array of size C, so multiplier[c] is the correct value for channel c.

The kernel would take pointers to input, multiplier, and output.

The multiplier is a parameter, so in the PyTorch module, it's a tensor that we need to pass. Since the kernel is part of the module, the multiplier is a parameter, so we need to pass its data pointer.

Now, the function to call this kernel would be:

torch::Tensor channelwise_mult_cuda(torch::Tensor input, torch::Tensor multiplier) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);
    
    auto output = torch::empty_like(input);
    
    const int threads = 256;
    int elements = B*C*D*H*W;
    int blocks = (elements + threads -1)/threads;
    
    channelwise_mult<<<blocks, threads>>>(input.data_ptr<float>(), multiplier.data_ptr<float>(), output.data_ptr<float>(), B, C, D, H, W);
    
    return output;
}

Wait, but the multiplier is a tensor of shape (C,1,1,1), but in the kernel, we treat it as a 1D array of size C. So when passing it to the kernel, we need to make sure that the multiplier is contiguous and has the right shape. Since the multiplier is a parameter, it should be stored in a way that allows this. Alternatively, we can reshape it to 1D in the kernel.

Alternatively, in the kernel, since the multiplier is (C,1,1,1), the data pointer can be treated as a 1D array of length C. So the code is okay.

Now, the second fused kernel: clamp and multiply.

After instance norm, we have x. Then:

x = torch.clamp(x, min, max) --> clamp between clamp_min and clamp_max

x = x * self.multiplier --> again the same multiplier as before.

Wait, but in the original code, the second multiplication is by the same multiplier as the first. So this is the same multiplier tensor.

So the fused kernel would take an input, clamp each element between min and max, then multiply by the channel's multiplier value.

The kernel code would be similar to the previous one, but with clamping:

__global__ void clamp_mult(const float* input, const float* multiplier, float* output, float min_val, float max_val, int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*D*H*W) return;
    
    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int c = (idx / (W*H*D)) % C;
    int b = (idx / (W*H*D*C));
    
    float val = input[b * C*D*H*W + c*D*H*W + d*H*W + h*W + w];
    val = val < min_val ? min_val : (val > max_val ? max_val : val);
    val *= multiplier[c]; // same as before
    
    output[idx] = val;
}

The function would be:

torch::Tensor clamp_mult_cuda(torch::Tensor input, torch::Tensor multiplier, float min, float max) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);
    
    auto output = torch::empty_like(input);
    
    const int threads = 256;
    int elements = B*C*D*H*W;
    int blocks = (elements + threads -1)/threads;
    
    clamp_mult<<<blocks, threads>>>(input.data_ptr<float>(), multiplier.data_ptr<float>(), output.data_ptr<float>(), min, max, B, C, D, H, W);
    
    return output;
}

Now, the max operation: torch.max(x, dim=1)[0]. The output is a tensor of shape (batch, D, H, W).

To implement this in CUDA, we can have a kernel that for each batch and spatial position, computes the max over the channels.

Alternatively, PyTorch's implementation might already be efficient. But let's see.

The CUDA kernel for max over dimension 1:

The input is (B, C, D, H, W). The output is (B, D, H, W).

For each element in the output (b, d, h, w), the value is the max over c in 0..C-1 of input[b][c][d][h][w].

To compute this, the kernel can process each output element, and for each, iterate over the channels to find the maximum.

However, this might be inefficient for large C (since it's a loop in the kernel). Alternatively, we can use a block-wise reduction or other optimized methods, but that's more complex.

Alternatively, let's see if we can write a simple kernel:

__global__ void max_dim1(const float* input, float* output, int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*D*H*W) return;
    
    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int b = (idx / (W*H*D));
    
    float max_val = -INFINITY;
    for (int c = 0; c < C; ++c) {
        float val = input[b*C*D*H*W + c*D*H*W + d*H*W + h*W + w];
        if (val > max_val) max_val = val;
    }
    output[idx] = max_val;
}

This is straightforward but may be slow for large C (e.g., if out_channels is 16, it's manageable).

The function would be:

torch::Tensor max_dim1_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);
    
    auto output = torch::empty({B, D, H, W}, input.options());
    
    const int threads = 256;
    int elements = B*D*H*W;
    int blocks = (elements + threads -1)/threads;
    
    max_dim1<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), B, C, D, H, W);
    
    return output;
}

But perhaps this is not faster than the native PyTorch implementation, so maybe leave the max as is. Let me think. The PyTorch max function is implemented in C++ and likely optimized. So maybe it's better to leave it. However, the user's instruction says to replace operators with custom CUDA kernels where possible for speedup. Let's see. If the max operation is a significant part of the computation time, then replacing it could help. But with C=16, the loop over channels might not be too bad. Let me proceed with replacing it as well for the sake of optimization, but note that it might not be necessary.

Putting it all together:

The ModelNew will replace the first multiplication (after conv) with the custom channelwise_mult kernel, the clamp and second multiplication with the clamp_mult kernel, and possibly the max with the custom kernel.

Alternatively, maybe the max can be left as is. Let's check the original code's forward:

Original forward steps:

x = self.conv(x)

x = x * self.multiplier (custom kernel1)

x = self.instance_norm(x) (PyTorch's InstanceNorm3d)

x = torch.clamp(x, self.clamp_min, self.clamp_max)

x = x * self.multiplier (custom kernel2 fused with clamp?)

Wait, in the original code, the clamp is between instance norm and the second multiply. So the sequence is:

After instance norm, we do the clamp, then the multiply. Hence, the fused kernel for clamp and multiply must first clamp the value, then multiply.

Yes, that's what the clamp_mult kernel does.

So the plan is:

- Replace the first multiplication (after conv) with channelwise_mult.

- Replace the clamp and second multiply with clamp_mult.

- The instance norm remains as is.

- The max operation remains as torch.max, or replaced with a custom kernel.

Now, the ModelNew class needs to have the same parameters. The original model has a multiplier parameter of shape multiplier_shape (out_channels, 1, 1, 1). The new model must have this parameter as well. Also, the instance norm is a module, so it's part of the state.

Therefore, in ModelNew, we'll keep the conv layer, instance norm, and multiplier parameter, just like in the original.

The custom CUDA kernels need to be compiled using load_inline, and their functions need to be called in the forward pass.

Let me structure the code:

First, define the CUDA kernels for channelwise_mult, clamp_mult, and possibly max_dim1.

Then, in the ModelNew class:

class ModelNew(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))
        self.instance_norm = nn.InstanceNorm3d(out_channels)
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

        # Load the CUDA kernels
        self.channelwise_mult = load_inline... 
        self.clamp_mult = load_inline...
        # self.max_dim1 = ... if using

    def forward(self, x):
        x = self.conv(x)
        x = self.channelwise_mult.channelwise_mult_cuda(x, self.multiplier)
        x = self.instance_norm(x)
        x = self.clamp_mult.clamp_mult_cuda(x, self.multiplier, self.clamp_min, self.clamp_max)
        x = torch.max(x, dim=1)[0]
        return x

Wait, but the clamp_mult_cuda requires the min and max values as floats. So in the function call, we have to pass self.clamp_min and self.clamp_max.

Now, writing the CUDA source code for each kernel.

First, the channelwise_mult kernel:

elementwise_mult_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void channelwise_mult(const float* input, const float* multiplier, float* output, 
                                int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*D*H*W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int c = (idx / (W*H*D)) % C;
    int b = idx / (W*H*D*C);

    float val = input[b * C*D*H*W + c*D*H*W + d*H*W + h*W + w];
    float m = multiplier[c];
    output[idx] = val * m;
}

torch::Tensor channelwise_mult_cuda(torch::Tensor input, torch::Tensor multiplier) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty_like(input);
    
    const int threads = 256;
    int elements = B*C*D*H*W;
    int blocks = (elements + threads - 1) / threads;
    
    channelwise_mult<<<blocks, threads>>>(input.data_ptr<float>(), 
                                         multiplier.data_ptr<float>(),
                                         output.data_ptr<float>(),
                                         B, C, D, H, W);
    
    return output;
}
"""

Then the header for this kernel:

elementwise_mult_cpp = "torch::Tensor channelwise_mult_cuda(torch::Tensor input, torch::Tensor multiplier);"

Similarly for the clamp_mult:

clamp_mult_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void clamp_mult(const float* input, const float* multiplier, float* output,
                          float min_val, float max_val, int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*D*H*W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int c = (idx / (W*H*D)) % C;
    int b = idx / (W*H*D*C);

    float val = input[b * C*D*H*W + c*D*H*W + d*H*W + h*W + w];
    val = val < min_val ? min_val : (val > max_val ? max_val : val);
    val *= multiplier[c];
    output[idx] = val;
}

torch::Tensor clamp_mult_cuda(torch::Tensor input, torch::Tensor multiplier,
                             float min_val, float max_val) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty_like(input);
    
    const int threads = 256;
    int elements = B*C*D*H*W;
    int blocks = (elements + threads - 1) / threads;
    
    clamp_mult<<<blocks, threads>>>(input.data_ptr<float>(), 
                                   multiplier.data_ptr<float>(),
                                   output.data_ptr<float>(),
                                   min_val, max_val, 
                                   B, C, D, H, W);
    
    return output;
}
"""

cpp for clamp_mult:

clamp_mult_cpp = "torch::Tensor clamp_mult_cuda(torch::Tensor input, torch::Tensor multiplier, float min_val, float max_val);"

The max_dim1 kernel:

max_dim1_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_dim1(const float* input, float* output, int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*D*H*W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int b = idx / (W*H*D);

    float max_val = -FLT_MAX;
    for (int c = 0; c < C; ++c) {
        int in_idx = b*C*D*H*W + c*D*H*W + d*H*W + h*W + w;
        float val = input[in_idx];
        if (val > max_val) {
            max_val = val;
        }
    }
    output[idx] = max_val;
}

torch::Tensor max_dim1_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty({B, D, H, W}, input.options());
    
    const int threads = 256;
    int elements = B*D*H*W;
    int blocks = (elements + threads - 1) / threads;
    
    max_dim1<<<blocks, threads>>>(input.data_ptr<float>(), 
                                 output.data_ptr<float>(),
                                 B, C, D, H, W);
    
    return output;
}
"""

cpp for max_dim1:

max_dim1_cpp = "torch::Tensor max_dim1_cuda(torch::Tensor input);"

Now, in the ModelNew class, we need to load each of these kernels.

But since the user wants to use load_inline, we can combine all kernels into a single module? Or load each separately. However, to avoid too many separate modules, perhaps better to combine into a single module.

Alternatively, load each kernel separately. Let's see:

Wait, the problem is that each kernel needs to be compiled as a separate module? Or can we have multiple functions in a single CUDA source?

Yes, we can have multiple functions in one CUDA source. Let's try to put all three kernels (channelwise_mult, clamp_mult, and max_dim1) into a single source code. But since the user's example had a separate module for each, maybe it's better to separate them. Alternatively, we can have one CPP file with all functions.

Alternatively, since the kernels are separate, perhaps better to separate them for clarity. But to minimize the code, maybe group them into a single source.

Wait, the user's example had one kernel, but in this case, we have three different kernels. Let me think: the code must be self-contained.

Alternatively, create three separate inline modules:

First, channelwise_mult:

elementwise_mult = load_inline(name="elementwise_mult", cuda_sources=elementwise_mult_source, cpp_sources=elementwise_mult_cpp, functions=["channelwise_mult_cuda"], ...)

Similarly for clamp_mult and max_dim1.

Thus, in the code:

elementwise_mult_source = "..."

elementwise_mult_cpp = "..."

elementwise_mult = load_inline(...)

clamp_mult_source = "..."

clamp_mult_cpp = "..."

clamp_mult = load_inline(...)

max_dim1_source = "..."

max_dim1_cpp = "..."

max_dim1 = load_inline(...)

Then, in the ModelNew forward:

x = self.conv(x)

x = elementwise_mult.channelwise_mult_cuda(x, self.multiplier)

x = self.instance_norm(x)

x = clamp_mult.clamp_mult_cuda(x, self.multiplier, self.clamp_min, self.clamp_max)

x = max_dim1.max_dim1_cuda(x)  # if using custom max

or x = torch.max(x, dim=1)[0]

But the problem is that the max kernel requires that the output tensor has shape (B,D,H,W). The torch.max returns that.

Now, let's proceed to write the code.

Putting all together:

The code for the ModelNew would be:

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the CUDA kernels

# Kernel 1: Channel-wise multiplication (after conv)
elementwise_mult_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void channelwise_mult(const float* input, const float* multiplier, float* output, 
                                int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*D*H*W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int c = (idx / (W*H*D)) % C;
    int b = idx / (W*H*D*C);

    float val = input[b * C*D*H*W + c*D*H*W + d*H*W + h*W + w];
    float m = multiplier[c];
    output[idx] = val * m;
}

torch::Tensor channelwise_mult_cuda(torch::Tensor input, torch::Tensor multiplier) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty_like(input);
    
    const int threads = 256;
    int elements = B*C*D*H*W;
    int blocks = (elements + threads - 1) / threads;
    
    channelwise_mult<<<blocks, threads>>>(input.data_ptr<float>(), 
                                         multiplier.data_ptr<float>(),
                                         output.data_ptr<float>(),
                                         B, C, D, H, W);
    
    return output;
}
"""

elementwise_mult_cpp = "torch::Tensor channelwise_mult_cuda(torch::Tensor input, torch::Tensor multiplier);"

elementwise_mult = load_inline(
    name="elementwise_mult",
    cuda_sources=elementwise_mult_source,
    cpp_sources=elementwise_mult_cpp,
    functions=["channelwise_mult_cuda"],
    verbose=True,
)

# Kernel 2: Clamp and multiply (after instance norm)
clamp_mult_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void clamp_mult(const float* input, const float* multiplier, float* output,
                          float min_val, float max_val, int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*C*D*H*W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int c = (idx / (W*H*D)) % C;
    int b = idx / (W*H*D*C);

    float val = input[b * C*D*H*W + c*D*H*W + d*H*W + h*W + w];
    val = val < min_val ? min_val : (val > max_val ? max_val : val);
    val *= multiplier[c];
    output[idx] = val;
}

torch::Tensor clamp_mult_cuda(torch::Tensor input, torch::Tensor multiplier,
                             float min_val, float max_val) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty_like(input);
    
    const int threads = 256;
    int elements = B*C*D*H*W;
    int blocks = (elements + threads - 1) / threads;
    
    clamp_mult<<<blocks, threads>>>(input.data_ptr<float>(), 
                                   multiplier.data_ptr<float>(),
                                   output.data_ptr<float>(),
                                   min_val, max_val, 
                                   B, C, D, H, W);
    
    return output;
}
"""

clamp_mult_cpp = "torch::Tensor clamp_mult_cuda(torch::Tensor input, torch::Tensor multiplier, float min_val, float max_val);"

clamp_mult = load_inline(
    name="clamp_mult",
    cuda_sources=clamp_mult_source,
    cpp_sources=clamp_mult_cpp,
    functions=["clamp_mult_cuda"],
    verbose=True,
)

# Kernel 3: Max over dimension 1 (optional)
max_dim1_source = """
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void max_dim1(const float* input, float* output, int B, int C, int D, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= B*D*H*W) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int d = (idx / (W*H)) % D;
    int b = idx / (W*H*D);

    float max_val = -FLT_MAX;
    for (int c = 0; c < C; ++c) {
        int in_idx = b*C*D*H*W + c*D*H*W + d*H*W + h*W + w;
        float val = input[in_idx];
        if (val > max_val) {
            max_val = val;
        }
    }
    output[idx] = max_val;
}

torch::Tensor max_dim1_cuda(torch::Tensor input) {
    int B = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto output = torch::empty({B, D, H, W}, input.options());
    
    const int threads = 256;
    int elements = B*D*H*W;
    int blocks = (elements + threads - 1) / threads;
    
    max_dim1<<<blocks, threads>>>(input.data_ptr<float>(), 
                                 output.data_ptr<float>(),
                                 B, C, D, H, W);
    
    return output;
}
"""

max_dim1_cpp = "torch::Tensor max_dim1_cuda(torch::Tensor input);"

max_dim1 = load_inline(
    name="max_dim1",
    cuda_sources=max_dim1_source,
    cpp_sources=max_dim1_cpp,
    functions=["max_dim1_cuda"],
    verbose=True,
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.multiplier = nn.Parameter(torch.randn(multiplier_shape))
        self.instance_norm = nn.InstanceNorm3d(out_channels)
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

        # Load the CUDA kernels
        # They are already loaded above, so just keep references
        self.channelwise_mult = elementwise_mult
        self.clamp_mult = clamp_mult
        self.max_dim1 = max_dim1

    def forward(self, x):
        x = self.conv(x)
        x = self.channelwise_mult.channelwise_mult_cuda(x, self.multiplier)
        x = self.instance_norm(x)
        x = self.clamp_mult.clamp_mult_cuda(x, self.multiplier, self.clamp_min, self.clamp_max)
        # x = self.max_dim1.max_dim1_cuda(x)
        x = torch.max(x, dim=1)[0]
        return x

Wait, but in the code above, the kernels are loaded outside the class. Since load_inline is called at the top level, the modules are created once