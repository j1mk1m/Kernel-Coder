api_query_interval: 0.0
base_traceset_path: /data/user_data/gyeongwk/flashinfer-trace
benchmark: KernelBench
build_cache_with_cpu: false
build_dir: /data/user_data/gyeongwk/Kernel-Coder/cache
dataset_name: ScalingIntelligence/KernelBench
dataset_src: local
gpu_arch: Ampere
hardware: A6000_babel
language: CUDA
level: 2
max_tokens: 16384
measure_performance: true
method: best-of-N
model_name: hosted_vllm/Qwen/Qwen2.5-Coder-7B-Instruct
num_best: 1
num_correct_trials: 5
num_cpu_workers: 16
num_eval_devices: 1
num_iterations: 1
num_parallel: 8
num_perf_trials: 100
num_samples: 1
num_workers: 1
prompt: regular
run_name: test_time_scaling_best_of_n_Qwen2.5-Coder-7B-Instruct_level2
server_type: vllm
target_gpu: H100
temperature: 0.7
test: false
timeout: 300
verbose: false
vllm_host: babel-s9-24
vllm_port: 8084
