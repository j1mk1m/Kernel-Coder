Please use the following template:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define your custom CUDA kernels here
custom_cuda_kernels_source = """
...
"""

custom_cuda_kernels_cpp_source = (
    ...
)

# Compile the inline CUDA code here
custom_cuda_kernels = load_inline(
    name="custom_cuda_kernels",
    cpp_sources=custom_cuda_kernels_cpp_source,
    cuda_sources=custom_cuda_kernels_source,
    functions=[...],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum)
        # Add any additional attributes here

    def forward(self, x):
        x = self.gemm(x)
        x = x * self.scale
        x = self.bn(x)
        return x

# Define any other helper functions or classes here
```

Ensure your solution is efficient and utilizes the power of CUDA for computation-intensive tasks. Consider replacing operations such as `nn.Linear` with custom GEMM (General Matrix Multiplication) kernels, and optimizing batch normalization using techniques like online statistics accumulation.

## Solution

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define custom CUDA kernels for GEMM and batch normalization
gemm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_kernel(const float* A, const float* B, float* C, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += A[row * K + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

void gemm_cuda(const torch::Tensor& A, const torch::Tensor& B, torch::Tensor& C) {
    auto M = A.size(0);
    auto N = B.size(1);
    auto K = A.size(1);

    dim3 threads_per_block(16, 16);
    dim3 blocks_per_grid((N + threads_per_block.x - 1) / threads_per_block.x, (M + threads_per_block.y - 1) / threads_per_block.y);

    gemm_kernel<<<blocks_per_grid, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, N, K);
}
"""

bn_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void bn_forward_kernel(const float* x, float* y, const float* mean, const float* var, const float* gamma, const float* beta, float epsilon, int N, int D) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N * D) {
        int d = idx % D;
        y[idx] = gamma[d] * (x[idx] - mean[d]) / sqrt(var[d] + epsilon) + beta[d];
    }
}

void bn_forward_cuda(const torch::Tensor& x, torch::Tensor& y, const torch::Tensor& mean, const torch::Tensor& var, const torch::Tensor& gamma, const torch::Tensor& beta, float epsilon) {
    auto N = x.size(0);
    auto D = x.size(1);

    dim3 threads_per_block(256);
    dim3 blocks_per_grid((N * D + threads_per_block.x - 1) / threads_per_block.x);

    bn_forward_kernel<<<blocks_per_grid, threads_per_block>>>(x.data_ptr<float>(), y.data_ptr<float>(), mean.data_ptr<float>(), var.data_ptr<float>(), gamma.data_ptr<float>(), beta.data_ptr<float>(), epsilon, N, D);
}
"""

custom_cuda_kernels_source = gemm_source + bn_source

custom_cuda_kernels_cpp_source = (
    "void gemm_cuda(const torch::Tensor& A, const torch::Tensor& B, torch::Tensor& C);"
    "void bn_forward_cuda(const torch::Tensor& x, torch::Tensor& y, const torch::Tensor& mean, const torch::Tensor& var, const torch::Tensor& gamma, const torch::Tensor& beta, float epsilon);"
)

# Compile the inline CUDA code
custom_cuda_kernels = load_inline(
    name="custom_cuda_kernels",
    cpp_sources=custom_cuda_kernels_cpp_source,
    cuda_sources=custom_cuda_kernels_source,
    functions=["gemm_cuda", "bn_forward_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.eps = eps
        self.momentum = momentum

        # Initialize running mean and variance for batch normalization
        self.register_buffer('running_mean', torch.zeros(out_features))
        self.register_buffer('running_var', torch.ones(out_features))

    def forward(self, x):
        # Perform GEMM operation using custom CUDA kernel
        A = x.view(-1, self.in_features)
        B = self.gemm.weight.t().contiguous()
        C = torch.empty(A.size(0), B.size(0)).to(A.device)
        custom_cuda_kernels.gemm_cuda(A, B, C)

        # Scale the result
        C *= self.scale

        # Perform batch normalization using custom CUDA kernel
        mean = torch.mean(C, dim=0)
        var = torch.var(C, dim=0)
        gamma = self.gemm.bias
        beta = torch.zeros_like(gamma)
        custom_cuda_kernels.bn_forward_cuda(C, C, mean, var, gamma, beta, self.eps)

        return C.view(-1, self.out_features)

# Example usage
model_new = ModelNew(in_features, out_features, scale_shape)
inputs = get_inputs()
output = model_new(inputs[0])
print(output.shape)
```

This solution replaces the `nn.Linear` layer with a custom GEMM kernel and optimizes the batch normalization operation using a custom CUDA kernel. The `ModelNew` class now uses these custom kernels for computation, potentially leading to performance improvements on GPU hardware.