The optimization should be focused on improving performance by replacing existing PyTorch operators with custom CUDA kernels where appropriate. Feel free to experiment with different combinations of operators to achieve the best performance improvement.

Here is a hint about how to approach this task:

- **Convolution**: Consider using efficient convolution algorithms such as Winograd or FFT-based convolutions.
- **Group Normalization**: Implement a fast version of Group Normalization using shared memory.
- **Tanh**: Replace the Tanh activation function with a more efficient implementation.
- **HardSwish**: Implement the HardSwish activation function using a simple conditional statement.
- **Residual Addition**: Combine the residual addition with other operations to reduce the number of instructions.
- **LogSumExp**: Implement a more efficient LogSumExp operation using parallel reduction.

Please provide the optimized code for `ModelNew` in the specified format.