### Constraints:

- Only use PyTorch's `load_inline` function to compile the CUDA code.
- The custom CUDA kernels should be designed to improve performance over the default PyTorch operators.
- Feel free to add any additional helper functions or classes as needed.
- Ensure that the final architecture (`ModelNew`) is fully compatible with the original `get_inputs()` and `get_init_inputs()` functions.

### Example of using `load_inline`: 
```python
from torch.utils.cpp_extension import load_inline

custom_code_source = """
// Your CUDA code here
"""

custom_code_cpp_source = "void my_custom_function();"

custom_code = load_inline(
    name="my_custom_module",
    cpp_sources=custom_code_cpp_source,
    cuda_sources=custom_code_source,
    functions=["my_custom_function"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)
```

Here's how you can call the custom CUDA function from within your PyTorch module:
```python
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.my_custom_function = custom_code.my_custom_function

    def forward(self, x):
        self.my_custom_function(x)
        return x
```

Make sure to include all necessary imports at the beginning of your code snippet.