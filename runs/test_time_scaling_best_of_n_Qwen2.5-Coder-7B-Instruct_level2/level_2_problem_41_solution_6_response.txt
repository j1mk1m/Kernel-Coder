Here are some considerations when optimizing:

- Use PyTorchâ€™s `torch.utils.cpp_extension.load_inline` function to define and compile your custom CUDA kernels.
- Combine multiple operations into a single kernel if possible to reduce memory bandwidth usage and improve performance.
- Consider using alternative algorithms or techniques that can be more efficient on GPUs.

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for GEMM, BatchNorm, GELU, and ReLU
gemm_batchnorm_gelu_relu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cublas_v2.h>

// Helper function to perform GEMM operation
void gemm(float* C, const float* A, const float* B, int M, int N, int K) {
    cublasHandle_t handle;
    cublasCreate(&handle);
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, 1.0f, B, K, A, K, 0.0f, C, N);
    cublasDestroy(handle);
}

// Custom CUDA kernel for GEMM, BatchNorm, GELU, and ReLU
__global__ void gemm_batchnorm_gelu_relu_kernel(float* x, const float* weight, const float* bias, const float* running_mean, const float* running_var, float* mean, float* var, float* inv_var, float* gamma, float* beta, float* out, int batch_size, int features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * features) {
        return;
    }

    // Perform GEMM
    int row = idx / features;
    int col = idx % features;
    float sum = 0.0f;
    for (int k = 0; k < features; ++k) {
        sum += A[row * features + k] * B[k * features + col];
    }
    x[idx] = sum;

    // Perform BatchNorm
    mean[idx] = sum / features;
    var[idx] = 0.0f;
    for (int k = 0; k < features; ++k) {
        var[idx] += (sum - mean[idx]) * (sum - mean[idx]);
    }
    var[idx] /= features;
    inv_var[idx] = 1.0f / sqrt(var[idx] + 1e-5);
    x[idx] = (x[idx] - mean[idx]) * inv_var[idx];

    // Perform GELU
    x[idx] = 0.5f * (x[idx] + tanh(sqrt(2.0f / M_PI) * (x[idx] + 0.044715f * x[idx] * x[idx] * x[idx])));

    // Perform ReLU
    x[idx] = max(x[idx], 0.0f);

    // Apply scaling and shifting
    out[idx] = gamma[idx] * x[idx] + beta[idx];
}

torch::Tensor gemm_batchnorm_gelu_relu_cuda(torch::Tensor x, torch::Tensor weight, torch::Tensor bias, torch::Tensor running_mean, torch::Tensor running_var, torch::Tensor gamma, torch::Tensor beta) {
    auto batch_size = x.size(0);
    auto features = x.size(1);
    auto out = torch::zeros_like(x);

    const int block_size = 256;
    const int num_blocks = (batch_size * features + block_size - 1) / block_size;

    gemm_batchnorm_gelu_relu_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), weight.data_ptr<float>(), bias.data_ptr<float>(), running_mean.data_ptr<float>(), running_var.data_ptr<float>(), out.data_ptr<float>(), batch_size, features);

    return out;
}
"""

gemm_batchnorm_gelu_relu_cpp_source = (
    "torch::Tensor gemm_batchnorm_gelu_relu_cuda(torch::Tensor x, torch::Tensor weight, torch::Tensor bias, torch::Tensor running_mean, torch::Tensor running_var, torch::Tensor gamma, torch::Tensor beta);"
)

# Compile the inline CUDA code for GEMM, BatchNorm, GELU, and ReLU
gemm_batchnorm_gelu_relu = load_inline(
    name="gemm_batchnorm_gelu_relu",
    cpp_sources=gemm_batchnorm_gelu_relu_cpp_source,
    cuda_sources=gemm_batchnorm_gelu_relu_source,
    functions=["gemm_batchnorm_gelu_relu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.randn(out_features))
        self.running_mean = nn.Parameter(torch.zeros(out_features), requires_grad=False)
        self.running_var = nn.Parameter(torch.ones(out_features), requires_grad=False)
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))

    def forward(self, x):
        x = gemm_batchnorm_gelu_relu.cuda(x, self.weight, self.bias, self.running_mean, self.running_var, self.gamma, self.beta)
        return x

```