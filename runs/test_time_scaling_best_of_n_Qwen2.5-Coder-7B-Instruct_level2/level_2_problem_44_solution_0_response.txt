Make sure to use `torch.utils.cpp_extension.load_inline` to compile your custom CUDA kernels. You should aim to optimize at least one of the operations in the original architecture using a custom CUDA kernel. Feel free to fuse multiple operations together if it can provide further speedup.

Also, note that PyTorch does not support global average pooling over just one dimension (i.e., it expects either both spatial dimensions to be reduced or neither). If you find yourself needing to implement such a functionality, you will need to modify your model accordingly.

**Note**: Your implementation should be able to handle any batch size, input shape, and other parameters provided in the original architecture. It should also maintain the same behavior as the original model. 

```python
# Example usage
model_new = ModelNew(*get_init_inputs())
inputs = get_inputs()
outputs = model_new(inputs[0])
print(outputs.shape)
```

```python
# Expected output shape
# torch.Size([16, 128, 1, 1])
```