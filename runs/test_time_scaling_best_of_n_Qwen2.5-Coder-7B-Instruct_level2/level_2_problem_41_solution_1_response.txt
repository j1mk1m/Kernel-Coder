### Requirements:

- **Custom CUDA Kernels**: Replace the GEMM operation with a custom CUDA kernel. You can use any efficient GEMM implementation, such as cuBLAS, but implement it yourself using CUDA threads and blocks for educational purposes.
- **BatchNorm**: Implement a custom BatchNorm1d layer using CUDA threads and blocks. Ensure that the normalization process is parallelized across the batch dimension.
- **GELU**: Implement a custom GELU function using CUDA threads and blocks. Since GELU is not an element-wise operation, you will need to design a parallel approach to compute it efficiently.
- **ReLU**: Implement a custom ReLU function using CUDA threads and blocks.

Your solution should aim to achieve maximum parallelism and minimize memory bandwidth usage while maintaining numerical accuracy. ```python

































