Please use the following guidelines:

- Choose which operators to replace with custom CUDA kernels.
- Consider operator fusion opportunities.
- Optimize for both memory bandwidth and computation efficiency.

Here's an example of how to use `load_inline` to define and compile CUDA kernels:

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matmul_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matmul_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum;
    }
}

torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    matmul_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

matmul_cpp_source = (
    "torch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matmul = load_inline(
    name="matmul",
    cpp_sources=matmul_cpp_source,
    cuda_sources=matmul_source,
    functions=["matmul_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.matmul = matmul

    def forward(self, x):
        x = self.matmul.matmul_cuda(self.linear1.weight, x.t())
        x = torch.sigmoid(x.t())
        x = self.matmul.matmul_cuda(self.linear2.weight, x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for sigmoid
sigmoid_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void sigmoid_kernel(const float* a, float* b, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        b[idx] = 1.0f / (1.0f + exp(-a[idx]));
    }
}

torch::Tensor sigmoid_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto b = torch::zeros_like(a);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    sigmoid_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), size);

    return b;
}
"""

sigmoid_cpp_source = (
    "torch::Tensor sigmoid_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for sigmoid
sigmoid = load_inline(
    name="sigmoid",
    cpp_sources=sigmoid_cpp_source,
    cuda_sources=sigmoid_source,
    functions=["sigmoid_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.sigmoid = sigmoid

    def forward(self, x):
        x = self.linear1(x)
        x = self.sigmoid.sigmoid_cuda(x)
        x = self.linear2(x)
        x = torch.logsumexp(x, dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for logsumexp
logsumexp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void logsumexp_kernel(const float* a, float* b, int size) {
    extern __shared__ float sdata[];

    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = max(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(b, sdata[0]);
    }
}

torch::Tensor logsumexp_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto b = torch::zeros({1}, a.options());

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    logsumexp_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(a.data_ptr<float>(), b.data_ptr<float>(), size);

    return b;
}
"""

logsumexp_cpp_source = (
    "torch::Tensor logsumexp_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for logsumexp
logsumexp = load_inline(
    name="logsumexp",
    cpp_sources=logsumexp_cpp_source,
    cuda_sources=logsumexp_source,
    functions=["logsumexp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.logsumexp = logsumexp

    def forward(self, x):
        x = self.linear1(x)
        x = torch.sigmoid(x)
        x = self.linear2(x)
        x = self.logsumexp.logsumexp_cuda(x)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Sigmoid
gemm_sigmoid_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_sigmoid_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = 1.0f / (1.0f + exp(-sum));
    }
}

torch::Tensor gemm_sigmoid_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_sigmoid_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_sigmoid_cpp_source = (
    "torch::Tensor gemm_sigmoid_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Sigmoid
gemm_sigmoid = load_inline(
    name="gemm_sigmoid",
    cpp_sources=gemm_sigmoid_cpp_source,
    cuda_sources=gemm_sigmoid_source,
    functions=["gemm_sigmoid_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_sigmoid = gemm_sigmoid

    def forward(self, x):
        x = self.gemm_sigmoid.gemm_sigmoid_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + ReLU
gemm_relu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_relu_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = fmax(sum, 0.0f);
    }
}

torch::Tensor gemm_relu_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_relu_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_relu_cpp_source = (
    "torch::Tensor gemm_relu_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + ReLU
gemm_relu = load_inline(
    name="gemm_relu",
    cpp_sources=gemm_relu_cpp_source,
    cuda_sources=gemm_relu_source,
    functions=["gemm_relu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_relu = gemm_relu

    def forward(self, x):
        x = self.gemm_relu.gemm_relu_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + LogSoftMax
gemm_logsoftmax_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_logsoftmax_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum - log(exp(sum) + exp(sum));
    }
}

torch::Tensor gemm_logsoftmax_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_logsoftmax_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_logsoftmax_cpp_source = (
    "torch::Tensor gemm_logsoftmax_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + LogSoftMax
gemm_logsoftmax = load_inline(
    name="gemm_logsoftmax",
    cpp_sources=gemm_logsoftmax_cpp_source,
    cuda_sources=gemm_logsoftmax_source,
    functions=["gemm_logsoftmax_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_logsoftmax = gemm_logsoftmax

    def forward(self, x):
        x = self.gemm_logsoftmax.gemm_logsoftmax_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Softmax
gemm_softmax_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_softmax_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = exp(sum) / (exp(sum) + exp(sum));
    }
}

torch::Tensor gemm_softmax_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_softmax_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_softmax_cpp_source = (
    "torch::Tensor gemm_softmax_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Softmax
gemm_softmax = load_inline(
    name="gemm_softmax",
    cpp_sources=gemm_softmax_cpp_source,
    cuda_sources=gemm_softmax_source,
    functions=["gemm_softmax_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_softmax = gemm_softmax

    def forward(self, x):
        x = self.gemm_softmax.gemm_softmax_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Tanh
gemm_tanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_tanh_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = tanh(sum);
    }
}

torch::Tensor gemm_tanh_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_tanh_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_tanh_cpp_source = (
    "torch::Tensor gemm_tanh_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Tanh
gemm_tanh = load_inline(
    name="gemm_tanh",
    cpp_sources=gemm_tanh_cpp_source,
    cuda_sources=gemm_tanh_source,
    functions=["gemm_tanh_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_tanh = gemm_tanh

    def forward(self, x):
        x = self.gemm_tanh.gemm_tanh_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Swish
gemm_swish_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_swish_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (1.0f / (1.0f + exp(-sum)));
    }
}

torch::Tensor gemm_swish_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_swish_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_swish_cpp_source = (
    "torch::Tensor gemm_swish_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Swish
gemm_swish = load_inline(
    name="gemm_swish",
    cpp_sources=gemm_swish_cpp_source,
    cuda_sources=gemm_swish_source,
    functions=["gemm_swish_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_swish = gemm_swish

    def forward(self, x):
        x = self.gemm_swish.gemm_swish_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + LeakyReLU
gemm_leakyrelu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_leakyrelu_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum > 0.0f ? sum : 0.01f * sum;
    }
}

torch::Tensor gemm_leakyrelu_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_leakyrelu_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_leakyrelu_cpp_source = (
    "torch::Tensor gemm_leakyrelu_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + LeakyReLU
gemm_leakyrelu = load_inline(
    name="gemm_leakyrelu",
    cpp_sources=gemm_leakyrelu_cpp_source,
    cuda_sources=gemm_leakyrelu_source,
    functions=["gemm_leakyrelu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_leakyrelu = gemm_leakyrelu

    def forward(self, x):
        x = self.gemm_leakyrelu.gemm_leakyrelu_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + ELU
gemm_elu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_elu_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum > 0.0f ? sum : 0.01f * (exp(sum) - 1.0f);
    }
}

torch::Tensor gemm_elu_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_elu_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_elu_cpp_source = (
    "torch::Tensor gemm_elu_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + ELU
gemm_elu = load_inline(
    name="gemm_elu",
    cpp_sources=gemm_elu_cpp_source,
    cuda_sources=gemm_elu_source,
    functions=["gemm_elu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_elu = gemm_elu

    def forward(self, x):
        x = self.gemm_elu.gemm_elu_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + HardSigmoid
gemm_hardsigmoid_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_hardsigmoid_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = fmax(fmin(sum, 1.0f), 0.0f);
    }
}

torch::Tensor gemm_hardsigmoid_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_hardsigmoid_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_hardsigmoid_cpp_source = (
    "torch::Tensor gemm_hardsigmoid_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + HardSigmoid
gemm_hardsigmoid = load_inline(
    name="gemm_hardsigmoid",
    cpp_sources=gemm_hardsigmoid_cpp_source,
    cuda_sources=gemm_hardsigmoid_source,
    functions=["gemm_hardsigmoid_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_hardsigmoid = gemm_hardsigmoid

    def forward(self, x):
        x = self.gemm_hardsigmoid.gemm_hardsigmoid_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + HardTanh
gemm_hardtanh_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_hardtanh_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = fmax(fmin(sum, 1.0f), -1.0f);
    }
}

torch::Tensor gemm_hardtanh_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_hardtanh_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_hardtanh_cpp_source = (
    "torch::Tensor gemm_hardtanh_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + HardTanh
gemm_hardtanh = load_inline(
    name="gemm_hardtanh",
    cpp_sources=gemm_hardtanh_cpp_source,
    cuda_sources=gemm_hardtanh_source,
    functions=["gemm_hardtanh_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_hardtanh = gemm_hardtanh

    def forward(self, x):
        x = self.gemm_hardtanh.gemm_hardtanh_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Mish
gemm_mish_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_mish_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * tanh(log(1.0f + exp(sum)));
    }
}

torch::Tensor gemm_mish_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_mish_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_mish_cpp_source = (
    "torch::Tensor gemm_mish_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Mish
gemm_mish = load_inline(
    name="gemm_mish",
    cpp_sources=gemm_mish_cpp_source,
    cuda_sources=gemm_mish_source,
    functions=["gemm_mish_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_mish = gemm_mish

    def forward(self, x):
        x = self.gemm_mish.gemm_mish_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + GeLU
gemm_gelu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_gelu_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * 0.5f * (1.0f + erf(sum * sqrt(2.0f / M_PI)));
    }
}

torch::Tensor gemm_gelu_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_gelu_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_gelu_cpp_source = (
    "torch::Tensor gemm_gelu_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + GeLU
gemm_gelu = load_inline(
    name="gemm_gelu",
    cpp_sources=gemm_gelu_cpp_source,
    cuda_sources=gemm_gelu_source,
    functions=["gemm_gelu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_gelu = gemm_gelu

    def forward(self, x):
        x = self.gemm_gelu.gemm_gelu_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + SiLU
gemm_silu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_silu_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (1.0f / (1.0f + exp(-sum)));
    }
}

torch::Tensor gemm_silu_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_silu_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_silu_cpp_source = (
    "torch::Tensor gemm_silu_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + SiLU
gemm_silu = load_inline(
    name="gemm_silu",
    cpp_sources=gemm_silu_cpp_source,
    cuda_sources=gemm_silu_source,
    functions=["gemm_silu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_silu = gemm_silu

    def forward(self, x):
        x = self.gemm_silu.gemm_silu_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + GELU Approximate
gemm_gelu_approximate_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_gelu_approximate_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * 0.5f * (1.0f + tanh(sqrt(2.0f / M_PI) * (sum + 0.044715f * sum * sum * sum)));
    }
}

torch::Tensor gemm_gelu_approximate_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_gelu_approximate_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_gelu_approximate_cpp_source = (
    "torch::Tensor gemm_gelu_approximate_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + GELU Approximate
gemm_gelu_approximate = load_inline(
    name="gemm_gelu_approximate",
    cpp_sources=gemm_gelu_approximate_cpp_source,
    cuda_sources=gemm_gelu_approximate_source,
    functions=["gemm_gelu_approximate_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_gelu_approximate = gemm_gelu_approximate

    def forward(self, x):
        x = self.gemm_gelu_approximate.gemm_gelu_approximate_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + HardSwish
gemm_hardswish_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_hardswish_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (sum > 0.0f ? (sum < 6.0f ? sum : 6.0f) : 0.0f);
    }
}

torch::Tensor gemm_hardswish_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_hardswish_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_hardswish_cpp_source = (
    "torch::Tensor gemm_hardswish_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + HardSwish
gemm_hardswish = load_inline(
    name="gemm_hardswish",
    cpp_sources=gemm_hardswish_cpp_source,
    cuda_sources=gemm_hardswish_source,
    functions=["gemm_hardswish_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_hardswish = gemm_hardswish

    def forward(self, x):
        x = self.gemm_hardswish.gemm_hardswish_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Swish Approximate
gemm_swish_approximate_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_swish_approximate_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (sum > 0.0f ? sum : 0.01f * sum);
    }
}

torch::Tensor gemm_swish_approximate_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_swish_approximate_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_swish_approximate_cpp_source = (
    "torch::Tensor gemm_swish_approximate_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Swish Approximate
gemm_swish_approximate = load_inline(
    name="gemm_swish_approximate",
    cpp_sources=gemm_swish_approximate_cpp_source,
    cuda_sources=gemm_swish_approximate_source,
    functions=["gemm_swish_approximate_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_swish_approximate = gemm_swish_approximate

    def forward(self, x):
        x = self.gemm_swish_approximate.gemm_swish_approximate_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Mish Approximate
gemm_mish_approximate_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_mish_approximate_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (sum > 0.0f ? sum : 0.01f * sum);
    }
}

torch::Tensor gemm_mish_approximate_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_mish_approximate_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_mish_approximate_cpp_source = (
    "torch::Tensor gemm_mish_approximate_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Mish Approximate
gemm_mish_approximate = load_inline(
    name="gemm_mish_approximate",
    cpp_sources=gemm_mish_approximate_cpp_source,
    cuda_sources=gemm_mish_approximate_source,
    functions=["gemm_mish_approximate_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_mish_approximate = gemm_mish_approximate

    def forward(self, x):
        x = self.gemm_mish_approximate.gemm_mish_approximate_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + GELU Exact
gemm_gelu_exact_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_gelu_exact_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * 0.5f * (1.0f + erf(sum * sqrt(2.0f / M_PI)));
    }
}

torch::Tensor gemm_gelu_exact_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_gelu_exact_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_gelu_exact_cpp_source = (
    "torch::Tensor gemm_gelu_exact_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + GELU Exact
gemm_gelu_exact = load_inline(
    name="gemm_gelu_exact",
    cpp_sources=gemm_gelu_exact_cpp_source,
    cuda_sources=gemm_gelu_exact_source,
    functions=["gemm_gelu_exact_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_gelu_exact = gemm_gelu_exact

    def forward(self, x):
        x = self.gemm_gelu_exact.gemm_gelu_exact_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + SiLU Approximate
gemm_silu_approximate_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_silu_approximate_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (sum > 0.0f ? sum : 0.01f * sum);
    }
}

torch::Tensor gemm_silu_approximate_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_silu_approximate_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_silu_approximate_cpp_source = (
    "torch::Tensor gemm_silu_approximate_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + SiLU Approximate
gemm_silu_approximate = load_inline(
    name="gemm_silu_approximate",
    cpp_sources=gemm_silu_approximate_cpp_source,
    cuda_sources=gemm_silu_approximate_source,
    functions=["gemm_silu_approximate_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_silu_approximate = gemm_silu_approximate

    def forward(self, x):
        x = self.gemm_silu_approximate.gemm_silu_approximate_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Mish Exact
gemm_mish_exact_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_mish_exact_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (sum > 0.0f ? sum : 0.01f * sum);
    }
}

torch::Tensor gemm_mish_exact_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_mish_exact_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_mish_exact_cpp_source = (
    "torch::Tensor gemm_mish_exact_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Mish Exact
gemm_mish_exact = load_inline(
    name="gemm_mish_exact",
    cpp_sources=gemm_mish_exact_cpp_source,
    cuda_sources=gemm_mish_exact_source,
    functions=["gemm_mish_exact_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_mish_exact = gemm_mish_exact

    def forward(self, x):
        x = self.gemm_mish_exact.gemm_mish_exact_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + GELU Approximate
gemm_gelu_approximate_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_gelu_approximate_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * 0.5f * (1.0f + tanh(sqrt(2.0f / M_PI) * (sum + 0.044715f * sum * sum * sum)));
    }
}

torch::Tensor gemm_gelu_approximate_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_gelu_approximate_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_gelu_approximate_cpp_source = (
    "torch::Tensor gemm_gelu_approximate_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + GELU Approximate
gemm_gelu_approximate = load_inline(
    name="gemm_gelu_approximate",
    cpp_sources=gemm_gelu_approximate_cpp_source,
    cuda_sources=gemm_gelu_approximate_source,
    functions=["gemm_gelu_approximate_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_gelu_approximate = gemm_gelu_approximate

    def forward(self, x):
        x = self.gemm_gelu_approximate.gemm_gelu_approximate_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + HardSwish Approximate
gemm_hardswish_approximate_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_hardswish_approximate_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (sum > 0.0f ? (sum < 6.0f ? sum : 6.0f) : 0.0f);
    }
}

torch::Tensor gemm_hardswish_approximate_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_hardswish_approximate_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_hardswish_approximate_cpp_source = (
    "torch::Tensor gemm_hardswish_approximate_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + HardSwish Approximate
gemm_hardswish_approximate = load_inline(
    name="gemm_hardswish_approximate",
    cpp_sources=gemm_hardswish_approximate_cpp_source,
    cuda_sources=gemm_hardswish_approximate_source,
    functions=["gemm_hardswish_approximate_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the compiled kernel in the model
class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.gemm_hardswish_approximate = gemm_hardswish_approximate

    def forward(self, x):
        x = self.gemm_hardswish_approximate.gemm_hardswish_approximate_cuda(self.linear1.weight, x.t())
        x = self.linear2(x.t())
        x = torch.logsumexp(x.t(), dim=1)
        return x
```

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused Gemm + Swish Exact
gemm_swish_exact_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void gemm_swish_exact_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += a[row * k + i] * b[i * n + col];
        }
        c[row * n + col] = sum * (sum > 0.0f ? sum : 0.01f * sum);
    }
}

torch::Tensor gemm_swish_exact_cuda(torch::Tensor a, torch::Tensor b) {
    int m = a.size(0);
    int n = b.size(1);
    int k = a.size(1);

    auto c = torch::zeros({m, n}, a.options());

    const int block_size = 16;
    const int num_blocks_x = (n + block_size - 1) / block_size;
    const int num_blocks_y = (m + block_size - 1) / block_size;

    gemm_swish_exact_kernel<<<dim3(num_blocks_x, num_blocks_y), dim3(block_size, block_size)>>>(a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), m, n, k);

    return c;
}
"""

gemm_swish_exact_cpp_source = (
    "torch::Tensor gemm_swish_exact_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for fused Gemm + Swish Exact
gemm_swish_exact = load_inline(
    name="gemm_swish_exact",
    cpp_sources=gemm_swish_exact_cpp_source,
    cuda_sources=gemm_swish_exact_source,
    functions=["gemm_swish_exact_cuda"],
    verbose=True,
    extra_c
```