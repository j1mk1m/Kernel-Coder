Please note that I am looking for optimization at both the level of individual operators and potential fusion of multiple operators. For instance, replacing a matrix multiplication followed by a bias addition with a fused matrix multiplication operation that includes the bias addition. Similarly, optimizing group normalization and LeakyReLU through custom CUDA implementations. Feel free to explore different optimization strategies to achieve the highest performance gain.