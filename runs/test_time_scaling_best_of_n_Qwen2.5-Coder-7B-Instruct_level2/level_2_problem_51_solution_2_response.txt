Note: You should focus on optimizing the operations that can benefit most from custom CUDA kernels. For instance, matrix multiplication (GEMM) is a prime candidate for optimization since it can be highly parallelized. Other operations such as subtraction, global average pooling, log-sum-exp, gelu, and residual add can also be optimized depending on their specific implementation details.

Here's a hint on how to approach this problem:

- Start by identifying the operations that can be optimized using custom CUDA kernels. Consider the parallelism potential of each operation.
- Implement custom CUDA kernels for these operations. Make sure to handle edge cases and ensure correctness.
- Replace the corresponding PyTorch operators in the `Model` class with your custom CUDA kernels.
- Ensure that the modified architecture (`ModelNew`) maintains the same functionality as the original architecture (`Model`). Test the new architecture to verify correctness.
```



































