    Your solution should be efficient, utilizing CUDA for parallel computation wherever possible. Consider using PyTorch's `torch.utils.cpp_extension` module to compile your custom CUDA kernels.

Here's a hint to help you get started:

- Consider replacing the `softmax`, `add`, `mul`, and `sigmoid` operations with custom CUDA kernels. These operations can benefit significantly from parallelization on the GPU.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for softmax
softmax_source = """
// Custom CUDA implementation of softmax
"""

softmax_cpp_source = (
    "torch::Tensor softmax_cuda(torch::Tensor input);"
)

# Compile the inline CUDA code for softmax
softmax = load_inline(
    name="softmax",
    cpp_sources=softmax_cpp_source,
    cuda_sources=softmax_source,
    functions=["softmax_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor
        self.softmax = softmax

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.softmax.softmax_cuda(x)
        x = x + self.bias
        x = x * self.scaling_factor
        x = torch.sigmoid(x)
        return x
```

Please provide the full implementation of the `ModelNew` class with all necessary CUDA kernels.

