Please include all necessary imports at the beginning of your code.

Here are some tips:
- Consider replacing the convolutional layer with a custom CUDA kernel.
- Explore using Instance Normalization directly within the CUDA kernel if possible.
- Optimize the division operation.
- Feel free to combine multiple operations into a single kernel if it improves performance.

Example of loading inline CUDA code:

```python
from torch.utils.cpp_extension import load_inline

custom_convolution_source = """
// Custom convolution CUDA kernel source code here
"""

custom_convolution_cpp_source = """
// C++ wrapper for custom convolution function here
"""

custom_convolution = load_inline(
    name="custom_convolution",
    cpp_sources=custom_convolution_cpp_source,
    cuda_sources=custom_convolution_source,
    functions=["convolution_forward"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)
```

```python
class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, divide_by):
        super(ModelNew, self).__init__()
        self.custom_convolution = custom_convolution

    def forward(self, x):
        x = self.custom_convolution.convolution_forward(x, weight, bias, stride, padding, dilation, groups)
        x = instance_norm_forward(x, running_mean, running_var, weight, bias, eps)
        x = x / divide_by
        return x
```

```python
def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divide_by]

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]
```





























s
