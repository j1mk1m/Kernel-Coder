The output should look something like this:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for the operations here...
# ...

model_new.py
```

Please ensure the solution includes efficient use of CUDA parallelism and any necessary optimizations such as memory access patterns, algorithmic improvements, etc. If there are opportunities for operator fusion, combine them in your implementation. For example, combining `matmul` and `relu`, or `convolution` and `activation`. 

If there are algorithms that can be improved upon, implement those improvements in your custom CUDA kernels. For instance, using more efficient activation functions like online softmax, or implementing batch normalization in a more performant way.

Finally, ensure that your custom CUDA kernels are correctly integrated into the PyTorch model framework so that they can be used seamlessly during the forward pass.