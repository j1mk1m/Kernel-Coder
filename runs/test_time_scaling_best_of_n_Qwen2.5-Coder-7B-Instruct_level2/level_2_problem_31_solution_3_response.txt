    Note: You should aim to achieve the highest possible performance gains by choosing appropriate operators to replace with custom CUDA kernels. Consider operator fusion opportunities, algorithmic changes, and other techniques to optimize the model.

    Note: The goal is to demonstrate your understanding of how to leverage CUDA for optimizing deep learning models. Therefore, focus on the actual implementation details and provide a comprehensive solution.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for min operation
min_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void min_kernel(const float* x, const float* y, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = fminf(x[idx], y[idx]);
    }
}

torch::Tensor min_cuda(torch::Tensor x, torch::Tensor y) {
    auto size = x.numel();
    auto out = torch::zeros_like(x);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    min_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), y.data_ptr<float>(), out.data_ptr<float>(), size);

    return out;
}
"""

min_cpp_source = (
    "torch::Tensor min_cuda(torch::Tensor x, torch::Tensor y);"
)

# Compile the inline CUDA code for min operation
min_op = load_inline(
    name="min_op",
    cpp_sources=min_cpp_source,
    cuda_sources=min_source,
    functions=["min_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for adding bias
add_bias_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void add_bias_kernel(const float* x, const float* bias, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = x[idx] + bias[idx];
    }
}

torch::Tensor add_bias_cuda(torch::Tensor x, torch::Tensor bias) {
    auto size = x.numel();
    auto out = torch::zeros_like(x);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    add_bias_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), bias.data_ptr<float>(), out.data_ptr<float>(), size);

    return out;
}
"""

add_bias_cpp_source = (
    "torch::Tensor add_bias_cuda(torch::Tensor x, torch::Tensor bias);"
)

# Compile the inline CUDA code for adding bias
add_bias = load_inline(
    name="add_bias",
    cpp_sources=add_bias_cpp_source,
    cuda_sources=add_bias_source,
    functions=["add_bias_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for scaling
scale_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void scale_kernel(const float* x, float scale, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = x[idx] * scale;
    }
}

torch::Tensor scale_cuda(torch::Tensor x, float scale) {
    auto size = x.numel();
    auto out = torch::zeros_like(x);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    scale_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), scale, out.data_ptr<float>(), size);

    return out;
}
"""

scale_cpp_source = (
    "torch::Tensor scale_cuda(torch::Tensor x, float scale);"
)

# Compile the inline CUDA code for scaling
scale = load_inline(
    name="scale",
    cpp_sources=scale_cpp_source,
    cuda_sources=scale_source,
    functions=["scale_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.constant_value = constant_value
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor
        self.min_op = min_op
        self.add_bias = add_bias
        self.scale = scale

    def forward(self, x):
        x = self.conv(x)
        x = self.min_op.min_cuda(x, torch.tensor([self.constant_value]).to(x.device))
        x = self.add_bias.add_bias_cuda(x, self.bias)
        x = self.scale.scale_cuda(x, self.scaling_factor)
        return x
```

```python
batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
constant_value = 0.5
bias_shape = (out_channels, 1, 1)
scaling_factor = 2.0

model = ModelNew(in_channels, out_channels, kernel_size, constant_value, bias_shape, scaling_factor)
inputs = get_inputs()

output = model(inputs[0])
print(output.shape)
```