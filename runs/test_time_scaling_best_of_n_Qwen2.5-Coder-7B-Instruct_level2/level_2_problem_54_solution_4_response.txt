    Your goal is to achieve the highest possible performance improvement over the original architecture using custom CUDA kernels. Feel free to use any PyTorch operators you wish, but prioritize custom CUDA kernels for the most computationally intensive parts of the model. For example, convolution can be replaced with cuDNN's highly optimized convolution implementation, which is already fast, but if there are other operations that can benefit from custom CUDA kernels, feel free to implement them.

    Note that it is acceptable to use the original PyTorch operators in the forward pass of ModelNew if they cannot be efficiently implemented with custom CUDA kernels. However, the overall performance should be higher than the original architecture.