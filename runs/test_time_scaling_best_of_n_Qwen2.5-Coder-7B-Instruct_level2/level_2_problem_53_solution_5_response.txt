```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Your CUDA source code here
gemm_scale_hardtanh_gelu_source = """
// Include necessary headers
#include <torch/extension.h>
#include <cuda_runtime.h>

// Kernel function for GEMM, scaling, hardtanh, and GELU
__global__ void gemm_scale_hardtanh_gelu_kernel(float* a, float* b, float* c, float* d, float scale, float min_val, float max_val, int M, int N, int K) {
    // Thread index
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    // Check bounds
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }

        // Scale, apply hardtanh, and GELU
        sum *= scale;
        sum = fmaxf(fminf(sum, max_val), min_val);
        sum = sum > 0 ? sum : 0.5 * sum * sum;

        // Store result
        d[row * N + col] = sum;
    }
}

// Wrapper function for the kernel
torch::Tensor gemm_scale_hardtanh_gelu_cuda(torch::Tensor a, torch::Tensor b, float scale, float min_val, float max_val) {
    int M = a.size(0);
    int N = b.size(1);
    int K = a.size(1);

    // Allocate output tensor
    auto out = torch::zeros({M, N}, a.options());

    // Grid and block dimensions
    const int block_size = 256;
    dim3 grid((N + block_size - 1) / block_size, (M + block_size - 1) / block_size);
    dim3 block(block_size, block_size);

    // Launch kernel
    gemm_scale_hardtanh_gelu_kernel<<<grid, block>>>(a.data_ptr<float>(), b.data_ptr<float>(), nullptr, out.data_ptr<float>(), scale, min_val, max_val, M, N, K);

    return out;
}
"""

# Register the CUDA function
gemm_scale_hardtanh_gelu_cpp_source = (
    "torch::Tensor gemm_scale_hardtanh_gelu_cuda(torch::Tensor a, torch::Tensor b, float scale, float min_val, float max_val);"
)

# Load the CUDA module
gemm_scale_hardtanh_gelu_module = load_inline(
    name="gemm_scale_hardtanh_gelu",
    cpp_sources=gemm_scale_hardtanh_gelu_cpp_source,
    cuda_sources=gemm_scale_hardtanh_gelu_source,
    functions=["gemm_scale_hardtanh_gelu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# New model class using the custom CUDA function
class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max):
        super(ModelNew, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.scaling_factor = scaling_factor
        self.hardtanh_min = hardtanh_min
        self.hardtanh_max = hardtanh_max

    def forward(self, x):
        return gemm_scale_hardtanh_gelu_module.gemm_scale_hardtanh_gelu_cuda(x, torch.zeros(self.out_features, self.in_features), self.scaling_factor, self.hardtanh_min, self.hardtanh_max)

# Example usage
if __name__ == "__main__":
    batch_size = 2048
    in_features = 8192
    out_features = 8192
    scaling_factor = 0.5
    hardtanh_min = -2
    hardtanh_max = 2

    model = ModelNew(in_features, out_features, scaling_factor, hardtanh_min, hardtanh_max)
    inputs = get_inputs()[0].to("cuda")
    outputs = model(inputs.to("cuda"))
    print(outputs.shape)
```

This solution demonstrates how to optimize the `Model` architecture by replacing the GEMM, scaling, hardtanh, and GELU operations with a single custom CUDA kernel. The kernel efficiently performs all these operations in parallel, reducing the overall computation time.