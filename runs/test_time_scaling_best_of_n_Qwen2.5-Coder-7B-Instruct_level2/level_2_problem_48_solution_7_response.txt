    Note: You can use any libraries from PyTorch, but ensure they are imported at the beginning of the code. The goal is to optimize performance by using custom CUDA kernels where appropriate. 

    Feel free to add any additional helper functions or classes as needed.

Here is a sample solution:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for 3D convolution
convolution_3d_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA implementation for 3D convolution
__global__ void convolution_3d_kernel(...) {
    // Implementation details
}

torch::Tensor convolution_3d_cuda(torch::Tensor input, torch::Tensor weight, ...) {
    // Kernel invocation
    return output;
}
"""

convolution_3d_cpp_source = (
    "torch::Tensor convolution_3d_cuda(torch::Tensor input, torch::Tensor weight, ...);"
)

# Compile the inline CUDA code for 3D convolution
convolution_3d = load_inline(
    name="convolution_3d",
    cpp_sources=convolution_3d_cpp_source,
    cuda_sources=convolution_3d_source,
    functions=["convolution_3d_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = convolution_3d
        self.scaling_factor = nn.Parameter(torch.randn(bias_shape))
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv(x, self.weight)
        x = x * self.scaling_factor
        x = torch.tanh(x)
        x = x * self.bias
        x = torch.sigmoid(x)
        return x
```

This is just a starting point. Feel free to explore different optimizations such as custom CUDA kernels for other operations, algorithmic improvements, etc. Make sure to thoroughly test the optimized version to ensure it produces the same results as the original model.