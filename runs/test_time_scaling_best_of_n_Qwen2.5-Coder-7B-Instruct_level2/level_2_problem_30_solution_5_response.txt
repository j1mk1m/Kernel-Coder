Please note that you can use any libraries from PyTorch and CUDA, but you must implement the GEMM operation using a custom CUDA kernel. Feel free to also implement other operations such as Group Normalization and HardTanh using custom CUDA kernels if it will result in performance improvements.

For Group Normalization, you can assume that the number of groups is always a divisor of the number of channels.

Here's an example of how you might structure your solution:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for GEMM
gemm_source = """
// CUDA kernel implementation for GEMM
"""

# Define the custom CUDA kernel for Group Normalization
group_norm_source = """
// CUDA kernel implementation for Group Normalization
"""

# Define the custom CUDA kernel for HardTanh
hardtanh_source = """
// CUDA kernel implementation for HardTanh
"""

# Compile the inline CUDA code for each operation
gemm = load_inline(name="gemm", cpp_sources="", cuda_sources=gemm_source, functions=["gemm"])
group_norm = load_inline(name="group_norm", cpp_sources="", cuda_sources=group_norm_source, functions=["group_norm"])
hardtanh = load_inline(name="hardtanh", cpp_sources="", cuda_sources=hardtanh_source, functions=["hardtanh"])

class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, num_groups, hardtanh_min, hardtanh_max):
        super(ModelNew, self).__init__()
        self.gemm = gemm
        self.group_norm = group_norm
        self.hardtanh = hardtanh

    def forward(self, x):
        x = self.gemm.gemm(x)
        x = self.group_norm.group_norm(x)
        x = self.hardtanh.hardtanh(x)
        return x
```

Make sure to provide the actual CUDA kernel implementations for GEMM, Group Normalization, and HardTanh within the respective source strings. ```