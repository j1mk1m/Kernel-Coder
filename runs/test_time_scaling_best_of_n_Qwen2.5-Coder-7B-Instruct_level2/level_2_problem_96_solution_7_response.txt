The optimized code should be able to achieve at least 2x speedup compared to the original implementation. To ensure the correctness of your implementation, compare the outputs of both architectures using random inputs.

Note: The provided architecture uses PyTorch's built-in operators which can be replaced with custom CUDA kernels to improve performance. Feel free to choose any subset of operators to replace, including combinations of operators. For example, you could combine the transposed convolution and scaling operations into a single kernel, or use a different algorithm for the max pooling operation. The goal is to optimize the architecture for better performance while maintaining correctness.