Note: The goal here is to optimize the performance of the Gemm_Sigmoid_Scaling_ResidualAdd pattern by replacing the PyTorch operators with custom CUDA kernels where appropriate. Feel free to experiment with different combinations of operators to achieve the best performance. Consider operator fusion and algorithmic changes that could lead to performance improvements.
```