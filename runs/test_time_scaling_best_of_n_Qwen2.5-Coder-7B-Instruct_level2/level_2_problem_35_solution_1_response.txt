    Your solution should include any necessary imports, class definitions, and function definitions. Be sure to use the `load_inline` method from `torch.utils.cpp_extension` to compile your custom CUDA kernels. If you decide to replace an operator with a custom implementation, be sure to define the CUDA kernel and then create a Python wrapper around it using the `torch::Tensor` API.

    Your goal is to optimize the performance of the Model architecture by replacing operators with custom CUDA kernels where appropriate. Feel free to combine multiple operators into a single kernel, change algorithms, or perform other optimizations. Make sure your final architecture is fully functional and produces the same results as the original Model.