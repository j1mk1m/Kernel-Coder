**Note:** You can use any libraries you need to implement your custom CUDA operators. However, please ensure that the solution does not rely on external libraries that are not included in the standard PyTorch installation.

**Hint:** Consider optimizing the convolution operation, which is typically one of the most computationally intensive parts of neural networks. You might also explore other operations like element-wise multiplication, ReLU, and GELU that could benefit from custom implementations. Additionally, think about operator fusion, such as combining the convolution and element-wise multiplication into a single kernel if possible.