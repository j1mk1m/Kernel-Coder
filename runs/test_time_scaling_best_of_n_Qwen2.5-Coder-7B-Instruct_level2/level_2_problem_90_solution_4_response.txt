Please note that the model has three operations that can be accelerated with CUDA: `conv`, `leaky_relu`, and `gelu`. You should aim to optimize these operations. Consider using cuDNN for the convolution operation and potentially implementing custom CUDA kernels for the LeakyReLU and GELU activations.

### Constraints:
- You must use PyTorch's `nn.functional` module for the LeakyReLU and GELU activations in your custom CUDA kernels.
- You cannot use any other libraries except for PyTorch and its standard modules.
- Ensure that your custom CUDA kernels are efficient and provide performance improvements over the default PyTorch implementations.
```