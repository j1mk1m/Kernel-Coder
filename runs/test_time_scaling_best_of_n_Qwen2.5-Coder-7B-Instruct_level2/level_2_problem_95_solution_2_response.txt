Note: You should aim to optimize the most computationally expensive operations first. For instance, if there are multiple matrix multiplications in the model, focus on optimizing those first. Additionally, consider the trade-offs between memory bandwidth, compute efficiency, and complexity of the custom CUDA kernels.

**Hint:** If you decide to fuse multiple operators, make sure that the fused operation still produces the same result as the original sequence of operations. For example, fusing a matrix multiplication followed by a ReLU can be done efficiently because both operations can be combined into a single kernel. However, fusing a matrix multiplication followed by a sigmoid is more challenging because these two operations cannot be combined into a single kernel without losing accuracy.