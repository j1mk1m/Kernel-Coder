The goal here is to optimize the architecture as much as possible by replacing operators with custom CUDA kernels, fusing multiple operators together, or using more efficient algorithms where applicable. Be creative and think about how you can leverage GPU parallelism to speed up the computation.

Note: The provided example uses PyTorch C++ extension to define and compile CUDA kernels. Feel free to use any other method or library that allows you to define and compile CUDA kernels if it's more convenient for you.