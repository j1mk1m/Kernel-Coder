Please note that you should aim to optimize the most computationally intensive parts of the network. For instance, if there are several matrix multiplications or convolutions, those are good candidates for optimization.

Here is a starting point:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for transposed convolution
conv_transpose_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Implement the transposed convolution kernel here
"""

conv_transpose_cpp_source = (
    "torch::Tensor conv_transpose_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias, int stride, int padding);"
)

# Compile the inline CUDA code for transposed convolution
conv_transpose = load_inline(
    name="conv_transpose",
    cpp_sources=conv_transpose_cpp_source,
    cuda_sources=conv_transpose_source,
    functions=["conv_transpose_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_kernel_size, maxpool_stride, hardtanh_min, hardtanh_max):
        super(ModelNew, self).__init__()
        self.conv_transpose = conv_transpose
        self.maxpool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=maxpool_stride)
        self.hardtanh = nn.Hardtanh(min_val=hardtanh_min, max_val=hardtanh_max)

    def forward(self, x):
        x = self.conv_transpose.conv_transpose_cuda(x, self.weight, self.bias, stride=self.stride, padding=self.padding)
        x = self.maxpool(x)
        x = self.hardtanh(x)
        x = torch.mean(x, dim=(2, 3), keepdim=True)
        x = torch.tanh(x)
        return x
```

Note that you need to implement the `conv_transpose_source` kernel function. This is just a starting point and you can add more optimizations such as adding bias terms, using different padding types, etc.
 ```