```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for conv transpose
conv_transpose_source = """
// Your custom CUDA kernel implementation for ConvTranspose3d
"""

conv_transpose_cpp_source = (
    // Your custom CUDA function declaration for ConvTranspose3d
)

# Compile the inline CUDA code for conv transpose
conv_transpose = load_inline(
    name="conv_transpose",
    cpp_sources=conv_transpose_cpp_source,
    cuda_sources=conv_transpose_source,
    functions=["conv_transpose_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for max pool
max_pool_source = """
// Your custom CUDA kernel implementation for MaxPool3d
"""

max_pool_cpp_source = (
    // Your custom CUDA function declaration for MaxPool3d
)

# Compile the inline CUDA code for max pool
max_pool = load_inline(
    name="max_pool",
    cpp_sources=max_pool_cpp_source,
    cuda_sources=max_pool_source,
    functions=["max_pool_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for subtract
subtract_source = """
// Your custom CUDA kernel implementation for Subtract
"""

subtract_cpp_source = (
    // Your custom CUDA function declaration for Subtract
)

# Compile the inline CUDA code for subtract
subtract = load_inline(
    name="subtract",
    cpp_sources=subtract_cpp_source,
    cuda_sources=subtract_source,
    functions=["subtract_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for swish
swish_source = """
// Your custom CUDA kernel implementation for Swish
"""

swish_cpp_source = (
    // Your custom CUDA function declaration for Swish
)

# Compile the inline CUDA code for swish
swish = load_inline(
    name="swish",
    cpp_sources=swish_cpp_source,
    cuda_sources=swish_source,
    functions=["swish_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for max
max_source = """
// Your custom CUDA kernel implementation for Max
"""

max_cpp_source = (
    // Your custom CUDA function declaration for Max
)

# Compile the inline CUDA code for max
max_op = load_inline(
    name="max_op",
    cpp_sources=max_cpp_source,
    cuda_sources=max_source,
    functions=["max_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, pool_kernel_size, pool_stride, pool_padding):
        super(ModelNew, self).__init__()
        self.conv_transpose = conv_transpose
        self.max_pool = max_pool
        self.subtract = subtract
        self.swish = swish
        self.max_op = max_op

    def forward(self, x):
        x = self.conv_transpose.conv_transpose_cuda(x, in_channels, out_channels, kernel_size, stride, padding, output_padding)
        x = self.max_pool.max_pool_cuda(x, pool_kernel_size, pool_stride, pool_padding)
        x = torch.softmax(x, dim=1)
        x = x - self.subtract.subtract_cuda(x, out_channels)
        x = self.swish.swish_cuda(x)
        x = self.max_op.max_cuda(x, dim=1)
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for conv transpose
conv_transpose_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void conv_transpose_kernel(float* x, float* weight, float* bias, float* output, int N, int C_in, int D_in, int H_in, int W_in, int C_out, int D_out, int H_out, int W_out, int stride_d, int stride_h, int stride_w, int padding_d, int padding_h, int padding_w, int output_padding_d, int output_padding_h, int output_padding_w) {
    int n = blockIdx.z;
    int c_out = blockIdx.y * blockDim.y + threadIdx.y;
    int d_out = blockIdx.x * blockDim.x + threadIdx.x;

    if (c_out >= C_out || d_out >= D_out || n >= N) return;

    float sum = 0.0f;
    for (int c_in = 0; c_in < C_in; ++c_in) {
        for (int d_in = 0; d_in < D_in; ++d_in) {
            for (int h_in = 0; h_in < H_in; ++h_in) {
                for (int w_in = 0; w_in < W_in; ++w_in) {
                    int d_in_padded = d_in * stride_d + d_out * stride_d - padding_d + output_padding_d;
                    int h_in_padded = h_in * stride_h + d_out * stride_h - padding_h + output_padding_h;
                    int w_in_padded = w_in * stride_w + d_out * stride_w - padding_w + output_padding_w;
                    if (d_in_padded >= 0 && d_in_padded < D_in && h_in_padded >= 0 && h_in_padded < H_in && w_in_padded >= 0 && w_in_padded < W_in) {
                        int index_x = n * C_in * D_in * H_in * W_in + c_in * D_in * H_in * W_in + d_in_padded * H_in * W_in + h_in_padded * W_in + w_in_padded;
                        int index_weight = c_out * C_in * D_in * H_in * W_in + c_in * D_in * H_in * W_in + d_in * H_in * W_in + h_in * W_in + w_in;
                        sum += x[index_x] * weight[index_weight];
                    }
                }
            }
        }
    }

    int index_output = n * C_out * D_out * H_out * W_out + c_out * D_out * H_out * W_out + d_out * H_out * W_out + 0 * W_out + 0;
    output[index_output] = sum + bias[c_out];
}

torch::Tensor conv_transpose_cuda(torch::Tensor x, int in_channels, int out_channels, int kernel_size, int stride_d, int stride_h, int stride_w, int padding_d, int padding_h, int padding_w, int output_padding_d, int output_padding_h, int output_padding_w) {
    auto N = x.size(0);
    auto D_in = x.size(2);
    auto H_in = x.size(3);
    auto W_in = x.size(4);

    auto D_out = ((D_in - 1) * stride_d - 2 * padding_d + kernel_size + output_padding_d) / stride_d + 1;
    auto H_out = ((H_in - 1) * stride_h - 2 * padding_h + kernel_size + output_padding_h) / stride_h + 1;
    auto W_out = ((W_in - 1) * stride_w - 2 * padding_w + kernel_size + output_padding_w) / stride_w + 1;

    auto weight = torch::randn({out_channels, in_channels, kernel_size, kernel_size, kernel_size}).cuda();
    auto bias = torch::randn({out_channels}).cuda();

    auto output = torch::zeros({N, out_channels, D_out, H_out, W_out}).cuda();

    const int block_size = 16;
    const int num_blocks_d = (D_out + block_size - 1) / block_size;
    const int num_blocks_h = (H_out + block_size - 1) / block_size;
    const int num_blocks_w = (W_out + block_size - 1) / block_size;
    const int num_blocks_c_out = (out_channels + block_size - 1) / block_size;

    dim3 grid(num_blocks_d * num_blocks_h * num_blocks_w, num_blocks_c_out, N);
    dim3 block(block_size, block_size, 1);

    conv_transpose_kernel<<<grid, block>>>(x.data_ptr<float>(), weight.data_ptr<float>(), bias.data_ptr<float>(), output.data_ptr<float>(), N, in_channels, D_in, H_in, W_in, out_channels, D_out, H_out, W_out, stride_d, stride_h, stride_w, padding_d, padding_h, padding_w, output_padding_d, output_padding_h, output_padding_w);

    return output;
}
"""

conv_transpose_cpp_source = (
    "torch::Tensor conv_transpose_cuda(torch::Tensor x, int in_channels, int out_channels, int kernel_size, int stride_d, int stride_h, int stride_w, int padding_d, int padding_h, int padding_w, int output_padding_d, int output_padding_h, int output_padding_w);"
)

# Compile the inline CUDA code for conv transpose
conv_transpose = load_inline(
    name="conv_transpose",
    cpp_sources=conv_transpose_cpp_source,
    cuda_sources=conv_transpose_source,
    functions=["conv_transpose_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for max pool
max_pool_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_pool_kernel(float* x, float* output, int N, int C, int D_in, int H_in, int W_in, int D_out, int H_out, int W_out, int kernel_size, int stride_d, int stride_h, int stride_w, int padding_d, int padding_h, int padding_w) {
    int n = blockIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int d_out = blockIdx.x * blockDim.x + threadIdx.x;

    if (c >= C || d_out >= D_out || n >= N) return;

    float max_val = -std::numeric_limits<float>::infinity();
    for (int d_in = 0; d_in < D_in; ++d_in) {
        for (int h_in = 0; h_in < H_in; ++h_in) {
            for (int w_in = 0; w_in < W_in; ++w_in) {
                int d_in_padded = d_in * stride_d + d_out * stride_d - padding_d;
                int h_in_padded = h_in * stride_h + d_out * stride_h - padding_h;
                int w_in_padded = w_in * stride_w + d_out * stride_w - padding_w;
                if (d_in_padded >= 0 && d_in_padded < D_in && h_in_padded >= 0 && h_in_padded < H_in && w_in_padded >= 0 && w_in_padded < W_in) {
                    int index = n * C * D_in * H_in * W_in + c * D_in * H_in * W_in + d_in_padded * H_in * W_in + h_in_padded * W_in + w_in_padded;
                    max_val = std::max(max_val, x[index]);
                }
            }
        }
    }

    int index_output = n * C * D_out * H_out * W_out + c * D_out * H_out * W_out + d_out * H_out * W_out + 0 * W_out + 0;
    output[index_output] = max_val;
}

torch::Tensor max_pool_cuda(torch::Tensor x, int kernel_size, int stride_d, int stride_h, int stride_w, int padding_d, int padding_h, int padding_w) {
    auto N = x.size(0);
    auto C = x.size(1);
    auto D_in = x.size(2);
    auto H_in = x.size(3);
    auto W_in = x.size(4);

    auto D_out = ((D_in - 1) * stride_d - 2 * padding_d + kernel_size) / stride_d + 1;
    auto H_out = ((H_in - 1) * stride_h - 2 * padding_h + kernel_size) / stride_h + 1;
    auto W_out = ((W_in - 1) * stride_w - 2 * padding_w + kernel_size) / stride_w + 1;

    auto output = torch::zeros({N, C, D_out, H_out, W_out}).cuda();

    const int block_size = 16;
    const int num_blocks_d = (D_out + block_size - 1) / block_size;
    const int num_blocks_h = (H_out + block_size - 1) / block_size;
    const int num_blocks_w = (W_out + block_size - 1) / block_size;
    const int num_blocks_c = (C + block_size - 1) / block_size;

    dim3 grid(num_blocks_d * num_blocks_h * num_blocks_w, num_blocks_c, N);
    dim3 block(block_size, block_size, 1);

    max_pool_kernel<<<grid, block>>>(x.data_ptr<float>(), output.data_ptr<float>(), N, C, D_in, H_in, W_in, D_out, H_out, W_out, kernel_size, stride_d, stride_h, stride_w, padding_d, padding_h, padding_w);

    return output;
}
"""

max_pool_cpp_source = (
    "torch::Tensor max_pool_cuda(torch::Tensor x, int kernel_size, int stride_d, int stride_h, int stride_w, int padding_d, int padding_h, int padding_w);"
)

# Compile the inline CUDA code for max pool
max_pool = load_inline(
    name="max_pool",
    cpp_sources=max_pool_cpp_source,
    cuda_sources=max_pool_source,
    functions=["max_pool_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for subtract
subtract_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void subtract_kernel(float* x, float* sub, float* output, int N, int C, int D, int H, int W) {
    int n = blockIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int d = blockIdx.x * blockDim.x + threadIdx.x;

    if (c >= C || d >= D || n >= N) return;

    int index = n * C * D * H * W + c * D * H * W + d * H * W + 0 * W + 0;
    output[index] = x[index] - sub[c];
}

torch::Tensor subtract_cuda(torch::Tensor x, int out_channels) {
    auto N = x.size(0);
    auto C = x.size(1);
    auto D = x.size(2);
    auto H = x.size(3);
    auto W = x.size(4);

    auto sub = torch::randn({out_channels}).cuda();

    auto output = torch::zeros_like(x).cuda();

    const int block_size = 16;
    const int num_blocks_c = (C + block_size - 1) / block_size;
    const int num_blocks_d = (D + block_size - 1) / block_size;
    const int num_blocks_h = (H + block_size - 1) / block_size;
    const int num_blocks_w = (W + block_size - 1) / block_size;

    dim3 grid(num_blocks_d * num_blocks_h * num_blocks_w, num_blocks_c, N);
    dim3 block(block_size, block_size, 1);

    subtract_kernel<<<grid, block>>>(x.data_ptr<float>(), sub.data_ptr<float>(), output.data_ptr<float>(), N, C, D, H, W);

    return output;
}
"""

subtract_cpp_source = (
    "torch::Tensor subtract_cuda(torch::Tensor x, int out_channels);"
)

# Compile the inline CUDA code for subtract
subtract = load_inline(
    name="subtract",
    cpp_sources=subtract_cpp_source,
    cuda_sources=subtract_source,
    functions=["subtract_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for swish
swish_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void swish_kernel(float* x, float* output, int N, int C, int D, int H, int W) {
    int n = blockIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int d = blockIdx.x * blockDim.x + threadIdx.x;

    if (c >= C || d >= D || n >= N) return;

    int index = n * C * D * H * W + c * D * H * W + d * H * W + 0 * W + 0;
    output[index] = x[index] * sigmoid(x[index]);
}

float sigmoid(float x) {
    return 1.0f / (1.0f + exp(-x));
}

torch::Tensor swish_cuda(torch::Tensor x) {
    auto N = x.size(0);
    auto C = x.size(1);
    auto D = x.size(2);
    auto H = x.size(3);
    auto W = x.size(4);

    auto output = torch::zeros_like(x).cuda();

    const int block_size = 16;
    const int num_blocks_c = (C + block_size - 1) / block_size;
    const int num_blocks_d = (D + block_size - 1) / block_size;
    const int num_blocks_h = (H + block_size - 1) / block_size;
    const int num_blocks_w = (W + block_size - 1) / block_size;

    dim3 grid(num_blocks_d * num_blocks_h * num_blocks_w, num_blocks_c, N);
    dim3 block(block_size, block_size, 1);

    swish_kernel<<<grid, block>>>(x.data_ptr<float>(), output.data_ptr<float>(), N, C, D, H, W);

    return output;
}
"""

swish_cpp_source = (
    "torch::Tensor swish_cuda(torch::Tensor x);"
)

# Compile the inline CUDA code for swish
swish = load_inline(
    name="swish",
    cpp_sources=swish_cpp_source,
    cuda_sources=swish_source,
    functions=["swish_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Define the custom CUDA kernel for max
max_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_kernel(float* x, float* output, int N, int C) {
    int n = blockIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;

    if (c >= C || n >= N) return;

    float max_val = -std::numeric_limits<float>::infinity();
    for (int i = 0; i < x.size(); ++i) {
        max_val = std::max(max_val, x[i]);
    }

    int index_output = n * C + c;
    output[index_output] = max_val;
}

torch::Tensor max_cuda(torch::Tensor x, int dim) {
    auto N = x.size(0);
    auto C = x.size(dim);

    auto output = torch::zeros({N, C}).cuda();

    const int block_size = 16;
    const int num_blocks_c = (C + block_size - 1) / block_size;
    const int num_blocks_n = (N + block_size - 1) / block_size;

    dim3 grid(num_blocks_n, num_blocks_c, N);
    dim3 block(block_size, block_size, 1);

    max_kernel<<<grid, block>>>(x.data_ptr<float>(), output.data_ptr<float>(), N, C);

    return output;
}
"""

max_cpp_source = (
    "torch::Tensor max_cuda(torch::Tensor x, int dim);"
)

# Compile the inline CUDA code for max
max_op = load_inline(
    name="max_op",
    cpp_sources=max_cpp_source,
    cuda_sources=max_source,
    functions=["max_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride_d, stride_h, stride_w, padding_d, padding_h, padding_w, output_padding_d, output_padding_h, output_padding_w, pool_kernel_size, pool_stride_d, pool_stride_h, pool_stride_w, pool_padding_d, pool_padding_h, pool_padding_w):
        super(ModelNew, self).__init__()
        self.conv_transpose = conv_transpose
        self.max_pool = max_pool
        self.subtract = subtract
        self.swish = swish
        self.max_op = max_op

    def forward(self, x):
        x = self.conv_transpose.conv_transpose_cuda(x, in_channels, out_channels, kernel_size, stride_d, stride_h, stride_w, padding_d, padding_h, padding_w, output_padding_d, output_padding_h, output_padding_w)
        x = self.max_pool.max_pool_cuda(x, pool_kernel_size, pool_stride_d, pool_stride_h, pool_stride_w, pool_padding_d, pool_padding_h, pool_padding_w)
        x = torch.softmax(x, dim=1)
        x = x - self.subtract.subtract_cuda(x, out_channels)
        x = self.swish.swish_cuda(x)
        x = self.max_op.max_cuda(x, dim=1)
        return x
```