### Requirements:

- Use at least one custom CUDA kernel to replace an operator in the original `Model` class.
- Optimize the architecture to achieve better performance, considering both theoretical and practical aspects such as parallelism, memory access patterns, and algorithmic improvements.
- Ensure that the `ModelNew` class maintains the same functionality as the original `Model` class.

### Hints:

- Consider replacing the matrix multiplication (`matmul`) with a more efficient implementation if possible.
- Think about optimizing the division operation by utilizing hardware-specific features or algorithms.
- Explore potential optimizations for the summation and scaling operations.
- If necessary, combine multiple operations into a single kernel to further improve performance.
- Remember to compile the inline CUDA code using the appropriate tools and flags.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matrix_multiplication_kernel(const float* a, const float* b, float* c, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b) {
    auto M = a.size(0);
    auto N = b.size(1);
    auto K = a.size(1);
    auto c = torch::zeros({M, N}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N + block_size - 1) / block_size;
    const int num_blocks_y = (M + block_size - 1) / block_size;

    matrix_multiplication_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), M, N, K);

    return c;
}
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    """
    Optimized model using custom CUDA kernels.
    """
    def __init__(self, input_size, hidden_size, scaling_factor):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))
        self.scaling_factor = scaling_factor
        self.matrix_multiplication = matrix_multiplication

    def forward(self, x):
        """
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, input_size).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, hidden_size).
        """
        x = self.matrix_multiplication.matrix_multiplication_cuda(x, self.weight)
        x = x / 2  # Divide
        x = torch.sum(x, dim=1, keepdim=True)  # Sum
        x = x * self.scaling_factor  # Scaling
        return x
```

```python
batch_size   = 1024  
input_size   = 8192  
hidden_size  = 8192 
scaling_factor = 1.5

model_new = ModelNew(input_size, hidden_size, scaling_factor)
inputs = get_inputs()

output = model_new(inputs[0])
print(output.shape)  # Should print: torch.Size([1024, 8192])
```