### Requirements:

1. **Matrix Multiplication**: Replace the `nn.Linear` layer with a custom CUDA kernel for matrix multiplication.
2. **Group Normalization**: Implement a custom CUDA kernel for group normalization.
3. **Leaky ReLU Activation**: Implement a custom CUDA kernel for Leaky ReLU activation.
4. **Element-wise Sum**: Replace the element-wise addition (`x + x`) with a custom CUDA kernel.

### Note:

- Ensure that all operations are correctly implemented and tested.
- Use appropriate data types and memory management in CUDA kernels.
- Make sure the final implementation is efficient and comparable in terms of performance to PyTorch's default implementation.

```