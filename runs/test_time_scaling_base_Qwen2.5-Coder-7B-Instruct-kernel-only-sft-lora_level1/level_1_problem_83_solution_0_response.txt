Here's an example of how to use `load_inline` from `torch.utils.cpp_extension`: 

```python
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel
custom_cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Your custom CUDA kernel here
"""

custom_cpp_source = (
    "torch::Tensor custom_function_cuda(torch::Tensor input);"
)

# Compile the inline CUDA code
custom_function = load_inline(
    name="custom_function",
    cpp_sources=custom_cpp_source,
    cuda_sources=custom_cuda_source,
    functions=["custom_function_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)
```

Use `custom_function` in your `ModelNew` class. For example:

```python
class ModelNew(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.custom_function = custom_function

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return self.custom_function.custom_function_cuda(input)
```

Please ensure that your custom CUDA kernel is efficient and correctly implements the operation. Make sure to handle edge cases such as varying batch sizes, different kernel sizes, etc.