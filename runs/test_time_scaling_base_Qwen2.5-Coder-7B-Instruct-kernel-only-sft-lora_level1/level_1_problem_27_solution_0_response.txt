Please note that the `torch.selu` function uses a specific formula involving exponential and logarithmic operations which can be computationally expensive. Your task is to optimize this operation using a custom CUDA kernel. 

The goal is to achieve maximum performance improvement by utilizing GPU parallelism and efficient memory access patterns.

### Guidelines:
- Use CUDA to implement a custom SELU activation function.
- Ensure the implementation maintains numerical stability and correctness.
- Optimize memory access and parallelization to maximize performance.
- Provide the full code for the `ModelNew` class.

### Expected Outcome:
A new architecture `ModelNew` that replaces the `torch.selu` operation with a custom CUDA kernel, resulting in improved performance compared to the original `Model`.

### Deliverables:
- A fully functional Python script containing the `ModelNew` class with the custom CUDA kernel.
- The script should compile and run without errors.

### Example Answer Format:
```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel for SELU
selu_source = """
// CUDA kernel code here
"""

selu_cpp_source = (
    "torch::Tensor selu_cuda(torch::Tensor input);"
)

# Compile the CUDA code
selu = load_inline(
    name="selu",
    cpp_sources=selu_cpp_source,
    cuda_sources=selu_source,
    functions=["selu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return selu.selu_cuda(x)
```

### Note:
- Ensure all necessary imports and configurations are included in the code snippet.
- The provided code should be executable and demonstrate performance improvements over the original `Model`.
- Consider any additional optimizations or improvements that can further enhance performance.

