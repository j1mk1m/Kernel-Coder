Please note that the goal is to achieve maximum performance gains by leveraging custom CUDA kernels. Feel free to optimize any part of the architecture, including but not limited to, the convolution operation itself, memory access patterns, data layout, etc. You can also use techniques such as operator fusion, algorithmic optimizations, and parallelization strategies to improve performance. However, do not introduce any external dependencies other than those provided in the original code snippet. Your solution should be self-contained and easy to understand.