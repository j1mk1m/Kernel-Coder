Please ensure your solution is efficient and takes advantage of parallelism wherever possible. Consider operator fusion, algorithmic changes, and other techniques to improve performance.

### Answer:
 ```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for 3D transposed convolution
conv_transpose_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void conv_transpose_kernel(const float* input, const float* weight, float* output, int batch_size, int in_channels, int out_channels, int depth_in, int height_in, int width_in, int depth_out, int height_out, int width_out, int kernel_size, int stride, int padding) {
    int n = blockIdx.z * blockDim.y + threadIdx.y;
    int c_out = blockIdx.y * blockDim.x + threadIdx.x;
    if (n >= batch_size || c_out >= out_channels) {
        return;
    }

    for (int d_out = 0; d_out < depth_out; ++d_out) {
        for (int h_out = 0; h_out < height_out; ++h_out) {
            for (int w_out = 0; w_out < width_out; ++w_out) {
                float sum = 0.0f;
                int d_in_start = max(d_out * stride - padding, 0);
                int d_in_end = min(d_out * stride - padding + kernel_size, depth_in);
                int h_in_start = max(h_out * stride - padding, 0);
                int h_in_end = min(h_out * stride - padding + kernel_size, height_in);
                int w_in_start = max(w_out * stride - padding, 0);
                int w_in_end = min(w_out * stride - padding + kernel_size, width_in);

                for (int d_in = d_in_start; d_in < d_in_end; ++d_in) {
                    for (int h_in = h_in_start; h_in < h_in_end; ++h_in) {
                        for (int w_in = w_in_start; w_in < w_in_end; ++w_in) {
                            int i = ((n * in_channels + c_out) * depth_in + d_in) * height_in + h_in;
                            int j = ((c_out * kernel_size + d_out - d_in_start) * kernel_size + h_out - h_in_start) * kernel_size + w_out - w_in_start;
                            sum += input[i] * weight[j];
                        }
                    }
                }

                int o_idx = ((n * out_channels + c_out) * depth_out + d_out) * height_out + h_out;
                output[o_idx] = sum;
            }
        }
    }
}

torch::Tensor conv_transpose_cuda(torch::Tensor input, torch::Tensor weight, int stride, int padding) {
    auto batch_size = input.size(0);
    auto in_channels = input.size(1);
    auto out_channels = weight.size(0);
    auto depth_in = input.size(2);
    auto height_in = input.size(3);
    auto width_in = input.size(4);
    auto depth_out = (depth_in - 1) * stride + 1;
    auto height_out = (height_in - 1) * stride + 1;
    auto width_out = (width_in - 1) * stride + 1;

    auto output = torch::zeros({batch_size, out_channels, depth_out, height_out, width_out}, input.options());

    const int block_size = 256;
    const int num_blocks_x = (out_channels + block_size - 1) / block_size;
    const int num_blocks_y = (batch_size + block_size - 1) / block_size;

    conv_transpose_kernel<<<dim3(num_blocks_y, num_blocks_x), dim3(block_size, 1)>>>(
        input.data_ptr<float>(), weight.data_ptr<float>(), output.data_ptr<float>(),
        batch_size, in_channels, out_channels, depth_in, height_in, width_in, depth_out, height_out, width_out,
        kernel_size, stride, padding
    );

    return output;
}
"""

conv_transpose_cpp_source = (
    "torch::Tensor conv_transpose_cuda(torch::Tensor input, torch::Tensor weight, int stride, int padding);"
)

# Compile the inline CUDA code for 3D transposed convolution
conv_transpose = load_inline(
    name="conv_transpose",
    cpp_sources=conv_transpose_cpp_source,
    cuda_sources=conv_transpose_source,
    functions=["conv_transpose_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = conv_transpose
        self.scale1 = nn.Parameter(torch.tensor(scale1))
        self.avg_pool = nn.AvgPool3d(kernel_size=2)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scale2 = nn.Parameter(torch.tensor(scale2))

    def forward(self, x):
        x = self.conv_transpose.conv_transpose_cuda(x, self.weight, stride=self.stride, padding=self.padding)
        x = x * self.scale1
        x = self.avg_pool(x)
        x = x + self.bias
        x = x * self.scale2
        return x

# Initialize the weights for the convolutional layer
def get_init_inputs():
    in_channels = 3
    out_channels = 16
    kernel_size = 3
    stride = 2
    padding = 1
    scale1 = 0.5
    scale2 = 1.0
    bias_shape = (out_channels, 1, 1, 1)
    return [in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape]

# Get the initial inputs
in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape = get_init_inputs()

# Create the model
model_new = ModelNew(in_channels, out_channels, kernel_size, stride, padding, scale1, scale2, bias_shape)

# Get the inputs
inputs = get_inputs()

# Forward pass
output = model_new(inputs[0])

print(output.shape)
```