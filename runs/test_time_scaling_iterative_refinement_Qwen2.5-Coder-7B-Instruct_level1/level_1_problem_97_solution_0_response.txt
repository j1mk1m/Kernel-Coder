The goal here is to optimize the scaled_dot_product_attention function which is a core component of many transformer models.
 ```