Please note that the custom CUDA kernels should be implemented in such a way that they provide significant performance improvements over the default PyTorch implementation for the chosen operators. Consider factors such as memory access patterns, parallelism, and data locality when designing the kernels. You can use techniques such as shared memory, coalesced memory accesses, and efficient data layout to optimize the kernels. Also, consider implementing algorithmic optimizations such as fused operations or alternative algorithms that can achieve better performance.