### Hints:

- Consider using shared memory to optimize memory access patterns.
- Look into potential algorithmic improvements, such as optimizing the matrix-vector multiplication.
- Think about how you can combine operations to reduce the number of kernel launches.
- Be mindful of memory coalescing when accessing global memory.
- Feel free to experiment with different block sizes and grid sizes.
- Ensure that your implementation is numerically stable and matches the behavior of PyTorch's built-in operation as closely as possible.

---

### Expected Output:

Provide the optimized `ModelNew` class with custom CUDA kernels. ```python ```cpp ```markdown
























