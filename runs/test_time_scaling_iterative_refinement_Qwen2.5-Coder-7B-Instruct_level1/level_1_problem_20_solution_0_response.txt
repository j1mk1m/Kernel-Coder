The goal here is to optimize the LeakyReLU operation using a custom CUDA kernel, which can provide significant performance improvements on GPU hardware. Your implementation should aim to achieve high parallelism and minimize memory bandwidth usage.

Here is the CUDA kernel for LeakyReLU:

```cpp
// LeakyReLU CUDA Kernel
__global__ void leaky_relu_forward(float* output, const float* input, float negative_slope, int n_elements) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    while (tid < n_elements) {
        output[tid] = input[tid] > 0 ? input[tid] : input[tid] * negative_slope;
        tid += blockDim.x * gridDim.x;
    }
}
```

And here is how you might call it from PyTorch:

```cpp
torch::Tensor leaky_relu_forward_cuda(torch::Tensor input, float negative_slope) {
    auto n_elements = input.numel();
    auto output = torch::empty_like(input);
    int threads_per_block = 256;
    int blocks_per_grid = (n_elements + threads_per_block - 1) / threads_per_block;
    leaky_relu_forward<<<blocks_per_grid, threads_per_block>>>(output.data_ptr<float>(), input.data_ptr<float>(), negative_slope, n_elements);
    return output;
}
```

You need to integrate this CUDA kernel into your PyTorch model by creating a custom C++ extension. Ensure that the extension is properly loaded and used within your PyTorch model class. The final solution should be efficient and handle large batch sizes and feature dimensions typical in deep learning applications.

```python
from torch.utils.cpp_extension import load

leaky_relu_module = load(name="leaky_relu_module", sources=["leaky_relu_module.cpp"])
```

Replace the `forward` method of the `Model` class with the custom CUDA kernel implementation. Make sure that the new implementation maintains the functionality of the original LeakyReLU operation.

Your final solution should look something like this:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load

leaky_relu_module = load(name="leaky_relu_module", sources=["leaky_relu_module.cpp"])

class ModelNew(nn.Module):
    def __init__(self, negative_slope: float = 0.01):
        super(ModelNew, self).__init__()
        self.negative_slope = negative_slope
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return leaky_relu_module.leaky_relu_forward_cuda(x, self.negative_slope)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []
```

Ensure that the CUDA kernel is compiled and linked correctly, and that the model works as expected when run with the provided input tensors. Your solution should demonstrate significant performance improvement over the original implementation.