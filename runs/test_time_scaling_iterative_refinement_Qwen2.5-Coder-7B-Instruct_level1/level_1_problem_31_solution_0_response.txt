Your implementation should leverage the CUDA parallelism capabilities to achieve faster execution than PyTorch's default implementation of the ELU operation. Your solution should be as efficient as possible while maintaining the correctness of the ELU operation. Consider using techniques such as shared memory, coalesced memory access patterns, and other optimization strategies to improve performance. 

Please note that the provided code is just a template and does not need to be modified beyond what is necessary to implement the custom CUDA operator. The `get_inputs` and `get_init_inputs` functions should remain unchanged.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for ELU activation
elu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Implement the ELU kernel here
__global__ void elu_kernel(const float* input, float* output, int size, float alpha) {
    // Kernel implementation goes here
}

torch::Tensor elu_cuda(torch::Tensor input, float alpha) {
    auto size = input.numel();
    auto output = torch::zeros_like(input);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    elu_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size, alpha);

    return output;
}
"""

elu_cpp_source = (
    "torch::Tensor elu_cuda(torch::Tensor input, float alpha);"
)

# Compile the inline CUDA code for ELU activation
elu = load_inline(
    name="elu",
    cpp_sources=elu_cpp_source,
    cuda_sources=elu_source,
    functions=["elu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, alpha: float = 1.0):
        super(ModelNew, self).__init__()
        self.alpha = alpha
        self.elu = elu

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.elu.elu_cuda(x, self.alpha)

```