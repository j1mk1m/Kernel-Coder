Please provide the full code for ModelNew and the corresponding CUDA kernel. Ensure that the kernel correctly implements the Group Normalization operation and handles all edge cases appropriately. If there were any issues with the previous implementation, please address them in the updated version. Additionally, ensure that the updated kernel is efficient and performs well on the target GPU architecture. ```python

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for Group Normalization
group_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void group_norm_forward_kernel(const float* input, float* mean, float* var, float* output, int batch_size, int num_features, int num_groups, float eps) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * num_features) return;

    int g = idx / (batch_size * num_features / num_groups);
    int c = (idx % (batch_size * num_features / num_groups)) / (batch_size);
    int b = (idx % (batch_size * num_features / num_groups)) % (batch_size);

    float sum = 0.0f;
    for (int i = 0; i < batch_size; ++i) {
        sum += input[i * num_features + c];
    }

    mean[g * batch_size + b] = sum / batch_size;
    var[g * batch_size + b] = 0.0f;

    for (int i = 0; i < batch_size; ++i) {
        float diff = input[i * num_features + c] - mean[g * batch_size + b];
        var[g * batch_size + b] += diff * diff;
    }

    var[g * batch_size + b] = max(var[g * batch_size + b], eps);
    var[g * batch_size + b] = sqrt(var[g * batch_size + b]);

    output[idx] = (input[idx] - mean[g * batch_size + b]) / var[g * batch_size + b];
}

void group_norm_backward_kernel(float* grad_input, const float* grad_output, const float* input, const float* mean, const float* var, int batch_size, int num_features, int num_groups) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * num_features) return;

    int g = idx / (batch_size * num_features / num_groups);
    int c = (idx % (batch_size * num_features / num_groups)) / (batch_size);
    int b = (idx % (batch_size * num_features / num_groups)) % (batch_size);

    float sum = 0.0f;
    for (int i = 0; i < batch_size; ++i) {
        sum += grad_output[i * num_features + c];
    }

    float grad_var = 0.0f;
    for (int i = 0; i < batch_size; ++i) {
        float diff = input[i * num_features + c] - mean[g * batch_size + b];
        grad_var += diff * diff * (-1.0f / (var[g * batch_size + b] * var[g * batch_size + b])) * (grad_output[i * num_features + c]);
    }

    float grad_mean = 0.0f;
    for (int i = 0; i < batch_size; ++i) {
        float diff = input[i * num_features + c] - mean[g * batch_size + b];
        grad_mean += diff * (-1.0f / var[g * batch_size + b]) * (grad_output[i * num_features + c]);
    }

    grad_mean /= batch_size;

    for (int i = 0; i < batch_size; ++i) {
        float diff = input[i * num_features + c] - mean[g * batch_size + b];
        grad_input[i * num_features + c] = (grad_output[i * num_features + c] * (1.0f / var[g * batch_size + b])) + (grad_var * (2.0f / batch_size * diff)) + (grad_mean / batch_size);
    }
}

torch::Tensor group_norm_forward_cuda(torch::Tensor input, int num_groups, float eps) {
    auto batch_size = input.size(0);
    auto num_features = input.size(1);
    auto num_elements = batch_size * num_features;
    auto mean = torch::zeros({num_groups, batch_size}, input.options());
    auto var = torch::zeros({num_groups, batch_size}, input.options());

    const int block_size = 256;
    const int num_blocks = (num_elements + block_size - 1) / block_size;

    group_norm_forward_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), mean.data_ptr<float>(), var.data_ptr<float>(), input.data_ptr<float>(), batch_size, num_features, num_groups, eps);

    return input;
}

torch::Tensor group_norm_backward_cuda(torch::Tensor grad_input, torch::Tensor grad_output, torch::Tensor input, int num_groups) {
    auto batch_size = input.size(0);
    auto num_features = input.size(1);
    auto num_elements = batch_size * num_features;

    group_norm_backward_kernel(grad_input.data_ptr<float>(), grad_output.data_ptr<float>(), input.data_ptr<float>(), grad_input.data_ptr<float>(), grad_input.data_ptr<float>(), batch_size, num_features, num_groups);

    return grad_input;
}
"""

group_norm_cpp_source = (
    "torch::Tensor group_norm_forward_cuda(torch::Tensor input, int num_groups, float eps);"
    "torch::Tensor group_norm_backward_cuda(torch::Tensor grad_input, torch::Tensor grad_output, torch::Tensor input, int num_groups);"
)

# Compile the inline CUDA code for Group Normalization
group_norm = load_inline(
    name="group_norm",
    cpp_sources=group_norm_cpp_source,
    cuda_sources=group_norm_source,
    functions=["group_norm_forward_cuda", "group_norm_backward_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self, num_features: int, num_groups: int):
        super(ModelNew, self).__init__()
        self.group_norm_forward = group_norm.group_norm_forward_cuda
        self.group_norm_backward = group_norm.group_norm_backward_cuda

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        eps = 1e-5
        mean, var = self.group_norm_forward(x, num_groups, eps)
        return x

    def backward(self, grad_output):
        eps = 1e-5
        grad_input = self.group_norm_backward(grad_output, grad_output, grad_output, num_groups)
        return grad_input

```