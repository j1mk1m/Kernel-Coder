### Guidelines:
- Use custom CUDA kernels where appropriate to speed up operations.
- Consider operator fusion opportunities to combine multiple operations into a single kernel.
- Feel free to modify the architecture as needed to achieve the best performance.

Here is an example of how to use `load_inline` to compile custom CUDA kernels:

```python
from torch.utils.cpp_extension import load_inline

custom_conv_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom CUDA kernel implementation here...
"""

custom_conv_cpp_source = (
    "torch::Tensor custom_convolution_cuda(torch::Tensor input, torch::Tensor weight, torch::Tensor bias);"
)

custom_conv = load_inline(
    name="custom_convolution",
    cpp_sources=custom_conv_cpp_source,
    cuda_sources=custom_conv_source,
    functions=["custom_convolution_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)
```

```python
class ModelNew(nn.Module):
    def __init__(self, ...):
        super(ModelNew, self).__init__()
        self.custom_conv = custom_conv

    def forward(self, ...):
        x = self.custom_conv.custom_convolution_cuda(x, weight, bias)
        ...
        return x
```

Make sure to include all necessary imports and ensure the code is self-contained.
```



































