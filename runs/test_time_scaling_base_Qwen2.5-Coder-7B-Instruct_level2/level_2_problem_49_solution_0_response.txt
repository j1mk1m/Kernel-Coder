Please note that you can choose to replace any part of the architecture with custom CUDA kernels, but you must provide a full implementation of the `ModelNew` class. Your solution should be efficient and take advantage of GPU parallelism wherever possible. Feel free to experiment with different optimizations, such as operator fusion, algorithmic changes, or other techniques. The goal is to achieve the highest possible performance while maintaining correctness.

Here's an example of how to define a custom CUDA kernel in PyTorch:

```python
from torch.utils.cpp_extension import load_inline

custom_op_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void my_custom_kernel(...) {
    // Kernel implementation goes here
}

torch::Tensor my_custom_function(...) {
    // Function implementation goes here
}
"""

custom_op_cpp_source = (
    "torch::Tensor my_custom_function(...);"
)

custom_op = load_inline(
    name="custom_op",
    cpp_sources=custom_op_cpp_source,
    cuda_sources=custom_op_source,
    functions=["my_custom_function"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)
```

You should use the same pattern to implement your custom CUDA kernels for the `ModelNew` class.

