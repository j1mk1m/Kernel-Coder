Please note that you should provide the full implementation of the new architecture, including any necessary imports and helper functions. The solution should be efficient and take advantage of CUDA parallelism wherever possible. Additionally, feel free to optimize the architecture further by combining multiple operations into a single kernel or using alternative algorithms where appropriate.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Implement the matrix multiplication kernel here
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for summation
summation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Implement the summation kernel here
"""

summation_cpp_source = (
    "torch::Tensor summation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for summation
summation = load_inline(
    name="summation",
    cpp_sources=summation_cpp_source,
    cuda_sources=summation_source,
    functions=["summation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for max operation
max_operation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Implement the max operation kernel here
"""

max_operation_cpp_source = (
    "torch::Tensor max_operation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for max operation
max_operation = load_inline(
    name="max_operation",
    cpp_sources=max_operation_cpp_source,
    cuda_sources=max_operation_source,
    functions=["max_operation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for average pooling
average_pooling_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Implement the average pooling kernel here
"""

average_pooling_cpp_source = (
    "torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride);"
)

# Compile the inline CUDA code for average pooling
average_pooling = load_inline(
    name="average_pooling",
    cpp_sources=average_pooling_cpp_source,
    cuda_sources=average_pooling_source,
    functions=["average_pooling_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for log sum exp
log_sum_exp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Implement the log sum exp kernel here
"""

log_sum_exp_cpp_source = (
    "torch::Tensor log_sum_exp_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for log sum exp
log_sum_exp = load_inline(
    name="log_sum_exp",
    cpp_sources=log_sum_exp_cpp_source,
    cuda_sources=log_sum_exp_source,
    functions=["log_sum_exp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.matrix_multiplication = matrix_multiplication
        self.summation = summation
        self.max_operation = max_operation
        self.average_pooling = average_pooling
        self.log_sum_exp = log_sum_exp

    def forward(self, x):
        x = self.linear(x)  # (batch_size, out_features)
        x = self.matrix_multiplication.matrix_multiplication_cuda(x, x)  # (batch_size, out_features)
        x = self.summation.summation_cuda(x)  # (batch_size, 1)
        x = self.max_operation.max_operation_cuda(x)  # (batch_size, 1)
        x = self.average_pooling.average_pooling_cuda(x, kernel_size=1, stride=1)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matrix_multiplication_kernel(const float* a, const float* b, float* c, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b) {
    auto M = a.size(0);
    auto N = b.size(1);
    auto K = a.size(1);

    auto c = torch::zeros({M, N}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N + block_size - 1) / block_size;
    const int num_blocks_y = (M + block_size - 1) / block_size;

    matrix_multiplication_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), M, N, K
    );

    return c;
}
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for summation
summation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void summation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : 0.0f;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor summation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block sums up 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    summation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

summation_cpp_source = (
    "torch::Tensor summation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for summation
summation = load_inline(
    name="summation",
    cpp_sources=summation_cpp_source,
    cuda_sources=summation_source,
    functions=["summation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for max operation
max_operation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_operation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor max_operation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    max_operation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

max_operation_cpp_source = (
    "torch::Tensor max_operation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for max operation
max_operation = load_inline(
    name="max_operation",
    cpp_sources=max_operation_cpp_source,
    cuda_sources=max_operation_source,
    functions=["max_operation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for average pooling
average_pooling_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void average_pooling_kernel(const float* a, float* c, int M, int N, int kernel_size, int stride) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        int count = 0;
        for (int kr = 0; kr < kernel_size; ++kr) {
            for (int kc = 0; kc < kernel_size; ++kc) {
                int r = row * stride + kr;
                int c = col * stride + kc;
                if (r >= 0 && r < M && c >= 0 && c < N) {
                    sum += a[r * N + c];
                    count++;
                }
            }
        }
        c[row * N + col] = sum / count;
    }
}

torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride) {
    auto M = a.size(0);
    auto N = a.size(1);

    auto c = torch::zeros({M / kernel_size, N / kernel_size}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N / kernel_size + block_size - 1) / block_size;
    const int num_blocks_y = (M / kernel_size + block_size - 1) / block_size;

    average_pooling_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), M, N, kernel_size, stride
    );

    return c;
}
"""

average_pooling_cpp_source = (
    "torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride);"
)

# Compile the inline CUDA code for average pooling
average_pooling = load_inline(
    name="average_pooling",
    cpp_sources=average_pooling_cpp_source,
    cuda_sources=average_pooling_source,
    functions=["average_pooling_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for log sum exp
log_sum_exp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void log_sum_exp_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

__global__ void log_sum_exp_reduction_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor log_sum_exp_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    log_sum_exp_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    while (c.numel() > 1) {
        num_blocks = (c.numel() + block_size - 1) / block_size;
        log_sum_exp_reduction_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
            c.data_ptr<float>(), c.data_ptr<float>(), c.numel()
        );
    }

    return c[0];
}
"""

log_sum_exp_cpp_source = (
    "torch::Tensor log_sum_exp_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for log sum exp
log_sum_exp = load_inline(
    name="log_sum_exp",
    cpp_sources=log_sum_exp_cpp_source,
    cuda_sources=log_sum_exp_source,
    functions=["log_sum_exp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.matrix_multiplication = matrix_multiplication
        self.summation = summation
        self.max_operation = max_operation
        self.average_pooling = average_pooling
        self.log_sum_exp = log_sum_exp

    def forward(self, x):
        x = self.linear(x)  # (batch_size, out_features)
        x = self.matrix_multiplication.matrix_multiplication_cuda(x, x)  # (batch_size, out_features)
        x = self.summation.summation_cuda(x)  # (batch_size, 1)
        x = self.max_operation.max_operation_cuda(x)  # (batch_size, 1)
        x = self.average_pooling.average_pooling_cuda(x, kernel_size=1, stride=1)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matrix_multiplication_kernel(const float* a, const float* b, float* c, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b) {
    auto M = a.size(0);
    auto N = b.size(1);
    auto K = a.size(1);

    auto c = torch::zeros({M, N}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N + block_size - 1) / block_size;
    const int num_blocks_y = (M + block_size - 1) / block_size;

    matrix_multiplication_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), M, N, K
    );

    return c;
}
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for summation
summation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void summation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : 0.0f;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor summation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block sums up 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    summation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

summation_cpp_source = (
    "torch::Tensor summation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for summation
summation = load_inline(
    name="summation",
    cpp_sources=summation_cpp_source,
    cuda_sources=summation_source,
    functions=["summation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for max operation
max_operation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_operation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor max_operation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    max_operation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

max_operation_cpp_source = (
    "torch::Tensor max_operation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for max operation
max_operation = load_inline(
    name="max_operation",
    cpp_sources=max_operation_cpp_source,
    cuda_sources=max_operation_source,
    functions=["max_operation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for average pooling
average_pooling_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void average_pooling_kernel(const float* a, float* c, int M, int N, int kernel_size, int stride) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        int count = 0;
        for (int kr = 0; kr < kernel_size; ++kr) {
            for (int kc = 0; kc < kernel_size; ++kc) {
                int r = row * stride + kr;
                int c = col * stride + kc;
                if (r >= 0 && r < M && c >= 0 && c < N) {
                    sum += a[r * N + c];
                    count++;
                }
            }
        }
        c[row * N + col] = sum / count;
    }
}

torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride) {
    auto M = a.size(0);
    auto N = a.size(1);

    auto c = torch::zeros({M / kernel_size, N / kernel_size}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N / kernel_size + block_size - 1) / block_size;
    const int num_blocks_y = (M / kernel_size + block_size - 1) / block_size;

    average_pooling_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), M, N, kernel_size, stride
    );

    return c;
}
"""

average_pooling_cpp_source = (
    "torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride);"
)

# Compile the inline CUDA code for average pooling
average_pooling = load_inline(
    name="average_pooling",
    cpp_sources=average_pooling_cpp_source,
    cuda_sources=average_pooling_source,
    functions=["average_pooling_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for log sum exp
log_sum_exp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void log_sum_exp_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

__global__ void log_sum_exp_reduction_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor log_sum_exp_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    log_sum_exp_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    while (c.numel() > 1) {
        num_blocks = (c.numel() + block_size - 1) / block_size;
        log_sum_exp_reduction_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
            c.data_ptr<float>(), c.data_ptr<float>(), c.numel()
        );
    }

    return c[0];
}
"""

log_sum_exp_cpp_source = (
    "torch::Tensor log_sum_exp_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for log sum exp
log_sum_exp = load_inline(
    name="log_sum_exp",
    cpp_sources=log_sum_exp_cpp_source,
    cuda_sources=log_sum_exp_source,
    functions=["log_sum_exp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.matrix_multiplication = matrix_multiplication
        self.summation = summation
        self.max_operation = max_operation
        self.average_pooling = average_pooling
        self.log_sum_exp = log_sum_exp

    def forward(self, x):
        x = self.linear(x)  # (batch_size, out_features)
        x = self.matrix_multiplication.matrix_multiplication_cuda(x, x)  # (batch_size, out_features)
        x = self.summation.summation_cuda(x)  # (batch_size, 1)
        x = self.max_operation.max_operation_cuda(x)  # (batch_size, 1)
        x = self.average_pooling.average_pooling_cuda(x, kernel_size=1, stride=1)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matrix_multiplication_kernel(const float* a, const float* b, float* c, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b) {
    auto M = a.size(0);
    auto N = b.size(1);
    auto K = a.size(1);

    auto c = torch::zeros({M, N}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N + block_size - 1) / block_size;
    const int num_blocks_y = (M + block_size - 1) / block_size;

    matrix_multiplication_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), M, N, K
    );

    return c;
}
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for summation
summation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void summation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : 0.0f;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor summation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block sums up 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    summation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

summation_cpp_source = (
    "torch::Tensor summation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for summation
summation = load_inline(
    name="summation",
    cpp_sources=summation_cpp_source,
    cuda_sources=summation_source,
    functions=["summation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for max operation
max_operation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_operation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor max_operation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    max_operation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

max_operation_cpp_source = (
    "torch::Tensor max_operation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for max operation
max_operation = load_inline(
    name="max_operation",
    cpp_sources=max_operation_cpp_source,
    cuda_sources=max_operation_source,
    functions=["max_operation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for average pooling
average_pooling_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void average_pooling_kernel(const float* a, float* c, int M, int N, int kernel_size, int stride) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        int count = 0;
        for (int kr = 0; kr < kernel_size; ++kr) {
            for (int kc = 0; kc < kernel_size; ++kc) {
                int r = row * stride + kr;
                int c = col * stride + kc;
                if (r >= 0 && r < M && c >= 0 && c < N) {
                    sum += a[r * N + c];
                    count++;
                }
            }
        }
        c[row * N + col] = sum / count;
    }
}

torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride) {
    auto M = a.size(0);
    auto N = a.size(1);

    auto c = torch::zeros({M / kernel_size, N / kernel_size}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N / kernel_size + block_size - 1) / block_size;
    const int num_blocks_y = (M / kernel_size + block_size - 1) / block_size;

    average_pooling_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), M, N, kernel_size, stride
    );

    return c;
}
"""

average_pooling_cpp_source = (
    "torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride);"
)

# Compile the inline CUDA code for average pooling
average_pooling = load_inline(
    name="average_pooling",
    cpp_sources=average_pooling_cpp_source,
    cuda_sources=average_pooling_source,
    functions=["average_pooling_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for log sum exp
log_sum_exp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void log_sum_exp_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

__global__ void log_sum_exp_reduction_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor log_sum_exp_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    log_sum_exp_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    while (c.numel() > 1) {
        num_blocks = (c.numel() + block_size - 1) / block_size;
        log_sum_exp_reduction_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
            c.data_ptr<float>(), c.data_ptr<float>(), c.numel()
        );
    }

    return c[0];
}
"""

log_sum_exp_cpp_source = (
    "torch::Tensor log_sum_exp_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for log sum exp
log_sum_exp = load_inline(
    name="log_sum_exp",
    cpp_sources=log_sum_exp_cpp_source,
    cuda_sources=log_sum_exp_source,
    functions=["log_sum_exp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.matrix_multiplication = matrix_multiplication
        self.summation = summation
        self.max_operation = max_operation
        self.average_pooling = average_pooling
        self.log_sum_exp = log_sum_exp

    def forward(self, x):
        x = self.linear(x)  # (batch_size, out_features)
        x = self.matrix_multiplication.matrix_multiplication_cuda(x, x)  # (batch_size, out_features)
        x = self.summation.summation_cuda(x)  # (batch_size, 1)
        x = self.max_operation.max_operation_cuda(x)  # (batch_size, 1)
        x = self.average_pooling.average_pooling_cuda(x, kernel_size=1, stride=1)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matrix_multiplication_kernel(const float* a, const float* b, float* c, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b) {
    auto M = a.size(0);
    auto N = b.size(1);
    auto K = a.size(1);

    auto c = torch::zeros({M, N}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N + block_size - 1) / block_size;
    const int num_blocks_y = (M + block_size - 1) / block_size;

    matrix_multiplication_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), M, N, K
    );

    return c;
}
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for summation
summation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void summation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : 0.0f;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor summation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block sums up 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    summation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

summation_cpp_source = (
    "torch::Tensor summation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for summation
summation = load_inline(
    name="summation",
    cpp_sources=summation_cpp_source,
    cuda_sources=summation_source,
    functions=["summation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for max operation
max_operation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_operation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor max_operation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    max_operation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

max_operation_cpp_source = (
    "torch::Tensor max_operation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for max operation
max_operation = load_inline(
    name="max_operation",
    cpp_sources=max_operation_cpp_source,
    cuda_sources=max_operation_source,
    functions=["max_operation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for average pooling
average_pooling_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void average_pooling_kernel(const float* a, float* c, int M, int N, int kernel_size, int stride) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        int count = 0;
        for (int kr = 0; kr < kernel_size; ++kr) {
            for (int kc = 0; kc < kernel_size; ++kc) {
                int r = row * stride + kr;
                int c = col * stride + kc;
                if (r >= 0 && r < M && c >= 0 && c < N) {
                    sum += a[r * N + c];
                    count++;
                }
            }
        }
        c[row * N + col] = sum / count;
    }
}

torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride) {
    auto M = a.size(0);
    auto N = a.size(1);

    auto c = torch::zeros({M / kernel_size, N / kernel_size}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N / kernel_size + block_size - 1) / block_size;
    const int num_blocks_y = (M / kernel_size + block_size - 1) / block_size;

    average_pooling_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), M, N, kernel_size, stride
    );

    return c;
}
"""

average_pooling_cpp_source = (
    "torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride);"
)

# Compile the inline CUDA code for average pooling
average_pooling = load_inline(
    name="average_pooling",
    cpp_sources=average_pooling_cpp_source,
    cuda_sources=average_pooling_source,
    functions=["average_pooling_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for log sum exp
log_sum_exp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void log_sum_exp_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

__global__ void log_sum_exp_reduction_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor log_sum_exp_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    log_sum_exp_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    while (c.numel() > 1) {
        num_blocks = (c.numel() + block_size - 1) / block_size;
        log_sum_exp_reduction_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
            c.data_ptr<float>(), c.data_ptr<float>(), c.numel()
        );
    }

    return c[0];
}
"""

log_sum_exp_cpp_source = (
    "torch::Tensor log_sum_exp_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for log sum exp
log_sum_exp = load_inline(
    name="log_sum_exp",
    cpp_sources=log_sum_exp_cpp_source,
    cuda_sources=log_sum_exp_source,
    functions=["log_sum_exp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.matrix_multiplication = matrix_multiplication
        self.summation = summation
        self.max_operation = max_operation
        self.average_pooling = average_pooling
        self.log_sum_exp = log_sum_exp

    def forward(self, x):
        x = self.linear(x)  # (batch_size, out_features)
        x = self.matrix_multiplication.matrix_multiplication_cuda(x, x)  # (batch_size, out_features)
        x = self.summation.summation_cuda(x)  # (batch_size, 1)
        x = self.max_operation.max_operation_cuda(x)  # (batch_size, 1)
        x = self.average_pooling.average_pooling_cuda(x, kernel_size=1, stride=1)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matrix_multiplication_kernel(const float* a, const float* b, float* c, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b) {
    auto M = a.size(0);
    auto N = b.size(1);
    auto K = a.size(1);

    auto c = torch::zeros({M, N}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N + block_size - 1) / block_size;
    const int num_blocks_y = (M + block_size - 1) / block_size;

    matrix_multiplication_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), M, N, K
    );

    return c;
}
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for summation
summation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void summation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : 0.0f;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor summation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block sums up 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    summation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

summation_cpp_source = (
    "torch::Tensor summation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for summation
summation = load_inline(
    name="summation",
    cpp_sources=summation_cpp_source,
    cuda_sources=summation_source,
    functions=["summation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for max operation
max_operation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_operation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor max_operation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    max_operation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

max_operation_cpp_source = (
    "torch::Tensor max_operation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for max operation
max_operation = load_inline(
    name="max_operation",
    cpp_sources=max_operation_cpp_source,
    cuda_sources=max_operation_source,
    functions=["max_operation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for average pooling
average_pooling_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void average_pooling_kernel(const float* a, float* c, int M, int N, int kernel_size, int stride) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        int count = 0;
        for (int kr = 0; kr < kernel_size; ++kr) {
            for (int kc = 0; kc < kernel_size; ++kc) {
                int r = row * stride + kr;
                int c = col * stride + kc;
                if (r >= 0 && r < M && c >= 0 && c < N) {
                    sum += a[r * N + c];
                    count++;
                }
            }
        }
        c[row * N + col] = sum / count;
    }
}

torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride) {
    auto M = a.size(0);
    auto N = a.size(1);

    auto c = torch::zeros({M / kernel_size, N / kernel_size}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N / kernel_size + block_size - 1) / block_size;
    const int num_blocks_y = (M / kernel_size + block_size - 1) / block_size;

    average_pooling_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), M, N, kernel_size, stride
    );

    return c;
}
"""

average_pooling_cpp_source = (
    "torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride);"
)

# Compile the inline CUDA code for average pooling
average_pooling = load_inline(
    name="average_pooling",
    cpp_sources=average_pooling_cpp_source,
    cuda_sources=average_pooling_source,
    functions=["average_pooling_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for log sum exp
log_sum_exp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void log_sum_exp_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

__global__ void log_sum_exp_reduction_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor log_sum_exp_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    log_sum_exp_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    while (c.numel() > 1) {
        num_blocks = (c.numel() + block_size - 1) / block_size;
        log_sum_exp_reduction_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
            c.data_ptr<float>(), c.data_ptr<float>(), c.numel()
        );
    }

    return c[0];
}
"""

log_sum_exp_cpp_source = (
    "torch::Tensor log_sum_exp_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for log sum exp
log_sum_exp = load_inline(
    name="log_sum_exp",
    cpp_sources=log_sum_exp_cpp_source,
    cuda_sources=log_sum_exp_source,
    functions=["log_sum_exp_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.matrix_multiplication = matrix_multiplication
        self.summation = summation
        self.max_operation = max_operation
        self.average_pooling = average_pooling
        self.log_sum_exp = log_sum_exp

    def forward(self, x):
        x = self.linear(x)  # (batch_size, out_features)
        x = self.matrix_multiplication.matrix_multiplication_cuda(x, x)  # (batch_size, out_features)
        x = self.summation.summation_cuda(x)  # (batch_size, 1)
        x = self.max_operation.max_operation_cuda(x)  # (batch_size, 1)
        x = self.average_pooling.average_pooling_cuda(x, kernel_size=1, stride=1)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        x = self.log_sum_exp.log_sum_exp_cuda(x)  # (batch_size, 1)
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
matrix_multiplication_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void matrix_multiplication_kernel(const float* a, const float* b, float* c, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += a[row * K + k] * b[k * N + col];
        }
        c[row * N + col] = sum;
    }
}

torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b) {
    auto M = a.size(0);
    auto N = b.size(1);
    auto K = a.size(1);

    auto c = torch::zeros({M, N}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N + block_size - 1) / block_size;
    const int num_blocks_y = (M + block_size - 1) / block_size;

    matrix_multiplication_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), b.data_ptr<float>(), c.data_ptr<float>(), M, N, K
    );

    return c;
}
"""

matrix_multiplication_cpp_source = (
    "torch::Tensor matrix_multiplication_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for matrix multiplication
matrix_multiplication = load_inline(
    name="matrix_multiplication",
    cpp_sources=matrix_multiplication_cpp_source,
    cuda_sources=matrix_multiplication_source,
    functions=["matrix_multiplication_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for summation
summation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void summation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : 0.0f;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor summation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block sums up 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    summation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

summation_cpp_source = (
    "torch::Tensor summation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for summation
summation = load_inline(
    name="summation",
    cpp_sources=summation_cpp_source,
    cuda_sources=summation_source,
    functions=["summation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for max operation
max_operation_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void max_operation_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor max_operation_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    max_operation_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    return c.sum();
}
"""

max_operation_cpp_source = (
    "torch::Tensor max_operation_cuda(torch::Tensor a);"
)

# Compile the inline CUDA code for max operation
max_operation = load_inline(
    name="max_operation",
    cpp_sources=max_operation_cpp_source,
    cuda_sources=max_operation_source,
    functions=["max_operation_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for average pooling
average_pooling_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void average_pooling_kernel(const float* a, float* c, int M, int N, int kernel_size, int stride) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float sum = 0.0f;
        int count = 0;
        for (int kr = 0; kr < kernel_size; ++kr) {
            for (int kc = 0; kc < kernel_size; ++kc) {
                int r = row * stride + kr;
                int c = col * stride + kc;
                if (r >= 0 && r < M && c >= 0 && c < N) {
                    sum += a[r * N + c];
                    count++;
                }
            }
        }
        c[row * N + col] = sum / count;
    }
}

torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride) {
    auto M = a.size(0);
    auto N = a.size(1);

    auto c = torch::zeros({M / kernel_size, N / kernel_size}, a.options());

    const int block_size = 32;
    const int num_blocks_x = (N / kernel_size + block_size - 1) / block_size;
    const int num_blocks_y = (M / kernel_size + block_size - 1) / block_size;

    average_pooling_kernel<<<num_blocks_y, num_blocks_x, 0, at::cuda::getCurrentCUDAStream()>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), M, N, kernel_size, stride
    );

    return c;
}
"""

average_pooling_cpp_source = (
    "torch::Tensor average_pooling_cuda(torch::Tensor a, int kernel_size, int stride);"
)

# Compile the inline CUDA code for average pooling
average_pooling = load_inline(
    name="average_pooling",
    cpp_sources=average_pooling_cpp_source,
    cuda_sources=average_pooling_source,
    functions=["average_pooling_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


# Define the custom CUDA kernel for log sum exp
log_sum_exp_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void log_sum_exp_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

__global__ void log_sum_exp_reduction_kernel(const float* a, float* c, int size) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < size) ? a[i] : -FLT_MAX;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);
        }
        __syncthreads();
    }

    if (tid == 0) {
        c[blockIdx.x] = sdata[0];
    }
}

torch::Tensor log_sum_exp_cuda(torch::Tensor a) {
    auto size = a.numel();
    auto c = torch::zeros({size / 1024}, a.options());  // Assuming each block finds the max of 1024 elements

    const int block_size = 1024;
    const int num_blocks = (size + block_size - 1) / block_size;

    log_sum_exp_kernel<<<num_blocks, block_size, block_size * sizeof(float)>>>(
        a.data_ptr<float>(), c.data_ptr<float>(), size
    );

    while (c.numel() > 1) {
        num_blocks = (c.numel() + block_size - 1) / block_size;
        log_sum_exp_reduction_kernel<<<num_blocks, block_size