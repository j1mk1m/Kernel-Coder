Here are some guidelines to follow:

- Use the `load_inline` function from `torch.utils.cpp_extension` to compile and link your CUDA kernels directly into PyTorch.

- Write your CUDA kernels in C++ and use the appropriate CUDA headers.

- Ensure that your CUDA kernels handle the data layout and memory access correctly.

- Optimize the performance of the GEMM operation using efficient matrix multiplication techniques.

- Implement the hardtanh and GELU activations using efficient CUDA kernels.

- If possible, combine multiple operations into a single kernel to reduce the number of kernel launches and improve overall efficiency.

- Ensure that your implementation is compatible with PyTorch's tensor storage format and can seamlessly integrate with existing PyTorch code.

- Document any assumptions or limitations of your implementation in the comments.

Please provide the full source code for the optimized `ModelNew` class, including the CUDA kernels and any necessary helper functions. Make sure that the code is well-commented and follows best practices for CUDA programming.