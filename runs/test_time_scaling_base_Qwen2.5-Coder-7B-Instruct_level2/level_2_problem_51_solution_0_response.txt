Please note that you should optimize the architecture as much as possible, but you should also ensure that the code remains readable and maintainable. Avoid overcomplicating the code for the sake of optimization. Use reasonable operator fusion and algorithmic changes where appropriate.

Here's the reference implementation using PyTorch:

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.subtract = nn.Parameter(torch.randn(out_features))

    def forward(self, x):
        original_x = x.clone().detach()
        x = self.gemm(x)
        x = x - self.subtract
        x = torch.mean(x, dim=1, keepdim=True)
        x = torch.logsumexp(x, dim=1, keepdim=True)
        x = torch.nn.functional.gelu(x)
        x = x + original_x
        return x
```

## Solution

Provide the optimized `ModelNew` class using custom CUDA operators. Ensure that all operations are performed efficiently using CUDA. If necessary, combine multiple operations into a single CUDA kernel for better performance. Maintain readability and maintainability while optimizing the architecture.

### Example

Consider replacing the `Gemm`, `Subtract`, and `GlobalAvgPool` operations with a single CUDA kernel to reduce memory transfers and improve overall performance. Similarly, combine the `LogSumExp` and `GELU` operations if possible.

### Note

- Ensure that the code compiles without errors.
- Verify that the functionality remains the same as the original `Model`.
- Keep the code clean and well-commented to facilitate maintenance and debugging.