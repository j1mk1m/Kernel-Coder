Please include a brief explanation of the optimizations you made and why they are beneficial.

## Solution:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for GEMM
gemm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cublas_v2.h>

void gemm_kernel(const float* a, const float* b, float* c, int m, int n, int k) {
    cublasHandle_t handle;
    cublasCreate(&handle);
    cublasSetStream(handle, at::cuda::getCurrentCUDAStream());
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, n, m, k, (const float*)1, b, n, a, k, (const float*)0, c, n);
    cublasDestroy(handle);
}
"""

gemm_cpp_source = (
    "void gemm_kernel(const float* a, const float* b, float* c, int m, int n, int k);"
)

# Compile the inline CUDA code for GEMM
gemm = load_inline(
    name="gemm",
    cpp_sources=gemm_cpp_source,
    cuda_sources=gemm_source,
    functions=["gemm_kernel"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(ModelNew, self).__init__()
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.hardtanh = nn.Hardtanh()
        self.mish = nn.Mish()
        self.groupnorm = nn.GroupNorm(num_groups=num_groups, num_channels=out_features)

    def forward(self, x):
        """
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """
        batch_size, in_features = x.shape
        out_features = self.bias.shape[0]

        y = torch.empty((batch_size, out_features)).cuda()
        gemm_kernel(x.data_ptr(), self.weight.data_ptr(), y.data_ptr(), batch_size, out_features, in_features)

        y = y + self.bias
        y = self.hardtanh(y)
        y = self.mish(y)
        y = self.groupnorm(y)
        return y


# Initialize weights for GEMM
model_new = ModelNew(in_features, out_features, bias_shape, num_groups)
model_new.weight = nn.Parameter(torch.randn(out_features, in_features))

# Get inputs
inputs = get_inputs()

# Forward pass
output = model_new(inputs[0])
print(output.shape)
```

Explanation:
- **GEMM Optimization**: Replaced the `nn.Linear` layer with a custom CUDA kernel for matrix multiplication (`sgemm`). This leverages cuBLAS, which is highly optimized for GEMM operations on GPUs.
- **BiasAdd**: Kept as-is since it is straightforward and can be efficiently handled by PyTorch.
- **Hardtanh and Mish**: These activations were left as-is because they do not significantly benefit from custom CUDA implementations in this context.
- **GroupNorm**: Also kept as-is because normalization layers are typically well-optimized in PyTorch and do not need to be rewritten in CUDA for performance gains.

This optimization focuses on leveraging hardware acceleration for computationally intensive operations like GEMM, which often dominates the runtime of neural network models. By using cuBLAS directly, we can achieve significant speedups without sacrificing readability or maintainability.