Please note that you should aim for significant speedup over the original PyTorch implementation while ensuring the functionality remains intact. You may use any CUDA features available to achieve this goal, including but not limited to shared memory, texture memory, coalesced memory access patterns, etc. However, keep in mind that these optimizations can sometimes lead to increased memory usage or complexity, so weigh these factors carefully. Additionally, ensure that your custom CUDA kernels are properly parallelized and utilize GPU resources efficiently.

```