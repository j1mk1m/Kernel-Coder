Please use the `load_inline` function from `torch.utils.cpp_extension` to compile the CUDA kernels. Use appropriate data types and ensure that the CUDA kernels are correctly handling memory access and thread synchronization. 

Make sure to include all necessary imports at the beginning of your code snippet. 

Ensure that the `ModelNew` class has a constructor (`__init__`) and a forward method (`forward`). The forward method should call the custom CUDA kernel instead of the PyTorch ReLU function. 

Finally, provide a brief explanation of why you chose to optimize the ReLU operation with a custom CUDA kernel in this specific case. 

### Expected Answer:

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for ReLU activation
relu_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void relu_kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = max(input[idx], 0.0f);
    }
}

torch::Tensor relu_cuda(torch::Tensor input) {
    auto size = input.numel();
    auto output = torch::zeros_like(input);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    relu_kernel<<<num_blocks, block_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);

    return output;
}
"""

relu_cpp_source = (
    "torch::Tensor relu_cuda(torch::Tensor input);"
)

# Compile the inline CUDA code for ReLU
relu = load_inline(
    name="relu",
    cpp_sources=relu_cpp_source,
    cuda_sources=relu_source,
    functions=["relu_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.relu = relu
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.relu.relu_cuda(x)

```

### Explanation:

The ReLU operation can be efficiently implemented using a custom CUDA kernel because it involves a simple conditional check (`max(value, 0.0)`). By writing a kernel in CUDA, we can parallelize this operation across many threads, significantly speeding up the computation when dealing with large batch sizes. In this specific case, the ReLU operation is applied to each element of the input tensor independently, making it ideal for parallelization. Additionally, since ReLU is a very common operation in neural networks, optimizing it with a custom CUDA kernel can lead to noticeable performance improvements.