Please note that the goal here is to maximize performance, so feel free to use any techniques you think will be beneficial, such as operator fusion, algorithmic changes, etc. Feel free to also modify the architecture itself if you believe it would benefit from changes. Your implementation should be as efficient as possible. 

Hint: You can use the following code to compile and run your custom CUDA code:

```python
from torch.utils.cpp_extension import load_inline

custom_op_source = """
// Custom CUDA code goes here
"""

custom_op_cpp_source = (
    // C++ interface for the custom CUDA code goes here
)

custom_op = load_inline(
    name="custom_op",
    cpp_sources=custom_op_cpp_source,
    cuda_sources=custom_op_source,
    functions=["custom_function"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

# Use the custom CUDA operator in your model
output = custom_op.custom_function(input_tensor)
```