### Constraints:
- Only use CUDA for the operations you implement.
- You can freely choose which parts of the operation to implement in CUDA.
- Feel free to change the model architecture or introduce additional layers if it makes sense to achieve better performance.
- Ensure the outputted model works correctly with the provided `get_inputs` function.
- Your implementation should be efficient and take advantage of parallel processing capabilities of GPUs.
- Use appropriate CUDA memory management techniques to avoid memory leaks.
- Ensure that any CUDA kernel launches are properly synchronized.
- Consider using PyTorch's built-in CUDA functionality where applicable.
- If you decide to fuse multiple operations into a single kernel, ensure they are compatible and do not lead to data races or other synchronization issues.
- Provide a brief explanation of why certain operations were chosen for CUDA implementation and how they contribute to performance improvement.
- Avoid unnecessary complexity in the solution; aim for simplicity and clarity.
- Ensure all CUDA kernels are well-commented to explain their purpose and functionality.
- If you implement custom CUDA kernels for multiple operations, provide separate snippets for each kernel and include them in the final model definition.
- Ensure the model maintains its original functionality when compared to the original `Model`.
```