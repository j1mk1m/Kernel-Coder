### Requirements:
- Replace the 2D convolution (`nn.Conv2d`) with a custom CUDA kernel.
- Ensure that the output dimensions match those of the original convolution operation.
- Optimize the performance of the convolution operation using techniques such as shared memory, coalesced memory access, and other optimization strategies.
- Maintain the functionality of the original model while achieving better performance.

### Constraints:
- You cannot change the input or output dimensions of the convolution operation.
- You must maintain the same number of parameters in the convolutional layer.
- You can use any CUDA programming techniques to optimize the performance of the convolution operation.

### Hints:
- Consider using CUDA shared memory to store intermediate results and reduce global memory accesses.
- Use coalesced memory access patterns to maximize GPU performance.
- Experiment with different block and grid sizes to find the optimal configuration for your specific hardware.
- Profile your implementation to identify bottlenecks and make necessary optimizations.
```