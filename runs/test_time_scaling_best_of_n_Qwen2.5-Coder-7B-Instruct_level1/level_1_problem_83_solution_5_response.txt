### Hints:

- Consider replacing the PyTorch Conv2d operation with a custom CUDA kernel that performs the same operation more efficiently.
- Think about how you can optimize the convolution operation by using techniques such as shared memory, texture caching, or parallelism.
- You may also consider implementing a custom CUDA kernel for the ReLU activation function to further optimize the performance of the network.