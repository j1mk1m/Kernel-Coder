Here's how you should structure your response:

- Import necessary libraries.
- Define the custom CUDA kernel.
- Load the inline CUDA code.
- Define the `ModelNew` class.
- Ensure the `forward` method uses the custom CUDA kernel instead of PyTorch's built-in `matmul`.
- Include any necessary helper functions or variables.

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for matrix multiplication
custom_matmul_source = """
// Your custom CUDA kernel implementation here
"""

custom_matmul_cpp_source = (
    // Your custom C++ source code here
)

# Compile the inline CUDA code for matrix multiplication
custom_matmul = load_inline(
    name="custom_matmul",
    cpp_sources=custom_matmul_cpp_source,
    cuda_sources=custom_matmul_source,
    functions=["custom_matmul_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.custom_matmul = custom_matmul
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return self.custom_matmul.custom_matmul_cuda(A, B)

# Example usage
if __name__ == "__main__":
    M = 1024 * 2
    K = 4096 * 2
    N = 2048 * 2
    A = torch.rand(M, K)
    B = torch.rand(K, N)
    model_new = ModelNew().cuda()
    result = model_new(A.cuda(), B.cuda())
    print(result.shape)  # Should print torch.Size([1024, 2048])
```

Make sure the custom CUDA kernel is efficient and correctly implements the matrix multiplication operation. If possible, include optimizations such as shared memory usage, coalesced memory access patterns, or other techniques to improve performance.