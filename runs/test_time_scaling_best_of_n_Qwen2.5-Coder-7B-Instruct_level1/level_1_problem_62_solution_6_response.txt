Your solution should be focused on optimizing performance by leveraging CUDA parallelism. Consider using techniques such as shared memory, coalesced memory access, and efficient grid/block configuration. You may also consider alternative algorithms that can provide better performance on GPUs. 

Please ensure that the custom CUDA kernels are well-documented and include comments explaining the purpose of each part of the kernel. This will help other developers understand your implementation. 

Finally, please ensure that the custom CUDA kernels are compatible with PyTorch's tensor operations, allowing them to seamlessly integrate into the existing PyTorch workflow. This includes handling tensor shapes, data types, and device placement correctly.