Your solution should be as efficient as possible, taking advantage of parallelism and GPU capabilities. Feel free to optimize any part of the architecture, including but not limited to the reverse cumulative sum operation. If you decide to keep the reverse cumulative sum operation, implement it using a custom CUDA kernel instead of PyTorch's built-in function. If you decide to change the implementation, feel free to do so. Remember, the goal is to achieve the highest possible performance on a GPU.