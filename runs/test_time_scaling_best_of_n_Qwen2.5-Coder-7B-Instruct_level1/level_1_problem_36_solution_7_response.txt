Please note that you should aim to optimize the RMS normalization operation, which is computationally intensive due to the need to compute the mean across all dimensions except one and then take the square root. 

Hint: Consider using a more efficient algorithm for computing the mean and avoiding unnecessary memory allocations. You may also consider parallelizing the computation across multiple GPUs if available.

```markdown
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for RMS Normalization
rms_norm_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

// Custom implementation of RMS Normalization
__global__ void rms_norm_kernel(float* x, float* rms, int batch_size, int num_features, int dim1, int dim2) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= batch_size * num_features) return;

    // Compute the sum of squares for each feature
    float sum_of_squares = 0.0f;
    for (int i = 0; i < dim1 * dim2; ++i) {
        sum_of_squares += x[idx * dim1 * dim2 + i] * x[idx * dim1 * dim2 + i];
    }

    // Compute the mean of the sum of squares
    float mean = sum_of_squares / (dim1 * dim2);

    // Compute the RMS
    rms[idx] = sqrt(mean + eps);
}

torch::Tensor rms_norm_cuda(torch::Tensor x, float eps) {
    auto batch_size = x.size(0);
    auto num_features = x.size(1);
    auto dim1 = x.size(2);
    auto dim2 = x.size(3);

    auto rms = torch::zeros({batch_size, num_features}, x.options().dtype(torch::kFloat32));

    const int block_size = 256;
    const int num_blocks = (batch_size * num_features + block_size - 1) / block_size;

    rms_norm_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), rms.data_ptr<float>(), batch_size, num_features, dim1, dim2, eps);

    return x / rms.view_as(x);
}
"""

rms_norm_cpp_source = (
    "torch::Tensor rms_norm_cuda(torch::Tensor x, float eps);"
)

# Compile the inline CUDA code for RMS Normalization
rms_norm = load_inline(
    name="rms_norm",
    cpp_sources=rms_norm_cpp_source,
    cuda_sources=rms_norm_source,
    functions=["rms_norm_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self, num_features: int, eps: float = 1e-5):
        super(ModelNew, self).__init__()
        self.num_features = num_features
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return rms_norm.rms_norm_cuda(x, self.eps)

```
```