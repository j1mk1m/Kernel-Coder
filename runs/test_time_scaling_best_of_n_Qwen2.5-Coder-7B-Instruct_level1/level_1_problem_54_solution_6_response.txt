Note that the provided architecture does not use any batch normalization, dropout, or other common layers, so focus on optimizing the convolutional layer with custom CUDA kernels. Feel free to implement any necessary helper functions or classes to achieve your optimization goals.