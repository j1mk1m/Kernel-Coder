Note that the provided architecture does not have any batch normalization layers or activation functions, so focus on optimizing the convolutional layer itself. Consider using techniques such as fused convolutions, grouped convolutions, or other optimizations to improve performance. 

Your implementation should be able to handle arbitrary-sized inputs and outputs, but prioritize optimizing the specific kernel sizes provided in the example (3, 5, 7).