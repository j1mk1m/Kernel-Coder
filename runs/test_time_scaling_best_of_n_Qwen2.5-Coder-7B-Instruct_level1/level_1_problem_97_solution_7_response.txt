```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for scaled dot product attention
scaled_dot_product_attention_source = """
// Your custom CUDA implementation here
"""

scaled_dot_product_attention_cpp_source = (
    // Your C++ interface here
)

# Compile the inline CUDA code for scaled dot product attention
scaled_dot_product_attention = load_inline(
    name="scaled_dot_product_attention",
    cpp_sources=scaled_dot_product_attention_cpp_source,
    cuda_sources=scaled_dot_product_attention_source,
    functions=["scaled_dot_product_attention_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.scaled_dot_product_attention = scaled_dot_product_attention

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        out = self.scaled_dot_product_attention.scaled_dot_product_attention_cuda(Q, K, V)
        return out
```